{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification - Cats vs. Dogs\n",
    "\n",
    "Welcome to your Third assignment! You will train a simple neural network model to distinguish cat pictures from dog pictures. In this assignment, we'll practice such process under a binary classification context. \n",
    "- A binary classification task is to categorize an object into one of the two classes. \n",
    "- Use all pixels in an image as the features. \n",
    "\n",
    "\n",
    "## Exercises:\n",
    "1. $\\color{violet}{\\textbf{(20\\%) Data Preprocessing}}$\n",
    "2. $\\color{violet}{\\textbf{(5\\%) Logistic Regression Model}}$\n",
    "3. $\\color{violet}{\\textbf{(5\\%) Cross Entropy Loss}}$\n",
    "4. $\\color{violet}{\\textbf{(40\\%) Gradient Descent Optimization}}$\n",
    "5. $\\color{violet}{\\textbf{(15\\%) Evaluation on Test Dataset}}$\n",
    "6. $\\color{violet}{\\textbf{(15\\%) Test Model with New Image}}$\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. $\\color{red}{\\textbf{Modify code out of the designated area at your own risk.}}$\n",
    "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
    "- **Need to install [PyTorch](https://pytorch.org/), [opencv-python](https://pypi.org/project/opencv-python/) and [pandas](https://pandas.pydata.org/).** Use the following command in your terminal.\n",
    "    ```console\n",
    "    pip install torch opencv-python pandas\n",
    "    ```\n",
    "## After this assignment you will:\n",
    "- Be able to train a neural network model to do binary classification task.\n",
    "    - Use sigmoid activation function.\n",
    "    - Learn a new loss function: cross entropy.\n",
    "    - Practice gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data and Create a Dataset\n",
    "### 1.1 Create an Annotation File\n",
    "Data is not always stored in NumPy arrays. Most cases, you will have to organize and annotate the raw data stored in your hard drive. For image data, you want it to be organized as the following way.\n",
    "``` console\n",
    "root/dog/xxx.jpg\n",
    "root/dog/xxy.jpg\n",
    "root/dog/xxz.jpg\n",
    "\n",
    "root/cat/123.jpg\n",
    "root/cat/456.jpg\n",
    "root/cat/789.jpg\n",
    "```\n",
    "To grab image information and store them in an comma-seperated values (CSV) file:\n",
    "1. Visit the data directory, grab all images' paths and corresponding categories.\n",
    "2. Save the paths and categories of images in an `.csv` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 279 cat images, and 278 dog images in the training dataset\n",
      "There are 70 cat images, and 70 dog images in the test dataset\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Locate train and test directories\n",
    "root_dir = \"./dataset\"  # locate dataset directory from this repo in the whole system\n",
    "train_dir = os.path.join(root_dir, \"train\")\n",
    "test_dir = os.path.join(root_dir, \"test\")\n",
    "categories = ['cats', 'dogs']\n",
    "\n",
    "# Glob training files\n",
    "train_cat_files = glob(os.path.join(train_dir, categories[0], \"*.jpg\"))\n",
    "train_dog_files = glob(os.path.join(train_dir, categories[1], \"*.jpg\"))\n",
    "print(f\"There are {len(train_cat_files)} cat images, and {len(train_dog_files)} dog images in the training dataset\")\n",
    "train_image_files = train_cat_files + train_dog_files\n",
    "train_labels = ['cat'] * len(train_cat_files) + ['dog'] * len(train_dog_files)\n",
    "train_data_dict = {'path': train_image_files, 'label': train_labels}\n",
    "df_train = pd.DataFrame(train_data_dict)\n",
    "# print(df_train)\n",
    "df_train.to_csv('annotation_train.csv', header=False, index=False)\n",
    "\n",
    "# Glob test files\n",
    "test_cat_files = glob(os.path.join(test_dir, categories[0], \"*.jpg\"))\n",
    "test_dog_files = glob(os.path.join(test_dir, categories[1], \"*.jpg\"))\n",
    "print(f\"There are {len(test_cat_files)} cat images, and {len(test_dog_files)} dog images in the test dataset\")\n",
    "test_image_files = test_cat_files + test_dog_files\n",
    "test_labels = ['cat'] * len(test_cat_files) + ['dog'] * len(test_dog_files)\n",
    "test_data_dict = {'path': test_image_files, 'label': test_labels}\n",
    "df_test = pd.DataFrame(test_data_dict)\n",
    "# print(df_test)\n",
    "df_test.to_csv('annotation_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a Dataset using [PyTorch](https://pytorch.org/)\n",
    "1. Inherit the `Dataset` class to build a customized `CatsDogsDataset` class.\n",
    "2. Instantiate the customized class to a `dataset_train` and `dataset_test` .\n",
    "3. Further create dataloaders to shuffle the data and access the full matrix of the features and the targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (100, 100, 3) 0.0\n",
      "100 (100, 100, 3) 0.0\n",
      "200 (100, 100, 3) 0.0\n",
      "300 (100, 100, 3) 1.0\n",
      "400 (100, 100, 3) 1.0\n",
      "500 (100, 100, 3) 1.0\n",
      "556 (100, 100, 3) 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAC3CAYAAACffxPBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOz9ebRtWVnfD39ms9bazWnuvXWbaiiqQJDuRY2BoL5RRAgasYmKvjpUsInmNQaNIw6Nxt9AGUkGqANeI2psiTHREI1GHTYEDfYoOuwVlL6KqqLqdqfde6+15pzP+8cz19p7n+becxsgwf3UOHXP2Xs1c8015zO/8/t0RkSElaxkJStZyUpWspKV/F8v9kPdgJWsZCUrWclKVrKSldweWQG7laxkJStZyUpWspIPE1kBu5WsZCUrWclKVrKSDxNZAbuVrGQlK1nJSlaykg8TWQG7laxkJStZyUpWspIPE1kBu5WsZCUrWclKVrKSDxNZAbuVrGQlK1nJSlaykg8TWQG7laxkJStZyUpWspIPE1kBu5WsZCUrWclKVrKSDxNZATvAGMO3f/u3f6ibsZK/I7Iabyv5YMpqvK3kgy2rMfehldsG7P7iL/6CF7/4xdx3330MBgPuuece/tE/+kd87/d+7+26xf+18jmf8zl80Rd9EQAiwunTp/lP/+k/HTru9a9/PV/yJV/Ck5/8ZIwxfPInf/It33tvb4+Xv/zlfNqnfRpnzpzBGHPkva8lW1tbfPVXfzXnzp1jPB7zvOc9jz/+4z++5bbdiqzG2/FykvF2+fJlvuu7votP+qRP4ty5c5w6dYqP+7iP4/Wvf/0t37+ua775m7+Zu+++m+FwyHOe8xze+MY3nvj8hx56iC/4gi/g1KlTbGxs8Nmf/dm8613vuuV23YqsxtvxclL99g3f8A187Md+LGfOnGE0GvG0pz2Nb//2b2dvb++m7/3hqt9gNeauJScdc4vyzne+k8FggDGGP/qjP7rpez/yyCP863/9r3ne857H+vo6xhh+4zd+44au8YHWcbcF2P3e7/0ez3rWs/izP/szvuqrvorXvva1/NN/+k+x1vI93/M9t+MW/1fLW97yFj7u4z4OgLe+9a1sbW31fy/KD/zAD/DzP//z3HvvvZw+ffq23PvSpUu84hWv4K1vfSsf/dEffcPnp5R40YtexE/+5E/yL/7Fv+A7v/M7eeyxx/jkT/5k3v72t9+WNt6orMbbteUk4+3Nb34z/+bf/BvOnDnDt33bt/Hv/t2/YzQa8YVf+IW8/OUvv6X7f9mXfRmvfvWr+eIv/mK+53u+B+ccn/7pn87v/M7vXPfcvb09nve85/Gbv/mbfOu3fivf8R3fwZ/8yZ/w3Oc+l8uXL99Su25WVuPt2nJS/faHf/iHfOInfiLf8R3fwfd8z/fwvOc9j1e+8pV82qd9Gimlm7r3h6N+g9WYu56cdMwtyjd8wzfgvb/le//N3/wNr3rVq3jooYd45jOfecPnf1B0nNwG+fRP/3Q5d+6cXL169dB3jz766O24xQdUAHn5y1/+Abn2gw8+KID8/u//voiI/MiP/Ihsbm5KSunQsQ888IDEGEVE5BnPeIY897nPveX7z2YzeeSRR0RE5A//8A8FkNe97nUnPv/1r3+9APLTP/3T/WePPfaYnDp1Sr7oi77oltt3M7Iab8fLScfbu971LnnPe96z9FlKST7lUz5FqqqSvb29m7r/H/zBHwgg3/Vd39V/Np1O5SM+4iPk4z/+4697/qte9SoB5C1veUv/2Vvf+lZxzsm3fMu33FSbblVW4+14uRH9dpR893d/twDy5je/+abu/+Go30RWY+5acjNj7ld/9VelLEv5tm/7NgHkD//wD2/6/js7O3L58mUREfnpn/5pAeRNb3rTic//YOi428LYvfOd7+QZz3gGp06dOvTd+fPnl/5+3etex6d8yqdw/vx5qqri6U9/Oj/wAz9w6Lz777+fz/iMz+A3fuM3eNaznsVwOOSZz3xmT3n+7M/+LM985jMZDAb8/b//9/mTP/mTpfO/7Mu+jLW1Nd71rnfxqZ/6qYzHY+6++25e8YpXICLXfaaHHnqIr/iKr+DChQtUVcUznvEMfuzHfuxE/VHXNZcuXeLSpUu86U1voigK7r33Xi5dusRv/dZv8VEf9VFcvnyZS5cuLe1U7733Xqw92St529vexgMPPHDd46qq4s477zzRNY+Sn/mZn+HChQt87ud+bv/ZuXPn+IIv+AJ+/ud/nrqub/raNyur8bYsNzPenvCEJ3DfffctXccYwz/5J/+Euq4PmQVOOt5+5md+BuccX/3VX91/NhgM+Mqv/Ere/OY38+CDD173/Gc/+9k8+9nP7j976lOfyvOf/3z++3//79e9/wdCVuNtWW5Wvx0l999/P6Dm0EX5u6zfYDXmDsqtjLm2bfn6r/96vv7rv56P+IiPOPL6bdvytre9jUceeeS6bVlfX+fMmTMnavdR8kHRcbcDHb7whS+U9fV1+Yu/+IvrHvvsZz9bvuzLvkxe85rXyPd+7/fKC1/4QgHkta997dJx9913nzzlKU+Ru+66S779279dXvOa18g999wja2tr8l/+y3+Rxz/+8fLKV75SXvnKV8rm5qY86UlP6tkuEZGXvvSlMhgM5MlPfrJ86Zd+qbz2ta+Vz/iMzxBA/p//5/9ZuhcHdhfvf//75XGPe5zce++98opXvEJ+4Ad+QD7rsz5LAHnNa15z3Wd83eteJ8CJft797ncfeY3rMXbADTN6N7OjfdKTniT/+B//40Of/8iP/IgA8ud//uc31IbbIavxtiy3Y7x18q3f+q0CyMMPP3yozScZby94wQvkaU972qHPf+3Xfk0A+YVf+IVjz40xSlVV8jVf8zWHvut22js7O9dtw+2W1XhbllsZb23bysWLF+Whhx6SN7zhDfLUpz5V1tfXewZksc1/V/WbyGrMHZRbGXPf+Z3fKefPn5ft7e3+OgcZu3e/+90CyEtf+tLrtmVRbpSx+2DpuNsC7P7X//pf4pwT55x8/Md/vHzTN32TvOENb5CmaQ4dO5lMDn32qZ/6qfLEJz5x6bP77rtPAPm93/u9/rM3vOENAshwOJT3vve9/ec/+IM/eKhzX/rSlwogL3vZy/rPUkryohe9SMqylIsXL/afHxyEX/mVXyl33XWXXLp0aalNX/iFXyibm5tHPsOiPPzww/LGN75R3vjGN8p9990nL3nJS+SNb3yj/NRP/ZQA8h/+w3/ov59Op0de4/8UYDcej+UrvuIrDn3+S7/0SwLIr/7qr95QG26HrMbbstyO8SYicvnyZTl//rx84id+4qHvTjrenvGMZ8infMqnHPr8r/7qrwSQ//gf/+Ox5168eFEAecUrXnHou+/7vu8TQN72trddtw23W1bjbVluZby9+c1vXlqEn/KUpxy5KP5d1m8iqzF3UG52zD3yyCOyvr4uP/iDPygi8iEHdh8sHXdbgJ2IyFve8hb5nM/5HBmNRv2kPXfunPz8z//8sedsbW3JxYsX5d//+38vgGxtbfXf3XffffL0pz/90PGAvOhFL1r6/E//9E8FkB/90R/tP+sG4d/8zd8sHfsrv/IrAshP/dRP9Z8tDsKUkpw6dUq++qu/Wi5evLj00w2K3/md3zlRn1y9elWstfKGN7xBRHQQDAYDmc1m1z33dvnYLcrNKD5r7ZG7i1//9V8XQH7u537u9jXwBmQ13g7LrYy3GKN82qd9mpRlKX/6p396ovsdJU984hOPZEDe+c53Xnd3/sADDwggr3rVqw5996M/+qMCyJ/8yZ/cdNtuRVbj7bDczHjb3t6WN77xjfI//+f/lG/6pm+Sj/3Yj5Vf/MVfPNH9ricfTvpNZDXmjpIbHXMveclL5KM/+qN75vE4YHezcqPA7oOl4249RCTLs5/9bH72Z3+Wpmn4sz/7M37u536O17zmNbz4xS/mT//0T3n6058OwO/+7u/y8pe/nDe/+c1MJpOla2xvb7O5udn//fjHP37p++67e++998jPr169uvS5tZYnPvGJS5995Ed+JADvec97jnyOixcvsrW1xQ/90A/xQz/0Q0ce89hjjx35Oaitfnt7G4A3vOENWGt56lOfyqVLl3jDG97A3/t7f4/d3V12d3fZ3NykKIpjr/V/ggyHwyP9TGazWf/9h0JW403ldo23l73sZfzqr/4q//k//+ebii7s5FbGS/fdarx9+I63jY0NXvCCFwDw2Z/92fzkT/4kn/3Zn80f//Ef39K4u1n5P1W/wWrMdXKzY+73f//3+Ymf+Al+/dd//cS+6x9o+WDpuNsG7Dopy7J3DPzIj/xIvvzLv5yf/umf5uUvfznvfOc7ef7zn89Tn/pUXv3qV3PvvfdSliW//Mu/zGte85pDTo/OuSPvcdzncgIHzutJ14Yv+ZIv4aUvfemRx3zUR33Usef/7u/+Ls973vOWPjvopH7u3DkA3vSmN92WXHUfSLnrrruOdCjtPrv77rs/2E1aktV4u/Xx9h3f8R18//d/P6985Sv50i/90htp/iG56667eOihhw59fpLxcubMGaqqWo23D/Pxtiif+7mfy5d+6Zfy3/7bf/uQALv/0/UbrMbczY65b/qmb+ITP/ETecITntCDzkuXLgH6fh944IFDQPcDLR8sHXfbgd2iPOtZzwLmDf7FX/xF6rrmF37hF5Y69E1vetMH5P4pJd71rnf1OwqAv/3bvwXm0VgH5dy5c6yvrxNj7HeWNyIf/dEf3Sdj/Zqv+Ro+7uM+jpe+9KVsb2/z4he/mO/5nu/pd1ofCkV2o/IxH/Mx/PZv/zYppaVdzx/8wR8wGo2W+vZDLavxduPj7fu+7/v49m//dv7lv/yXfPM3f/MN3/+gfMzHfAxvetOb2NnZYWNjo//8D/7gD/rvjxNrLc985jOPTB76B3/wBzzxiU9kfX39ltt4u2Q13m5dv9V1TUqpZ2Q+2PJ/k36D1Zi7kTH3wAMP8N73vpcnPOEJh675WZ/1WWxubh6Kxv5AywdLx90WfvJNb3rTkcj+l3/5lwF4ylOeAsx3BYvHbm9v87rXve52NONIee1rX9v/LiK89rWvpSgKnv/85x95vHOOz/u8z+N//I//wV/+5V8e+v7ixYvXvN/p06d5wQtewD/8h/+QBx54gM/7vM/jBS94AePxGOccX/mVX8kLXvACXvCCF9xSEuKTpgO4EXnkkUd429veRtu2/WcvfvGLefTRR/nZn/3Z/rNLly7x0z/903zmZ34mVVXd1jacRFbjbS63Mt5e//rX83Vf93V88Rd/Ma9+9auveZ+TjrcXv/jFxBiXTC51XfO6172O5zznOUsmnwceeIC3ve1th87/wz/8wyXF9zd/8zf87//9v/n8z//8697/AyGr8TaXmx1vW1tbS3qlkx/5kR8B5oClk7/L+g1WY25RbnbM/dAP/RA/93M/t/Tzspe9DIDv/u7v5r/+1//aH3sj6U5uRD5UOu62MHYve9nLmEwmfM7nfA5PfepTaZqG3/u93+P1r389999/P1/+5V8OwAtf+ELKsuQzP/Mz+Wf/7J+xt7fHD//wD3P+/Pnb3qGg+bN+9Vd/lZe+9KU85znP4Vd+5Vf4pV/6Jb71W7+1p26Pkle+8pW86U1v4jnPeQ5f9VVfxdOf/nSuXLnCH//xH/Nrv/ZrXLly5br3/qM/+iOapuETPuETAM0k/lEf9VGMx+Njz/mt3/otfuu3fgvQwb6/v8+//bf/FoBP+qRP4pM+6ZP6Y5/2tKfx3Oc+90SlTF772teytbXFww8/DOgu733vex+g767zp/iWb/kWfvzHf5x3v/vd/e7rxS9+MR/3cR/Hl3/5l/PXf/3XnD17lu///u8nxsh3fMd3XPfeHwhZjbfDcqPj7S1veQsveclLuOOOO3j+85+/pOQAPuETPmHJl+ak4+05z3kOn//5n8+3fMu38Nhjj/GkJz2JH//xH+c973kPP/qjP7p07Ete8hJ+8zd/c2lR+uf//J/zwz/8w7zoRS/iG7/xGymKgle/+tVcuHCBf/Wv/tV1++EDIavxdlhudLz9xm/8Bl/3dV/Hi1/8Yp785CfTNA2//du/zc/+7M/yrGc9iy/5ki9ZOv7vsn7r2r0ac8tyo2PuhS984aHPOobuuc997tJm4qGHHuJpT3saL33pS09Ukq5bl//qr/4KgJ/4iZ/oK+t827d9W3/ch0zH3XL4hWhUzFd8xVfIU5/6VFlbW5OyLOVJT3qSvOxlLzuUJfsXfuEX5KM+6qNkMBjI/fffL6961avkx37sx4QD+Wfuu+++Q5E6or0jX/u1X7v0WReqvJjt/qUvfamMx2N55zvfKS984QtlNBrJhQsX5OUvf/lSbp7umgezZD/66KPytV/7tXLvvfdKURRy5513yvOf/3z5oR/6oRP1yStf+Ur5iI/4iP7vF7zgBYfafVBe/vKXL6UCWPw52D5uIB1AF+Z+1M9in3dRTwfzAF25ckW+8iu/Uu644w4ZjUby3Oc+97ZFFd2MrMbbYbnR8Xa9vFAHIwtvZLxNp1P5xm/8Rrnzzjulqip59rOffWTaiOc+97lylAp68MEH5cUvfrFsbGzI2tqafMZnfIa8/e1vP9G9PxCyGm+H5UbH2zve8Q55yUteIk984hNlOBzKYDCQZzzjGfLyl7/8yConf5f1m8hqzB0lN7OmHpTble7kWrpzUT5UOs7kRn7YyZd92ZfxMz/zM7dUYHolKzmprMbbSj6YshpvK/lgy2rM/d8j/2fEAK9kJStZyUpWspKVrOSWZQXsVrKSlaxkJStZyUo+TGQF7FaykpWsZCUrWclKPkzkw9bHbiUrWclKVrKSlazk75qsGLuVrGQlK1nJSlaykg8TWQG7laxkJStZyUpWspIPE1kBu5WsZCUrWclKVrKSDxM5ceWJb3zJs4gx4pzlwoU7iSGwP9nn4sVLhLZlOBiwvrbG2XN3UBQe5ywpBnZ3d5k1M+q2wRcFZVFQeE/hPBITFoPzjnq6iy8c1bBivL5BUY6wvkSso41CiJG2bWlDIMZICC27u3vMZjNCDFhjMdHQ1A2T6RTvLePxmNFoyGg0ZH9/jxhbnBFtW4qkFBmOBqyt6XHj8Zi2qZGUENHiyzEmQki0bcA4B9YQJbGzvYvFUZUDzp89z9k7zrGxsYkrRkxnezTtlLrZYzgsGY/XGQ7XCbWn3rtMmO0TU8s9j3scZVmSRHjgofchInhXsL52B9Y6nPOUZUUIgaIoGVRDEGG2d5V2uk+MwpkzZyirAmMSk+kubaypY8teU3P5yhX29vbY290HDM5p/+/u7tM2LcYY7rzrLkLbIiIYYyjLkradMZvtM6tbBoMxzhVMpzXT2YSiKtjcXOfq9hZb21fY3d2mDTNSElJMtK3QtELCkozjB3/icAmZk8gzn3sXMUKKIK0ACWvBFwbjPSEa2mhomkgMFokGk8AXEecF78EVCVeBr8APDMYYjBGMTSABg2DFYLGM/ZiRG3HKjDGtUNiCUTliVDpSaJCmhthSWoOzYCzsbE8RwHnPaDzCFB7rHc4Kw8EQX1X44ZAd79hNiV0RZHwKMQ4Ro+2eNEjTQDPFx228hcJbhpWjKAyFtxSlo6wc1qDtNwZjEsaQ36vFWosxBkwu+m3AWAMCMSYmk4ar2zP29mfs7E65eKlhfzcwmyWcLTh7epONtTGnTo85c2pAVTnK0mEEQhBiEGZNYrI/oZ41TCYNe3sTZk1gVrfs7c1omkSIQoimz7SubTIggojoOEmJlISY9JiisFSDglOba5w6NWR9bcj6+jpr4yFlWVBVBcbq8xpjSCnROQYbhM5LWMTwzf/6+25qvAHwywfK+RhANOsoBnKHw+Kz9a0AOMJdOR8jh865EenOOdodevGSH0iP6f4ZltpiDn+21IauA49uWP+p6HH9k570QZbaNM8Saw60qU8pe1wf5v/LfDCBJK71tiSPafv5v3Kyth6Qem+LGCMpRlI7o21mxBgIIZDaQNvMqKf7XLlyka2r2+zu7HLl8iUmkwmzumFa1+xNpkzqhlkTIATqFEiSIAo2xX7eSUyEGIgpQRsxJBDBipBE9DgEmwTT9ZNJ+GSwGAwOQZCUiClRp0iThJCgTVpDtpsjvihwvsA5j/MOLBhnsM7inOvnQEqp78OUElESaeHv/t0Yg3UVznm899jC45zHOk9VVb3u63VDd72UtC+6a1iLNfqzOA+TRMAiop81TUMILaFtqOsZzaymmc6YTqc00z3atibmNRPm7FjXhkUxxlAVA5z1WOtxtsQVBdZ5KIb0o9cahoWhLDylLxhWBeVgjCsqXDlCigFiPWIL/vN/vnb5R7gBYLc2HjObzXTxbhpiiMQQscZovTrRDtnb26MoHM45vLOEGLUDRHQQW4c48EWBLUweRGC9xxWesiypqgFY18/WEAJ10zCdTYkx9j9t2xBi0ALOzmCMxVlHWRRYaxiUJaPhkLW1NayFpq5p6im5OYhAjJEmgxwRwVmtbVcUZT9Q2rZhNmsYr63p594xm9SQBAuUhcc7izEQY8vOzhazeo8Qp6Q4pPAFo+EY56CqKpyJTPbbTpchCM45rLEKfktPGxIpRYxRgFn4Aucs08mENjQ0oaWpA00MOHF4r6AzRQgxMJnuU9dT2qZR5WM9RVFQVUOaJvT6rSpL6tmMEALOOYbDIcaWYCLTWUMMgRiFuq5pQwAL09mM/f092rbBGKiKSvtRAqGd0cwCyTiSL046vA5JDwIiqmPMXEErnplPooxvMmgTPdYIIknXFVE13y9MAtABICEJBBKtRFoTcQIOAWvwZQXWqFILgkkJELzNqs4YCmeRFCEZJAmIVR0nQooRnMNai8OAq0hYJOlkFgtiBWyNjWTwpsDRWgWhxlqcE/0cWVAeaUGZSF7gBVEcRQqJhOjmJMYMqkCiIGIOLHH6tySQZBCxSLIKwiIZZOs7iVGIIRGjgvmUpFfQOq+k7+tuw9D93n8nMn+PVoGptQarHYcQMwBMxJhwRq9tTL4G3XW68UKvmG+nSAZ32kNdVx28jxwGAN0z93/eatsWGrJ0m6M//0DI0jNIYqFHMpjKfy89aoeOheOeYVFu9klOep5h+b3kD+eA2Cz+MQeJR99Ulo69ZTFzAIJzeR0qKIqCsigpS/0JIZCAIFCGQJuEmLTtKUFMqgdJdg58Q0SsQWJEkvQYt5uLqiuzvuyf2pHy71ZnHAlUp+QNlQDJCCn/bTAkASOmw4ZgBJtUzx0E7Is6QRY+O/ivcUIyOh8F28+4lNKRgCpfaX6+MUs/y2IB02MAfRVHv3WBHrPIgg7Tf45uR5KEEQXMB3WggjqwRvDWUliorDBwhsKBczoeknX6Y05mZD0xsCucJ/mCOtZceuwi3ntlmKzDW0dKiel0SoitMnKFZzQeEmMAFCyFjKJFRBmowQALtHWDKwyDQcHa2ojR2gZ7+1NmTYMrCra2LrM/mTKZzpYQvmSw2E2G9eEaZgQxBYzR4sEbGxuMRgP29yv29va4ejUSYpPPcUiCpm5JMRFCYDwe4LzHF54YIsZafFkwtJ7BYEBRFBhjcAnaNtCESGhq2qamqT1tnHL50vtp2ynDUUFoLdPJHs4YRsPTlKXH2wHTyX5+BnBFQTUY4AtPUZQAWJsXMSLj8RhJ0IaG2WzC3v4u0/1dppMZtvKshTWGw4rpbMr+ZJf96R47k20mkyl105KS4JzB+4LBYMTe3j7D0QjvXGZh54WknXNYIKWW2XTK7u6ElIQQIRJxraUJNdP9XZDEsKwYDAakJDRNC2GPttmlaQOz5nDR75NKjJEYFNhZLFZRDZB3hzLfk3diDD2rpcBtWS33k8nEObOFIRFpY4MVS2NKRr7SsWEMzhcU3mJKx2S7yTvJpCA6tBjndDNgIMVAEoMpK20vEGLC2SGVL0jOMTNlBlWqOrz3SILUTHQ3aQXrwDnd2zgPRWGwNmGtwxrXAxt9TqOaxnQLrSw9b5RImyIhKbCTpAAoxayw7QFF1AG6YAgGSJa2jYRWWevQpgwUU54zyp4fpbAP/t4DvzzuO2Wqz21x3qJ6S/s4pkiMlhgVdHbkH+guWxCMWL1uBqO3VZZJqaPlAIt32+VGr38z9N1NPINknuLEcLVbkA6yZsfd89ok5TFtWj712GsePOHY4821++RYQHGDYtDNjbFgMwskgC+QFCnLirKqqJqGwWBACAEwGLEEIwRjdbMYIxJ1M0ZSHdiDjzaSgoUYiKJfG8BlMCOpRVIAIrr7ywyqtUqA5H6Ikgiim8dospYV+s2kADGfnh8FB/1GbHGjB8ubwEOdokegOlpvpFpOFYG1y/P94LWBQ4Cu/74f82CNPQQqTyLW2gVm+HhRIJ0Qo2SNTQ6xCSMJjMOg61bhDAPnGHrHuLB4bzHOQuGIzhFxxBPOuBMDO28sTRIkROrJlFQWWGvxmVpt2oYmBKQVrDM4nJpIncUahzcFtu1AhO1ZBIOhFcH7gqIcUFQDBEMbAtPZlGZ3l729XWZ1Q9u2LL7wGJMuCM7hC8faeEThfF7fhOFwSOkdg6pE0hBJibqumc6M7k4MOGcpigLvHUVpKYoSay2hbfOiFYkp4V2pxzlPCMpCeufUXNu2TPZ2CU1DmxLOCuItbduQpKBppuwhFMVQqeYYsN6RJBJTRCK0oc0LWcKbRNO0eK9g0lpDEiHEwNbOVXa2r1DPJnhX6gTLO6g2CXUITJoZTTtDzZeGZGBtvM7a2gbj8QZbW1uIRIwD40CMEFKgaRoGdUWSQNPM1JSXmZKUEtYrhR1iRFLUPitKhtUQYyyxEgo3IrSJIBOm9eykw+uQhAQJg/QT0ebXqtu2DBX0/0ZIViBFUne4ZCCUQZfNQAIgJWV3jdHJBo5GAiI1DUNGbowxDiTRSoMzQkHCFarzTAQJCUtH6S/sxhCctWoGiIlkEs5avK8ofUXA9schClrBYI0qS2tE7+eg8AZnDUgLFHT8LiYsGP8inbrT3ZyZs9EpEkVIyZCSI6SGkCJBIkhCkhAjOGuyaTQRUqQVsAlMILs9JEKEWQN1yD8x0kZV+FEU7EkyPRHas2rz19RNy56lBsFag3EdMznfWYtEhEDKi4rIfKdsjMkbxLmZRUSZgtsqx+nQxQW/A63XAwGHxNJ31tLN5MjrL9/+YMPMEb9pO0+0UC0ec0KQd4i9XGBcjj3m4DeSjvi425wcg8IO3eRwvy+fbfrDlg+S+a2OuqYxIBGdTwbVRsto83bww8aY/rrGml6nRBFMclhX4H2J914tP4Mhqc2uR9bRGKGJQpNAJ26hup2EkZjnnyAuYKOliJ7GKrNnjaGwJusCT0qRpq5JISirn1TfYRQg2gTJ6E+MJvdd7kBr+s22knTdZi7mnupYscy2GZ3X3fZb+3ves7K0SU1guo2s7TeDxlisdXQ6bz4vFiH+wrjp9sHG6NqSDzH9PeczUkRJBRGbn4v5NY3BcnjYLALXQ6ZeXUTBQkwWosGaGjHKzDpnqUxB5SyVd1SZsbVO79Si/XhSHXdiYJdE7dWd7dpGvZH3DusdNgYwC3bz7hV1ZhanNuxuEIcYMU0DGGKMWOdy3xvaGNX0Op0yndXUs5qQmT9jTTbl29yJOiacc1RVqcAOsNZQFkXuNEdVVYgIddMg+Vm6c733eO+UTSs8Bl042tgSYyIJVKXT41yhLJu1OGsZDYdYCzG0NCKEzDboIFR/PhBiaJlOdimdx6K7jZgSbWgxkjJohbZtMammaVqKomA4HJDSWr+Mt22jIFAiw6rEeo/pgARKk6eUiKFFMiPjnKWqKoqiwlkF4pIHYkyRlAIxBWKKzOoZIoG2bXDeUxgwNtFGZasUVxmSpJ4hMcZincdaw2iofhDOWg6SQTcm3Swk30PvA0ZZJ9PtlQxiOiYuZSWs56oCyColTzghryeWJQNaIhEkqDk5QyVJSf1fbEIk4ro5nk0XJvMWpocw8/sqO5i3rn1bnG6VRfJWt9tFgphEzEyimmH1eXsGSzJLadLS/YSkLCGmB69JtI86f5UO6PWgqmtC/rGJns1LgpparRo81Hyr/nAhJmLM36cM6NLcJJPfVA+4YGEHvIBTFnfpxih49V53qOpSYXrWVZ8vzi3neQHo9Azozlk4ABJuk9wQI3PD7M0yw/qBkKPaf02wpyvkicjKAzc6dJ0jlldlufpjjr1YxmsL4KtXBMttktze/j4nBtdH9/1horC7YdY3wpHn3Yp0fqi9PyrduumwNq872QpWevWT1nlkqHyg8C2uiFn9WEQs1iQMLusqNWNiDcbm+SnK1vmsf1LWdQDBWmLQjSkdwMu9Eo0hmrwpmZNe/brev6p+Qz7X40tMHd3mb96b8387MJ3/ypvng2PyeBPsQr+KnmnyfwjZtWbhvMXNZres5PYdfNVzwu/6G7neUpHdgpIYTIrYpBt+E0N2wwGLwRuhsFA6Q+HmvtwYIWagflJkd2Jg17QtIUaiqNU9SbZ0Z8YrpIiNraL/7iSr/jMuD8oY50q9rmtmdZ3fuTpHNiHStIGYGvYmE3b39phMJ+rbZSzOl9gOyWYqVmlpBVpFUeCsI4ZAWekux2bft9FIHbG1bWrm6ZxBO+ZgMBjgCpv9egJ102Y9Z7GZnSqLAmet+gKWJadObZJCzItNC1YQaTEknLcMygpjLCkJV65cYnO8waBSk24bIyGpH1TdNIQQiCHS1i1t21KVFd47xuM1dRa1OsCtNxS2YLyxzmA4wDpHG2Ke1xbjnAJEo06m3qnfoppeU2Y3FKTXjaEJDSG2JInsZROrSFLHdVsSolC320xmM0yAotTrQATTUqWEz/cuSp8dZTUI4GZFfUQkgxmUQesmqmRg1znC0iDZ7JBEMGLwxiuYQn3hrMz3gCllBdor7AxySUzamrEJFDhKESSqmT5KwBmIJBKRmEIGWQab+1LsAt3f7dByX87NFFG7LRlssjiADOpEGlSR2fzMqhR1h+tQ37N5e0WEROpNEimmrBQzQKNzjM592rMDVnFiFFIUotHdaYogyaj/nDW4hceIQc2uMSo7l0L+NwPCTiP2evaA3pu7UEj/t75nk/VDSVVVec7md24FTOzBozFzrbsI7PIdlgDlLcvi4nRQpO/Q+d8LAOMoeLB0abMIKhau0X16YMG4Lea+G5ATgaN+8zIH6EvX6A7juF7oLmPyJQ4u7/NFu1/Sl/+5dTniQv1HIrDEy5iFL/NT3eJ76dlpM//dWDt/3iRI9rMry5JQlLTZ3w7RYLHWBypfUPs0nyOiPrk2w5WUEtFksOhsv5myBnxmwNQcm3DOUTctbYi6HtVN1g2GaDVYItgEjt5Xz+CQuPxy5lhYLUbzqSK9bljyrzvwLiQzgf2GPh+fUs/xLW0OF0Fxx7CZ3Ige1C32/eJ7zA1Xs3U+tgPYC++40z8dQdBJp3+PmzcpJbAWk4Roovppm857UZ/dGoe3UFqoHFQWnEXxhIm0EjApkuLJxtyJgd2sqUkGbOGJEpGQHSazt3cSXWAUOAhJ1NSqCtrgkkOS9OAlohEwXUc1oaZuG+rQ9MxaF6Wp19fOt0awhhwJWPYvJKZAG1p9IgN1XevxVphMU46cgaryFGWBSxaMkFJAUmfStd0rni/8eUApI6a+VWVVMRgOKbxSpUXhSVGvI1aISc3BVaWmW2vVL2p/b58ggWQEZwxt26rpq22ZzWa0Qc2/qQnZdxD2J/tMJvtU1QBjDMPRAOPWEBKbmxs9VavPNkSIiER29wZzv6MMjlI29TbtDCFirDCbzajrmhjUWb1J6jOVYuDs2fMIDiuJoixIE/URiBKZNY3qNmpmdVLg6Av1GSOqCfSWXJ6EjG/0r4WdujJNCmqWFlFFMzrRZeEnqW+JGFUqViwpdbtOfdcecPkmTQhMk8G0ULmI8wbvyD+WkAwpJGWrncUYZa+s8bj8rvs9YGhB1GzvqgF7cao+LjmKOIQaE2eI1GBCBnRJd9ZIbzZOSf1qrDMg6hspIojr2MMMeGIGf9b2Zo5OefYBDwliMsSkJlhrlYULOQI8xkQMVv05jCEGIQa9dgfs9LiYAZ8QWu3/fqe9uBXv2rAQZAFkVw6fF64K77z6FlrlnpMESPoOxWZTDiYr9znA7SLobjl44mZOP1KZH/VZB0pPCnk+BNItZjJ3nz+pafbQpVjEQcdco/e7OwjqPhhynXt1D3CImTkwsG9FMoumys1mf7ZuswjiOlKkYFBWUAWkqqGtmUmEACNfUPuK6GFiwVuPIeFtwprUM3bRW1zQDWhZJpw1ONRNZf64QjkcMJ1MadqG2AYar6RB00TaJpCt0rp+W2XWTLbkKLIUnJNsJjH9sF8CcmnuYrPYm70xI807qN875Ej7YCJFv1nW63VRsf2mV/euqjey3jG9+SiDswOvQqNlmfvsGw0K7YgjIAM/WXIJWZRFoHlQlDiIOGN6veYoMUSMVwuFNxqIVzhH6R3GqT9MxGBiQqJBwm0GdupHE9UkagwRAUn4zJIs7c7zi+zAibWW5BLeF+rXYxJtE3I4si6sYmKOeNGOSTFinaW0GlmpbGGHzlUnWONwQc2QMWr6lcIpU+MLp86KUlCiVLYxGliRUlhYDDsSNhFCi3G2B5zWWo0KEqP+StkPqW8zCjCKPMC7PbezFmMN3hWYvCMymOUdCh0LGmjblhhCz2ZYazBGQUNMkdlsCoj6WhQeHz0QKb3XCKm8YBaln49qsZlNESDR1DNa22raitkUY5X5A6ibWoFCUkCubYlqLrfaH23I7yslJGoUloLBSAgT6jbiC09ZeBIRa5VWviUxqHNcAsjKo2Ob8k4ndMyNIhhMspicwsSSQYEAYuc+J+RJmCNTe3bGZJ8W63DGYpOayKPxilJT1E2FsXks6vuOKRFNworgBEyOJEUEvIeUkBiJIVC4gtiGPvggxoCJASMBY3TjosSkJYkqGB1nEUR9MwTplWBnrULUZJEywyexY3/IPnapPz6luUJVdk/9YpLMwVtw+iyIISTJgU+x9zsNISjISwoWpWPy+xe30M95hgmdrkjZzKwLl3Ne0yBkU5Exc9/F7hrd3FC1s2j+XVB0t0ps3SgDIye76bKin+uco+79weDmFqOUD30Hc1C38PdJZB5RyNK/y2zmUkMOfXey5z++RYsmsqWjbhCYQrexn/dGZ9K7fcBO22qsRYyO+/l3ok792TLlnaXwuuAHZ3tG3QHeGLyxeCMkn5OTGIOztgd2uvlP/TxyC+dbO4+eLTLr7RpHcK2CpiaQaAlJMAls1PVfkw8IlmzezS4x1unc1nncsW6ZBbPdBscsvfdu5HTs7SKeVhcueiZucfb0G8UFNr8Hbua4d7UI7OaMweIVetP4QTkG1B332dKpIiSJmLxJN2SLUwaMNnePyz/GoP7jEjUiJRr9OYGcGNiFqMo8hIBYXSiMSOcFo+wdfd8jSM/YWedIkiiLIQj9rl8DBlJ2N+oW7hyZmX3znPPzxSHE3IP645zFeYcEjY7dn+xhjZppR+MhSRSIiim71tG2M9q21jHmLOqkrYte09ZYiv4FqWNmyotfIoqCnRSzv2EGfAmrdvScisF51wd1kF9hP36ks7en7C8XFLTm3FzGWs3VZsBaj0hkOp0gIgwG6rPnnc0skKWeNn0wR+E0yrdLT6ELriqjWT0l5Qjg6XSCKywu+8g1TdMzQKDMnmRQ45zt053EFNW3Tl0vaIIQ2khNwIegeQhLRyJgXaJwJx1dR4g1862bIQc5zFmPzn0tJvU7MEkxoBGDFYvD6eToSPcE3Wjt/dOM0RQlGVA457C+oLAFhWh6khBaoreIOCRFDXYwvbOdgmES0SaKJJikKVFiSOAcxjml4EOgbVq8H0AKSFAAbWLAZnDnTOoZaQ2KMGimxxxIkkQBH4BY/T3N/UxV6dg5eEsCKIOYoiIh6YARpteQUdQvr/OvCyHhnBBc/r4LrIiRNgTaVhnlmDpzrPrB9Qxhn9lp2dSiADU7YhtdALx3/Xw3nV+hsQs77G5MziP1epCqyJ/eQG2OUuI3INJr7cVbL6ONQ58d9eVRlzVLlz54y6O++0DLIYCXF8hDkYW5cXMmuv/02tfP/8rCPQ6d1Q9ccwAoX8OsvtRnhwNEDr6FY1u6PMQO8an9VwvvWvWpXfj21sacbvpBrEaj909vchoPOwd20VuKwlFYQ+isDAgeKIyhAGJ2PXLG4u18/mlaEJsZcbsE7JxzC/fVd+WcIzivoM+2BAHXgQsT1f9WwCYhmaRG63wv41zvH2i7/JMdarFW+08OBzzMX+Pyi1kMMNG/OfxS+x1ud6Y9NE4O9jscvbk57pylpt2kqHUhB5SIkj90mr7rIiPKpBrFUTYFPSUazO0Gdlt7uzirAQt70wlFUWiy3rwELbqP54QQ871pZhPG4zFlWeK8pw6tsi05MWG3aNStpTS6C3HZ8d9ZS+wUQLcDyJOsCw6QlJjWM83dlTTasSw1krUJtfqnSSKGmtlsojmCyoLCOzr/rCY0VDLU5IrWZRbB0gGKum5ompbZtGY2nVJ4R5SWulYzrnOaQ2881qjKEIKyStnMWRTKPsaoC+d0qubXlE29HRgcFi47hOtOaGdvh7ppqJsa7zVi1xohhZhNwKpsDGgiyjZQ1yFHrRYMByNms5q9vQk7OzvsT3bxVUERtT11U2vUq9W0IsaA8479yQRMQ4iJyWRCGwOQTXlRTZJNVHDj0gzbKtjzhWBdYnDzaeyArLQRRO2rQLcQKmKcJ7FciMhb2AxaAy6bNUi6Q0oCiYiI1UUsm0IsJcaWSCzx5ZjSOryPVKHGpEg9aylKBzEhcZ47qRvfDgMxEQlEGxHvslJ2vWINoWW6VzObBepGx4SkGZ4ab2I/cyQDLHWptWA8xIjFKCw1ORIMFGAa1z86nULrbRgGSRYIypaluXmmZyqF7HNn8vFGTauiJnXd0EVCC22T9KcV2iYR2kho23mCUubvaZGhXsx1171ba11+T5LNz0mTgHuDOA4oUXW3SDHka3SmF/1R5X+LFPEiVbXQo9eGCgf5raOOPwiIjmISjjv3uO+udfziYR1IvR0s07WvsZznbvnY6y9Hx137hM95nStf8/75dVz3DrfelCWxtkvVo5t6a3yvyYyJvY5xzuVAByiMpsVoTcKngIsBL4kCYYAQcu4l71zO/Zj69FAp+5WrL7oGTvgMHF0mUtQfTl2g6ulMfV7LGinUgtC2LSYEkiswKQddhICx6nJiIKeJ8jjrEVqcy0y8dRinXs/dJhTo9aM+t1nqn3mAhO39x032rV5Kd7I09oxufHOuxS6lk80s4sH7HCcHQV8HMG/V5zWlFg3GM0q+JCB6nCSs6Obe54wViURsAjYzpeYa+51FOTGwq0PoF4SQBGnzQpF97EwGXV1EqDGZ7rWdo2angB3eFarUM3InscAItFjvgNitOXkx63Z8c0XV5cjrom6NdYjTQTKrp9TNTKNjq5KyUFMsqdVcMs6AEXyhC2GSRBuVkbKiKTAwansHbX/TaIWFumk1YjYFYmypraEaVAyrisHaiFObZzDGsL8/6Vm5GAPOeTAZXIihDZG6UT+77vsCkCKb01KkaVtl7nIenCIWSn9bixhN9Oy9xbmCwXBAGyPeTymLiuFozHAwYmNjg8uXrtDU2V/AqDKJIdCbA/J7VkdV7UPnDfuTfeq67RnJFGMOLBHqJtC0yqIW1qjpLiVM0hxLzt78QqsBAzqmUmbFDKj/j8yjpkwP7owyaUbBkJguJF45HWW4TJ84U4EdOaI0J38Uj4injgkjDvAMi4SJQGjVfy37kDrvNZdaAsRRGI2mlhxFaq3PEWhRkx+3NXUTCEloGyG0qnCt1CTTIi5HUnf9LAkjXn1tUgRre3O/RulmZSfKVEre+ZtMm1sxmk8KctoCyzzDZ9cPmQ2U7POXOneLqF2ZffHaELMzdU59EhKhTTRBTcpdXixyfr5eH8riT8fI5LeV56zzYD04v5hrqlO8861iZ8KFlK0FGYTmd5w6559bkV61yMGPlo/pvz+SOrjmLZZZr8VrHHPekQlJl6iJa9934VmOgoVwskXu8FnXP697UrPwiTnqOt3/e+C07Dt7yM1t0YR3lDn5CDamnx8Hz1tijA63b84IdsjPdJPsIJ13w2KNzZURMliwuhYqEWI1XN3pdsUaNa96q+mTrMsgKTNvHiFZi7EecVZTU+UdmyFhveo8Y4SqLHFWYZXDaBYD53oXIklJ171stk3WEK0htgIzTZZbxkgIolrZdbnr9HiXk42ru5TPbHye77bLmQoiBRCzvlS+v9sAzjNLgHNe53t2m1mMhp2DNTvPpZfVXBIg56jTtF+ZuFhM19CrpezPZ+xBVdXL4kb1JHLc/OieL6WGmKpsTcmBbv1BJluH8nWyKc6Ek+m4EwO7NuadRXaQjtnslLKvkOqf7sVqKaRBqS/VW6sGk47qz07TZaE0ZKrVTk9mrmLUNHwmgstgC7qX2HMC9JMe+hxAkn2y2lZLg3XgwBhN0ms6s1B2wFTzTzcSMpuRBMkDrdNMxiiQjNnsJBKJSSeBOEuRBIyjKocMqxEYaJqWJpco6zJkx5xjwsSkUcBBzcUhthRisvlX2xdToGlnWFcguSyLQK564DRaEINxFlcUOF/higpfDBgMRqytbTIerbG+vsHe7gzvJ1pWrKz6nWFZlth+N9cB2axsJBFCQ902aiqQREiBNjS0rS74Iaqjf+fLlgJ4202OW9nZyFyBdsBe7Yj53XUqOOcuMfRmvLlt0nSvP8uCr8aCUtYAE4dWWzA0SXAkPA4pMiAympbHZLbIWIdJMd/TZzZ7Pge6tuZtCSkquI9kP7aQgRQt1gYFa0YygJdsTl3I07aQLT5lkz/dAkBPvGn+ufyspDnQUtO1xYilW+Lm5o1lk01KOrZNioho3kJ1xehy2gmhC6TIWe+76h5kcLv0GjNwma+lpo9q74JsbJ/GaFEhZjBnQgbzncP0ArDr7oncOit1UnbrRMO6A2vXpnlOhKlM/t9yt95Ycw7IdVmsW5UlE+lBTXCg9ddg+pavecwljm3CMgC87jN3c3vBpD9/hPzdofbdfC92EZjSs0C6uCmwy/tUyW45RpknayUzbJr7zOS0Ui4DP6xFjE4qK+pfq2yZZgswlpyOSoGdFdRv2/metQul+lvHoOy4WEM0QltG2gQh91HHnHcJiLWf6POvLfqKaySuW4ogFXHZHNuB24S1c2AHZJck3cB2ZXkOJRvu+q1/HWZhDeiapX9LZ2mRTlt1vvHza11r3ZLFZz34Pm+AydOMBoGUyoW0VJ1e1xt1WR3mm9yE5la8vpzcxy55BSQi2TypA8WRUb7RhKLWw3BYsjYasbk2IrZNduLX7Pd5LDAejRgMBjRNw/b2NrYkjzR6f6+YEnXb9ibCsujqzCkA80Whte+S+k5Fgi6iaHoKYwRnDcaEfgHGOJIxhJQwoSEkT1Eq8IyzGi8DUow0cd6BxmhUnnUGZ4UCg7TKGlk8VTlkUAwZlmPWx6dxpiSlgMUqq2gE6xyTyRRXOExrkWlgf6K+bRhhNmuRSp3JMYaQWprYMGsneFOSgk6wIJZhMcRKwe5Ey3w55xm7Ai8O60cMx8Kp0zPO3nGB4XANEYv3OxTllNFonbX1MZPJHiG03HvX49jd2WU2q2kaNTUXzlF4w6SZYr1gS2F3spMX+cCsrgltQRuUvbWFJ6aESYLHUphCgdCtpJ8wCx5b2VdPIAMeBRyKezrwlxVct4OTLt/dgeuKoTPl9ppAHOqLZmlqIToh2ERLoEXUj8Ua2jpSdGlnYsw+Kw7niqyQuue1hKbBSKSqilwARzcPGryjoK1tWzAN4hqMa/FGk6mklAjobnrum6TJvDFO05Lk6yFgfH+Hvo90s65KKk8RfU6jUdr0iUMXGZ2OHdOgjk5xh1YDfHqTbPfTR8keZrh6S/BRr7bzHXJzZW9sjoK2JoPUTtGlhVqOaZ4nMwPGjsU0pkv6dOvSK+iFXFrz/y8e2EPKw1/00sGJ+YbpMNjTY7rErX2N0kX/s4Vj+9ZcA9x0Xy225NoQ80ZkASIdotPmxywsu8zbbfq/l55rCetZDiYvPgjSFr7Qqyxuqg4cZ/L5uqk70Nb8t3S7H9NZA/rZtHDZxXNvvRfnj6BkRxfY0L17Z/ImTDKwywl5C+/7FChuFnpfNpvyBthmt4QeHaYcfJsywPJ4lyM/LVq2zPm+2IAG5M3JCBuUHWzrSBCLmJaapH5gMSFBLTZd4012KepNqShh4L1fei+dKRrm0aj50enMtF0glc1BVV2ptUUfdt3UmWwhmb8Xa22vL5bSlmB6gmhx9nZm1uXN5e2X5bEcSOLUxz7nEowZe5iFTY9yo1ov/SRyQ5Un1FlJNXZX22xud7ZgHTabCX3hGY3GkAY5UCKyO9nvO957rUbhnKFtB3RpYZOZT7TO70t3Wx1VZ/qFoW1bOt++eUWL2DMdmvF/XnhYE8omZQ9jpE0tofRYoz4Hpa96Nu+ol6Ao2maHUKXR29CSqhKs+qV575nVM0JoNBjDGVJQ/6AmNNSTGhFDUZQ0zbRnDZFESoHQNuzt6YuMKdA2GppS1zP29lpOny7ZXKsYD0eE2DCdaZ82zUwHhmgKjZQ02a21yursT/aRBKdO3cFwUOY8dHDq1CnObEbqumZ7e5v9yS5tPaFtJxjjMQSscVTlgP1ZTduqX1VTJ6Qr9ZVC9mEU2qZhPwQKJ3h/K8pPwORi1YY+WSYx5+CTvAvoxgTzsaEKJeexs+q/JaQFXWx60KKKwwEFSJnNFblOo7WkWBNEdGebI207p31jHNYWOFfhfcwbEu3XQVlgnEOaBhdbimJIVYyoJ7t5h5qTfbuANYFkcnBOzhJcWJvzHeUdPB71v4skXB9AYLPpvBunmkl+HnfSA0AcIm1m9HPfdbvAvoJGN++0QK9Ix853tZnbDP6ze0FIGTAuzJGe9EpLa2i3uIrQB0YZI/OqE7nO7+KCqc+o7DgYDq3J8ysv/NxG6ZKrXkOOH+GL4GAZ4szPXIRaC89uDuRPO+qcHsF3elH6uaDvPB0+dbFpJ3uIWxMj/dhYZkVk+d/+WW6/9Iv6Sa5vMwBf0BPzBnLE77dROhNhUmCEWCBmo0U3LvIaUxQURejTBFVBTaGpbbHW9Dk+k8naSpRhs9jM2BXZ7KqPawtNU1Xk5PVlWebNUvbxC+prPq0CQRqSMUQi8xI1Jlu4tH8k4wKyCdYb2895FpipJPMqDR0IIz/unJXLblbk/jGmB3awYIo1pi971r+pDtj3a8LJ/OMUA3ygJsWyJFHXps4FJsT5OqKZBrS9znRJ60923RMDO2s7rlMnqze2L1AL811HD+ycpxoMIOdpC0lomv3+ZVqnCXOts5qaJEfHdgsK0A+AbmJKbofN4SMppH5N6Xwhup2GtVpmrPAO511mBtTfqvBu7sydhBSS+iuI0/JFC0kbjLVKcXcbt4VB2fklzUUd/UPQNCZCyhNNzbghtLRNJKU80PLLS1HROCkSQ0uN6vYkURfOKJktaVhbC3Q0bYyBup7mNBQNvlCn1BADdTulbWvaUCGSmEwVRG6M1zm1eTr7hkE1qLTyh50ymTSUpZYLa2ONlRyFmbr0JrpYe1eAz9GSItknI7+j7Ags2cfipqVfrEQVRw4qoPevy2we6mVvjD20XplDC+pi3qz5wmmM+nt26YxLX1IVltIGXDKQd1ISc+68vAt11mU/MYsvLQQIQTSfWxJlHTNAs5IVq2Q/POlSlki/pseEmndTwlP0FH0OfSAnGSCKyTkKQazF92SVgoleMXU+b2nOQkiX/mVh3syzN87bZTq2LMmC4gmEXKGkS068mAB5zt4cp32k333bvkTQ3F2i64jOrDqPhM1zcknZ5udEsnP07QZ28zG2pOJPvJPv+kGvMwfOB78/6r4Hv1pmVg+3Yw4gpb82nao+dLkFLqr/4CQBxQcjUA9FpB4FoAzM8wse8RyHWnW0HLvQynGGsXkbTyK9KXTpYgvjkUV9cvCYmxOdkvPn1iZI/2r7Nyo5HUYOxrLZx845TfxfFJEiJpqcikzQKPeelurNkhlI2S6wS1lzm3Ot2kyAOO/xqct5V2bQlCs0FUIhWpnJEDHZ90PMwlwxi0ybVeYxFyvQNFN5PneMYnbjMgsbxK7KUD+m8zpgj1hUlgjWA3L4/S+wZUt/H/Fue7UyZ86ucauj5dB4OvB1iqSomKGNIVeCUouJi46OojPdCnXCyP+TM3Y59YEuFuoY73NSXwPZKdH24dPOewbDISm0OVigZtbWPeXqS4czDuPAeoNPLi9YcQnQdXSw9GxdzrvjLNBmwDVPqdABu6oqqaqCsvCaIsR2u5eIq0pStHmxNqQWzftlTPah02c2RgGsBiNnJ3XU/KSUqeZ062zgSSJtaNTcJkHHrLfQir64tiUGzUdmUgZ9MRHaRnViCqSQqEPELSSBi0Fompa6bmmbtk870rQ1s3qfEFoa5+hCZmJK7E93mcxOYZwnRsPu3h6DwYDx2gbnzt2TKwxoguammdE2CYzH+wHBtzg/IxpDSoa2Feo60DSa7mM0HCKFMMvVQ1KKecKpEvTWUnhLWd7KQmv6udYFE8yVgkfNVVqHWId9Bii9bu7AQv4QyFkz0djxHCAjBoPH4nLuu4JROWBcGgYkfDSamqRtKIJGqyZr8L5Lj+IwhaEclEhNdg2ItHXAeEvhPCIBibk+a2Ce4FcMvvP3k5ymxiSsaRG8BsVLrg6Rc0IaowEN0p3nFERLyiqnMP08VdOOsgCp87fL07hTlIYu1UGOP86bHe1zySBOfUC1QonWm+1SoCwt9Hkx6RfJ7k1IXhxzbi6TQZ1abzIrRwSTd6rJknJ9SiMJm7pqNtpSay0Ek3VR558Dt8sUOx8zvSI4wbHMj1/aNCx/tox9lsFdXnr1G1n4Zmkcd6lduvmxuDB1g39hs3nSFegocuqow/p3e5ILzlHlYtDb0YcauhRCt4u9W2ShDy3vB0EpMDf/df6ghx3lxRzlZ3eTYm3esBp1c+rnha5B3bykM4larb5kbJNTBRUUZaIMmlx8FpS9ihit5ZrVnWQCsK8Qs8CSFc7jjMc6p2sVaFLkhWcUYzTdSVHgBQqjqZaMbTVVU/bBm+fLM30fq/kYZSGtyxv/lN1kRP2Fc7owTI6qlazY9Gpa5cp2Lna2xwf0x+o8WnrfC23o/u3MrDmcrpf5Pbu/tePmulKHp5rDyR4w87l7Au/NfI/Dn3duL23b0MSGNhbEFGljgw0KurtgOCf2xIDt5MBuIXWPCBSF7aMOTXZ+dtkch3UkYHdvH4NhVrdM6wYRdSBPIkxn8/qObWhwvsgudouZ5BfAXR99mwdMrqEXMnsQc8JUZeISg+GQ0SCXAMv+BKSIhCnD4QjQBXY2rTXaMSQFdnnB6561MwfDHDQ2daPlxqImSe5SqYS2YW9/V0sjFZ7KeyYzfZshagQs0kUIC4ii9RRDTtmiFQySabBS9s7lMS96xkaaMGE628dZoa73adoJMbQksVy9OumTCCdgWu9ivaGeqnlW8jUH5YiULMFE9ic7TKczmqZlMBjQtjN84RmYMWk2JeQo4LqNeF+yPl7nrvN3UhnDxUuPcenyJYJon1hjWD9zB+fuOE+yiTpOTzq8Tiy9Y61xWGNxVrM5mS4MSoADE/+oUlPWaISy5j30iBhSRP2+xOBjwscpsr9DO22JTUDIzv5OJ1q5toYrLEYzdBCtITlLDJFpXRNnCdc2lGt3MEs1+7KTA2cara0sjs5k0oiwuZ8YVhWVDJlWk8z8GaTV0HhD0jlkJJuBVf2FEDUvnLWkqGXzkpCVqOligjC4/NwOaz3WqrkeYSHqVPu3S8MQc9BHyqyd+tTFXHUi+4LkRXJuVjFzP5H83TyNA2oeXzS75ganCClHtXffz5W4W+LOluFQjoa+bX4xixuBkyziRx1zDSZg6doHz70R0HCN570WrXBwv3ObcMqtyS00otfX17rGwYc+6aUXju8vsfjZrXeeWQBbmLmD/yECscPJVmtz+7LEJ3BR8DFRhUCb1L84GUGSJyEatY5Fc6YbQlKTLtb1biddIXpjlGzo1lnIhIfAYDAgWYdxEYNmrwghYIyhtm2/l1bT77yPtP7z3Af2oM9b/3DCQlTsQhfTBU7NgVv3Y+0chPVXWgDtB0FeV57sKDnku3mgjUtBG0JfC72rfHlSORi1HSUxCy2z2FLHVlPBxQIfuzRfposbObGcGNiNR5Umsg1RnSAz+u0iXjolXBYFMUYm0xmFLyBpebD9yVTNiVEdM6cz05tlY0qkGJbu15uobLajs/yyOl+61Dkd5mjabmB0xKnp/6/jyhUOX2jdMeuEtk20s0ZzyxWFto9ut2EgWc1PJjp81Nco0DZBeSJnl2z+dT1T/8GiwBclRSrBaqWC0XgN23lqGaNlW5JGGFqnrIvu2ho1yxnfV+tQX7HI/mSb7e2Stp7RtPuE0EDnhyai5rIQMc4xnU0w1hKDpQk1VRqSRDTKNWnU7tb2FTAJWxgKccyaKSHW6seG6q2Y9Lmd9RS+YH1tg1HpaZoZ9WzKdDahnSUKX3L3nXdx/sLdhBTZm+2dfCTekMyzmXcRuXT+JMb0EzslzQmk46wDFmbBaddhTK5BJ5oux1pHHROVCZSxJbbKokoLrdEyPSmJ7lpjyCnCBUQTIntrCCKUZaFZ6ZyhyL5k1hmCMxA1jUlbKyjRsjywPoO14YhiOGC/3UOISDKkEBAb8ih2RBNzFKmO9Zh3vyJdYnKTU5ho8t7YMXZdihSrwSKaUoBeYygAU3Ovamgt6xWTzltNShz7uXcwL10nHahbFJMZWNsFSFjAzK+ROmDXuTfEXENDOo5qGSh1PpLWeqzR3FbmFtLrHBxfHTLqrf5LwCB/eEjRztHUYSU839V3i3Z/1fwspvt9iYlbvMICO3HM6qSMYA7UOG4h6G5xC5jkqPd+Lb+kxajlY9enawDNY3GqmQcNLdkwFzcBR5mIj2v70Yf290K693CjEPG4Sy5sIoyof1wOzuo2qUiX+Fz6Bd5Yg/GOwheURSQWidrVOU+nmuwC2S2mQ4jZ2tbpAhG1XXRPYzJJYZ3TSFdjcSJ4DEWCsiiJYkBabGVwwdL2urQDX4nFeHtQ8qMDrN00UCtb14tCt9wnyO4dC+Av5mwcTrAmYpySSr1lLx87x91K/Cy6SfU6wsxr4x71LpYCblicJ7qx7ir5LgKtxS1af9VuDPbP261VR9xP1Ie8yUGbIQWaGCikyM/msuvKyXwE4QaA3WhQkGJLCpqVWjrnXLtAR4ooixY0aa5FzUFt21LXM6J05p2ENFoT1uWEgyKhz7JtjOnrSnaRKpg8cFDlj8yBXbfI6O8d+NPJmkQrAaipGKx3OTFqxzaqqTgmTTIbk9YmMDllB5LUzJV0oUkxKbALGvVqjevz9mC0QsNgUCF4rUAR9F5gWVtbo/QVBkvTNMzqGlAmyPlS2x812WNKBnEG50qlwjOAmEx22baetq5J1Ii0CgrFZHCr0YvWFNTNLN/bE1JL29ZMZ1MmswmShKZp2NnbZmNjTFFoxGQbG5pWwWIIXUWMbJ5DsNYxqAaMhiXj0YjxcEBqa/ZFKJzn/NlzXLhwF00IFLs7Jx1eR8h8Biy7VmkcYrej7HZhRmxmeeaSklL93VxYjNBKecIY40BsnxHdWk+b1F8lxkBoI9KqyT0YwaJjOzpLCI2aTsQhRalFta1GNld+oFHMRhiVHhkWjMqS2IomocSwHyQvTIYiGTajY92PcaM1Htp6MJsjUZcBiXR+gClncyePeSOWrkBeDKqOdJ7ovNTExPTK0xrbX8ssAYDOv7SLJIukpMCu8/1IixsqmbPry4WwD6Yj6DeemKWfOSun1+yAnYLKTpEinSvGInDTgApnPV3C0tsH7LreYHEYXkcWAN81r7momLuBmRfGDGQXv1pujSz8fpyC78y/R/i/9Zf6EFB0S8+1gJ5k4YtrrVnHgLP+6yNudfAIWfj2ABczP7NbxI/qwwMnLpnLb0KWF+lsjrQyz2iRX7lkgGfSQqSxNVjv8IWhaCPBR7x1BCs5nqE7sXPByFqz889NRvPTyWKPZHbIqanSWYdNCScK7IqyIua2+M7XL+tSdf0IpNQeZrpw83fSreWHOrXTPHPDS7fRmV9OSDbrHjvXG1qWi3l+uv4dLsH7rO8WNlcc3qAs6a3c/4a8z13wve+SHnfniNATIb0srVvzoX/U2EtJaGKkzZvoJkUGkrJReIGgOuGAOzGwg4iVCCnQzALlYNAnMxTA5vxbnaJv28D2zi66qzYaol2WWOsWIle7SLdceUFyvhvn6AIbOrHWUNhCFzL0JWp5q/muf/Elqd1affZC2zAYlBSF+kTlZAJEEeqQ9zV5EFjvlnSgLorZrCQgUReg0KpZ1QgYp9GZKSWaMGUYhhShpGkC+/tTkljG400u3HkX6+t3UPiKyWTCO9/xt9i9PXzTcOed59nZ2WF35yqh3WYw9FjnadvEYLxGFIOZTZlO97ECoakZr2nZtBRSTkkR+2SyLgqz2Sw7vwsQuXr1MltbOzijZdNiiCSpGQxPMxiUxFiwtj7i6lbN9s4uu9N9Zm2da1RbQlRwGELDdKYJpTWKylMVBYOiYjhY444zdzJrA0HKkw+vQ9JNbZMzc9scYKOfp44xnqM2TPRoliWw0iWC1knvuulhdIJ6Q05GqWk6vS3wrsKbISY5iC3MInEaiU0ghYi1rQJAMYQmsn1li6L0VNWA0o2w3lFUBetnLjBcX8dXJa4sqNZGmGIIfkCc7XJxa5eLOzPqUDMVQyWWs1Jx/9mK4elzhPEmf33lzzVOXDT7uCNhJeocST57BDrIAR02K5c2SK8GRLRSRZcHMsaobgepC6DIDs2xU56qYENscEh2i2hJqc2+r9lU0iUzltTrLt05Z71j58vEHGt3WtLk3JH699x9IptsRedcCgGnViUiDn+wqoT1dPmReqbhtuK6brff79sX/j4oi1TTweMXgEvXwHSU9p8zRsui2rVjDOTQta9Bcx0nx1mBP+CSge8CU9NLt+u4LXL0heZv4jj0fC3AvCjXh5E3J8v37oBLZ5FKks2qxmhibmfxpeBDwmVLmk0Ri5YYa4Ky72p+zGuqEVqJFOL78oupSzjQDcEcGIlontMukGE4HObASEcbIoUrabwGbFg8MaqlqWt7D4plrosWo2Ajy8x+WnjOo2TRTDtn+q+VAuTwRutg8M/tkA7Y3exY6J6lDZrwv24TTdsQYqkbV5uBcZ8C7PpyclNsVWEFCu+1QkSuQBCT5sHpfGI61ixGBRgGS1lVFIMR1jmwAUwgNo2mrMj2eOnYlaSltw7u+HVRSTnvG2C6NAypNw8B/aBJKWlaBoSmniEihORpk1HAlxeVaZNTg2AwkrKZKkcECnRJUSGr1243kEnZlKLuh3Lt2KIoGI3WGA5HGu0rlqIcUA0K7jhzJ8PRKU2RUY6pho9hixGj4ZB7H/84rl69wtXLF5nsDlhf38QXFZIsG6fPsL2zhaFg6+o+uqtS82xTK7MkCeq6c2rX3d3u3j5Fobnupo0mJx6N1igqT4rqmzWd1eqc6hzGDSjLMTFeZWd3l53JjlLDsUsKrNE70+mEpnbUTcBYTYVy7o6K4XAdcOzva6BMXd+8j12M3a7IqimyS4Jr1OFfqfUC0yU/lGVlKxIQsdma0Zn+9N1Z43JSXQ2MKaoS5wdYN8DYAWI1GjkaaHICbqLkeo4CkggGhqMRviiwrsCXJeNTpxhunmJ89gLeoxVMQmB46hTiByRb4kdaxm59MGG9nLEzTYzMgHsKS7n1KMXGOvXpc9TvzfWXRWl6axYVmbJwVkTLgBmba8nmtCHM2QZlyEXnaw7+WKwLrDJnmzQSlXkN3FwbuUscntI8YXjHiuvvuRi46YrbkdfwA3vzJcZD0ATOIbMKlpACEnIuKVlIk7B0DWUV9L1np/NcWvDWxdDbX49U/EeBAu1v0yf3PgYgpAWw2C8E3Wra9WM+tTdFLVzHLJq4Dt5jEeAtAqYjmnIbQN1BxuMEJyzdnn57fUM3zRe4icb3jF/HaM8h8vwZDgKB40Dzwt+3CRwcaqs27AC4E10zjeaVs9Zo+q4i4IpcPcLllOgLfdSZXLvNcBSh1R0jNqHsXfbvdsblMovkoP4u/5yhDJ3eMDiv9bGddaqLTEsbDCHqmrnI6HeBXQd7atFtpvv7+C5ZPlafS+a6QQ4PdMmU6lFBFSd7DfNAsPm/nOid61KT73sNom2xbW0baNqWWdsSgqNtA9a0FGWXueHkE/fEwK4stI6dc5Y2RKSukTYSDmRC7l5mzMg76yjI+cKsEZKVrMi76L5uwebQy1u4MiIacaspGVJOG7FgEspJCm3226NbhEOgDVoWK2IRqUlJCEnr0+p9LR3/0DET3e6hX7ww86wMmDzxumdWcFgWA4qixDlPjIm6bjBWq2xU1RDvKzAerY1ZUlUlp06f4cwdF3C+wruCvSIyHm9SFBXg2Dh9GmNK9vZmePf+vIBI79OQkpaoapuQqXU1JXSloUIMGJPwhWM4qvClJzS6+28aBYMi6itYFEMEy7SumTUz3cnllBcGTcFS1zN8MQBjKIoKV3gKX1JVY6LA1vYO03rK9v7Vkw6vw29bX51KzKAuU/59AqaFagfziLZOOaeehVL23/aTkxyZ2Z1vnc8pALJJ1mZgZKANESuSFWYG90bNra4o+3P9oGJ06jTrZ88zPH0GEyZIqEmp7Qs6J2uoXEkajTQ5KIaNMjIicr4w7O4CgwEyWqeVzsdmYXfagbluzGWWo8v2oa4Lnf+hzE2boqzt3NQ5Z7q1z7oZph2v5cESpvNdzSH5/XzoUgXRpSbR81P/FvoZm/HR8YpQowJzfj4JObDbYI0n9fn1Ft8tC7/r3/OIudu5yC74bs0/OnRM9xQHRZ/d5LbOPzUHj7nulY7/9EZkKWXHB52p4wAYOwCMTgjUrtVHRzExB9eRZV+7Y1jVPOlkIUK2O1bn3fz4o9Of3KQYMwf0LMwvSXkzJf2PppJAGTcXl4IfnO0A4MG5OB/NyoqzcM38d7d5Zg5iNPE6OK+kTsqbQhNz6U0Dpfhs9UpgtL3GLNSozWW9uvV9/sgH58fhOXzknD60d8nvQ+YXm/vKzT3+ruWjdhh3mEO/da+o17FdM7rTZOGsY5p9rRETM2GmAaG6fhfqeNw/xUlH3ImBXZcPLg3U0uwnU6bTmrg/6UeQYYFO7erMiYZiN03LcOjzouzmm1erEYadOaWLgu0Qf2dqM+iOfDQa0YaWWdNQt21mE5S5s0ZTN2jAQDaxioaKt21LEsGbgpi04oNm2JZcYsUSSVrUImneuIPMRlfew+H6nYuaKANk0Kt5hjxRYDqtuXxlm6oaYu2AGKFtNRHt3l5NGwyDQcXa+ilG4018MWA0XGO38lTVmMIPKIoB1WiEYchkP7K+9iiGfWLUZLE2D9aQTbAabeJ68KFBHJq3b1CNWF9fZ1BVzGIL0tC2kaYWwtDiXcWgXMcYT93MaGOuBRojTRMYDzSaeDqbsF6MqKoRo2pAVcDa2gbWlOxNE+9/7wPsTXfYm1w+6fA6JJJ052kkQbJIpPcj6/1ErAYFdPV8Ben7A8jR2vRl49RXU8efyak3kvG4HGGKFYxLGG8x0SFiqJuWUWEZVI5JCDjrKYuStc11fJkzonvP8PQm63ffzanzd+GbhqZtCe0EqfepdxJusIYbrCFFiSlHFKZkMxnOjip8qPH1nppSiwHTtQ2iNBAzeE0aZap58DRaNWUftK6GcAdedZzmaPKsrGOSPgdhx6bHhTm25FNC59OXMCbXjm0DbVAWUwFhyotcyu4S8yU3dW2UeQCTSK4qYWBZNambQxJDSpaYaojZsdsYrCkwKZtY85q8eI3udTo/D+C6JekWfq1fxLKmPnhs/wjzZ5G51UARwHwhXV5PjmhnR9wttuXI4/PCxfwe88XqiEWxJ3+6zan0Rx7VW9dbOE7M0M3POPD74dQjS6vkzbBx+RqylFP0qKYsAukbvY9hzsZ+IMQiOQ2TkTbnSdWMCSk284h0k5DCaz3uJDhRXzhvNVdrYVrECkkMHiF0uU50dwwYYvbJzbwHEQjJYGPODNAXHlAAY50Fm6iKErLvfOMSrWuRKJTGo6mM9N12wXbJ5GpQC2C1Y+C7NGZzM2buZbMY3Lb8ef9zkJnLPy4nl5Y84eZuOtmv+Dobv6V7mM41ZD5fLCZjV8GZHsBkvsH0aXCO4tWuB+pAsUEbQl53tdZ3yjbyrm8PZ2s9Wk5eKxbXJ0d0xrBmS5yviQnqtiEJBLGkjhnAYF3RU7zT2RTnvLJLIdIFzvV9bzufKc3tQmYGsJBCRApHUa5x7txdJElMZxPe8c53aOLcZEhJ8/B0gzJK21M+3jpN2JgEYsT36Lz7SejwtrkWZqJpWzpzSN6/YBB8jgq9//6n0zYte3s7mfFqiTGxt3eR/YmmO5ns7zPdvYhNa8jI0e4/yk7zMLMmMpu1THYvkdoxO1fXkbtKXCopbIkvZxTViMIXOK/O4QZP4Qfcc9d9GDMlpSlXt68ANvteRJyLRInE1DLbt6yN1ymGI6rBiNFAgyH29yZc2bpKPZmxt7PH9pXH2Dm1iZFEMxpST65A2GNgArPQaO68mCAZdfpPAWKD8+r7FqJhrRgxHJ6iLIeM1jyPPvo37G1vc2X3ykmH12GJZGCu7G4yyjT006xj9PrFWN916hzujSWJy9nqcoDFwuSOqH+ec9nPzngMLk/okkBLbR3OeoabZzh99jT37j/MfhCScRSlJ6UGVw0ZnTrDnfc/lcFwRJrsM9m5jEk10rakOuJLi0xrQh0o7ziHndX4WYONkTi9SmynSDNlFgwxGOoWmsaRcs1kh0WoNSIXep9BJ2p6MWLmNSUl5pQ96ksXJbPnOQq6bYOWAmvTgapNHfCziEk9KxdC95Pyjj4HY8gxi7wIElO/WC/r0i5kYw5PEKt+gjGSQoFWntQUMl0xeBGTGYiOcV0MztBowhNl2L0hOQimDvwtR6nvG5QlU+tB1ujIE67RxmveaPn44zDjbZeDfXRtXrI/5Kax3Y0yaEcst+Ygo3mCNn+gJG+eQiYhYkwY7/NuFXIx2L68VvfjAYzgA7jeAb+rzqSSkuqJ1oKNaunyyah7StYl0GWk0OTBVVXNgVcIOGsJ1mETONFatc6qnokYAgpKUtelQr+Z7B8x65IbMe0vVqQ6Svr0ZJ2Cu4bJ97io7iUfQebsbJfYufMl7MByz0ZKl8TtsBgzV1NHPW9KWqGrbgOztmEY1G1nsFjT/oTj8MTArm6jmjycghvnC4oi4b1n2mj6DCzZPKpdAWgahqQMQJ2PU4dp3YmreSk32Jj+4U3utBRVuQ+qgo2NOzhz5oI6d+8VgM0L0NxUY7B5bs53VzEns5UkSM67o/56qY9m7WjvGDXnXMjRNp3bskaFahTexsYZzp27h7qusc4znT2KkRpnEt456ukUUkXpCy6cO8twMGRtNKSZ7TPZ36cJkaoasTZ0QGR36wqz/QngCU1gMmsQHLEUBhbCLNC0NcYk7rzzAsZEmmbKzl5DWRmK0CBYJvvbSNB6uc515bEqTm2c1xQ0k312dra4cvUSsWlpZzNMaqhnE6bTAmwkNLt4GsaVYzKFJuPhDiCpX1dgb28PTRBsmVW646tswXgwZmN9zP60wtxCtpOO8YUOn3eLoGTFc6CKgnRsRD9zoK9G0fko6F8W09P2Hcurb1ndBRJWNwLoOCmGa4zPXKCYPMCp8YjkSoKAHwzZOH0H5+55PIPhGt4IhFpB2nSKxIjHkNqWEKbEEBivr2ElIKYhtvu0kx1oppjQMjp/N3uDMXXdqK9KIldlUqdnl1OQWKR3CTBdYtOOCenNAhpdGtPcP65PEdT5jQp05midQ7l/0ZeeRPJGrYuqlf48yXapeRWQPE+Q/jvNPdm9xc4knv89uO/uTbzaJsnvs79nZmjnppV5tPxitv5bFVn87aYumj0cDyju49VxHsA3dK8bBxmHLJ79huiWLnvswnjoYifBRwZ6h6TrLfAL1+vh2bFuPIdlzsPoef3lhDzpjn6WQ35eLHfhrYn0hLFuoLIrROjKTKHuIqarFhC1Zcb0GSWs1RQlQsJZoxGyC5TxIjuvJtmclFyy9pT5Ey1FigKSTF98wIsCNguIj5ktTSBO8+CJbtqSMbRGlt77wWDHQ71wAGAddc7yO1hmBOdy+M0c59d3lP/d9Xz+9LyTT5u85CxpvsW2iKjffxO0tFibAiH5nGpMdetJB9uJgd1k1mJtxDrHcOgpC02S6LzXklgWSKav9do/jHTJTqFuZtlRMy+aogtUEsm5oAWwS50dogZgeF9xx+nznDt7J9N6ogECxmVzEJrugvnCIb1JBUJKOKuImCbmztRJ41wuTIwuYCEqY5f6hWlZQRe+5PSps5w5cyez2YyYIle3HwCZUTjD2qigaQKFr9jc2OTsqXOaDsUY9nb3mO5tk0icPbNBimP2dlt2rl5id+syzg0IIbK1s0Xd1gyHFdavMZvsM5vtYgjcffddQMH+ZMqDD11mfb0iSYP1BZPZXq70kSgKT0oGQ8GZ03dqvjuxbG/vcOnSYzgjuCSUNhGaKbOpBxeJ7R6ljWyOS/YmBSGAkUhMlrIo8M4hMXL1ylWcKymLCu+GrI0Dg4EwHAw4d/YM02aXK7vVSYfXIVHQDQo6OtBgFoCIWZpcebRlBd8pI2XqrNGAiw74WJR9VC6si5dVIGiNy2yUHptixA/GjM5cID0kbG4MMOWAy7szRpubnL3zTh533xMpilL96toZEhomu7sgsLG2Rqhr6v09mukecv60RoTZQNvs0u7vEGdTJCXOPOnZzPwG9bRGrfv6YCmDBbGaiscvRDV2ufowc+Vs+j6M88XhQJoSWeo3NdcoUE5zpR+7TVhWYOmgcs0d2vNvGpE7/35uCjFWMis/B3hdom7JC1pKKfsQzsFj/4PN+Qpzk/tFR3riy9wqa9d3yHzxW0bL/YGHTl0yKS28hH6MHjiz27h2A3bZXLsIHhYXnEUGgeuAwSPOud7CZTgQsXvEVfM5s0aDciAxrOzc7LUki9fq/HXkWMx2UoC0BCYzsjvRm18CZQuL64FrK7ZJh6+7hI7NyRt8jCxjx4WxLqjLQyYfYs79ijFIdqjt529eotT64PqMEoACO5vTpBxkqfK4TqLzL3WbtQXpQVLeQIkkXIp4SQoGo+mDtnTnlzRrhOsUhpBsVMvEgX7ugGv/zIsb+QUdc9Asmg6Mz658aLexv6Z/5YHrHfX9/O/5sbcLuHe37ADe4r2751NrYVRzbFCmNrgIvtPPJ7vXiYHdla2rPQV65vQZhqMhMUbqtlXwJeDM8f4eYGjqFvWl0zJQMXaFwgXr1LlcuvqWxuakq4m19XVOnT7F6dOnOHPmNNs7ht3dLVLSKM0YE86V9LnpjAIA5zzWWtomYXK+nRAiKdW9P19ZavUCtftnM3HSPHbOud50FBEKX1L4Ac5WpNgSU02Ump3dLUaDio21MzzlIz+e8XCD4XDE2toaKbRaTqyesbtzkRAjWFhbv4e19cRjjz3G5Stv5x3vfjNF4Ykp8Y73Psgdp89w5vRpCn83lx57jDaXHfP+AnULbfIMxmc4ffYU0OLLAVd395g1W0xnu1y8+AgXzjtGw01Aw9Z9oeWtLl+9xKm1MZujMWunN9jc3MBVA2IKVIOKO86dZePUmHLtKruTljYZRuNNNsbrxBDY3d7ir//8rVjvGQ6G7OztsL+/x+lTZ0ginD57muQDFO1Jh9chSVEVnBY+dmBcvwlgwZhnjKPn+g19tYW536eycGjWQQX/grLEopUYlCHu/CMdpY240BJnU0pnIal/2Uc8//9DfOgvsM0u99//FMxwHVOU1PuXKOMaYbpDmO7STGdsrI0AIbYzQpuI0z2Y7bJ3+RH16TOGjcqy5Tw7s8ijF6+y9g/vYuiGnI5XMWI1elu0nE5KiWQTzmX/trx5MZLAxAxwcjWXvJFKMc7zRnZO2DGRQr5AP081OlhEzeutaL3glLT0maZI6TY/XUDR0Qqyn+3dgmOVTXDe4LzNi032TTRqHrJdMMzCe0XVQt+++Xcd6MpgFlh2E79dMmci57939z/O2LLYF0cvB+bg9ToQceQ5J6G5jmrvwXNuAPAuoxwO7AB6+Y8/f5G/fXBCS8urvup+zqwXhy9klLU5vqmL313HP+74ixz9qq51WnfANcsFZBP/4njLF7hd7DBoMJo6EEt24TMQLSZFJETNCNAG9ZHFUBYNyRa9f1wXKJF0MlFWJc5rROUg+583kpjlZ+ldUZJug50xensnBFGwbp2uvyzkle36RHWPozFTXHRIUCYxonlgi2ztMF4wIZIMlAmQRCuxD9TQt5Brb5NwEqhZfHWiPtVJ8ATElKRctSnlggwp5g3/4jBYYN7mGu4AW3ng9/6O+UJaOkxddJzR7AhdgGZCcUqfqcWo9edao7cHm0mWQF1nOVw8JonQxsCsbanbhsIZZoWnCo6QXVJOIicGdk3dauSnMezt7TGdTklJqJu6z2VHXzfVLDWe/mFy43V7MLcGmITrsmGbzgwDWEvlK9bXNxiNxzhrmU722d3dZnv7ql4rm5j6CgJmHpqs7ASUZUWKoM7NFk1LZ7RChC8pywJrLU0TMEYDITp2o7tW4T133HGe06fOMhyuUbcNe5Mdtncu46uC8fomG5tnKQcb+HKEcRVRLK4oISXaJLQpImaI8yXl4Ax1vQPW4kvD1vZDOK8L+P7uFcYDSz1ytM0GxgSc1UCOrauPMWkcu/st+/uXERlgbSJJk39akkSGw5Gmc5lMmUwmVFXJZDLh8uXL7O/t4lLAS+Ls3Y9j8/QmxpfszyYYGaJut0NqcYxqAVNw6sx5Slewt7vPbNoyXlvT4AZnmdUzrmxdoY0tg1HFxto6k+k+N66sFyTKnOfOfh4aPK3BFNYYbHaW7c2ShszSKRjofOoMZFcBZeaSJOh87sjmixxNbU2GOSkS2hZnoZ1N2d7aZu+uOzHRUbUBF2rqrX3cYEC1sUnTTmmne4R6CiFhvTJme9vbSIxI20KMNLMJPpfxAY0mWz97J6ef8Pewg9OY0OBpcNbSYAhJwZszXhVYUnWI0QXBGYdZCJ4Q2j5PXZ9yQLpEw51ZdoFlyzVnO3NrShoxrC4TCwxCd738ehaVIAuK0+QF3Rh12zBWg4o0Ys8qwHMO7wyFy0lQnTp+azqh7M0qZv7+sqmpv76QgWqOCJbOp+bmh5vK8u5cDv128zdYXkbm15NrooQFYHHsVY9q23Ho5gRA8YSo5aFLV3j7w9uEGGjjPbqsL9TqlWx9OdyUPF56gGUOEHsLrFj+57FtLQ1+x/rideaMTpfWQj9eBl6HmJn8/2tiv37DskSr9GtW/4Q3UkfqCOmsApLBs3R5WTtmPUatUx2iAjtjiKEgOmXxQ7fxMVmHedcHTolA6Voam/BG1E/XzNfHRQKmS4kUu2pDuXEpzU2b1ma3pIQCtv09Lm3t0MZIURQUg0rLMTowVggUeGvAJGoXlDW0x4xLYxDrKeOE06fWuOv8WQpneOj9j3HpyjaT1mKM+gqqi4y6Vqnin7tcHQfcFp938d9FVnDZn+4I6fA9i4Fh3Uucn3HUuOqv3eGdhc8PAsyOlWyTVqQKSQgp4WNCDqX1Ol5ODOzmSYVhMpnmh5Dep87kBnXKVzKQU8ZlvhPXBaRDqWoWS9lPLo/z/scYQ1lVVIMhRVEgkphM9tnd3WFvbxdnu5crOXKuO5N+cINlOBzRNAFarcogmfKwRoFdUSiwi1FZD0T9kBQwarSu955Tp05z5sxZBsMxu5MJO7s77E12qQYDRqMNhsNNrC9J1hIkMWtrLIm6njJtZ0QTKaoxZbWG8+tIPcPYAl946sm0V3YiLW2Y0bZTQqgpvMHZAhCaZsbeXsPO3pS63iK0G1gnxDjB2ZDN5YZTp86CFAoGt7YYDEq2t7bY3t5iMt3HppbSCmVZUg0HGFdQhwCx0AXVeoajgC8t1lWcOXMGoiG0gjUlw9FQ2UegaRvaVovEF4WjPnMHIdY0TXPS4XVYEgvEhr7Xbm+Q957K9KSFydLPqjm41xNMDjrQBSV12yajmch1V5rBojHKPEvMSXKFFFvquqa1Y3yxRmz2mO3vMav3qcKYalAwnbXEZkZqG6wYorc0bcve1jbOqsuBNYZ6OsWUJZaCoOUVqcYbbNz7kUyLETZFCpsonWOC0ZI/2X1Fn0z97UzSqNVcDVa5SNEdcMq5qSQvFkli/lEFnqKaNpdNP0LKGekxXVoUmZthD2is5aCi7jObzaKqB7Q8EThv8M7g8o93mgDVWzKo1iAVa2L2fzTZfG5zEXHb65Xubov1fw8twjct2Y+l/+uQ3uag6r4eC3CULAzTQ9ebX+tEl7oBubELHga1y1fY2t/j4tY2IUbe/r5tjIFzm6P+uEWIvOT7unSlhR1ZBinztByGNiTed3HKex4TNkaO9UFF4W+gn49kkxfYnPkKuwShF83g+s+8vSJLJ3JL4643OeoEm4O6BbeJEElBmXYxalVKmUVKkvs5AxfvfVabyux45/Au4kwOSjwAaHvAIx2g6Ahayex9tpp0c8+CTS2unWD2d6l3r2qC/8EQZzewIvgUiQQcFm8cYi2x8pTeEVpDmzRvbAySwael9J71tTE+7HHn+TM84b7HMXDCxtDxvlHJ1gwuX93JeXENw1IoCoP1Guw4fwUZ/OS3cj3Xg+PG0eFRoyNaui8X9KH+efJxcPCoQ764eZMdUlJQF/X3mLrycCezTpwY2HUJEUWE/ekklwIzy4MjAnR5tPLOycxNKG0bCKFjDCxlUeJxSE6quEzTgrGWohoQU2I2m7G7u810usfVq8o6jUcDQmxoQ6QqB2DDnCnE4Zynqobcddc97O/vs7+3z+WmJRE1RNwVVOWAqioV2AWwroWQSCHSxIB4MN5S+Io7Tp/j/Nk7ceUaf/nWt7IzuUI0DefPX2AwOI3xQ+pU07YzpIm0bc2jjz6EIVIUjlOn17nrwhMZDs6TZIDzG5TVPsPxHVRDz6lTI6rKU7fvJsRI3QTaIKytnWYwGDCoBrRty87+O2nqR7Byid2rDcZCPZ1wZsNhKRlWnmf9/Y9nut+ytbXLX/7lX9I0E6azffYn20zrXeoJtLN9du7dYrh+lmLgcEXJZD+RYk2SQFMHMAU4CHVD0yQm05qm1SznRVmCgccee4ymqQlXA+994N084f77qSqfYcnNisnBjror64GagHEOIw7Npt7t8EXTlRxAIfpn9rtE+lnbwwRjsLbMZtgcHWbA5ATXDqGqBow3TrOxeQ5b/AOaS+/hHX/2awwGnvVkccMJVx69iJWI64b8rGUyaXj/+x7jzKkxrnDgLenqFoNz5/BFQd22OCNM6hkXH3mMC6fvpywHjAfrbAzG7NatMtKdX3IGO5LBE7mChoU5iCNl3zh6NlzIKU6yr06fozEDN4AkWkNS0wYZzaMUEimaDPi6fpsvCC4HF83na96sWZdT/4D1Qlk4vNfPCkMGeJbCaSS7pmuwWJdZUwweS2Fs9oKcK+uj5Khot5uTRZPo0lJ/i9e9lnQbkOPucTPtOJI3uLFmXceZZ9okJo0mR/+n3/lH/H8/+8l8/ec99eiDO9PM4S/mbeu6QHpPTd53ecY/+//9BRe39nnOU87wLV/4DB53wePdEZe6BTkKci7QC8ecIYfOuPEbhy55HEYCSEBSyO47DSFvmEOdq7xYSEFoiMRMXHRpPIy1FEVBspbkE9JqGi4fI961ZAXSt1c3sOAtmpTf5IpMRs2qSRI+u2ZI3gSD0OxcYvLYQ1zam1J6MCT2t68w2dqi8E6rO5lIaTWF1MawZbxxFusHRCn5i/c+yKNXt7kyS6RkuXCq4qn3XeCf/KN/yGB9iPWF6ve9x/jk//ezia6C6SVe8QP/nbe/5/1UwDPuMlTjIcmNeed7r1KbzhsnINZj0ErY/eu7Qb3Q5cnt2IUevC2Qy5p0mb4EWpdVAK4xmxe+h6P1VUqJthWaoERJGyxtKLBEza9qTzaPb4Cxm++QnSsWlGn3qc2pSjoKV6swDAYDjDF5YZE+TLnzZxO02oMPkEwkWaWGm1mNCIwGI9qiYILhUitsrI8pC8/Zs2eUqUqRpmmpBiNcFWnblulkiojmHit8QeEr1tccBsvOzg5tUIzdRwcudLR3Wvu1+zy0ASuGjY0NxutrDMdDMCVNm8AUjMYDXLHO2sadbK6fJVBQzyaE2BBCQy0aJZtSy9333Y/1Y4Lx7LcCUlEOz3L+Tk+M+4xHJWXhePJTz/LAex9UsMwG4407GVZDqrJkOt1nVD3GqcE6Z84PKAuDMcJmuYatKkbFDtu7U9bX1hkNHdZVvOc978MYq7Vd10re9/AeMbS0rf4Y63G+hBS4fGWH2WyXpp1ydWsHYxxlNeTs2ZqLl7a4enWHxx67DL7Fe5cTQguz2T51U2ON5eFH30dReIpb0cBR8zoJaKVBE1GTg8NLAVJgoiUmNa8LapY3lt7JtCtTlSRRWJtNAdmkYg3Gg/G603V+ziKRZqQ4I4TI+qkxZWWIYcJ+3VCWp/HnPI//2IaH/vwt7O3XVCFy4QmPp51MCU3NYFzxyIPbTPcb1qoBW1f3MYWlHBbc+cR7sYWnSWpeaU2FGYzZOL1GCHuYpIzyhdEZLk8n7MeGCJqWxVqSNXQWDUlCa7SUn0WwxN4frmNAYtS8UiFACrowKCOgu/ooBkciRUsw6hfjuqTX2XFbooboGumJTrK/MoYcZWxzeIvVtnpnsU6Pcw4Kpxnsvc/1dK3F5t26cwrqjHP5PDv/zIIxorkjO9OLmQMCm5y2zd2OyhN2iUU7yPh0DOeiaed4H8NeffdtXcYAulCaQ6zdQWV/EETIge+Wr3f8sSeTudmosz0JR+FKyZutsqyQEPi1tzzI1tUtvu3L/wE+v4fjI1Q7On4RQJt5wQ/g9996lTf/5SXe++gVYhIeubrP7//NRT7r7AUlFa71DDf81Nc6b96nhiXybuFZbk0EkBBIuT51jIGY86h22Rtivk8IkWA0/ZPmh1QfVa0clPoccQK0RYH3gaJo8XUgGJAcOWvNQf+53JbMGCnDtxDQkVOPzSYTtreusrPfMFpbp7AF60PPeG2kGzgnXL50kfc9/DBb2zs0TeLx99/DuXN3cM+Fc3z6P3g8F3drtictT7iwycZoyPrakI1TnnJ9A1MMwBZI5TGpVf9mv8m/eMnn8vCDD/H2P/1Trjz0fs5UQx73+A0+4amP481vfZj3PLrLRCxG1MpGBqkfGJlbhIwszjntwH57eA1Aea3vBKEJkSYk6hApQ8Bla8ZJ3YlPDOy0MPc8X0sfWZdNXXMfuvlDaiWDuVOtsiKZdl6IloU54yDZkVRE6ed6NtENX4j4gSUNS1xhGVTqG1eWHhHBe0tRGCTFDNCKXB7M5Lb4njUQcgFjZ0kSe7OOMjZaTFwdG6Uf3IPBUPPXxEDdTjDGcerUOe5+/J34wrExPsdosIn1nnK0gYgyJL7coJ7tkULDxqnH4e0dGDNWFkkGVHHMYLSJpJqy8nhv8ePzJDRQYePUWYZrdzAoBxTOUzcGwxDLgGE5oPImL4KecrxOaIfEuEMIkbIYMB6NOHXqFPv76n9lbKAsC9qgyZ29LymrAWU1IpgW5yti2mMybdjZ3dd32Kjf1mOXttje2WNrZ4fh2GnuQKt97pyhLLR6Q5sZvxBvoBTxQckVSXqWzWQ2yFgsHpKDpL4XixFVy9UoOgZJfXSM1Wt1O1yjDnVYa3BWzYTWAm2LxAACRVlQViVFUSLWY6shtvCk5iybd96LpUbyhqUsSwqj153NaqbTKT5F9Y2JgmkjRgxN3SBti00OcUOMK7DOEdoabw3eec5snGG0d5mdZkabgs61nJanTzMAXYrijprUSMWcJkVZuC6QgqWo1kXfuS4owuTSQgoa9XjSMlRYjGo1dDv/OXPfATvnTfa5yeZXb/He4pzgTQZ23uKd78EfeU4625lg50EWNi/8i2Y9I4tb6GWm9mbFiCzHldwWWVhkehV5vbZeG5gtf2OO+O3617je3SGzNZ2ZaPFyQq9n2xh55NKMP5FwjVdwFOt4FCOm37/9wW3+6G8vUbeanioKhJP4Fy3exiz3y7VA+DwtTQbbB7H2AbaFA35ZtyQZOM2T+6dsfu3SnGSTba7FLhk8LDLVtsvGnv/WjA8dU+5wsSX5rsa26de4g+BurhO0L5bSjIi6ZMWc2N8aQ1V6yrJgvD6kns3Y3tnh4Uce49FLl9ndn0CyXLx4idIJd22WXLjnHjZPbVLbAY87XVF6i/eOYujxgwH4AWI9MIJmisQGK4777j7HME7ZewdcSoHJ/oTdrW0ef88dPOHckNImdmPFw1enNCHpjvJWXsmhXxZfV7eOzA+6EX9Lnf6yNO6WxRCi0IZEG5P+bhPOJuxRwUhHyIlX3q4xdL5rCzvSTtV2/il9uLXrnMT1M+c8SCLmjogx5A2hRQOnE6Tsj4Q6fe/t7jCd7hNGG6wVQ0SiOmznxbjwThcl0chX5x1lWTKo1jTZcP7pIredtzhf5cXHEWNLjB5rvS4qxmNNQJfOjtEzVNWAtm3Y3d1ha6emKAc84QlP4eP/4SewN9mjmRpicLhiwB3nzlCUBSkm9vZ3SKEFiRp96kcYU/SdKillEKHMpXUGVyQu3PMMQmihbRWUOgWjTBIxjgnNEGMsw+GYqqooqwGDjTOEsEHTPsbW1W3O3FExGo14whOewKVLjzCb7TNr9hiPxuzOaiQJ6+sbrK9tUo02cW3k1JnzNG3L1s4OddNSFpo89uqVq2zvTdmfNexNZ0BJKBzOGVJoWBsNwY7AQNM0REHrEd6kHCQyOmVkrcMYj4kexJIk9OWxjMk00sK22hlL6Qy6v4hIynST6xRhTomSAygKl5BZg8QWi46vqhozGp+mqNYZDsdIKHi0dfy//v5ziNuPcfnBdzK5usN6NaD0JVf395ju7TLZ38OTGKxv5PD1QD2ZUceGBIzXT2OGJeJLmpAwdUMxGDCoRpw/dw+ndx5lt55QB/WliyI5fjUr3dxBWlpMq6akHHRixPTRqyJzH9nOb6arbSySkzqLzcETpl8EVVk5OlO3bnySWgOs+ie6PA8hl/ZyXa4rPd45iy8Mhbd4p0CtMJmR874HdtYJONMzebaPUs75ueiCqswS0Nf3rmOl8wG+eel2mQcHY7dJWHA6P8BsHb8Dzw01Zn59ut8PsladDtXPFq8oC/+fj+7MGiwcJYtnGHPDeZsP+SHph3rFfhEyvd6uvCe2hp2J8LARmgRO0Ez5S111FFI6wBblY2Ib+PN3XuL3/vL9WFcCMBqU3HNuA2cP594/xDot/P8kuPZAr+UWLr6b+RXNkc9zc7KcvyweAHaB2GrKix5c5SpAIvNle77ZMrmmaxfkJJkhLzTRfZpqPFrn+2pt/2/nD2voojR9JnHm6Uh0jVWGylkoipKqcKyNS86c3cAX8I53Pspf/vXf8rdvfy8Rgys8mxtrPPr+R3HtPnePEu4J53jcvU9gdP5e2v2rSLOPNYlyUGLKAWJLkrH4qqQVTZNiBaSdYeKUKu3Slob3XrzMg5d3uPfxp3naPSOe/rhT7IU1/ufvv4PLbaO67RbTHyl5dWDDuMBgGrOYNkp6QAxHjEmRfqYelafvoDQh0cRIGxJ1m3Am4HOVnZPIDVAqcxZk+Xe7NIG1oTYDN8kBCcr4GbooN12IUux2HJa18QBLLp+SIuNhibMa2RlTIsWGK1cuUhSGtBMIqaUalhTeEdqWyXRCnRKDwZDz588xqDa5ePEKOzt7vO/Bh/CFxdjEcFQyGAyzD5Bhf3+XGBs6E08SLd0Sgi7EnePfzs4uF/2jDAZDxAz55Oe+gDsu3MtgdBdlFbCnS4wtEFvmUD3FD6eKTbqoPtvtBnPCxs4RV+jAss10f8JVAVepAyyikbqSIn5YQzkm2JJJaFmzBcmWtDhia5glT50cD73rHWAKzp49z33338d4rWRr+xKXLkfKsmQ0GlO5gqocgxS0wTCdRM6cuUvzE5YFO7uaOy+FlrqZMmv2SCIMxwXSthgH3heId7jCKWguHFuxVj+BeGuUR78e9n9nRWQtRu2oGONwhN7puN9kiOCtxTvBu0jptepJJJFE/UMXb2BFsJLzpbdTTGwpnWNWt1x979sJ73+Mc/c9mVDdiXOWUxvnubp/mf0r+1y+uEXlZlwWgxXYTYlzd53nnnvvxrSRwcaY973v/bz33e+jFrh0ZUIb4b47Hk+5fhapNmmpGHn19cNY1tbPsVGMWXMFW2ZG2wEI0WCJkPNGOdstDqiDtdAnOOWAIlnMYzfP0bQY/kAelxYRTYIqBOZphLTot8lz1hmLtaK5sjKTZ63BF1AUXtk6bykKBXXOqhm2tA5nHFiP915z3LmEKUxm6twCm7Dw/udTp6cVxMyTWR+dS+1DLKYDcAcV8qLp9Ci9OgcUhz/Xf83S9901F+UEC9ux/m9HiwBNq3Ow8JbKWkJRYIDdOvCP/tX/4hVf/tG84GPvPPBsh2585NV3pw2f8c3/m609LYHofAHWcNcdY579pDFFTn5wG3DVgaeS5XlwqJ3HAdObH3NRwMQAsSWmQGgbNcG2DW1oiaGB0CA20tVTFyDE1JtUI+CMAaeboOx4p1WCMkte+ezvalU5aDCFZGCnlgyTd0chgRddC6NRgkUk4SMMrbqJBOvZ3BzwuLvPMRyWxFTzy7/yZh5+9AqXr+7StCASaZvAdK/m7Kkhe7sTHnzXg3zyZ/5jyvEIYsPIJ5LbwBiLNwZTlOAKEIO4Ai8OaNjffpid9z/ExYcf5rGrO9RXd7i0F5ng2bq6S6gKysJTlVM+6xOeyN8+MuV3/ur9WmpRwHYmjmvI4ZyOmjOw87TrAW7+W5Ms5It20cRz1H+kiEj2kz76vovsqAmGpo3MXKBoW5z3iJje3/h6cnLGTpZ1QG+OMYvh5fPdpojMwREdY6D5wDrnbcglutbXOX9uk9hOmOxvUTct62tjBtWAlIS9/QmhjcxmU7a2rpATkeGLrByNDqSUBOssg8GQqqwYVAMmfsaVK1cZjSvKylENTDYJuewrZNXXLrYgtk8K2LaBpml0IYnClStXqQr15Rqvjzh1+izj8SZGSpyrNB+acYi3BM2nkn0ZOk3U/V+fe04smZ42FnI4t7hsRs7JlV2JpECUhBsOKMYlpko8/MhDVAMhuQ2G5Zi93Utc3d9mZ7LDYxcf4+zZC4xHGwwHayymjjh9+jTjakhpvOYqiwnTarCGA6zzFGVFiAmH+ni0oaYoHMZDm8BHw2gwYDQaMqmniElYZ6iqkqLw1EHrzN68dNnVBUjzTGa9KUGzJjlx2f84YCRHDGRzrDWxj6rEJIwVZYLyzpUu6pKQAZ6B2CDNDBsDRWGp28jwzAXG5+/DIKTUImJomkgRZ4g0FIVh1nqSRApnWD93AZlOMSaxeeYO/NqYU7UwaQQ/Pk1Re4iiu97RBqEY0mTWVrKTtB9ucsfGOXYnezy4s9P7BybRH+WTte5wlweuM53Q9RNdFF92c+i6No87m4GtMuYGukCUPDh7FZT73CpRnv0qLd7Y3t+u27BZo/50hQfvF02wmRE1VutaGoc1LgN1Ufc2k9lTY3IOSWVOTH4GIzIHdl3bRJm+TNndwnijCzNcVrbXOeXkARtzkDM/5yBYOPYuS79L/1FmZ/vPD7T2JODnBs2JMcKlK+rg7YxhWBU5F1si1IH3X5ownUV1NenrJx1sy9HP+lfv3uY3/vRR3n+1weCULS81yEgk4W0GX9drcvdMB467lulr8V/duMzZtI79pBt7C89zqwE7cxck6c2cIQRiaPs1Uzditgd2S+ZRQPqyXyAp6Taiq0KR/50nB1/4oQtWmkufGD4fLykR0X7zs12chbW1NTY2N3BO0zEFhlzanrA3bTDWU1Um55eNWr4wokEN4zWsLdW9IwXwFdaNdN00gnG5lnxeczWAZArNlN3tq0zqmnLjLHXYYTYLTJPQ1IkpGpRZhcBgbcCFseGp95zhre+/msHUMkFwVLDVoXHRm+b1//3cOnCdnkjoPjTX2B4c+GIx3clhf95cpzsG2hRpU8KJaL7CE8gNALvFG8/z1fWDjmxoEOkrSrRtoGlbDGTWLWlh2yTEGMBAWXpOb25y4dx56tkOhhbDlM21ddbW1rTDUmIvTthvZmxvJ4rCUVSOFIfoZkV9dXAJ7zxFUTKohoxGNZPJjEff/wjWrefUIlqtQr2y1XQUU0SiQHKEEGnbSNNqXU2SkGzi6tWrbG6MGa9tMlrbYLy2SVmMkGCxVUlemdBBkIuBOwUOnf8EziuV20UiQm8G7IaOiEGiRgoiiRjUITYmtbsXVcVoY0S5VvDY1Ue5444xfuQoTMHFnW2ubO9wZecKly5f4urWFutrpxiP1/uSNM46zt5xDtmIODTFS103ED2zekZhNCVMTJG60TJpKWqQha9KEENoI85ZhoMBG+vrYIVpPQOEovQUhcfWCjpuXhzzvZGga/9cKXW+dlrjOmFsZkZynWEwGKMl45RA7XK9qW9I6K+hDrd6PwOxRtoGkwJFaWmicGbjDHc//iMwVkhJfe9CGwkyw9jIcFjQmCEiNbY0nLpwJ1vvfAdtEoZr65j1TTaCISTDYPMsI8aEEPGFp6hGquDaVpdsCQgGO1zjzOY5dve2MfJuNUXSFe/WCi82A7t5dKphbuJD2aI8Jo3p0sV0frIKmnSXv5j8V9P9GMm55Lqr5fNcl0RcDN46jGOhzrPgrFA4Q+kN3hsKb7IvnS4yheSoWeP7RM36DnOFEDOPuO0Wq45NMGhi6bTArhiEmBd7c4uO7JJ3r1qCqevPY+QkCnbpmMXfjwIXecweeZ0FpvLQdees37Wzs926GGOISXhsAdiNqhLQPIl1o8E523uBq3stp9dd1/yF9FsHgVleMhP8xbu2+Yk3vJuYDEU221eFZ9Y02YoilAV5k3PNhh4PWI/4rksZos1ZXueORZHHLOA3Kl0gYcopLbq6sKlPc5LXEmN0zTrQLum/M9g0X/g7YNeBO5NNr3IA3Fm0Jo/kR0+iKZNMDiRK2ewYRSimuxQW3NqYzfU16tDSREMqNCBQGXiLtb7DwZgYNOF/UTG84yySLCkmbIqkYoArhmrp6sFzvl/TEpoZbTMhNjV7+7tM20C5eY7k3kcba5om0NSCIRJsJLT/f+b+JFaybEvPxL7dnM7MbutduEf/+nwv+8zKpMgssSkWySKpkkAQKmmugQAJUAcIgiaaaSQNpYkKEKCBgKqSBpIAqUqQqphskkwmk5kvk69/L1686NzDm9tYd5rdabD2OWb3unvEjSYJbcQNv9fs2LFz9t5n73/9a61/Jay+4NAu+NaDI959tqFzoutqJnbtkxm15/9OYySKjIMMxg5Yq7GMIrtxesH5dqbcy5jh5y6GEAXYSYnTICFsn/5J4DMAO4fPFkFiGAbqssQam0uYQFAKn3Zln1JMRNfReT9lycXg0NqC0Qxhy6JqOJnVvHXvLncO7+APTjg4uM2Tjx9y984tDg8XKCKrywu2JHRKFBrq2tLMGwprqaoarSz6dk0X1uIC7hJvfOVtFrNLimLGxx9/JJtyCviguVguKWtLWVl632fGR9w+PgZcGOhdjwDXSPKR7WbL8fFrfP1b/w6/+Xt/FwbwQ8QNHZUBVZQoY0hAocv84O8WMXkv5k1VBtaO+hA7zkRaJqsUClsYoh/A9aShp2kKvvnNX+POnVv89Ps/p5rdxpRzfFI8/Og93vvoAx4+fsR6teZn773Dpu9xMVKYRGkLXrl9j7KsaZoFxlSsLlZ88PB9nA8iA2M8T88e8dGjd3n87ANR+k6w6XpS79FjBm2pUU2BntVUKfL04hLnB2xtaJqSzkfU5vNXnsi9lkmJMXg3S+loESc2Y4BssKAiAT8xGAapr5fzZUcyRmJRCqlCIm4yAYWSdZRIwwaLp7Sa0li8T3Srj3n2wZ9xevcExRxT1Nx9cJf+6RNcW0N1zLd+9XdQ3Tn056QUWB4d44aB88dPeOXuG6gnF7inTzn6ld+lOrhk6DakFEWw+HjO3ftvwXpD12/oth3NfMHBwRFHh8ccVTXLJG5kFASxoTOr4PFJoaPEyakMAkajSxaXXERaX2PatYIoPaStxWiFUiFXuhDX/yhQKouYySBZkaLBZkkDnTdaowXYWUsOitbYIutAGmHJCwzGlMLWgQgvazGelNoxCxpJykDtQc689o8Z/+Myp/Ozsi+Q+7lbzLpYX+RcEyj8pAU87YGJvdeec83ufn2O+PrU7/jyW4iJZ2tH0gVVXTGvLQfNIXUpMcrJ9Pxv/i/v8P/5k3P+j//zX9td5qf0Rb9JnJ31vP94zaK01KWlKTRHpcXHGm0qlltFU38JN/Ep7uf9Edj58K4bDV9mv+fEieClWkwQDU2pHJO/a3xer4FaeVuhjMqmH5Owvp7YOmHJo5ZqEiMbNyY+aSXG0nTONHrUFN4nrE5YpShTjy41baf5k+/9kNe+8yvMj0/RUfO3/v2/xvf+7Hv82Z98j3o2J0SPzt465xKHiyN++5d/mcv1GXZe0Bw0BDdgCmEIY1KkbiUlyVxg2DyCYElD5PFZxzYs6IJntdzw+le+zjr+gosPPmLbb0mUKIQlXDlFXcGihL/+tVO+//EF7z5bwRidrPjMLPWu5YTKMLKoahqDF4kN8wmv36R5Hxi0YvCeIgSKIF67m7SbZ8WGHKqbEHYsKXL5ugmdj6h/miMp4YaeaDQUBpJDa6ntVlWKw8Oaw8MZi1nFyckxPgXstmC72VBWNVobXD/QVA3mluXenYrl6gK0SDaQXblGl4SkKVQpml8otLYsFgtuuRPm8xlKe1KKOOdIaLRLkqFXSCWKFMQa1VpjjMVoS9v2VLZkMZ/zra//Et/61q/yxutfJzmDGwaJaSJdsUZHu3naXOUfRqz+SZbevlWu9hd7NerbDGgV0QZqe8zf+Pf+Gzx5+C5d1wGR5eoZ6/UZbbfCx8jjp4/pe4fVhrfffI3SKoL3OB0wLmAJJGMoKouPA8uLp1yunrBtL+m6NQlH2wvNPTgpNRODx/uELjVd37PZtmy3LSGK1IjOsTf1kGiqz88gjCzNc5lZ+b1R7yxpLW5ZZbJBoSQBhyiVKbIbMoYkmk85wJ8g7B25aoMmYnNgsNUGkx8gyba2VLZktrjHoA0hDLj+DIatZG5jefjeOxzMCkyM/PhP/iVvfufXqY9L2icf8Wd/+E+wWjG/dxtvG6K+ABVIdkbfdQxnHzFsn7K5bKkOTqgOjlFJY5ShtJZFVdH6YZwNpCwBAomkRIB4TKYo2Ln+xxko5cbSBMLEMy2WfFKaGHZuemM0OubwCqVyJQoBiWL1xjyHCzQ796wwAzmw2pIFRIWtM8ZM7tfCWLSSKgVWK1SUbFfR47KZ+RsTJnbjvWMZRysZclAvI8GmvmBM59hjU0fv/fmiY166Xqf8v3EOj3+jri7yVx6PT2D09uKrJqX9tGMxX/iZv6BmjeaNOzUKTz8MDCFycniELSuqquTRT97D+8B6W+Gdx1p75Sqvt5QgBfhf/6c/4F/+8AmFNcwry92TYxazGcFLBZhhGHI5wS82xjvm5NOPAXnO0vW5sDeeX+haRqM1SmIEWbw/BvnJYbQiP56BXTIGnRnzsSWlSMoQde7Mkc1LeddRwrSbcZUY1R+MljAGpVG5RlmKUTThlBLWPuhcRsuw9QpdHmCbigfz2zSzI1SSRMCT23c4PD6hqEu6viOGiNGaqrQcHh1zcHxMtZgxK2tIiW7oKZsjQtJE71F+BTExtC3deoUyicF5utbRblYQI0kbOlNxcGhQhWHVO7phyPu1oXeRsB1wQ2QoPa/ef5NqfotXbjX8yU+f5L1EdoaQ95OpHu4ntqxGEEVpICkDOk7MncyTq3MhjnsWEr+9v++POGlc057/NvnxSb5mCJFyzI69YXjTzZMnws7C1GqsLckUUDhe6PQQ5EVXsj0VVguLZ40AKl0ZmsZSNwZTQFlZdNKUvqTMgsExRvp+oCjKiWUafIfzAzEEcWfZmDeusTYspGSwxtDUNYvFgtm8oR82xORzMkckWJGAMIURYKqkpIqEF+RacRmVlUXN1772TR7cf52jwxOSj3jnrtYt3Ru1ka4dX0y7t665JeT8V1aZfd/+5FZkisHoukBZKqxZ8PZb32LYrFguH7PZXODcFu87gu9JqmSz3UCCi8szvL+LVZYYAv1WgveL0svF6EjE0/Vrzs6fEkJLio7Catwg41hYiX+IEbxzFIsDQDEMjj4vulrLxmusoSwKquKLAbuR/d2xdmT3oJr6V+IYDVHp0V6dnBRqfCBhkrFQ0/hmxicjaPm2iFGJwhoMCZOVzK0xlLbAlg3O90TfEYY1oR8krtOWRDfgh0iMjmHwlNWMej5jWF9w+dFPWRwccnT7FqpsJDnFaCgbhm5LdFvc1tFtBop6hjUFRNCmoCxqDuqGZRcgRhxMwc4qz5HR9T+G06s8j9KuMye2clxIpv7Nm4aAtBFQaWLKlXWnAJVcApDRjaPRaVeGTWLowJiU4+sQFiGz9WZKtthV+Bgz8dKE2sbXd+yqjPIIpHYQYYQ1KWlZONk9gl9kzk0tW2eKq3FZicycjEHOn2aNPwcCPs8zMY7ldfB3Y8fOl9JSShituHVkMCrivKcdPLeNpbAiPTRWTbjc9Pz5z1d8680jmmqMl32+rdvAOx9s+S+/+5iPnqwxSnG8mHG8mDOrZ6y3rcSaBWHgdwBX2gvv/TO42643NboDpxk2bnsvis/7YuBOjafYW+MncBdHQzY/02I9CQjLz/Zo/I5nmwLzr8QHyt9jjLImyyVlQy5bUjl+VWLmYl7HJVM+EpUoWURdURQzbD1ncWCnDH0UNM0cWxaEPP6kLE1VSP1aW1hQSYzJKHOnMhUhKlIYUP2S4GBot3TrS0yzoOs9bdvTdb2wkFpj6wY1tKA1Lu+Jzke8grb30HuM1VTVwGvac++ooqotP/j5E4aQdpL5aY8xf1m7RlKlTBrtpy2Nz+X1U1356Au+b4cRxr3t+XkZ01hSLJd8izcPI7555YlceFfpUSIhkXLQ80j9KkCn/O0pCrukNU1dcXQwx7uIsQZTWKDIoC7iU4sPOR07eazRBO9pvePicklpS4qypKpFvmO9CQz9QNt2dEOgKGqOjm9R2JKxkkAzK2WrTp7j4yOePuuEZcsGr9GJYMAQMxMhqvsq7/UqQWkrklcQDH/5L/915rNTrFIk39G3W0BqwSotWcCTntqEzvc2hGkQr/Xr3u/7rrN9YIwSZe0ArJYrbp2cUlQNKSm+9tVv8f4Hij/8o3+DMQljIyiHtQ2u94TgiMmz3a6gKdEkPnz0BGNLqmrGrdM7hOjohw2D6+i6NVpHqspy++SEpd0wuEjTHLBuB9ptx9BteeWVB6Qo8XneO4FPStzdWkngc1OWN51ezzWJO8xSG1F09MbEGyV6OIKnowh0ij1q80SVMUgjLlS7vh27Xydh+8bFMCILmCJR1RU2WNQw4AKU2lKXBVE7kluT2hW+X7Nc91RacevokMPTB2we/4J+veR3//rfwitR57371jdE9kQZTDLMDk8Y+nMxIg5OWa1XuGHAGphXBXVVYIoCN3TYesHi6A73T+6yftazdH0GE+K+0CqBDyhKcV2mYoK1u9R6YTGVGrNZZclRjIkY5BicEbAllDGYENEY0a4ggywtwFmMH51Lf5mpdJg1Sf4t835hFbrQ2a2qMSMQz9duUGByESSlJUxV5+zWPQA6bh5X3QEZ8imDmZbrL1iSYHLR7TZUrrlcdkv2DQDaWJMYSd65+pmrm/Jza8VoIE7vZRBxRSF3T1D5pnjxU9yQn9aMTtw6cdQFBB95ttzw4HZAW53ngiGEwLuPtvxP/+Of8n/4H3+Hrz6Ycd2VOd7bjz7Y8D/73/+YDx+v8TkT/WtvvIbVlhAVVTMj5P1kXpjMst/wfq92683b1Ef7MW353+fO9yUypYmplOV18fxRhDgZTS52Lh9Ju2vKuEOmyT5YTKNBm7BaGDmtdwaa3PLumRs/E4kCvJInqgjNHShLkjFYBaNag9aawlT0g+Ps4imnx3cRPVpomoYYPV23Yb0859Zhgx4suqzBznCbLa5b41dntBdbCB6lI0MsabdbtpsNF1uHCQN1YXhw7zYfvPsLIFEUFqUUfe8YHFyut2y3W4pCc3A44/Txx9y9f5/T4wNOFjVnm4Hgc3/chKlTV5m3/b4c4xHHLn9+WuzNlZfgR8Xenp+mKZCBucqJjQEXwEdh8G6k5chnAHbHi8NcQkpxdnGOT1lvJ4if3iBp1ClGyFlSCrh9esq9u7d547X7tOtneQMFFyPr1Yp+6Fmuljz8+COUMvgQabcbVt7hhoHlcsnx4SFzpakqYWBCjDgXsFbRdwPOw3whwrvWFhRlxXJ5TkiJbdtKML9RV0ZAIdIUKkSausBqQ6UjUEjmTlI8/viSX/nlX+e3f+N3qKsDjK0kDij0bC7PsNWMo2Y+MZjTgE5DNLId4yu7pZqrR00xZALqxuzOXTZOUVia2YyHHz2kKSpKQCmPLWsOj0554423MY+EPWtbx/laxJ6HYcuTx+/z2v0TfLC07ZZn5+f0fY53VIHjoyOMhcFviTis0ZSFpT4+yokXiZQMKS4xaGZlze1bd+m6jpiW+HEhCoHNdkNIjqFPBPf5g9m1yfZB3sBGdjilJKn9Rlx70UvgmBo1mbRGa6mjmqV7GevridtDrkll4DfJ76gEyVMET20MBvDGEIZWkjOsYlg/oV9f4rsW7TynB7dQoWVoV5ydPcOt1/huwF1sefDgPlVVMPRbDk9v0a22rC5WvNafE/tLUneJKkrmixkxNViVWF98jO+WxPYclCIMPQWGr73yJj95+pGECmTGWym5D3nOfV7QRYeRmMEEmTVTYvEZqxiZ/JEBHftC+k7+NloSTJQ2V8MDjMTqjNZ+irmmq87CwzqztiphC5sTK8R9J/BO75hBNbp4R4kTjTJjwkw2rjSMDKwmC0qnUWtrfJJMfsy+GHuya3ugbZKau26Of4bNPOWTqBct//sgbWe97w3M8987lhUZPQJXwO5fcNv7mmbWUFYd3dDz9Pyce7dOOT06oS5Klt2WLgY8BdErfvzzM/7Rn77Lex9t+Mqrt/nmW7f4vV85Bm3wwXPZbXHeMa8rbh8dEr2nOjggJM0P3/uZlDAsYH6w08a7EWh70ftqr88/U7e97OAv1vcqr0FjLDDjv0mSEEOKwqjl5IeoQRU5czRJ4mJICRcDpZKs+rEuZ0p7xlthcyLEmCiVPRpJQqOmyjHZqBiTOrSGpLyw8kRUAK2tJE4hYRiWhNaWWVVxtJizdY7alFSlJDIeHtYsmgrlI63rsKGiiAE3dPhuhd+s6baJ9eoSYkAbzeXTDdt2oHMeF0XSxeqEMo6qspweL3j9lVNODhes+8TWD6zagXZIGB/xqeP8yQV1XXFq4W/++uu89+HHfHje8oOngBqQjHtD5jGvjaW4olWWn4K9AplTLsHeHj8954gxuAfAzChDk7cZzfgRedZHw1UpRSBl52giZ5URvWcIAesDRY5b/LR2Y2C3mM0pqwqUYrleEZybNtoYY/bhK+LINiFMwMFixq2TYx7ce4VuXuFTwKfIum9ZLddstx2XqzUupCxWqhmcp29b+q5nu93S1A1F6aZi8ygpZaONFR0gRE5FqTL7zBMXF2eElOjdQEq7YNIYJEhb5x+joLSGqiipCoWnQOtEYSoW9Snf+Oo3+dpXvoHWJQoRwI2up2u3NKaQslpqzIxJEOOudBWKMX17f9NJexLu8ltesFNCpQjJk/ZAHWSXVlHggqfvt/QWmkbKpDXNggcP3qQfVlwuNzyr15ytLrJLK9J3G5arc8qyoB8GNu2avnNoZVivL9EGYet6KYVmtCEFi61qrDEkFCEoztVK2BprKcta5FCMOCxTXiT6YRDXrr+aRv9Zm1JSniqNAkFTVyjIMVr72V3kfwWgjA/pKCVwdYOUSiNmz8Un/IwmUSpFdA6VEtoYUAljC8qqwRJJrgPfURUFOnoMClOWRFvQhkg/OGZVjSpqkrFE7VGmIsYNfbel3a4JLhCVoSprily2ixyU6/sW114SixqFaPFVuuT20S3iVnE5rEGPtXMRpiSIG5msd6U0eWGRBSWpqyzTdbywyzQW3WajVXaz7tycEqijKHI9XaUUKSBgTSthi7NWljIjC5hFi7PAsEI0pcbfhY3bZe9ppaZnQ+5PxlHuZJftOwJC2YR2z9qneVY+tb0MsF0DWp8lIHpk+dTuUX7hES//e/f5tPf+lwFhP2vbEZo5czGIeoAPgd4NmN5M67M2hpjgn/xgTdeu+KPvn/P4ac/DS8N7Z5GPzrfMF0e8+8Rhi4pRSFtrze3TOyhtWG17toMTjxC5Osn+3U8s7s3a1SPVS5m/KW7qRW9+XhbwU9oI8Kb9YPyucW27Ildy9dZjimIEm5EPyMkYe2E9k0cth2Bcnz8TXlZqx1KlseJUDsPIzOlIOoxhGKMn7+69u/zyL3+H7/7gXaytKMuKWVNz53TB0aImRke/XtHMjwh2QXIDw3bNsLmkXa9ot2sx2pRlsx1YO0UfFEVhsVpTaFnvDuY1i5kk7pRFgfHiTux9oHWSWBaCY9Nu6LqO4AMP7p6SwhZtFcuh42yjGKLGZ+3YqcvHPkbInyvyMLKYSXxizBIt4+DFzHqiJpB41Wwbo+93MflXYUEG1Gr/z+wdTaJdKOzdlxxjdzhfUNY1kURZSBmZURsrRkH91mh8HB8KRVkUHC8OuH1ywit37+EO5vgYGYLn8fkz3nXvs163WL3i7NkFZVnQNBVVXdO2W7q2o+v6rCnnaLuWru/R2jKfN8SkCDkGyHmHUjPp4+B5fP7xZPH44HJmrhHmwhip5qAtpYW6KGjqCm0rXLLUlUYfFty98zrf+uav8sZrbxNjRvYx4bqWtt1S1POpZJoi5ioSUjYq72gSGKv0NIi7ovV5aFWWKp0iZSMkJ+dCLLKI0PC2LEFB161p1cBifosQErPmgDff+CbddsX5+ZqPm3NIZyIKS8S5Lefnj6nrGrRms13hnVQCuVxd0LsW5wc22xX90KGiZdBagEdRoLNbROoWhizuLFp7oreX3bAx0g+BiJesVfP5XWM6gzpZaJjmlLBF2YpSY6aY/D5WPlBasXN9XYtfyIvePtMxATuVqJTBdWuCEsCstaYoa5rmkMoUaN8TQ8vB0ZzucovViYPZHF8fsnys6Z3n6PiQZBsGbYgleEqGEOn7LcvlGu3Bmhl2fortW5ILOO8lQabboFYadXBCqQq0NvQh8ta9VzGXhvbJBqdCVutRAsKzPInWEXJJvbH0jIj3CvAbY0alluKujXIlSoOxSkp+JclCN3sLjTJGQF/ud1nYBGAbG7FjhmyWQJFKFdmhpVQePySeZ9ygrmXCjsXGx7GaEFF2jU2s97SICrhTaped+8Xa3lxRefPfY9Q+9zekxGSaX/+evfY8YNwdt584kvL1TWz/dMzLPp3f+wJu2JF1TIms9SlJDVpB23Ws245t12GLkqouiCHwf/r9p/TthuV5R4qKn5+v+IOfbEi/H3nw+huUZc2sOUCh8T7ifOTNV9/g8bNnPL1c4V9ijFy9pnynLyJE0/Tu1XsfGc9PZOKuMTgq97WCqy7xL9YUTBUgduBuByyuvDcasnn9Gg3qEJE44wzG4rg/T0bJ3vlyub6x666EO4AA98zYxZByiISIHu/UCcbndxQ5Vrz51pucHM746S8eYU1DVc04Opnz2r0jbh2VhDgwXJ7Tn75BaQ9Rmy398pxuec5qdclmvcQHcMnih551bBhUze3KUNpEaUCXhpOjOYezksYqjLUC+JWid5FN50jR0+vAql2xbXu8S5zeXoA6pa4Mygf+zfuec6cZMJjopnG8nqQ3Gb1a9rNkZL3V1/aVlNcuAcDq2lzLc0lJ+U1GnuJKp2fXbu7TESZEJS5YF6S8mPNfdvKEivjQE4K4CuvKElJiCPLo1WXF0WxGCJ52u8FqwxtvvMbt23eo6xl976iLGU1h8Sny6OyClAzewfnFmllt0Qq8kc3KGE1ZlfgYqecVysDl5QXOeZpZTT1bsNls6XsJ3G+aGV3X4bwwicvlJf0w4KPHFCKQWtgC7wOL2YL5rGZWFRB6tClRxtI0DbfMgqo85NbJHb7y9ndomiNSiDgHVjnc0PPoow9oypq6qogporwoh6cQSK7Dq8mMwdazXfkqbffYp8zS5bFNbpDAY+/ot+cobYQpahq8GzC2xBYVb77+Oh//4nt8dHHBya06xwxqoOS1+7/Ex49WlPpDLJrCRnQKuHaFCh3eRYaQGHyPdxEGxaOPH1JVBRDphy39sMX1iqHrAIPWAwlN3weePj1jGDxFUfHeB+/Tths22xXNvCLS45zoWW03A0YnSvv5Fz1lJH4LBcknxD2t0clg0Bjhyog6ybFWo01EaS9u2FE1PFeDiApM1KSkcURQQVi5zCA3StEoIAWqpsmaeZGirDFFAYWCusS3K/z6Ag4PqeZzQoRnQVHPblO99lVm8xnv/eIXvPLVGQcntzk4OuXHP36Hy77Gq1t8Y2bpmTGEhN0s2UaLKmfM5rA+f4rvepRSHN19le5yTb9e0W+23Dk9YeM2vG8MLvV5U88AzSbG2MLkYF9c2LIDAm5kv1TKkjHCUypkcTZatAILY3MfawoVGDPyVE5UGd2oQY0aW0hlFzXuswpjsm7guIqNuIaY98QMOLXK4G7Pkt03fNKOKXp+c08Tkzfa2F+8vYzL+KLnuw6xPompu/a+mjpm72iVC5CPC/0IiF9IQV09zxdsSilms5rZrMbFwPzgkLbveLa85GhxQFFXaGvp2pau6zBKcXR4h8XxMcvVJevVEhcCH7z/HkbnEpCzillhaQrDR4+f8OjsgieXa2ZNxdHRnDu3Tz/nxfIJ2O0l4PomH57G5PNd1pVvU3vGyt7PGF5ird1VObjWUkxZ3ywyaJH9iChCEmH5kV02xkhCBpLlanKJykkSJetBiXxZglylxocoYbZJarpeiVM2RpJqkVj6cnbAQhvuzA1972iM461XDzlqAia0bJaWr/zGd9hsLvn5H/0j+s1T0YJNiu16w+byGUMyDGbGMAxQQtEYZod3qMISi0OpQJEcjQ7MCo02hQBG59n4xMW2R5OYV5Z3P3hG09ScHpe47WvUpub2saEuZ8wWF3x41vL+WcvDlRO5lchUm3c3zDvZGGsl+VD6SQidkR3VV57NcGXVGIkDSST9hKmwF7MXiFluYEzjM/ikuGl0042BXdtvUYOSQFYiZWlJgPGycRwdHnLv1m2hPhcHlGXB/fv3OVgsSBGePHnGvCqxVYEncnF5kSUyZJM4PjqmqUvKwmbXHqgQUCpKPVClRPy2PKZuZhRlTds5+sHj/EDdteIOMtJzITpCcvIdOcBegi2bbAGJnlaIQTouaQnC7AdmteH26W0KrSEEfBpAVSSgH3o+/PBD3vqlX2N+cIAyFqU0JiZCGNgsL9DWYIqCsqqJroNoUcYIrpvEYDOwS5EUA8H1RO/xrmf57ClV3VDW4gpVSKwTMdE0DV23ZXX+jKF3aF0wOrfqcs4rdx7w1Te/xvd/8l1U8mgVKA24YYOPjiGIte1z+am278ja4lLGJscvDmEgBiNik7rA2oo333qbrndcXiw5vzjDh4EQHCbuFgjnEl3nKUsoi8/P2Ckj9L9WSnwMmR3arw0zBvzuXH65a1UiRTWRciklwhhzlsSFhAGDQbTZEmVRUWmNGTqMlcwzUqIsy6kGo+83qCjlyeaLQ9p2S9+1LJcbPvrJz2kaw6y2zOsZMRn6zjP0l1SlfEfX9Zx19/mzP/3HPH34c7726gm3To6Yz2tY1IxxGDEEseysxdQNs3hKn3os0BQFmz5l2j7lRSQHQasoawE5Bu8K1x93GwdccT3sYhPF3SWlwzJzl8GXFBkXsD1a/hOLpVKOa9y3csfx01cZ0z0Xunx2H2+MrOJoDY+fkwOuukBH0BcnK1d/hkLcL2/7GW/Zhar3+/EznWrvcyPr9znOk672w9WrnA76ZBDzZYG6/G/XdngfOJjPWG02uASYhtnM0w3C5CljmM8OpGapNiwOjrLBWuJ7x3J1gXMDzg0YLTFg275nud1iCsts1tA+/piht3Rt/8Kum9iVdPXv5y46XWVDPy07Vj60J/Y9vqrU3hx90TjcvKnkMwzbZ8AknGESo9USrrCbRtPNEJPKkik5DjXEyVGRkiTbhYRIeygY/ab7/TD9rrLBN2Wej+FFO/WLNMZ4poBSxVQnWph4ea6rsuYv/bt/lZ/+6IdsLi6I7Ypt0tAUHMxKNq3nct1zsVzhVhfM5hGlLa33kt2aFD552rYl9h7TOc6N5ujBCbOyJvgN9mjBvTsnbDabHFZDtv4iEYn3HmLABUNTFhzPKtarlXjOkpBQd44X8rdSuBC4bB1dBm25o4G0x0qaibWT/t8xfGiVPWxASux8DvJ/o2welD1Utw8Ec9iV2ntNJ5vDUtSUTJViIvibif7fHNh1W7ELsmvHWpMXfOmJxXzO8fEp2+2GFDx1VXJ6ehurNc4NnK3O6BsBdkElLldLQgxooyis5eTomKauMFbTDz2AaNVltf+EZNPOm0OKsgZliXE5yW047ye3QEz5MykAQR4ArbC5NqUxhiLHTbXeEZJm8AnvBmKssdpwuDiQRzt4VNSosiQS6Yeep0+f8UvNjKaZ7QRcoye4ju3qkqIsKetKQKpHkHoq9jRz5OFUMUAMxOAIriM4h+s7tstLUpD6p7aoMOUss7niBvfO025bXB8oqjKDH6m6cXpyymsPXhPxVxKlBlvZCTgOUU9FomNSOO8ZMyV9EGAXvCcMkeDXKGWxRcnBYc1br79B1w348D5nyzNQEWMgJomNNNGS0oDzUvEhfv41L3taZbPSMTNLSn6uHHclBiKDnbywxZhIRpbeMOKElIgqMHojdV5QC1tSGjOVsiJFog9UdY21BSTETUrEWkNVNawvLxj6ga7z/PTH3+X01l3u3rvPrTfvoU2Bc4G+39I0AtrOzy9596OWP/vhezz54IfMeYMGhwlzdJK5lGSlJniHMhpT1RQpSeas0tS2hC7fi0qkFDKwz7zVSPuPLiM1/su0eYCAlivumdFtmpNPJs07dq5XlRfxkQ0cjShIU6wNSs4xuiXGMRqlG0YQus/uTf/PiQGyvu021X3XyOhO3k2AUfol8UnG8Gduo+119duuQc0Xtz17fw9PfXmM4vNiC+Pvz59fungaiE8//Q1dtSkMqOgoyjnLtiVSENAUZclmu6UfHLaqKIoKWxRopaiqOjOvllA52nbD0Hc452gag4/QusCybamqmqosCVno3g/Dy6/zC7Nm+6fd79OxV3d/jaDuy2gC6vaD9nchEeM0V0rt27LZ9huVKbIKxZ7rdCy7N16leC0YaSOYaIDrAI/dFN17lkVZImHGNSVXA1Iqa+PlPW0EdoUt+KVf/VW67YpHyaG8p3NQVIaisGw2A8t1y+VqQ9q2KGPRtmSICLBDEVKg7weGMIDuaUjw4ARblRjtMJXm1ukh7fZIriVfh8rMZcTjIxhd0FQl86pis96ALaVcpilYzGs8CZ8Sq00v+MFdr+wwgrvsrTDykzAkPRrOSSp+TIR5muSQRn2tXU3ZvfPvu3xHwLxnc+tor4hGgwC7sRLJp7UbA7vlZon3EtRfVTWFlbivkSSxWmLQbFHTh5aIYTY/wA093XrN07OnnJceUxUkBZfrZwyxxVaaWyeH3H3lLk3VYJXGJ89ydcHl+pLWbfHJoYNCm5qj42NS0mxbx9Onz1hvNmijmM0aqsLiw4DrugxCc609nyjKSpiJFDk6OObO7Tucnpzyk5/8kPW2wwepR/j1N97k9PgEozSVKQhRWEprFUPfst1s6LqeopDEiRQjod+wvnjGdnnO+vwpZVUTZjOMBooabQu0DTm7SZaKBKjoid7l7KA1buhxXUeRAt1qxdD2RB85uVuibSV6bcmwmJ3g5ls2G8ehmZFUwg0bKtPT1Jq7t484mi84rBtmdWLWJJ5dXtJ5T0iGwtYYoxhDzfq+l37CiWvZWIpKY3RNDKBMyfHJLe7df03iHX3i6cXHoALGaslELmq8LXBOYbuI947L5ReoPGER8VoSaDCqQJtC2MtxUcpsk0hy5izYFBDmR+4vpLwwhzE9PZJUoki7IGSVEqUpaKoaU0l6fvCOEB23D4+omoqAZ9hcoPEYDX675v0ffhe7eIPbb/27LJ79Me9+eMb755rf+6t/F+c29EOHj4nTV7/Cn/7wXf6Lf/4v6X//n/GVb3yVX/nt3+Q/+A//mzz7xfcYtpds246Dk9tELyzodnmOnZ+gjSGkgK0W1PUB87rBbNgtvinkRXiMk8mAaC8Kd4IBKXE9kWRcTLQS69uYNIE7q1OWKBn7+7oAxGjRph04nL5W70AmCJjb4wImgDddghy7X9NWsdu/r/Em0/1PwDSD2y/Uxhi9PSD5udsEWl8EBV8GF1/0fnrJ69e/b39n/vzZ6Ddt37yjefxw4E/fX3PnlVfYtkvW6zV3b9+TuGetsNZSVBVFWaIU9IMjJDBFiZ1+CrSGvt9iywZTzfjwfMmi6bHWcv/V11HrDziZvWBD+xIB3fMnnqjavXaVLf5Svmm0QmGPHdIZ0GlUEvfgdFVpJ70xFghIWVw4BD/JjY3asiN4kL0vTWaB1ErPz6NSWWFjdDsmETuGnMyQiDmGOsbdszH+jG7dcY2IxvDt3/rLvPXNX+XpL75Pu3xEURjm1YJ2veHZwyc8evyU+UyT4pqiqIjFnLPVhqQstta4qDhfrXHec1Rbzp48xHDC7Xt3oO84Ppzj7xwyhMCsLJjV9eRGTkHWm7tHDbN6hk81q4sBj0dbQzOvmM0LZouCu6bGD4cMUdGFlm7bT2M93aMx6MJiYsEoajxWgBiZTBV38YwqmilzX0tZjb0ZZKa5k0IYF3FEWHof2eWnOYPKGCPe+xsKKn8GYDebN4LeY2TbtnIz7Ka+D55N1zF0o15c4HK1IjjJwtx0GyoVMcqBVpSlRuEhKnR2pxhjKYsKkzzl0FEOLQpF13YiVGwdr79mKWyN0lJDTah9A0pJOTDn6IcebY1Iq5AgRsocqOP6ntOTUxaLI0xRU8+P2J4PtL1Da8XtW3c4nB+yWa2p7VwoZyPxR8v1iq5rOb19B10UEuw+bPjgvXc4f/KIzfIC5QYW8xnzgwOM0dQHOcZJK0gio6LID1kMotHmB/zQ4/sO17f02w1PL1YMPlE0c04u1ty6c49bd1/BFCW3bt2nVJahC3C4EwbVpcINHWHo+Tt/8+/w0Xvfp2+fcXxcsNy09MHtQFGU63LB4wapp4cKKJ0w2mBtxaw5pO8dYBi8x/uQpU8EcHnvCElRz0qqspBsWQeVB+96ou9fOp8+rSkt2UWj7IXRopE1ai+pHAAc4mgNkS2inSRCVKL/k3zCRin3ppRU0NBmrCErgp0xJaLzImViNN57uq5DGXERDW5AA5vNlm7d8ouHa15/9Wsc3n2T2d0FX/vFVzjyj0AlbD2nbZdEH2jqmroAa0rK5oS/9Tfv8tbX/yq373yFi4ePWZ6dsbp4xuXlBXcf9Mzmc6pZIy6GkCthVDWkiNElKilKbQlK5BCYkpXGNm5KkRiFKYsgCu8xq9nvs0cjcBoXMRJGSeyMJmKMnuaMhE1cGye1++zutR0A2XlO1fTe+Bm57pGVGz+S2ewrd5WNoSi/S0LGVZe8XPuXwYh9MqD7pPeu7vXq2r8vQgL7xzwPxnaSCnkDn+55n7UbD96B6Jdc+Mvfmz5/w6bgv/v3vs2DB6e893/+Aev1Bf0gyT/r5RkhQVEWnJ6eYqym61rWmyU+ZI22ECcNMmMM3vVoJR6TddhwXlY8vbgkBk9hLP/L/87X+a1vHL3wOtS1y74+Pi+u//nCDmAHnK+yq1KbeK9+xjWy8MuZdTvANLJf174KkmTA5qVuem0Eh1fYxBdc1PiMXH9r/+8R2Gn16cbB6JqcXJTZElNKYQkUOlEendJuznl2ueHx4x/zhz9+Rh8ihVV859UZOlTMmoi1c5LSDD6wXW253G5ZrtYMg+cD24AytF1kXtaUlUHrgqqYAyGHgSiqsqQsS1QKnDSW3/zlX+L09JjeB0xhc21Zx9Y7XGzyGqeZzxQPjg06WS42w9QpuxhERdQ5gdFaoh7BmAD8SELl2DzBRRa1X+M3xr11bkenSuLhyLbmogjZaNglveyt7GMY0Q3azeVO5jNGnbnNtsN5YUdSFifuh571eiUxdsExBMuTsydoPP2wJRkv4sTZpdOUlrawkKSqSWErirLGFjXRtdm3r2mqBm00MaYcnJ+RscquIiMPgQ9Sa28YBrq+p1SVuBXzxud8ZJSya+oZdT3H2oainGPtiqLwNFXJop5TFyXRJ1IIaCtljlQKuL6DGLl77xWMsQTv6bdbfv7Tn/Do4Uesl5fMjebe3dvEGJnPF5RVI2nScfSzx92/o8QJUnpt6LZ0mw2XZ+e89/5DltseTMWdZxe82XY0dc3B6S2a+Zzoe1YXl1KOxUJCytL4PhCHxNuvfYXVs48YujVj8GVSAZSe6r8moghiJmE2JY4tiptSG7S1aPEWs95uuFxekpJkIFtriXGYWNGRci6KAmM9IQRC+vyM3aQtJ1Xgp2oFVzbwK+vvTh5mdNcJ7kliPCQY3XZmZHsy25Sy1RtH5fwU8E7uASVbaIwB5zwxxzmslme4O6eAp6gCB7MHnHNJ128JbitMGnmhjgNHB4e8+ebXefP1e7z+6hvM50c8+8lP6TcbvAvEpFktNzLvtWEWsgWnDRGFMQWFqShtg9FagDh5AVC7RWZcFNLUJ4L9Qoy5JE4kRT3FdbDHql1hwSaLlSt9fn3j3He3juOwA2hM577yqeyimMBcks8ndjpa12bD/of3GL49xkF9WRvs8+3zkDM7bLvP2t38e67cy3M78VU48VxW7AgKrr/+JbY3HhzxG9vAX/uNM/7LP3/CkCS+07kBUxQYYylsSde19H1H30kixagl6VyHD34KmdBkt57WbLotfuiYlYnf/eY9fuMbJ7x1f3atD/bu6CVg7TlQd4P7yvb/aEo895mbj+ant0lEOIOEMYJYnr29GS7+a/KCJiXEEhJHPD77+8A0L26T4tHIBOYj4sQSqklTbTKzlCSNyXquCFfOvfsZq9TsJ2EAJBUnA1BpMGWNLkrWZxf84r3H/Oi9M6LSzJqSN44UhyVUhcXkJMwQE857uq5n2/Zs2oFVf8bZasN62/LKyYJbtw+k7FqU40cWURIcCgoUB5XmlVfuUpQFCUVpLCFGkg8472i7gtIqrIG6NBw1lq4T/dKxpFpSewatltKV2oxxvtKHknMirwk2GefRnmEbNaMbdtzSxvGXH6FeJ+aUdGVK5+1M+ufLrhV79/QEdEHvAk/Olqy3A1orykKjFSzXl2y7LW3bQ7b02+GSZqapast8XnF4sMBqjYowKA0zRwgJqyyz+TH17AhjStrB0fURPyRun9zj5PiEtu14/6MPs/xJwPlAWReYXnTU2s4Rw4a+b9lutyy0zgMvlq6PPZU1HFQFVTmjqg6oqgVVecZi1jGrGl65fcKiaKgpoBBhQK2kbizO4buOQhve+ubXKYxl6FrW58/4F//0n/LO+x9xuVxxb97w69/+Jsl5jg8PqeomU7lShUHtiw8neXistXSbDdvlBeuLM9575x3++Ls/5qNnF7Qu8fr9O/zmb/wah03FfD6jnhXE1PDk4Ye4foHCooFh63GbSGoNt09uo9OCtrV0YaB1BpcsKdfuDMmRvIc05lQlQpSyVQmpIDCkCNYQU+DjJ4+p63cobEnfDyyaOZsU6IcNbhgoyxprpXyMtgOxh8HdzLp4YVNxAnfjxN5hhV1MSf7zyoMwbvQpSf3fqCIQJt1CzA7UKU0WhQzEKKC3a1u8c6I+M8aokei6DmsLFk1Nals++OGfM/ie2b3bmPprPHz2Ho8ePuJvPP0ZZb1AF5ptNxC2PW+/9jrl33wT4xKVmaPdE568913K8oi6njO7dZsnHz7E+0ucG6hPGgor8Sf9es1sVjGrDjia3cZc/gIf4sRQpjSyd6BSvJJEMLpjYkiEkAhBZ5eKAHutdnGze90LjBIMV8HbdWAXcykXPbF5e6B7r8l6JbtM0hpijolMV6VPJG5QTWWJ1N5WOm44AravcQ9f1m77ae2aJX1dGuEKlXIFBrzIhXL92PF+skjz+D81VsEYX9+hWNmfx88+R1995tu7Uctf852vnvC//R/+Nn/vf/H7vO+3DD5kOSSJZXYu8uTxI2IMlGXJMPRYW6BQXF48y5mbgYRCK0tVN1SzBc+efkxpFW+/csj/7n/0qzdwiX8ZcEv6cB9spSnjei/hTclGnv/7Qm2/6kCKIi+kk8Atjax/QlKnvC4r8JFkdM7iVLluNPk6U9aJAkIGcciTba0RUJjGOxHdziJLUkn1pDyXx/ja7AJkH4BkY9Aq0arcJRRkMKOkTjtaKu8oNEVdcdEH/tWPHtIrzbrznK/XXN5T3J0bYl0S3HYy7JSS8KD1tuPjyy0/evoM06/5lbfu8vrdOfPZG7Rdz7ob6Ice701+DhJ1WdOogePSc3zndiZkBmxRURaG3jnUZkvbOmKpaCrFrCiJdaBrPLWJ0zoV98BrSoloJKEhKQGA4g1JBCJm9IiM82dvQxqNmbEylRrLIZEIyTP5H3Jc3g67pR2UzmXdQvySGTtbWoytwESstaw3G4xRVOWMotCUhaEqC47mc7Z9i3MD3XZNWdSYpmBezyjsnNKI2ODRTDMrj0U+xRZU5RxjahKa3kUGF/ERjuaHmKKmSHB4eMjTp08pqwptDP3QEpPDOc9HH72PH7ZCAxcFm81AShBCZLVpOTw8xCxmlPMagBgdLnZ0fUtV18zrQ7761leo9UwekbTPWijcMDAMPUlZqrpi8J6zs2e889Mf8Qf/5Pf59q/9Fr/567/J5snHPHr0iL7bUlQlb5U1R1VNU1i0sRD9ZBGOKFzqAyYuLi74+IMPeffnPyfGyBuvvc53fu3X+f3/73/B9/7Nn5Gi48Gbb6JL8sIeJwZQG41ymtKU1EVN9Ik7t+/RDmt++v73WPcdMQs8qiQaR0oJtRt8xMfM7igtiROx59adBccHR5DgZz/7OR89/IhZ03BycsrJ6Sk+OlbbFZtn5/gEVVUToqHL4tLDcDOV7E9sGVTFPResIkEMpOSAQEgdnoGAxOKhpDxYSgEdZMnzOmKyGZwIRKVEjFcpCp3EHZ48xjm2qxatNAeHC6nLGCElTfKedrPFd45B1ay6Lf4XH+L8H/Abf+Pv8+obf4+Ljz/i3X/9R7z9m/815qf3qA5KPv7+v2A2O+a3v/0G+ugVfvRH/yXv/+i7vPeT97n3RmB+cko9m3F49y5EDzoRnCP0LRpNVVb0PlCUB9y/+3XeufgBoRNWkZStZBQQdrFmMUpBiqhQIWfPRcnAjh5CsKAi6Egg4JPGjFlbOrs71VVtOJXH4srw7DOre0BL5Qk+uQ8ndL7H7kVEC29CjvnAKQN6/9vyEpfIcYJxZ/qS0fpfGGd3w/Ycc/RSDm7HwFwBd+Nx+9Ileu80+2ju+nm/ZGSrXvRd1w5Bam3fn7cslwNPo8aakqapKeuShJtc74nEfHHAyfEJpbWcPXvIMEgd8FcfvMly+QzXd/RPVxRK8z/422/x937nfgYqn3ZvXz6qV9OcusKNXr35L+mrRy26q2xPIBFARUIcxHBCQRifoTGZTLRO5VkVtyGjjurItquR9MvhJ9ewwRQfx6hJq6f4uyvXOYa65GscQY8xhjG+biwGsO9Krpojjg5PuX/nkGWviazZ9h0hKI4Ojrh1fEiPotIwBEe/9VxerrhY9ay2kSoVUC54shz4f/3+H3PvqMKjWA0eG2W9M1pR1nOKdcut4wW/9PW7HN9/k+ha+u0Kkw304COzZsajszUuOjZ9ZFEUmCLRNIp7h4mHSwhR5E32gVoqkpShzOLPKeVkFWSPmFjLEbDv/65Fy1Y+Ix0jbJ7aeU/YLaUhhomwkKIWMoi7qjuf3G4M7LphwEaFCwlUpCg0VWk5OmgoC0tVVlRVTV3NMJeKbQvei2J4Cgk/REJJ9k9LokVVzQCNKStmzQFlOSOiqOuG+cEB2gpqjtl16Lxnvd0IiNGKEAZJAkiKoe0oy3qSbohRQJ1znr4X8OdcIARo+xa2Gj1sBRykOF2P9i+wrBNT/T6UsDshKs7OzvjpT3+CUYp+s+H86RPWZ88I3QZj4Pz8nFcHB0ni1qRdtS6V1qhKMsfG+MXzszM224hLmrOnT1ApcHH2jB/98Pu02y2NqRkDzyFXq0CSRZRWFGWBUoqTkxNW3Sn9zyTbFb3HuOR/fQwS+5LTtSNk+RVPVTbUzRytFAeHB2zXW8koLiyFlfG2tqBzWwY3CLMQrdSOTVKx4fO2GMYgYTIAlgoKarJQZTGMMewsybwHjgRGgokqF1eHnEyeq7EEjIhGBhXwSdysIUr1BBFh1pMkT4yBTdviWkdRz0gG+r7l4ukTlk8/QnlPpTzd6lLcAmXB4AOXTz7GL2R+duslm4uPcf2Wop4RlCXZitnBqbAXYUAlR/CJGCLeOfr1GlNXmLJkUR7SFDO2fUsfBzLSmRaGccJOf49Kl6NMQlLTYgQRlQIqGVQuAbjPBgmDcZX4ea7qwvRrYlS0Hxf+6fu5CltGDCgGzripSe1ZyWqeaIXnXZJXNr/9tg+Svty2/003/oYr13f1DGnvNXX9mLHb9pnKlK4ec2UN2QHqT4W1V2ntTzv6eUA1IY9xDIV1/fu/9yblv37KP/o3F6BgGHoUSLlFhKGtqobf+u3fQqNZXy4BqMoSozTt8jLfTiJFz8nM8uqdhtfuLz79Gj+h7bKxp8u+8fhNwGR0ofGCvkvXJ+hnazHFXBN2NNB3oupyfkmEGMuN7aRH8iGjSLsScfXdXeawld0FA7vcIKNAyrzLMVrtl2oTUXJhq9T0LErpv8w7Jybwtouv233/mEQlWCSgTEEzq7l7ekB/Hlh4qG3JrGw4ODxgfjDDr1eSTGMTWnu59eyJULGXMpJKYVOkbOZEH/Cx5+zZOWVZQ9QUWlFXlqOjQ1599XVMUYECGyOEDmsKbJHQWGbtwLoLbN1ACaQQKFTipDE82YjBm6LKczxnxCaT2TMlABa5Rj31UV53J2A3Mp1pb61DNAMndnRcl3fADvJ4xJ0Eiho9VzecwTcGdpttiy0CITMedW2ZNzUnxwfU1lJWNVXd0DRzQnCQAt6BUYkUEn3n8HWiMPKg+5iwtsDYirpZCLCrahKKTTsjxGOK0nC5vKAIkc55tl3HcrUiBE9CBBSLwqB0om0jJ8cnKGUYnKcbBoIPuCHgfcC5kJXNk1RYCJ3oz+AIAYIPWF0+F+MjYQ1xyghWREIMhKS5uLjg5++8w9Fixubygve3HZvlBQezkropWW/WU7Dj8xZQXoyNRumCspByaCEEtpsN67UjbXt5GqNns16zvDxns1lTzorJglPTky5ZUUpBYUVe4PjkmFV3jA8+l+WReIi092D6EPAhEMZKMQggh0BRVBRlhVGag8UBKcQpG3iUdKmqik2n8N4z2rnee1BKagt/zhZj2t9HGBe8HWskD0wIHmFv8oGTpcTEFMgrkYQhKfZKoGWeK3qCjviowAfBIibrthmb+xpC8Gy2La73nBwcoCqDc57lxRmPfvYjZmUBrsP37WQp+2HD8unHuM0aheJsvWVz/hhFYnZ4RDAF2Ip6ccQQEoQBFTu8WxNCJOJYXlxwePuEoq6pqhnzYs5SLfdkDWQB2PcC7jNdsiHE6W+l0rRZaDxacl/H5Z9RjkQ2kx1weGkpLZUt/RxYc7UChJ4WuYy9p2FKWbcuqRFsq90k3NuGp3u5to9e0SVTSWJI/wLbZwN1n7LjT+NxbftNezBijOVRV46YWJPPcWV7H5kQ8tV/x9dfwpJNr6rdI/cP/+Y3eLrV/OPvXeYMfYdVlsNa4kFRmmZ+yO/9ld/j2dMz3vnpT0gpcTBfoGJg/fQp5vAAkDrFb58abp+WzA6rG7B1V+/+CoS+9tldn45z8Plzvxjvvsxo+CxQ8fkmwC5rVmaAl8bVSTYeqZ2a17BIEk260VtIynFgsk1ITN1VSSG5J1GDUBnsWSXXLlJGOgO7lJWRkoi7axH1Fz1MmZNa7ZJI9qthGGNIegwxkux6PcpVpQjGUNc1d08XPG17SAU2JW4dzDg8OWJ+WLPZXlJWFVVQlL0Xxj4zkpoO9Iy6NNw5nDE7OiVsWxJr3n/0mFdunVKUDaWGpi45Pjrk3iv35XJMgapmpM5jiiqH41gO5i19GBjaSBcHCiI2RY4qi7UeXCJlt4X0VI6/Q9aaOK6WI3M3xi2PgC6O0k0py4vlmZQQ9JbdtqM8m9rXxctjllQUEf1xoQ/sAehPbjcGdmcXF7l2JlSlpi4LDhYzTg4XmAS2LClKS1NZ5k2JZoa1h3gvGnPrdcvJsaZpGuZ1xfL8jBACBVCyyMwLaGMpqwq/TFysNvzopz/FlCVGgUkBbbLeTfDcvnsq4MR7rIFvfes7aG1Zrtb89GfvMPSSxbnZbJkvDkBZinLGx08eYQowhUJhMbGm0hWKkn2eWmKTvCQapDQJKtvSMnjF4uiAN15/nV/84HuoIFUuHj36iIOvv83hyTGvvfE684MFprDiBvN+txrm/St6T/IObTTHxye8/sYbrH/pW3zwT/+Is+UzkoJnj97j9PSEr37t66yWa+bHh5SlzexHgCBVQYZ2i3JREgVyUGkEdFHQd57UezyJqq5lI9Rk0BtzcfgsHZIMRhecXawpyxmltZxfLAnB4YPn8VNPUdU4H2gWc2ZhKzpmsq5grSFGeRg+f9PZks2LtYqoHHeXGB98CLHHeUcMHvE9+jE2XyjzvChajGS/qlHbaReflpLPD5zG2JKiEmHismkom4Wom4cowd/aYMoCUx1werviycMP+fk7P+GorLh99xbNrERZzcXHDxm6jjBsKa0iRU+7XnF8eMyH7/6ID95/j5UrmB0es9m0bC/POX31KxzNG5qi4Nlqzcw7VEysl0sOjhboCKUquXVwn00rtWfXsd/jyXYLixrjNdjjebTG6OwaMyLOqXTC2oi1aaoWQVJX3AkCBr8ANUEGhWSWIGSdhmz+jxbtWJ5MWNRsDCWme7p+PhBwF4Iw1cl8sWu80X3kf/eBwXN9M07ATz2Tesnv19sNgOJL2yed98tpyorHoSo0Q4Q3vvltXrn/Gq+/8oCPNxsSivuvvs6rr72GMYanTz4mpcTrr7/J6dySzhP/6t0lRMfJQcF//L/62xzM7SeDuskA2PsbriCzl47P57/T8czX/v2cLSVUTLlUoydGn5PRfF6TPESf13g9ufCS9xLgbyyQgZ0WUKa1gIE4MWrPx8VOd5NBmbFpIgimGtzA4FR2LwYRf2fXnyNbNwI8baRQbYpxT/5EvqesSqqqolIFv/uqZggik/U3fvstDu88IBmLC5G47KgbqQz14cfnXHaOtiwIJ69zUAS+/uoxv/Ob3yDqAlMbFqd3ePTsgltHR8znlrIIzLzGpki33bJ99iGpOiTamoJCjE6rKWzDop/T+0DXD6wuzmisMJJNpbBGYzQkI/hA6tArPEyxiKO8yQjc1B7gijGizFXGLsaU3beASiQtn8uFSiejecfwici8xO1drYZxk3ZjYDcMPcYatDEUpaEoDNYIY1TaEqUMwUeePnnGer0ixEBVVYRgUCqilRZZjNpiS4j0dJ1Huw60Zbm6ZJE0zUxs+rZtWS6XdH3HsNlQFpajRSMqzErKSB0dHQEJ1w8MXUuZy25JXVNxpdlcy7Wpa2azGWVV04ULXBxQPmAowA1USoSJlYrCXWgpwTJqC4ac/WmwaGvQaE5OT/nGN7/F+z/6AT5IhkxVl7z11pu8+dabvPb66xweHVKV5cTai6tLSVD+uJlpQzObcXrrFip6Ht++zVe/+lUOzy9JKL761bd45ZX7vPXVr1PWFSpf29B2JO9lk46eED1WSdZxt20J2hNTpHM9g/dEIkEltC1ISVzVMQtDhjBeoEZri1aWy4sVMUSs0XTDwNBviV4yk+tmgSlA6URRFtnq0yhlSXR5At+8Yt31pmLC5LVbrNOYWdogLJ0S5jgmYRcTPh8jbgZSmhwb46eTCmJtJYn9GpNGGhKWvCCGkX5XIp6pIgmPjxqSoZgdYYoZJ6++ybyZUZQzNo+fYEpFSI6uT7RDwipDYwsCDev1lrpKHB4rTu4cc3R8wrbtuX/6Gk5bgvNsVlvCh+8Sjw9Jh3OaqkAZQ4oK7TtCt8ZXBYM1zMpDSlOJ9hxq56JB5UVHQdT5X3lfp4CNCZImIZIGYukrCsASsSiMkgUlqURQuwVin63bgb44GSnGGBJjoLV+zg1GHosxXEQYn9FlkxijvSeV/fx8jMzQ1VOlyY2UxmPV8zzWX0T7pG/YeUyvy7VcP8d1YLDPTo6ds2M5rzN2O1/c/jdorkum7JjuPavrpe7VG/TdCz9/9e3F/JDN+TM+7Af8tqPrWowtCd7RdS2b9Zr1eoMpKp6dX7BZK/wW/BBQKeYSYwpjbsCGveSQ6yLDL2ovP/OLPvWiSXiN4fwcLflA9KJlGsKQK/l4EgLwRlCXope1NBcOTSgBBipeecZUlodiGlI1AbXrdybP3ehKHeO+MqDQEnJUlIbCS73zMfPzShmy8bxKXL8JMdjGuDsBeEqUBKLsPcG3HB8dcnx6DNUcXc+wVc2p1RTNhu2mY7Xc8JU371PWM44uNpx1A/cODnjt9oKYBop6RkRTeoWqZihbTNfwyq0TFos5y9ZRbztStCQrwNkWOmdfG2xpaaqCeVVwGQIDIspuVMBqjdWKqCVEZOxjY8wutk6pK1p1Ku4MYfHwXNW5G4FdjFkjMKWpSohKLwZ2o2beDjzKeN6k3XjnDd7JgpXV6aUUVsT7QDWriVmD5uJiyeC6a0J6GeEbQMtmENOAjw5NIiTRnqvcQBkqUooMQ0/btbhhoO06Ylkwb0QgEK2wyrBYHECKdLrFZOsFNFppScpAUuiNMRRlQVEWwp75iI+OGHoKFYl9ZKj6nZxGtkDGQRRLJP+kXGJJG+aLA1558IA333qL7bbF+8idV+5x/9X73H/1Aae3btE0jSRNjBbmJEWl8porg1WUJbPZnHh0zK07d3nrK475swsuLi84Pap58OprvP7mGzTzOcZYUpQi3LIAJAmKj0G2PGWIPhBMwHtP7xw+BgE33ucaghAyGA0h4oNssBIKKP0o9+TQWsa/a1vc4PDOMfiUgXquZWiywnayE5sRv8Cip/NiGhMT6zZmZonLImSolxcUxteFKgcR8Bw3/UheCIExKHocEjUqNadEGB9QJcKUEKdyMQpD2cwo6gOKqkLZElvWzOYLilmNqSqpc1w0aGXRSSq1+qhJ2ciwVcHhrVt4VdDceYuz5ZLNxSXrbYvzayrlqE3g4NYBJGGjkh9w/RbXleiipDANRou7XU2u0j1WJ4FKCpX0iIrRSUnWHTJOuW62VNzNAE6TMkCPGTjLyfaH8Uq2Fy/QZ1LXjtln/vJYjvE/etqZhT3VuxML+Mk44gqeuH5+YKpS8SWTUtcZn+s44uVM0B4ae+4Z2EdCN76QF7/+IqD1adfzSd9xA3B35Z73GN0RhFpjadcr2u0GMDjnmM0XnJ6eTBuhVG6p6fqerkv0ndQ1NVpcU1fQycsvZLoGtf/31YN2lzf+OV3yzSbL7rRqeh7G2bADzp+vpRiIIf9ETwhuIhBiVihQUdgaNfpfZVEjoSTbf1zE8vDt8ZVXBMOvztUd8BWApydXbMqWdErCXFmjZf3NArqjzt64v4+xdDtD7QXgT+U902oKZZkvGo6ODvC6BFti6wZbSmlMm+up37t7i4ihLAuK5SWvHM44mpe44MGKHizaoMuKZj5nvpijzwdeuXeH+WJGH6DtHdCDBR8CVWOwVhN1wlhDUVrqosAohY8x681JtabSgA9ImMheUsTISia9M15TdqWmDI7H+TJOjZF9Y889OxpnKu9R6gXzUbKSkxQ1GA3rG7LPNwd2bqAohKKM3rHxHcF5Ckrefu1r+ASX6w2XyzVKJ8rS0g0On918CcV6c4k1A7EyuNhR1QVVVXN8fEBRWhKRYRCBY+8GvOvouy1+cKjk2Ww01gqQaOqG26e3iSGw0is+Co/ouoHURVbrNX3XTWKOi8UcbURKQUt5UJLLKN4UpBBEBXradCQoVcDJOCAS96WybIQyBVUz5+j0Nn/pL/9lHj9+wrZtufPgAbNZw8FizuJgIQXkr4Hc0fpLE+WAaOTYgrqZ8+1f/hUWtx/w7PySp08fQ2h58OprvP2Vr3L73ivCyK2X+bo9MWqC7zNjZ1HKUJUVq/aCzbal7XuZY7KrE5Mi+oQbEsGLJEyIkgGp8n16vJRqc46UNfy6bovPSSidh0NmKFtT1SVVIQ9yChpbaJyXxJXP27Te6f4JSB+TJXLChEpEJezu+Pqo8aMSUwwDCpRWU2mdnaN9B0hc8DjtMRh88OADZSmiwt470IWk7ivD0eEJ2lievfs9fvLOhwTnmM1q5ncecOf2HRZ1TdXMSdRsVhHlYHH3DRaLiur4iIdPz7n39W/xSjHnFw87Ln/6M1ZnD3GbC2aziqFSdKWlOb5Fu+3xbqDve1arZbaGpaSNokCpghhb0YacMoZzk+KQgnmksh4qanRSew+9ADmrxwzhNLkGdMrZXvuAIJEZ0vzHxLZJG2UPxmN2DN+VU+xa3mjFvtljHia2b7cxjZbrDsrtQOWOqfoSGTulJsPk0wIK9jWr0vR3unrj16mtK20HvHKFTsaNYR9EpBdVlJjAyqdky90EyHwOQ2wXkixgYLvZ4nwPWrPerNHa8LWvfZ1/+A//2xweHomhreGf/uMDkioY+p7NqiUZAQxFNqxvDFpfwNpN8+eFjF76jFhsHxWOPzsj5os07zOYC16UCQZPCD6/HiB4lHOkqHMNVGG+UoSoI14ZqijyG2lk5jLQNYq9RLNcJ3qakmECFtpEqQcOE0jUSUHSFNpQGotPo9YguwqFSoESAxsVp4I3qKvxd1prCmOY1zXHhwvu3nqN0spa2vdanmJtQBUYtaKZzTDVIW1waGtZHMwxTw8pUktQ0Paw7SNt23H5+CnRFLz9ja9x/9YJ773/L/jV3/h1tDU8fPgx296jQke0gW4I1CUURFQNykjt+FldcHx0yJPlkm3XUQG3K4cKiX4YjU4BeMYYCIGUZWD2gR1j/42zJZnpb5mLUX6yZ0KUT5T4mEYsMI3BOFDZxZuQZI4rUiif3G5eeaKqaOqasirphh4XEqq0LA6OuX3vVZQuOe4HlLJcLJ+x3a65vFwx9I6QckCodlyeG+pKYzXcvfuAk5M73Lv7KjGJAKULA2VlJBYoeMIwUFuL0oq22zCbNyzqBScnp1RFQ33QMG+O6LaRhx99zGqzYbXZsF6vpklWFJa+27JaQWUj56snlKWmaSqCzxlJKgADWhfoMbQ8RmJ0JBVJyuSsxUDwCdWU6BKKZsbs6Ii3jw7FirEFVVVI8fjCSjas1kSFjOaoCDm1cWfTKGMwRUm1OOL1t4+486pjvbrEtRfYqhZQYstM82aKNjjiIIxa9IFkDBj5mm27ZbVZEYlgdkKLMYJziX6I9INkCscxSTQlIBADbLabXZZOSqQkpVK8C2z7DttrTC9A1ehSJlOSgsghBLr+hrPwBU3i83bZWiIgPD5EIS8aIp5MFhme3HxIEW2VQR0jxhlBwgiA8t8hRXyMeBVlDHRFWdaUZUXXDRhKTFVhbUW/OaPbbnn84btcXDgevPkVfv23f4smrUl+oNtGXvnqb3H+o/d58ugjPvr4Ed/8q79D3WgIHWcfnzEUhmJWUNaaqp4RFwfERuI5oq3plcZZhcKToog8V2NpJm05WpwwXx1Tb+as2jYLfUiIb1Ti2lfJoKMwhipFVJIYHZMk4FfWedkEbAZ1Vgl7Z7SAO5lnO1gzBXWPfZeyztenWJFj4saLKkMIA860EYyDIoyAunLcziWRx3yiwMfEnS8O7F7EyH0aDriiZScvyGdeAEz2te4mspWrS8K0yY5vvAxHXD//Hot15fe/oDaRX95DkoouIbvntdYUVqNSwcFiwZ27d9luNmituXv3Lv/g7/49fv8P/yXvvfcLYt+TUkRZ0BZiEKPsZqMpjNAngrurb/AcuPvEvtpJz+zxXNNvN2VQXtSC73PFoZZ+u8blet4hBIIbUK7PwM7I05ggOC+GuAqEpEgVuRLLzogab3uUIHlpy4ugznVWFWpK9hv3Tec0YxnC55i4F9gq40v7Uiht1+KDZ5aJDm0MxlrmhwcUxkqt1ejZXl4SMMRyTkRhbUlVQT0LhM4TtSKakrPVmo8/fsR7P38HnaCYz7j75qv8nf/o7/Dg/utsN1uWqw1PHz3C2IAuAmsPQ98Sy4RtKrwSvsVaQ1OVVKbAKYc1jtsHNYHIh+uWMU4RyBqNu3Jh10NN9tfBEQCPa9a+6zrlTEUxxsV9LYTKziC+Dhp3x95sbt0Y2B0fHFDWFboQ1Xvvoaobjo5vUdUHojVXJ+4/8BR1yfnFU9abDUPwYolVNTFpehdI0VFoSFFhdEFZNoRY5iQgLxUfVMJaTV0UFNYSdcIxkFJEG01RiJibNSW6Kjg5us37Hzxms16zWq0Y3JBdi5rCljjfs20jZ2pg22+IqaKwFhXAJCnGnJJH64IxGH1MmIgpoYqc9RKR95RGaYuyBbosKY0wiRhLWRZYm1PRdVYAz+eb7PJ9gzSR3cC5bElZUqsCUwWUhlgpkrGoqkLizncLUQqeaBTEMNHi46To2pa2beX6tZ4YuxTFDetdxPuYM1AVo+xEyllSzg1TbWKjDSQNSQsQDBHnAsPg0CpiVCJai1bFFFMwVv34PG2UexgzV/NenvmMxOhildq74+RnJDqkjRpp42vTQ5J2WWVJEaPGp0AgUlqLSQllxuQNiRFJXly/vm8Z2hWbzZrtBkIqqA/vcWAOiN0WlRSzu2/xwZ//gnUfKI7uomaHqDKiXBI3rZFn4ez8MV1Q6PqAo/kB64s1yhQkLfVhU9oJSGeSl5AURVGLyHYxR6tnMm8QaRYd9WSqJ1R207Nn6Y1uUPkdlSYZAwXoHGM6bV+KF294e6Bk7PG0D0auNXXt3ysnSuO17H8g5+nmU06g/moQkbAG47z4Egm7yfUxXuNzd/JJH335vL/y3kTxcRWMfda2z25d9Zt/vvN91u9m94z54KbKBiE4vvOdX+ErX/06xhjOLy8orKXQmq989Rv883/9r/FDP7mmZP4l0XJMInD7qffyQlZuvLQX9GdG058+V1500i9zgokXzA893vW4/G8MGdh5h/IeHTwpSPZrTCnHOIuRGzDEIKLzKelcOzYPxD7zk8YpIp01GiAT+GNnlEygQiVMFA+BlCZkd9xk8Oxiwfb7ZjxmZOxiZg3KskHpMn+xpqhLCSfyHjcMrC4u0NWMqj5gcXBE35+JR8IYBqXxCVyE9bZltVmz2q5JMUrRggivvv02TTHDR2jmM3xIhCjsm0+KwVucl7Alxpg6I/GcpdVSbAHDoi5YDSJoH8d42bSLgdvvs5e1/c9cAXxT3+z+vhIn+YJj5XeeO9cntRsDu/t37pI0BHKBYTwHB8fcvfcq2tYU5YzSlFRNRbOoqZqSRx9/jNKOsqo5Pb2FtoEUW0JoGTZb2m5gGCIpGoqiJKWE14m269Ba0dQVRwvRUXPJE8MuO0QphXfiSrS65OjgFloLqzYMHd45UBEdwVY6y520rFadyHCkhMFQmSLXxQsZ2An3EZIswsFHXAxURTk9J1pZIlrcc6YEY/A55rCZlTmZQMpQSbqoLHwxxUmAcAIu+f9j3EIyBkb5iQRKG+rZHFWUUC3oQ8wJHeK0CSFgIiikpI+4OjRDDGy2W9abjSy0uZSbVrIAeB8Zsls1junzo0im0mgDfeeIQR5amfSygKQoTJnznn4QPcEYBsqioLQzOU8GTZ+3pb2+UemK9jtBj9pLCWWkxi3EDPJyU+QsCsW0iicgX9tY0UK0kgrR8zORxpaYHATshpaiPJD+cgPJD8ShI/Q9MSbatWeziWyGgjfe+hquX+FjpHrlbS7SP6WfHfK1X/tdWn2GSj2zsuDo5Jj6cMG2d/z0Jz9kaD23jk64/9pt3t3+GFNqTGHxrieVBSQvsZ4hylwMiVIV1OUBs/oIqyLoEpJBJ01SZgJ2IYNSgghQjwBK6ZyvLCt5zmgWOYPRJbvrxF2CxMheXGGo1IhHRoFVSUrZX7CUUldcONPro4sChSR1ZPcGOdMvqekpkUsX4yuRsvgAYzXovbiVL9Y+ASd8+W3cJD4J3L0U0Kjn//63DOrGvlJGns6YwPV9Dj1R9N2Wv/v3/0Nee/0NNpsN73/4IaUtOJzN+cqbb6KSwnVZ0smJq8+SwA8QxcB56QryOW9vtCGee/GF/TUOjPxIF6crw/Wi0920ua7F9Z0IM3ctwfc53i5K/VznwOd9T4lXJbogzJ1K+GSIvhBgpxVhYt+YYsKzRcukY/lcZ1xtU2x8SmD0pHAw6uJeAXaZjUpRJE32T3wdiGhdUJQH+X1JfjOlZb1e4dqO7XrF8uPHLE7vMLttuPvKAy4v11KvG0VIMERFHxLLzVrq0ntP8p6zJ884e3bJg6//BjhH6QLzwwOSUgzOgRtIStP5GW1QLLxDWVkmxaGWqCpFSIbgDLOqZO48RhtifHE40Y4R3uuPvWQKxsz/F/SvkHSR66Dual4CE2iW414+Zi9qNwZ29+6csh06tn1H7UpSFFFirQ3L1Ro2PeiCw4MSW9QsFie8/tpbFGVJVdfMmjmr1TnbzRParaOPHc+ePiIFRVUeMTu6hS0MSiW6rqVpam7fuoUfelaXl8RBynYMXcdmteTcFOikODq4RVPPKYuaIXZgIvW8JOmADyKUqzUYK53oQyD0PeSYJNNYimlORpGAkOKhUt0gOFKI6NSgM3hSWpgrlELZiqQ0PovlmmHAGnGramWQMkl54WNkQ3Lx34l9krgplEFpizWWEES2pKwbgodmfsT8+A69rtApVxzAE+MAwWCVIVmFKQqUtajesGm3LJcrnHPYymC0xipNiOBDxPkgNUSzsm/Klpls8lKmJUaZWMYUFLoQANF7BufwQ8LbgCpgyBpHVaGoygrnPK3+/LpiOypby/WpXX1CpUQYVekAKaBNwqREciOGk2QKqYoQ8gNmJjf4JAOaJC/Wh45QNJPLPSFq720HB0dHpOSJfU/f95RFw/z2MW/f+WXK0x/haPnDP/gnzJu/xrypKKs5l+uBb/8H/wCiwqqKR+//KV3/jMF3PHj9DT687Pno/JzQrrCbJzhX8ShuePPNVykXM3RZcLY8x256bAocHR3lIFo96URaXVJVIuodkpb4OSTGUTyUiaBEI0tiFMe2z3CqHGMXsCZhtEjUXAVuavqUWP67Rf35dSsxat+lNFqmz2dxyetyLNMGML43ajqFXApO3ohItuColxWmrLMcS6lvnjH2/5dtH0t/ps993g9+SW2PRU8pYKysHcZYCREw8Cd/8sf8Z//Jf8pqtSKSWBws+I/+wX+LdnNJXSqig8YajIr0IbHZGopaU9jnojxf3l5Ern7h9knf/PnDTMa2WW0YXIsbWrrNOsdLS1k213VYF7AuoBMkjICrQQTlnYoMVuGcwRox860tcE6eD5UgDgNpcODE20COw9MgrFXUqBgZhamUUoQUcgKLwlhNLCw6BXQoxPMDMLJ0WY5KZFpG4JuNr704W20sugJdGN750Y955e4Ji4NbvP/eJbgt7WbNBx8+ZGgvOd0OBKW5/41fZrFYcOf0hO7xBUVRYooSNT/i/fd+wZMnj3j69AmzouHJ42c8+ugxv5IgaY0tCppmzmLWcHZ5yabrUKZgCJqARRlLpRVKR4KxKKWomhmYgtVqjS4syookSa57yMR0qlFjUvbw8b3xfRgVApiSLbXWWYhaXkMLxkijJy0by1NEQUrPzWNxVqQbC4jdGNgpIk1VUVYVRTWj6xNl1bDerFhtOpIuUKZgu52hUsC7wGJ2RD1rMNaitMZ5cSsVheb0dIFWgW644MOH73DsWupmRlkWbNo1PnqU1ZjS4lPEhSD1Lr1kzG63S8kM9UNWnlas2zOGsCGpiLaa5FMW5y0weWcKIWd+qqworcUdBZK5I/BEHhSdAjp5dIro5LBGoa1of5EBoFZGQOWwJSbR59nFCo3nyz96dIIZdIKAl4FVFkIiKUNUEuGnM9M3+ECkwGIJGIw2JO/w0ePTQIzCqqWYMEUpyREqYZoSlyJD8BhbiIvRCDvjh5xPahJFrfCDIgSF95k6VxLUabQFPaa5W0ISQOhDlDisqFBBXMfCMYpMSlUpfPDUzt10ej3X9Nh/KuVlRwDaVLaFuIc2JIB39OZlLu76BJ7ei0RJICEnXKSE9wODsiRdS2WGvAgabUReJQaqeiZVVEIghDPmB6dsvKX1iqgN56sO92xDudboVBJiovMDlw8/wuKoisT25++yHhSp7Xhw9z7Lpz0mBXxqwZYyB1A08zlhvSQMPToEQnCYfqAaHMUciqKiruYYU5BGqZq9jWjKIB5XiGmhyJQxGcCLPSwxdQlU1JNf9kUuh8njx9VsWTlu/KKRxtsDWpkVHM83snwvjovLWblJT9cakVqJMV+/ygLWcb92YvriAsVpdyP7d/aZzvEiqZcdo7b/+gvOGzNr96KvvHJd/1a5xZe3lFC51qVRCYew6VoFVPR8+Iuf8/jxYy7OHnF4cpduu6JbPuNP/ugfc1DBa/dOeff9FWGIojEZYPC7+sU3a/v98uLP7GdmXzl2fzNlf66r/Q8zzuudmfPFgXS7XuN8i3OdaGQGyeyPMUrSmgvgQt67MlPnAjHEHGMHzhVYo3NViRFwgUp60sSLwcm+osd9aU+CY6wFO7lnx2c0SSyc0UQtunTsxZaNn0/PaV7u3h/ZJgGMkiyyaXu6weF8oN+IoP9qveLsYiOs5MWGGD/g8P7rpBjQRrFaLfFFA6qgVzW9G2i7nu12YH5c49zAMAyT4amNoSgKyspm0f9E8D3Oe/HEmYKkgsRT2wKrtXjtxrAdNU6TKMBulCYb3RPjvrPXZPm9+nyOrKUUB1C7ai17IDHGsGfUqempHtk/eTmrHyTx5d2k3RzYKc18NsOWFTOfaLuA84mLywvW216Ctk3BarZgXleU1lKXNVXViJba0LHZbrAaZrMZR4dzNuuWfnB8/PR9hhSZzxc0zYzV+hKjdc5+BBc9Pvo8ESX2q+02ON8S8RhT0LUDm/aCIYjkh8lp2rIJB4ocBCL5C5lN0Hok06YNUB5hsUYE1AV0iqjoKAxom6VeMqdvMFRFjXc9Poap3JlsTBLPFg2iv2cyezdlzOQHTRthB1WOlVAxM30w+Ai2wGPwCQogxESInogTYOcTyYMtGtlUNJiqEHW3GKVKRGHFQzIqnSMu3arWknUVFTEoYpBEj0BCK0vSkqhB0iJm7AXY1VqJqyxqCl3nAHwNaIpSU0XNzH/+xU+PllGee5OkhdoLUJ/irUaGjt0o7i3Gco7dudN+8gkRnRIuDBhlUFUtJVS1wliLsQYTAzolbFXRtwMhdAzbS4r6NQpT0gfDEOHicsPl+ZLqbA2xwoXA5bAiXD7iYD7n+PCI9eoJ0QuIun16C0OPH9ak2DIEEbG2GspmxubigtD1WCXl4kw/MPQDRUoSWFyK7EmMPjMbabq7kTWb+kEx0pmZlc2LN0kKjieyRMrItuW+n8ZgD9TlRea52JCRZp1e3GMH2QUPj8fuuKZ91ilNv05ZoAlikiCQOBphyOIXoyTSpDg5bD93G10rX4aw7fXPpr1/92WjP9N37bNzL2RLP+3j18brC7SrWnEyX0wG+4qE1VBVJY8++IAnz57ghi2HRwfo2LO5XPKjH/wp/86v/honi5oPP/qFZF4mCAExZlPc3WK6mozyXFLACxiO51zTMD0G1/vi5W7Y5/9+DkJ+AZf3drPG+w7vO/q+F+M+uzf94Eg+oFyYssJiQmLsfCQqI+5JV06CulOlmKTQiEEaR+FjZfOeMoKuOLH50/Oc+2WKj8uuXW0kEWLss8kFm8jxdyOIk16JoyIB7ABMgiFIktrgPF3X0217npwvuViuOV97bGNx3uPbM7r1UjT9FLTbLSwawNIli48e5zxDL8LkIYqsl8rLh9YS714Udqq01Pc93ktpzagtSSW0sZiqzJ6K3VBmDYZstMioK/bXBaZ7G/tjfCb3z8Peeqf2XeHZsB1NhbR3zuvxiuOvYwjXTQ2KGwO7k9tvcHh0TDNbEJXmydkFDx895mc//DHPzs+ldqXvOGxmvPHaq9y+dYvbt07p1xsuV0s++vgjNqs13/7mt/nm17/FV15/iw8f/pyHH3/Aj3/2Pd57/yc5iEBcsScnJ8yahq5riW6VS6sUqKxRJwWkG9p2QwiJ7aZjGBxKGYqyIiVN6YTRmioYaE1ZSixfCEESLOYLsVKmJz5KBmZwgvCDJ0URQdZKyqX0fU9UTjJoraWsSnpnicnvBjSPVBwtKKUpjBWmITp5KJBSMuQgedGhi1MsByjRPmsaqSUbIkMa0NGjSRilcG7AxIhJVgBoxvQyWTTWFtR1Q1kqUgpSC9UnEb5VhlmzYKxJF1zImcCSCSTxdwJyuzQw9I6+dwx9oi7kFrVWWGsoS4Nkg7v8t+XoqL7p9Pr0pgqiMiQl2bDoPNO1QemQYxZAja7AvFIpLQkWipD1BxVWW0yS2LxIxCUgDthoQOX7qSrq+QmzxQK97XFesd2ssSphdWQbNZfnH3JweMQbr9zj+3/2Q7Zdwg0eO/yc9aaXDSpCXQQWb32F2/fe4C/9rb/NP/7P/6+884PvMq/n/LW/9rdoN0v+9F/9AT//yfe5/cpdTu/dI/YGH5ESQn6gsJUIKLsN7XpFiB4VLFUxI/glkYDCCK+VE0J0hEGEEoT1TCPLpigQLTszgjulMbkMUbHnsh7n8Pi5qQj1bpIx2pmKXWaqwl5h48ROucY4aZNthkjSAaVsFiAfxYfDNJYir71jIFOEFBLRJ0whRbnDF8N1ct4Xxbl9Se16csnLQd2LGKEv4ea+7DahVcmsH0LWfSQxXyz4S3/pvw5as1q3XC47Hr73Q9ouMQyJO6czfuO3/zIqOv7gn/8hzVzCN4aY2PYbfKiAvZKEnxVAvQDUPf/C1X7+5HF4XgD6i47J5cVZrjvuSUMn2182VrwbsMETvKeJUcyvpPBeEt48Cjc4+qbAkFDRkfZKOCqMSIc5jw9iyIcYRVrEkHVPEyEU+DCglNnpSiaycWswSKa9MRK/HrPiQfDDVF0ihIC1O5A3lUmLQWRXlJF0Z1ty++5djBpYPn3C4/WCH54XPNvMCfGQOiQezBNvHINOgb7v6Pqeg/mCywRt3+PWW5wbUDpS1pqki7wueEhCLGhlpOylFmAXgcE56Q/vCKokGYUtNcVsgTUfkEIiuIj3ousqSh5KwoD2zbJpLdwf+52pNgUOZGYuAWn0BCaFUlKXOxEzOy8ZyTtDI02RQ2L/yuuiHSzhDjdpNwZ2vS/Y9pqQy2EMQdP7xHKzYb3dMMSewMC68yw3M6pac3BYc3mx5PzynKfnjyl0TdOccHz8NlX9FvM5LBaK2eIjurP3ia5HRcmw7LcrfL8lxsB8XnNczmgObnNxuaIbNgxDS9d1VFUlG1kS60MboZyHXhC8qEXvZBm0NvR9T0oOUHT1gDKFZBRlVi/FSAqB6B2u7xmcZ8gPlIqRFH122wpA1MYK8CLkgcoIP/+Qcgq/82KVxQyy0qidFycWamwx0yq2sBRFIQxmkhIkKXhU8DRVIVZXSvnaY3bjJnyKUgM1So3bFPW04UuArkxCYyyKzOCNm3TSxKAY+j09shQZBp9r5uZprQQ4mVyOJaXIdrulrEqMqTD289eKvSr+qLKrYLchSuC9AJKJ+VRjoKss2mOaugIKIzUSjZZg2V0ybWaBlRNgZ4xkqWUhZ2MsMa4Z+hWQCLqiqI94/ZVfo374M1rnef/JCuLAW9/4FY5O7/CH/+z/hi4M87pkVi9IbsPBwYyiLvnP/pP/B3XquPfKawzbS777J3/Iyeltfvuv/Hv80T/7fxJCZLtesrjTYBZzvEpszh4TgsZUeWH1A0VZ0dSzHKOrcz/I/aRxDicjrFbKAqdjP+X+1Vri3I2OeQ6qDPbsNLYhqp3hk5gs9zE+UdgTjRrrtKp9ADfWjJRj0hhzly37/AQxxuxENaqrIyXvIhOwCzFKDGReBJ1TBBeJ3lMqOdNOY+/ztzGG5rNCu09lwFJ67piXfmTcOD7xnOrqMTcFPtMOlHa/f14gO5JiStHMF5yc3OWjh+9TaEuMiYvVkt/79d+iqWre+dGfk1xgZjQHBzV3XnkVVVT0rcMFTzcEfIB5Zbh3OKcp7VWO7NMucY8QuXK71/rl+YzlcVxe9AX7r+3m1t4e/JnnyX5brVaCRUgQnKzJuUqDc4HCewgem71QMSG6sD4SlMZrKXMoxlm5V51AAIT3XkKRggBDg4CVQJISkkoJixXElauylIdSGYy8gOEdSRERrpf4tBAE2Cu12xNH1k4pxTAMOBdISbNcrjCzkqKq+NGTSz56esm6HVBKM2w0pTPMypKvDx1WwayuOTw+4eKyI4RIoTyb9QrvPWVZURjFrCiobcFYk9V7zzB0FFbkxnRmb51z4uImg1ZjUEpRVBWse4KAAIbgcDlh5bO1vSzWSSXnKgC8crTO8Y7p6l4ne9LuIxPn9NwZXt5uDOw6p/CbHtsHMJrL1ZrlZsO27+j9QFQeTMJHx7bbst6WLDdzzi/PWK1X9ENHsoaQNKiGGBuK8pjZ/JSj41t07VOxyt1AYSUbxQWJ0To4OGRxdMrJnQfY4hmXS81qFeiH7ZRKPYI4bSzW2gnYjcKmiV3clvc5oDxTtEWppodCFrwkwC6ELB7pCChQBoUUnjd5g0khYrUWt12ye4vEyNrlNPUQCDGQgmFyLaUgA6WF+ZokG9SO+Dcma+EpsTysRuQAoqepSxi2hCQ1AkdAIxaACDA77/A+YILcW4pKLKv8M35Zvu3s9hTrInhxCY+eDylBJhaF0ZIKb62hKCzGCHj13mOtQeko2aWfs405sXB105TrHP0GIzk60uDpCq0tlHdO/shs61j1YNdP44+MR0BUymOQh9wasaii9yQDi8Ux1eyUsqk5Pj4hbjou144j7bh1vODWnVu4/hKtaqyFuky0/cAwtGw2G549+ph7J5amsXiT6LoN3h9QVpb50S2i3+K9hzBgFFn1XBGCCD67waGiw+qGoqhQo1Uadw7Nsfdi2gEhUhTRzDw7x+mp9Sh/kF2yKFFxzBt+YOzbnFiTf4/sgOKV/t4bw/3tUuXzTczdRA7sXKgpS68kxa6WL5nFyPUSR8DnQ5IyfiGhA4zl5r5I+7yg7ibA6GXAb9+9deWYfVbgGvAYjcUdKPkcbX+/+SIsZXaDayUanDGvKSFEzs7OmM/mHB8fcXx0xHq1RCtNVZUcHR2jMgBEK/o2Sk3u2jJrKqzdCxO/YiyMl552wOpFbtdPvel84uls6XlG+drnnu+hz5Dc8YLWdR2YXFM1ZeHhHPfmfET5iAkR5z0SOiIxdiEkAgqvA/Q9hRGB8f2yU1pHQjZOR5Y96uyZCYGY02y9Hytd5NjqKCE6Koqk15Xeyy7YMcEjRI0eGbygpvKb8rzGvbVV7suFRAzgfaJVkYfnF5wvl3S9k/2vqFgVJedbNe3ZoNBFSaTHh4AfWrptR/AJYyylVlSFpbR2Gt1x7wOu1MoNQYCurEMiD6WQWrZjvFuMUb4nhJc8E89bEC+KRd73Ao7et6k6BdfXg6vP8XNTeTw/z331S9uNgd3ldqA9u2BwPRg4u3jGxcU5q+0Kj0NphP1RkW3bkeIFvXN0fUtIAWsLVustZxdLnp6d01R30Lbk8PgWb/BVatOzuXzGdnkJWrPdbhn6gZgSxwfH3Lpzj3uvvYGxBTq7/Miumhgj1hpmszm2KKmqmZS+8iZPOimHkiBbQ24KUN2UFcVcEUPWgcugKMYgheVzGZAYPLYSANMPLaVzMlBZ0sQWhbgHtRatuD1wl6LE+nXOZ1+7gih1XLXW6FJcUAmp0Sm0rYygsWM2p4yptQYXBpLvODyYsXmywvuELUtG9k9n1O/cQNd1dF0PKuRUeAWZ1hcZj5wdm8uMiRVjiFHhvbB/JmtjCIMisYlVY5jNSmazGfP5DHLVkHHSpihJLp+3heQYZXJ3DEOCGHMwqsThyX2JHpFo7OWyYUbvJF40KCOZ0VaDJGII8IlJXHwqKmElQwAXiUWiKKGsDdaIlEiIibe/9qs0swO++6/+c+6+/m1Ub9k+7Xj7YMPxokYFz/DkY4qyxjcNnWp59vFT2t6xXAd+51tv8ejJuzx79oS6cPzSt38DN7T88b/4f3PrtV9iaC8Zts/oVmcY50mDx9gK5wb6oWW51JwezlFJY/WMZn5Iilt88Pgx3zftaTCOxaqz/qJkPGdB0pGxU1NPSyyH+GHkuCytI+M6ZlWkDO7GgOvsLshswW5lErV08nlHUC3ZytmtmkaDRDbXMbIqpB049cGTkts9UyjJDAw5HjDKM+aGz1/p5N9GG3vlxkDgBihzDLKeDvsUAJn/uMo66b0x25e7+KTv3Xcju8jQ9Wy32/x5Td8P/PznP+fJk0c47zm+fR9jLYMLGFtwcnAkda1Toq5LLte9sCeFRdVFls9Q17+Uqze7dx0vuc4rcVBXbmkyOdhVlb4qxj31z3PnnN76hG/+9Na2LcoYlNkl3MWYhHiQ7Q3jI4MLWdsv4b1UcvIJBhS9MRkUeooi10zVUu8b74k+IZrDCaVHoeOQ62UrvPc4Z6ZnU+RLEknrSfSY8fUM1LyHYRgkGU+rHbCLPMfYpZQoy5LeRdabLYvFIX3f8vTinEcPH7HaimoDWNStE8o+srxUUMzpti2X25bWBdGhcx39s3M2a2HditJQK0VVlZRNNT0vwt5L0ggg+rKAGxxukNeMBCUSY6SZzdD6nOB9rrQkceTj2F5JDslGxhUgdmVuqGuvjgl/shYqxlJhozeMyTs1ftd4nuknn+56CPMntRsDux/89Ic8ffqE5eqSqDxFpUFFMLLoxqSJ3lIpy3rdslmJ1EZZV5jCYAtLDIHv//C7PHnyMb/7O3+FW7eOqStNM5tz/96bDAcnDJslLiY+/PAj1mnL/Vdf5eDgkGZ2lGPGLIvFHLiF83POzs7pup6iqJktGqGrQ8z1CC0in5LwzqNC1oNJGu88fecYZgE11xhtSQn80BMHz9B3pOCnkktRgcpCwJfnZ5zOH6B0RQyR0iqKskRHRbvZUNkCpQ0hpgm5K6UIgyd4cb/WVYGOMdf7HCsoiBs5xSxSmRTalpRlRUQTkmQI9WdPcReP6VfnDF2LSZpoKoZuC6rApgIP9L2UZ9NKEbIenTVWEhOix7tIik76oU84B9YKkxWjIQYBAjHT7XZMJjCak+M581lF3VSUZUUIg4gr24K+d4Tkienzy0/smKbxBQF1Kaa9INLxwQuMgfYSipCy+1FjtMJahbWawuRSO/kz48l1ZkxRid51lFZS4kMI1HVDM6+oFyUUt/je9/8lwQ10g+fy3Q/QGA5SYrUq+OlPf4pHyuB0mxXtxZp+ecn5s0uUnuOOW/6r/+r/Tj2fU9UVnQ385Mc/wepE8IFH7/2A41v3Ob7zNVbP3ie5JdFJaTelIsYaqqoiDgMqRawxzKrbDOszQuqBkdIXvOTTVQY4Eqc0+6Ak+UWDJBbtNzXCtZ2ESBLKe3cISmrR5krWgvnSNCwjFo85vlUMlohRYlgkEkmPaus5rjXpqfZtQjK9vY+iV5k8o3SQ85HORYJLEIycO0FINxUD+JLbDRivGxraN/++z34Je8/SS14fB+0G7Qo7UVhMaSnKgvn8AJUcKMXhwYwf/+iH9F1H36347/33/yf88b/65/z5n/1rtpsljx99SLu9pCgrvvW12wxB5tvD9y+59+CQ+UH1govlecB7k0u+vt9eeetl65Rs7i/H119sVH3nUIVCWY0yO+JxTDRQPmJ8QrsBnXXpfEj0IeJSgghLBSE6gi+omzKDOoMyWQkiju4YSwxjPFfIrr5IclAPhgDoQsCY2ICaqPSVRIjx9xgjw6AwWjYulSJWl5MbMXiX2UIhXoIXl/HF8hnWa4Y+sto6Bt9zeXGRQ6pKNsun2Fcf8I1Xv0UkUdc1i2j4xdkzDk7vEs6f8c4PfkipE2Vhqazl6PCQt776Td766rdQeLzzhL4n9Z4QxePmnCMqhU+SeBhdIFZMpEvvPb33WbVDasQ6r0VLUcVJSimG+AKG/EUs78j/ykIYc4wkOaQJxjqzcRrzUSNUEcXozSUhVXZLTRm5N0R2NwZ26MTgezbtmsBAnazEVhmIyaMogFz2xMvOolXE2ChlrLTsLm275NmF59GTn6CL+8yaBhUDZUTKOBmpmdoPiWq2YXF4irElISrW2y0Xl+d431OUBUWpadsOpQzWVpiiwQWZRHpka4zCWCP+82xRaCVAjqQk/iAyxSVF73c/URInRv5NgIXU8IsxSNFqLZN5rOM3uiu1Bm0M2hhx7eUJ5QYpn2NSIsWIMeKObWYzqX+aYgYqGbnrUe9G4gNjHwlDh+s2BNeTokzGGLwAQq9QBnRZYsxY/kVLfCAJjMZ7qeHrMq3vc5A/aee2vhJEnsGC0mCNoSwrjo+OKUqNMQrnRrX0KCygz/1x07ifF7TRxT4Zzmm8DgFsz63rSQACWU/PpOwuVuy5YXdVFrInZ3JdqGzp+eRJEo6MFMK2mKJCFxUhJly3JviBpGY459Ghw/oNLXdQmzUhSU1BKXfm8UNAEVksGu7dvc0vfjxwevwqh8fHrDeXUkpIR8nMSj2uX9FZKf4c81iMgcv7C6z3A8lYCtOgc8abYtSHIy8Co0N7r8po7tJd/MhOTT2Hwu32/wzwVP7c2Nv7WFuNX2WYQKRSY9YqIuGQYWVCYiW1kthLUc/P7HiKqGjz+dTEMsYEPruo0JGkZeH1Mcoi7QEji2TYkwj4Qu0TENLVTLjPel6mefeFmzwQXDnbi0Rop/fYfXna+5xS44OwO+4Gm8eVflAKYwua2Yz54TGbiyckPzCrCvA9yfeo4HnvnR+zOj8TEeLQ47cXMLScHi5oFkes28jgHHVdTIwKL7icfZfUZOO9CP8979O6xrY9f/6UrvXpC9rutJ/Q3zdoYnzmihvXXPEJkQhxMWJ8QGdJqxAlFEiUEaDTov06BtlLOFLE2CxYnEDFnAaQVSYmN2mQEJPgPD4/tylfj9G7NWeffRszap2DwmQPkQZndwK7wUsoy/7nRENEsR0c3eDZ+IgxRsgCY4gx0BQVs6ZhPm9o+4G6bKgqK6UcvSQ7DmGgMhVaKwqjOT094fjklPniUIScs2ByDDEnfcm+IF6svNZ40d3TRrTsYmKXFBIDDoXPMelX5pBi0qa72q7O0Gn9muZLujJndtuqyiO9R1IodeXv8XzTt3zZwK6e15hSk4wI94asnZWUIiWHykpm3ou70ShLYa1sxkGYAwXE1NG5nkfPfkJR98zrOXGA0/mceVUwa+bMD48IFMw3W7S1YqEPgfXwjEePPqRpKg4OFxhTMJstsMZL1mAhGjchhhGKoRQURUHX5uBILYGTRamxKT8oWUYlhjFpIqeJ54zYkbmIObbNaGTCqCTxb0hmkBvTqaNsZsZatDWkMT6qH+jaTrJffYAgLEzwkYODQ7EWYiBoPy3ZSmtCjBNQlHivgTi0RO/kqBTFbZwp+RDA2oailJq1ymm8yxnDUTMMgX7wDIPPsRjkTNhdfQdZr/cskP8fcX8SbFuW5vlBv9XtvU93u9d7F+4eTWZGpaoyU1lFSSVUZQIZMihUmCQ0AgwmGBgzBmDGHAbCMMxoJoIBMowBZoBJJk0kmVQJZVBZTVZlE5kRHp237/nrbne63ayOwbf2Oee+xv2Fe5RpRRy/7557zj5nr732Wt/6f//v/y9xnqS8G05OT8g54n3PdrMlIyXnMSRizHKzvOngekXLSYIBSRuPxSW6PL8f6LmkaDMZVNyNe60k7WqKwb0taJ1WeVffthPgLWhSUomYPVE1JERtXSuHcQ2mnhGuO4xKaCP8E2ssod+wuX5CMMdY+vLZGufEocP3nrpx3L5zxvsfvMdP/nTOe+++xa3bd/n480/ZPP9YUK1qxrTWDO0FfX9JXR2VTYWirmvW606KXkrVtg89WSusckUI20gKFbUP0pRYJ6Xy0GOvlefHCtTdPKIV2Uhx1Jj+P5SdyWovP7KPjEt0Z8oip8p7C+o28uNSHl+qMKpsAkmEnMT1I0Yqq4qG4DihiqTD+KBMqj5R3pcEAdeWTNr5XP66281d+ldAP191jFLIc5jie/nYr2kvBFsjl/RFdG3ECQ6rb9V4WW68PO0CARkPB1wP9XJk9yYyKdY5ZosjHrz9Hs+/MHTLS2pnmLiMDjDRiv/Pf/IfYLRhUlU0OpC3F5gUeXDrlMEd0cWW3idOzmbU9cHS9BVBtBoD06+6Hgenml/1/KtervacO/UVn/9t2i4oHe/VXRpOk5UmKoXPGVv4dkJDEa5YjMJZ60ex+yyZDMmqgK3i6F+EyaBUFF4Zo35nuZ+DiM0rJYVMOYsQfCqbTRgrdfdoXS7gg7UJ0RDNxcFBgIQw+t2WlCzI35ypOd9u2HYD6xBxzjGZTIBM33ecHN/i+PiEybRive5wxxNcVdNMJjx/fsl2uyZbRH5Fa5xR3Ll3h8XxCXUzlQKrLOBIGmMAJZX/RilUFsOBFDwpGrJRmMJN33HrfGBAISoz+kZAe1OcXe2v4Y3hUYq/Dt43Xu2ba+rB+lMGwRhUj4DDjfYr8n/fOLC7vLpivd0yDAFrhW9EFMFeoww5J1LckgfP2ekpx/M5JyfHPL94xhA8MW6prMbWFlvD+cWXXDx7xMQ13D27hz85ZTaZMptOOdGKq3bNuuswrmbb9ayWVzx9+hnbzTV3795hvpiglKWuJlgDtZvSBl+g38AwtIVAOQZfBfPMiJl64SK4glKFssvwbUfqPdH3xGEoxfuCgsWcUSbgFCUPH4g5UjlJEWdV0W1aQhQrM6MtfT+w2WxZXi355OefkAs/rHIOnWTh1KWoZHFyzGQ2IXnhFqZyQ0TEvisHQT+JnuQ7su9E9sQYjDUyJ6coAYHKWGvKbihjdE0YMpebRvCqewABAABJREFUJdvNQN8FBh9ROuGjlrSlMeJNG+VmNmavui3LhqTcrNOslmtCGOj7juvllRRMqMLbi/FbB3YUQvwumBg38ImC7phSWBD3E2T5nkpJkYSxEeukwENrCfxkHOw5JSiIWgqCstLEtGXQc3xMaN/Se8Xi6BZVPWNz9ed0yZKyYaIj9777XZ49eshPPvsY6Dg7bVhMLHVTo9QcHzLr9TV33n6HeuLwfoNTAz/90T8WFxDrMcliG4ebBBbH99i2l/TDmntnH3DxVMrzaaYM6xanLc5qfPCkYUvE07ZXBB/JRfldYQTK1+InOS4cWUPOEaWE6J5MoogPEquM1hFMBm2w2so4UKKhlRQS7BkJosfKaT1el4ISSrq1FLDgJaAEfM7kwletdUNGKqu97/FDxKdMyBmfB0wtO+mMwSepRI8hobLIGeRyvD5FcXtRkuohf/viiZfb4c45v/Dz695XXqvfdCr+Z9Dy1ywF459HF5DXpiS/pinFBM9JXpNj4u/8nf82Kg38b/63/yt++O4xv/XWff7mX/4XuHz2kPnEMGscMST+7h/9BRfbgbtvf4dPHm9pO182sOyR5zf6fF59Wb4RoJZ2Xfb63vs1BXpaFZFcUygK7LI9SjsSAZ8VTVQkMdkpoIpwomNIDMowpETrI0NINE1NlTJ1ylilccU3S3itQcT5NYLWFfSuayFnS5WlmMUojTXCLR6rXkMIO1QrpUjOEa0GYjDE4vBkzLjepF1gN1bRam2YT495dvUJfdczDFKhKhzwjhgD23bJs2eaj21mku5yfrHGVhPmx2e0n/6c7eYKYxfENGC1YzGZcHr7FFNXRCSY8d4zDINIkmWoKsd0WrPt2lI9nMraLeLPHrFx64eetu8kwEP80F9liTnGDTeDtjIqCkI4BnBjsL6TiLoxmsYil6LB+atMK2/Q3jywu1zS9wGF6N3oAm86ZQXFyxllFadnt7l1csJiPmM2m7JpL0mtx3sREq6qCldZ+naLSgGVEl13yZdPljT1hOlkzjYOnF9es9p2aFeLoGG7pu02QGDwLav1iroKDH0ui0yk70VrbawOGgeVQotkiDGE5Hc7C601hEzXdaxWK9brNaoLZC/Ey5zKYM5AzKimKb6a5SIWzlc0GYyk4JwVgTcBkjJt2/Hk8RO+/OIRsZdKW5C0rEHIq13fs+4H3vvwAx68fR9j9lCu1pohBHyCmBS5pF+NyvgkPCutJKW83awwtcFlhy48h5SkQOPO7Ts09RRna/7RP/wLzBAl4NUF8cojlL7fmWmt5RzJZZGQFGDbbtluDSF40Rpq+yIzM+7Y0oi8/9raLkWS8t5eNJaCk1jQo1GQt6zHuvjmGqOwRkau3p3LIQrBLhhMZFQRt94OA1iR8tCpw+jMYjbBOEtT1VQqorJHKfjgg3fp247N6ppYUGMAZx3WNFxerPAf/UIIzCkKfUEZbDNFGeg3Gy7CExIBpWGzXe6qz7aXQuwNwTD4gcrVkkL3ofjjvjAr7BA7qSHBKPH8zZR0utATtAWMcN0wyFgwkiLVujixlONnjaQuys6TdMhvPyD/MlbnJamyU1KwFFIUREAPmCSFOEMn/RRzIpCKWn5B7FLCJ4qrSkLluAMJU/ksrZWkYcdz+2dmKfbiEv91s/BBrWQes5wl7aIOxuh4pBFge9Ovo3aypnKMse9fuTocwFWA8FHyQZyad9rf+/PcfemvbiP4OAxs25bz5YrF5DnaQDOZMq0dl5dPOZ0kbs8zD5894rtv3WZa1/z08SXPrrdk6/jeO/d4fPEpV6nH91vZBOTC4XzlZ+aXL8mruml3Ll+N1B2iKjeQ0XHKYRSvOnjtryG2U2V+stoUT9bRTi+B9uL7jJPK9EyZp0UjLsWSbvSSlg0xYXccN0tykcY4MBZd9Du1Er23lEQdISObtgFgkM9VSgTyU86YQXjTI2I3BmsS3PkDq6yEUZpkLclYASVSKoUZgRAlxb7tt8xqQxhEeiQMPc4YqGsyDm1N+Z5QN0J12fZLVDaEMGA0NJVFF+WFunJUxu3O2/tACIPMv8EXExeFNQZnikOHVviUAOnHHAPdZkXXd/gQscaQg6hd7AOyg3ToDrUr9/MIeOye3wfncCB5Ju8uurYHN39BJUZu+I5bV9QcxvuwQBK/fsRus24JHsT7QMzCK6NpnCuViJKiuvfgPqeLGdO6onKaSW1EcyokrNVUrsa5im3bolIiKE/frxm6AWNr6npDMIqLqyXrtkOZGqUdOQ6AdFjwA5vNmuBTIYRacja0fcfg97sH70fCZ8TaSnhfPhZOlXCSUk60fc96I0LKk0GhUxbkq0DJkoMHm6XyNKVETMKdEhN5sfrQyuBsJVA6sgBdX1/z7Okznnz5mLPFmaRzcyZEubGHfmC93nC5XjOZz5kfzzk5rvcDSitCQS5CTPjtCh0GCVDIO+Qpp0jfb7FZ3CJMGETDKAuX4e7de5ye3GLazPizP/klbe8JKZHVWERQBmPZiYyDc3QxUGp/s/Z9x2brSDGUnVeGENE64awjltT7t6idKE0m8FzySDshx6LBNAbQ8iUl3yhVu6NmodpVfZpifK9LwJ1HW60xFVLSFllJ2X4Cep9Qpmxi8BirqGuLcxWuqkndGkJHPal5+8E9Hn72OZebFTbtd7jWWrRxbNYtq/axTByqFBRgmMyPSWlgs14S+gFbV1RNTddtirNCYrNaYawlxsAw9LjpkaRkgoztF+92Vc5Ll6COBFHvgyFtFbaSfhKL4iJDMu7mC9onfLsxzV1eN14WpdB2H8zlUTW/PCgFDQmRvggpiniyl8AuRalSy/IsUeUDPlAmRrXT9RoFtEvYvTuPkR4wTqxaf8viiVcQo18X33xtCnX351dEIIfvHYO8rwoUfoUN0mGa6Ku+Yh4/O+/vgSKqRbkhDr7u6w4k1+azL1c8er7mcr0lVueEFGkmM96+e4LTkeQ3xO6Kdbsm5RMqq9n2PWjFpGm4f3YEKeCHlqHbEIdAri0vxemvAk2/LsA64AnDIZaaX3hvWWXH/viq+O1XDcRf03YbT10yLkHyDypH0BGUIStL3CWF5TxS4ZHlKBQSX+4Tp4PcT6koHDiNzopoZK3LhWOXSaJrCmSVGTSoIHy5Udst54wPBgu7eegwsItR7DBFNiXglIAAyRZh393rww7xG3zP0axiu90SwkCKAaMVylky4lVNKTBwrmEY1rR9Rztc4L2seY21pKxxztDUFVYLPSmniD+gUMUQyjJhsEUCTTJSxSs8S1CXfGToW4ZhYIgRZ12hZb2KS8dL99buPjp4UkCZfaC324AdjpqDQ4+pWzm2hpzK5nw//4x7r1+7pZhIKNTkbMm0zFzL8dRy+2TG5UWgx6Cahve+95epCZiwIa2/5GxmyNmxDZHJ5ITZZEZVO/q2o23FSmW98Tga+qFnvezpYmDT93RDIKWa997+AZOFRqWGi4tHhODZbJb0Qy/cOjTb7or1kNDGUjdVcYMYCIOXAWSrMmAVIWYxP9aK6AM5Jp6v1/zs8095f3HMwlXUqhgqIzsInwOmlFD32w2qbTG1xhgnshBZPBNdPSWXII8AP/qnf8bmeoWJYGKi0iXdmTLWaGbTBlc5fvbJJ3z885/Tthv+xr/815EqGlDWiSdsjNC3rB7+gno4x+ai/J1kB55iIPkBTyASwbf4GCTotDU/+P4PuXVyGxXhzp07BDJpLRw5rSSIq11mGMqEkYtuXWRn9TOiDX32PH22kglJKZSayU2VE35QGFOBDuQ8vPHweuWQy0K5h3EBl0BddMuy3AC7PGMpjkklFaikWMKgMOTCrctFo00d+AGCLWidUhIFhlzSfLnB5kRdGfRsgq5nDN2K7eqadZdQfsBUDd/9wfdYNA5Ch2/XzE8mLJeXdNvtzplDOU22CkdTdqwD1xvPD//qbxC7FR//6EtcXSRZyuQinZDw/ZbKzkh+YLWK2PoEYydgNJvN8gZCPS7KSmesEwK2QlFrTcwlwK3AuCIDo8Umb6wMFv6goHZZS9CYx/7SqkjmCMSX6HdE7EAgHvQ/Me40E/tYuKc5MzCAj5B1qdAtQqBaMaiESkKA9oNoviObayqtGLlg1ugdnJK9Kpwbi7XuW403aa+aOF+F1r2Yoj1shxHJq/7+NZGc+qpjv/4wb6Zpp8aV5ObTOYukhn6FzMjrWkrEduB/9L//Ex6et4SUWW4+Y/CBB/dv89/723+D0wa871iu1vzzv/k+d05OmE0bfv83bnNrYRgQ7btffvwJzy6XzBrLZjnIHO7eIFD/uqD4xjm+4etuHP/FAPDX18Suy+wtu5CNDeTCdzVEY/Fai59pSgi0boEgjyhFRD4ldNaEDFURGa7w2KzI2hACewRIU5DyTFTyb5KVDZSKOGexliL9ISc/BmdjsOb9QEyC/mcnRRo2BIwTRYgxW+a9px8GUkw0tubt+7dZr6551C+pasvgO3zoGYZAXVX0vefZ1YYuQrINQUd++vOfo2yisoaZrfG5ZzGfcnp6LCLqg8d3HabR6OTJyYsWaBZbO63herXFWulvyPi+JeVADgMZaIeBTdsxaaZ0IdLFETF45aDYBWPjfXS4odr5mR9w8/b8ycM48Cbivjv6zkJz316W6/nq9ubFE7qlz5khCxHxztkZ906nPLh9xpduzbPlhpWPDD4T+xbjtzRDzwfvfYfp9Ybl8Khom3Wk1KMJxd9OMUSpwhkLGNq+pxsGfMhU1nH75C4nR1OcO2W7vhI7Ja2L+rWUDnehw0eN1RUxm13JttKGVOzDRMOHHclaIaiBc5Y2Bf7spz/m9g9/yNQcERU7h4ZIps8DujJENP1wgb3Vo10DxrJtW0zJvStbkxGyd+oDTx4+RoXEYjoTNCKX6tQYSAVaUQpmTc12teLLLxLb1gskW+xQJpMpgYTf9myefEa2PY0JqCA7KaM0lbWEkHG16ED94tOfcb29IhZexWa9YlZNmNgJjc1YPCoOOBzRRFKS6tZ+GAQtiVokbCKieZQzRkTgxIc3GXJSBKQKyliRUQlBzKmNVZg33za81LIuO8ycCvFV7XbTqviCymAXqEllyMnvuPtjqkoCPIMlY8uCO+qljWUBVpvyr4QmgjMkNH3w9O2apppRVRNMVrjZAmcN96spv/jpj0k5UFnNZrnkaFbz7lu32a6eoiOYZEDB0PVUVaKaOOq8IGjNdDrnu999h89+8WN8d01UHoJCOyFJb7Y9rhQdVFUjQXYX0QH82UDOAyGJ/iIZKVjKmqhSqdSW66NtiXszKG3QSlKowjuUWMk5V1KvMh7HDEFSIxm/VLRqKdsfzcZCHonUWazLCr8kAwZHzvJKX6RXIoJUkwI6KUyS76OtaCOGIYAWxK8fghC/AatFUFRy8BGTDeQBQ8LZCaRKhJrVN3c6gXHevDl77n1zZTxlteedvj78SfsahBuw08sB28u6Vwcv+8oAqywKh8UOLyEMXxGkHqJSB4vSDgJPeg/9vqqlzHLZ88kvn3O56tl0gawU8+mEf/RHf8TV84e853qiF05xMnOaZoaqa7xx1EcVR8OMISictbRdZttm+sHzv/j3/oz/wd/+Hn/jd+6JiKw6+M6vPJ9Xr3iHqOTr2wuL9wuv32Et+RXd+22b1jubQ43BasQhRmmMMiQViVqBEtP6dFDopCiyqUhAR9b0KpKDJquMjZFOgdZJNqe6pF6TpPXknoaYNUPxDtc5YaxkkjIadEbnPX9V5ZKiDAHfd4QAwShUMBiMFO2FLA4WRQtu6GOxppQCwJgzH7xzn7uLKf/xP/wzVBrIKZC1YbVc4buWtt/wB//gH9I4sai7WF5irWZSNGd1ijiraaYVWkvG0Pc9IXnIiaEf6AdP221BSfpTq0RMumj6CaDjfYtvN2w3G2IIUjlrKrZ+oBv2QdlOS449DWRMXSglxXL74Ey9ME5ExoRxHJX3oIryw+7YNzePL1bjyk9V0vNf39546Z25gd5FQszkZFnM7nDn9m3efuce6+5jrrue3Lb47ZJ2dY32W1xjmM1OmHtDZZ8S+q0IJ2JQKkqAkI0oyOtxN5EZQmDwgZCgspnppOboaE5TW7SysqBE8DGhdBIj4Cg3aM5idyIIaOHREfGFtzFOBDuF7xgwlSXmxJPzZ1ytrpkai65UuZlz4ToNpQq1ZxOuOcup8O0y7XaL0aLhVpkalCKGQL/Z0rcdJkOqGvwwgC6ppmLcPlY/Oavptx3bTabtBlAU7lzE+0DyPTl0xHZFqiPZgUpSDauUJpV0WlaZmDwPH39B269RWrgXq+UlTmlCPed40XBx6disFCEZVCpaekkXNX921jYplrkuF424UiCx86zLMkmoLGXaMcUdpG70t4js1H5iVrsAbuS5MMJ55YWFx5D3qcJDZEUpwZj0SFDeVQWOt5RiLADQJbUn1wa6rmU+m1DVNVpFQWiNhRyEC6ky0XtW1xek0FHZTJcDViliEc2LIcjYiZFqfiIdWfibm+U5flhhVUCbaodKDkOgXBScc+JZXM4q5gTRE0ZXkDKsNRJ0j5zBXbl+6acxFaHNKEQt7xXSs1R5j+8pboZjLp6yOjNKw1Am7jGwC7lUl4871JRLalz6MSHyCyFnMTePYFLCGS0irZRCoSy8uhCS0BuM2gklJyWJWzdq3wEg6SqxT/vmw+317SBwGturEK8X3rF76cF/X/7rzWPmg3H7tZjZYTz4DZC7V4ZDmd19sAtkD87zhaWHy/XAP/zoOZteeFQohY+Rzz7/AhV7Tj88JuSA1XIN+0ERiLRBMW8cpprhrBwrxOImkjP/6OeX/O3rXlIFr6JzvJjKfl17EQl5w864cZ6/Rk7dSx+7C+z0bo7KY/ZgRFYVZK1LBfv4RrULCEzebzDS7h4s61a59fcjqhwH2ZaM25OUlYgepyxzqMSWJEZOWN75Racskl4hBnICGxUmJ4wVHdicMspm4c2mxOCLBFiW7620ZT6bMDWJW4uavhO9UJUVpEjKHj8k1pslwdlCuYikKEWRPniaA8oQQPA9Q28w2UkK2Qu3L8VY3CUythCCxfpcROmDF474thvISVLBSml8BJ/2wMAoQCK9KIMljwFaWahePTzGgaW4sRE8zPWPaMXBAV4C0w+u+Zvy1t945T2bdYisSWa9aTiav8/9B9/ne7/1IZ9fX2OuzsnPt/Tnn3P57DnExPzD98EusC7QGMVFd46iRutGkBirIVqBfOsyWHVmiImuaKzV1lNPMvOFY9LUpKTxgyw8g4+kFNDGkIuuXsqJrt+Scy4cK0kdeS9pQVsGiwRrnhgHFDUpRS4vL/ni8RN0BHvmmOpaUKMUIAwQPCHBcrnBKuEYZjLXV1c466jrBlNP0FrT9x3Pnj6VSr0k1mVroLaF9pQzue/JShEPeHKhKHTPZiI26Yeeq/ML1LDFDhtU6lFBFrUckgQIWpN8BqsZomez7vjFZz9jM7Rom6irmidPP2dzdcm8OeKd+7dp19f06zWrlWIbxTgeZ0n0xCzBnWgRUcQsR+sp4Wul1O52NNZpkZjJ4nZgdS2EVTt50+H1UttbmwEhkcfqQrn/5X4wsohJoCmT0p7nsHs5inEcyB/G53J+YeIuqGvKnqwsztVsNmsW8yOcm4HuiRhC37O+fMakMfikWa/WDL5FK4/OPVpFmkp24VklYvKEdkuwNc273ycvHe3ynJ/9/CMas0VlT+899WxOUhofIkPfk5X42zrnJHg2ClNb2Q2HQECU+kPIKJVQBipriySCUANyIctrZSRIMhrtLDuBTDLWapHwYVS2F3QtgozfPGptWagdKguHpvdh139D3tsIkQ0qpJ1GlzIayoKTUiL6RA4RHUVvUcnGnGZa0w0eHzLei4SCKuTyrIvHZU7UesBmBcoQsaQosige/43H26vbiOGNMkAcqIG8HinaDbwXjiP/UgcvGZ8/KPkGduTBN2g3nD7e8D033jt+r4P7QIEUzez2Rgf31MGq88X5lv/733/IuhOiOimzXq15+PAhpMiH3/mAE93hCGgVeP78mqxkzH5w93QnzSHONxTaADzbRNqhiBgetvyKX15xCV6nMZhfevGLx3gFenrw66/Wu1/fxirSsfAsJXbeymPBQxydjHZEyJvNInQTEQtnd6203gdxcgoCzyttCo1LArVc0n4JGBLkrKTETBmyMrKJzBBzKYTK4kPeB9nYB5VJSRD1GDPWZnQSfcqQMr1P+Jh23rRVVWNiRNvMD94+KxtYRSaQtEWFhI2BWyeVAA0BppOJZNkUDN5TNzJmttuelAP9dgXJU88XIu489HgvscpIvamqSqp0UyKFgWzFcq3re5ZbT8wKa2VeHFIiJPYBt7qpmzo6H72YSj1E7V5se77c/l7SehSUf8Xm8aUDjOoEbzYK3ziw+63v3eeqy5wvA3/+kzUqT+j6mvNlZDAOUzumjeZW3bOMWy7XgT///ApfX9Our1g+u6RykdpmnE50bYeyDhAUIRIxTlNZQ99nwjrQth2WwGr5KZVeEpPmerWk74eyyxNPPesstnKoLDyeGEbD+ljmSF2cHaSD64kjRoP1CtWJfk9MYBvLLx8+JPrM0eI28+NjTPQYr8AEKiOl3+1mTe00tdPif5fElkvZuJM3GAbP+fkFH3/8CcO2xSnD0XzOrZM5s0mFVtCHSDd42q5j8AFX1xyd3SJSPGLR+H7g45/+hCr3nDUZqzw6etH0inKTZSBHRWWnPH72lM+fPWLVrYk6Y2vLdFGzPr/k5O6C3/j+97hz+oDaHJN8zUebX3B26qgmM05v3eVnv/yU6+WWTdsTI6WCaj8OBMlSaFN2cClTG+h7sXOaTAxVVYiqvwKF88Wm1DjwFT5JikLv0iay+xTeJzurs3Fz9Op5Xex6iFkCxazLQqXIBjDFZSOJGX3WEqRsNy1dt6WZNDg7QdsaUynm9S26zZpNF/EBamvLJkBRVZrFvIZcYWtDVA39EBn6K1CGanIMGfrNBX7zTKxvskPZCslfS2q9NsLdDEMoE4HBVDXZaknNxwwqUteObCH5KJNxkABNa0hao/K4iAjsmsx+wyPCIiBeq/IYgnAMA0JKHjvUhECfSv4W6Pth9zevclkewKhSf5wzOQWRO1LCy8sqk42VbJ/3xJKWtThO7CnRQKwjQ91xuX6O1ZlJXUPjaIv0iUsJQ02Miu02MnQdQ070+tsGdq8K1r7Bcr5bhA+PdTh5K7gxlmGHrbzBHP/qz+Tlr/5N2niMWGCb8cA7NDEDmuw9wwAX3RylV6gYiEVJ/+TkhA+/+33+tf/mf4uP/+kfsLl+hu8DdWVkvms9n3wZmNQTkaOonMhPlLlaK80Xj5d89MtzfvM37339d32T89m1IutyI5g7RClfcdBfV9++0LTW5b60ggqpMagT5AilRdLJaFnQdQEAxvtRKZoEndIiYUTxii0BibVj0YBF2QplrHAotUJbmT+VMhTdLXFWMglT1diqQhn5W44RqVsXQCWkRNsNZPlKCJ3NULmMs6AtwlOOCT8EtoNs6rSkCjDJYHPNYjbje+/c5n57xHYIBRyIqBzx2x7bVFSNJQ0Qii7upJlgbGC52vDZJ19w/9aMB2+9z4mtqVMkeU/yAzFG+q6lqhzWyToUQyAMA9F3BER3tu8Hnq49AS2ar0rhMfhyP+690g+E3EV9/mYhxdfk6XcJ14N7e8e9OwQYXn+Aguq/2UB845X39p33mCXD9CTy5dNPWK+f8eSJxk5anj29oOs99XTKnbNjrtaebW75/LJj+NkXaL/Gd5F7p2ccnxxRNQ0pP6f1npTEYzYRsNrgnKaKBmIi+0A9s8TugvV1z/UmSjVNjqScReNLQVYZU1CNsfekGkdg4NFdIJNQKhdyqBDGRWtHAsSQM1cepsslXz6/YKJrJs7gVLFpKVFDzrk4GghIVFlLKrt6Y0SQWClFVTmWqxXXz8+JvWc2meDfusvJ8QyjFW0/sFxvOL+4pB8G5osj7sckhR7aMHqNnRwtcNky1QO9ziiB0wSZybnc9Jar9TWXqyXXmzXd4FGVJsVIu20hS7BobY02R5jqGFsvsHXizr05R0cnHB+d8uzRE/pVzzpmQeviXlgxj2XyCWxlMbWlqmru3b3FMAS6vmN5fQkjO+pblMUaLTIWiVHjRxbLPCJraFQ2sgt/UXE/U6B9dmnkHc+1PEZXBygZn0OQJIvI5ZAS236gGzwhRkw1F5kQlWAQmN5qRe0kwI1BkbIBarQJaK2Yz2esNgMqDZAGuuUlohSfmM7mrNtKxLwnC5QyIjaMJumAHwP4nKGgzKHvmJjbaG3QKkrxAwWsRJFzgb8EniyOX6MTihq7UQK6siBAGUvF07gfxHc26IzNYIpAtOB7A0kJByeGUjUORC1ouy58SEH1ZPMl6aWScYhyL0WdiQiNQGWNiqpY3ClyBJ0c02pC3RhOTxZsU8KnQKB4RCdbDMWHgjAmQv51eMUeIGplh51vPD0ix68Yd4eveG1wNkZuI153iODdPN6bCAMffFG+Iif0wud/3Uv26SNu3F+7O4Z/8hdP+NOPLgkj8lPeprTinbff4nvf/ZD50RGnd+6jSFyeD1gbsKmIgWPIWrismyGWZbTc6xr+9ONrTmaW3/yNe2PnHHylrziHG6nTm3PCDo78ZxGlfYNmxqrYEWVTpSpflQp+pTBKkZTBjJ5jL6RkdS7xt843x5zaB4pjxoISHI7UBoVGKYvWqQj3K4y1GOsEWLB6H9Bowb/GtK2PkLJYbhI1RkvWJyYwKYuoeEoMsYgh51HBQIJ/ZcSasqmccOC0FqpFyaS17SCVoCMPbTwfLVX0IWeGGNiuV2zWS+qqZj5taLdbMQGISQSZjUVpU2hfcnyFxAUxCau67XsqpXBWuNYpKXI+5D+Od+YoDS59LVzFkYsH4/okIPqIxO0zXS8ifDutuzfRbMyHx/z69saB3eL0XWZ2Qt1Fzs4uWa0eEtMlqtpw8fSSFCPTyZRbt29xvtU876959MUlT559ztx47kwyt04fcOfuHSbTCW2fxfM0eWxlyblDRHUtlVHolFAxMncKFVa065bnz1ti8rsBHGOQwRolNURMO87CXi2bHdw9clics7voexQ0DF58XJcK6tWaL8/POZudkKcN00qQxdHGRY8kdAVJKera0fui/aMNPkS0VkymU9rtlvPzczbXK5yxOJMI4QRjYNv2nF9c8sXDR/TDwOmtW+iqwlqptCWLGOSd27ewcUsdrrlWJf2aAjErwgHmcr664nq7pvWePkSkeDAzDJ6pOUJhCD6x7RV9ciRdYWt4971j7t6+zbw+4Zc/qlkaA1GQulJ0i1KZVPgXOUnl5tHRgpOTE77//e/T9x0XFxf82cUlYk1mSpDxzdrOn7esGuMERUHZyBqlTNE4k+u6Tz2U1ERGNuilfF1eJe8fAztF0R0aoXJVtNlTos+RTSeBXUwJW83QOqLSQIjyHmc0k1qJHA2KgCtjSyQMptM5681zNAFLor1+hqtqjFFMJlN6U6OMZnZyRkyDIBZokbgZBnJKWG1BK2IKxM6jtMWUiVDAM5lYslbkIPxGVdItUmVaNupalQBQ0itaGawyBGSjFJLQG4YhMqhINAmDlZRGznggpaGUmNiioZUIAEajbUEVU7nuKYu9oJIUk1agk8Yr4fdFVQI7JJjrew9eZsysDJPplPm85vT0BH+9wagenRNZZUIUR5oQg8iz5CTV4N+qySqpdoiaRBP7mO5w5Syk6FdOtIcpGXXj+UO2zosB46Ee1q/0rW8EgAcf/0I7jP9e/tvNz1W7XdBBnLd7b+YPf/SEf/TRdVm09wKrSmnee/c9vv+97+Kc5fTOW8QYWW2WuBBJWaN1QmmLrStQmrbzO93LlMWb+48/XhKB/+G/Aa9yw/iqPtinTvOLT+1/2/XTiPR/VbC4/3GImO366iu/2eub1Qat9J77q0uRGCWoAyIS2MmaY3bIDaW/pN4po8yLY0cfDNdS8b9D/SRokZ/yHZQSsMJZV5A+g3Va/MJzFuut4lc+8vFCCJLqNBZjpNArpoQprg0pw5DE9zSXAFHnsfpeF8RS44wGl8FqQtIMA3Qh7dL7mX0BU85BqEJKUs1D19NulmzrGn90zGa9pet7ccAyDqVdCexkzU8ponMixiDAhbJ0XU8zaaito01JArsSDI8WlONmLRVgXSmhfY2LzKihOfa5cP5LYMc+iKOMn0MBY/n3YfXsq8fi6KrxRmPrjV4FPLk+QldTej9gK812+TmkCafNCR8++JBn109Zt8/Z5pp1six95vHVkpk2JJuZKsMH3/ld3vvgXWaLKV1wXK07tv0lOgVS7FCupspwtVlh4sDcKe6dHfHgdEEfE08v1ygdcLZCG0c7dDsUovcdzlToPMLQjqqy4q8aBRUMwZPyIKXQpaN2KtJkrKvoOuiCJtmGt7/7A5bPn/PF06ecHk3ZrteErJkf3wJjhRwaIpO65unzx6y7gQdvvyuBgjEsFgucs7tS9kdfPuLoqCZlsXBZb7ZcXFzx7Nk5R0cLciyijmXHk2Jks9pgixJ4jorr9YoqtjidyW4KdcNm6Hn85TOC9uhpw0lzh6pd0fYbYgpU1sDUcHm95s9//DNo1vz0k1/yxZNHmNrx/gfv88Fb95moiofvzdmuljx8IjeUBHfFpiVDVRmapsLZKT/8rd/h93//9/md3/ldfvKTH/OjH/2If/KPf0SnoZkIl+abtlHSaLxR9ibIRX9NHU6wCjCQPSi7e1/eLTryuhgjOUpZWMr746qUpQgwiQRITF6qOrVl0/Vse083RJwd6FeXED2T6YxqWpGywkfFxcU508mMFDUPH34itnVOFq7jk2NSWhATXGyeYfURlorWt0RdU1eWZlLTdxnvI23fcfnsGSeLKZVzpKywzqCScBiFUiBpnNrUItlTijGUczvEMfqheLjKyaYkCGgsu0pdOHDoiuATfRdYbT39kAg6kkxiaqQqThxdNFnVRCDghYM5phAV6HRQPRYlLZLbHpQXmyOtYDAENCSpII86i29yiKxWG5y3mGSJtmbSNJAtMSb6TU8YemIK9D7TbweGLtL6wGRiMDlRhDa/Xcv5V1upd1WvrwvybryYVx/8pcjp19/GlelXOfz42hGSOwhYP/GnfDpk+uF8JyptjHCRTu7c4/aDd3DOMTu5D8ZRTSo++fnPMX4oHKWEm0wk06E8zaSm6joSidl8zrrb8nRTLBNRB9/hVz7p0r5mQfwvAMTbWxqq3RZifE68zksAZi06Otmk9fv3Gq3RNmNUxuaMH4MRLUK8SVmSHh9GBOmVRmXZGGqtSUV+SzZfiqqW+65y8nzUkaC86NUZT1IDAU0fMr2Q8qhNJtOJWL61GCccvYQqrjPSuVppVGNRRooaRpuymDN9FP9WQfmz6MnlUrQRhG+vVSneGAYmTcPUVWg7hQShazk/P2fbtgyDZxgCqqpwL6DeKWW2bUulIt0Q2A4i5eWqmrqpubrektkXs8gbx2zIqJepdxJTomxiduvJIRo3/tQHQd343OuQt1HLblec+A3bG6+8f/bjz2hmC9CJy6tz7pw6Htxd8M79M8LjCdeba3qf+ex8xaOLK86vrzFpW6Bei66mBGXohoRqPQmFrRqayUxMn7NiMZ1wNJvTdVu6jVzA2fQIrRsMmdn8mGUvFz7EiNZCFE8+7ozBTeEViNOEuE3EIItPCJoYDSkh5PMwumFkwDN0nu9+8EM+fO+7/O5f+iu8/eFvcHbnba7vnfP80eekqKnqCXff/wHGTcjKYHQWRaEQpMy6HwrXSgbtvfv3aVdb4hA4ms1xVcWmbUVHKGYmsxlvv1vTtR3HJ6fcvSuphxQTKibIivV2Q6V7apVI2ohgpUr00dOuI5frNZ8/fcLpnROmsylV5VhcnJNyZPDdbmBdXF7x+WfPic1nXK1XbP2GqZ4S/QKnb/Hg9jFvv33Ez754JlzFUs00VkTtUi7kEjiL2PQweIbeMwyeELJwr17HV3nDpk3RChpRg0PkZNzVKMhpj6ykrDBmnM5VQftGwWWB33NMEKSmUpUgUee9EnsMkAvZO6Ho+wHvhU8xnzXoNCEFS9M0xGEL5LLrVOQYUSlxvGi4ul6x3XpMVWFtja0kpX+UBkC0E63R3H/rnZJe8MSk0NpRVYa6qoVDqQLZGqx2aF2hsILEaeHo+N4TC1HYWovTNSkFgpfSf0mfS6CnVC5pFOGOYCMk4b2EkPEhM4SMD1HUEHUgWCe71swukItASILqaAPK6APVeCU0iiAFEiqJnlUcBOFstGGI+6q5utIYY6mwPLtYkkJC4ZhMINegXaLZKLabjj55Qg6kLhGzRTnDojlhMs34OOC6bytQ/CtSBw4RklwC6IICvRy+fVW0qA5+3lyIvrYdBjwj+PQianfjMGqHbKvXpG5fm+7J+39YU+FsQ+Ucw+CLzZR4ld66fZs79+/Ld2kmNPo2ZtJQHT2QfiIz9D2mFrJ66CO3/qO/y7bzbPseoysqG6lt/QJK+gZ9sPuiL5RKvHiY3fS0R1XGc8+Hr3nhc1762zeF63j19RVusd4dOyklIr5Gl0BP30B/lBYdTqsUoQRrY+GZXGFVdCjHNG+Rh1LCr1PGgpHUrDGaqm7k4QRVD8EDSnzWlUbEh2Vj7OPovhTIWmFyxmaFzZGsRFIqkNBFrcCotK/aLRzAsc9TTiTlGKL4qVvndueotd4FnjmJ+cDpyQnv3n/AfHFEUop2CJiuwxpNMoaeUo1dNAKrqiprgKwkbdfy/GrFl+dLZrMFlROppBCjBL+6hNpjCpgxoIulyGXUrhWaSz4oMBwdK153/76I3uU3ROF+lfbGgd2nX3xBM51jbWa7XjO713B6cszx0RHNpcJZMSN/fLXmYrWm7bY0aqDWEasBnVhtN1ytlrS+Yrleo42haabUjWVeTTmezTiaHxHzmqHPDH3HfH6G0jOMysxmmmbj2bbiMGFMVeQ5ROJDe4V1EVdl6qrGGLFqAaiwGKOJUXTnvA/4wTOZmoJ+WKrK8J3vfJfvfe+3eP+7v8HR7QdMFz3N/ITNtkP1LVUz4/TOAyG6lxtoF5UDwzAgyvmS7rpz+w6riyuGbctiNmPbreh9x+AHrHXUkwknpzOur665dfsOt2/fKcriareD64cBbMRaRVBSzeiTZ+0j5+uei/WK58trJmcLJtpg65qmbmgrK1wBBOlZbzu+fHJJmlRESfwQ04Shc6QwYTo94/SW+NXm3aArA0Dt8YaRQzAMA+v1mvPnz3l+/pzl9RIJoAM7a5xv2IRP8eLaOd4M+2dIuXiUsuNFH8aUOzeElEshSBbifrkJR7H98XUSkAeiEQ9aKcKR6s9J3RD8hDCIxp5CJqkQI03lCAP4lHBOi59yRooYjEYri7aWps50w0CMGVNNqSdTMgHfSYrTGIPT4IwjJykAQkmaTmtTJFFEm0+p4qXqE1plnFYYUyaklGVjUM47JgnsQswEL6nZSN5ZrIUgjzhyK1MiqkjAYMuFF05nKtImYFVGG/lMQRiUTOKlsEc0BYU7J04t4CpLCrIwxJxIcUQsFEPn8VFU923VE0JF8IphUHifiTmSlBSSWGexuqFupjS1ZxgUKn5bjt1+sj0ch2/81p1V0F55fkzvH4qZHsDQBwcoG5XxfW/IpbnxshHc2n2fl7/ijc89ePNXcnde/AzY2bhZYwVJQYK63/jN3+TevbtMJxPC0ILWmHqCqSvqyYnwnpSi6zuME3SdEGmmU6xzaB+FxmAsxrh9l7103gfX6KUv+arfd2986c830quvCOZe1x/qKz7mm7WC9jAu/COiV5ySdrIn5Z4pQYdWCqsOUaLx2+2Du8PfKQFe1kJn0UajjXhqu6rCVlXx/pZ5NeWMjgalimQIB9IqMYmskVHYrOQeJZAQxC6pLFaOKoOSgoqUIBvhfKsStI3yK7LxLDJm5d7R6jDAyjTOMZ1MmM/nVHUjEikxEfxAVdclozMGk6rEAZKKHQPLru24Xq15frXizq17OKtROUqB3fh5UCTNxn+XvteU4E6xp/XvkeXDNOvBBdkFfa/m2v06x9GvENg9evgTQspok7lzrKntAyb1A0x1SlQXTCcNZ0enfPzlJav1hpx67tSBuopoE+l85KNP/oyLzR2apuFHP/4zbt+5zXx+jFKZD9//PrdOb3F0dMKt5YrJ4hM262vuvPU2RohdnLrI9Rb6/gI/rDC2EaRsGERZ2mWxGmkC5tjhXDFUVglXGWy2xKBYrbb4IdB1PSDwrHMV773/Lr/3u3+N99/9Lrdv38MsbmEnkXp6Sj0/YbtZk1HUi1NMNSGESAg9xlhmszkRy3a7lcLYJKHTu++8Q+hEnmE6mfD5l5+x6Vs2/cDcVjSzOW+99TbvfedDHjx4wIN336VtW47ruQx8ZfEZDJpkKrY+MLRrhn7Nxdrzs88fs+k92RnOYsQNA9GANdDUQsQPwdP2K1Zty6pbM5s0OGtRWIaV5/oycLVUdGnB0d33mRw9kR2S2c9rxpSHVVir6fo1n3/+S1Lq+fLx53z0k494/PgxxmZi9AUO/+bFEyC7IluEl7OSiSUrc2OnXYSJyJQAaLQzE5tFcqZULatdcDfi5sJrKNIuCoTcFSVtgVTGtl2g61r8sOXk6D4hJNoQWT79nNOzM/wQub5ueev+HYZ2y2a94vllz+07t9HG0nWBrh+IWQIvW1X4dknne6ZVxdXlNVlllIOMOJk4rbHaMgwDKUR0gkG3uGaCq6bgGpSp0ETioPCDBGg2JkwcyCFAHzCDoMI5QcgILzNmogdrTAmuFCEF+iExhMKJGyuOgzhFaKvFCxLDEKKkZQBbG1wl9mSYzEiUUhhB8xD7sm7YkkMQYvQQaZNiQIOydKMepIJGNdgGstXoJmHcDKVmpDBBM4gvb/bU1ZSz+V2aaoGpDMQt2xQZ0tW3Gm87JxMZHYyjjINnXreO77Wu9hjzi4HiuM3Rr5zFDyGgwhfNeyTptU0d/lOVteVmNLQTIC7cLJWkkOyNQKcXuGvj2S2vL8XqzpjdQjafz/l3/tf/DovZnNC1dEOPeNZlMIl2vRSCvjEYo1DJCAIUg1T3l0U4kcuuThP6iK0NI8fqsKdebnv0fv/C/eI6HmK3oPJCXuEVQeRLn3UjyP32Ud2hAG5RjiwIl8GoiEWRjEMZj9GCyqUCUKrCyZP6dl023eNIVHvuXhqvtS6PRBZCLFhB251zVM5RNY6qqXBWeNsYTdJgckBZQ9EgEumhWB4qkgdLNJJJ82lAVAfGdK9Cq0jUiS4amqiplMZVNdYZrDdob4lR7hntHH0XUFrE1o2u0UUmzAC3jxY4Y1hvN8ynNeN2KHovgvmArgw6i2WZs3YXUMUYafvE1fWSy+WGyy7wg6MjqtTjO5EmG/vTluyPKWihZGcUWuj8pBKUppTJWdZ7FMKR5lWbj5vXfc+vQ6S7UAe+1/u0/DdpbxzY3TuDqCQf7tuevlvQtqcs2yldeshs7miau3z5i0coYFZlPvygYdJMWA+RJ6ueNp3z+dMrYsisuxVcKJpmynSyIOUp2p1ST+7SpIbTO5HZ8R3uvvMd4rYjxB6dOqbnG7TqCMOGaT2V/L/PXF10NJOMqxS+74uavizeg+9w1qGUxZh67wWXFH4I1HXDnVt3+Vt/87/C++9+l1u3HjA7uY0fEkY5cI6ejunJFGUMAXkklVHGYlXm6PiYqAw/+clP+OD992iqmhAjZ7dv8dknn3J+dcnz589ZdRv6GGiHAWV6zGqDfXZBXTe8+/4H3Ln3lgjSpqIDpjT1ZEZTZZzzXG1brs6fs1w+47MnS1YekjIoHJ9/+YjL1YTppGLorzEmMWksbdcz+B7jEvPjBqcM+EiS/DU/+os/ZtuumB1N+fknHefX4jLhEugiWmuNxlioKktVWVydWa0v+PiTNV88/JjLyyv6oWdxVLPZdKJb9KZqiq9pu9J9k3eLVvmL/CgoyL7CVXFYiSvXmaK/FIuO0R4V0eN0l3MhxY4fIEilthW3js5w1ZTNxjOzD9ms16xXK64uLshkmsmCB/duUxmFbSqsnuHTKbPZnIRi7Xr8VSep/zYxmZ1imym1MaAtogCfyVFu4uX1kmG7xSJG2d73hLDhtjmGSiY+ZxcMYaDzHe0mkHxC5USkg6bBxYQNGdcqwghEmOIiEpKggFmEu2McJGAbEt7HslbJAqlwUgikdPG8rdAx7Ra0qCJmrIDWmhxVqYzTDCERfSb1nnYbyDGiVWLaGGx2pIhUbueKbDTRGHGOmThMXQpP6gVHk1ucTO+Q84R1XzOEC+5N7zBr7kA2bNbPGPKGYegJ3zqj8Wb5tfxVr9ihJa/401dO0uoV/x79U26EHwd/zy8/9catrCakFwKUEU0MRTirII1jejBDzpHgBTmtmxq1klxUUvDw0SPu373LbDotpPy0ow2o6WRno2WNJY72f26C1RUK4S4556jnE45PJygbdoHKvufH/tA3u+XlUG3fTfnloPDFgO6/iPa6SscxEM9j1ZGS+QJrUMVqLaeMMogjDqAPdPGUEqQuIZs6X6JBQc908cQuqduCaDnnqKqKqqpwBeECLfp0xmNsLRkDZQlRUrE+iodPShFjhJqhywZJFf9nnUvdRinQykqhjHCem6YhKU0X1nTDQE4Kox1UBh86vA9UxlAZh9OGxgmftttuOA+Bxmoq53DWQjb0MaOtRVeOnAaMUbhK1lOAtmv57NEX/PQnnxJ0zWR+VvjDkRgTxkj/6CJbpktQN/4umy7Zq8Qsihr5IBCTayrcu921PUDxDtG6kd+/B2FHhBZ2aC379/wqgd4bB3an84wvpMnzdWbVJZZdZNMHfBjQOWIV2OjBD1gS9+6fMZksqNYdK39BU9W0XUfbtiQiq+2abe/ZdonL1YpmtqKeTAjDBp09TklqLGVFyo6MYvAZUqR2mQe3F1xfDySfCR14LZwekyKx7+iNLrYdGbJHq1JqrQLGgLMa7yP3797m3bc/5P13v8/x0S3qesZo45J1sd3VFuUkvSvl0LL0aSMmxK6qUXrLzz76iNunp5hTI+KNlaE+mrG4dcKnv/ilSDaUGyKkzLb36NWa791/QLU4grrCGIrTgNwRtTNYKybqq65lNXQsh46rzZpoGkGbBlittyJX0TmcjdSVlHpbnTCqpzKKWW3IwYtDAIlsNavtFY+ffsFPfvon/PInn3Hx+DkuK4w19ERiEhs5q4XLoUgi+YFM7iIZ0wMerR3OFUPr9C0CuxdTDuM+Sqtdmpq8n7N36YbyhCB1Bw9VdNoy+wBOCcJkkZtxFxfERLYZpbVw7EImKYuxmewHVAjM5segbPEmrsldS/KDPPoebw3KOKxxzOaW7dCz9QNZaZR2oEtFp9JivTMM1NYJehE8SovEgFKKGDwpjrzNJO8Jkb4f6PtAGjwqRiKKBotOWezmhlhSB6o4hmhycQhJqOJ+Esm5EemSUSFWg8oaXarNY9I7CRNVLLViFNmAcfHQJQYQ1XtNVgUNDAlfbHy0ViRG/UCK46xU/Clr0VVTzlljlKUyjto21G7OpDkmqRYbMo2doqL0y9B1BOOFD/ptxpsMiN2wGH/P+6e/smUOUmGviRS++jCv+OtYqflK2Gg/2Y8hS6kfvDH537AL2x335u8ZKdr56JfnrHoR3c66VJKPXyEG3rsz4d5JzXxqsEYW1Ho2w1xcMJlOuX3nNl98/jnH8wWz6ZQYgsgDIZWXk8mEkb9klEGPSFLOVFUlqdiRmK6FgP/0vOXOmcG5giC9sFlUrwjV8gHSuevCcfN3k9vx2t5/ZWr6ENH7NaB1hxpm432VEV7dPhDQovtoNFlLcKe1QWsJukS2SJAsUyps9U6go7hK5L28UybLnq5UyKLHggsjAberxB7SSLomxYg1BqOdfMcyzcbxuIky7lJhwkjadZe1zJKUTUqkWUKUVOgYeFLmo6pyqDbuYG3rLDGHHV9NK40zlumkZj5foMr664cx8AdtLI11xW1C6GHGGDk/gJTwg8f7lkeXa6Zzy4PbYtdIlDStBLsgNtn7gGwf2MnaH3VGJ9nIJrXnRY70pRv33IgWcxOpG38/RKN3gd3OJUvWvhczCV/X3lzH7hjaYFh3isdp4LLrmW02nG7XIiScPTZH6tCiui3aZe68/S6T2TGcX/N86TmdnZGGczZhgzaa1XqD92uyWnPn7hck5UlsILT47RYStFdTYrSEbOlxXC83pNhyNI385oe3ePK0QwVgsHg9oFOkVpCHnk3K0A3M5lOS71EqY6zBGo+qwGpLt+14960P+O3f/Od5+/4PmM9vY2xNDKBNJRNMSsL3wJCQQZ+yKH5n40ohiAShf/ZP/pgffPd71JMGKkOrE9N7p7zzGx/yp3/+p2htxK6rlIR3IdCv1/yt73+fye3b9EZjVKAbIhpwztI4jVIDIWxZtmvWYWBLos0BQ95ZUBmr8EOkqxwnC4dRNdZYKl0xcM1EJ4zTdMOVQOkmo10mhI7nl1/wR3/0kC9+fM7qOjGNijytSXSk4OXGUhYDpCgWLM5aphNLXTuGvqhox0DjHIOC1n/zyS8rAzqUQMNIFaUyKGUppZ6ovEte7CbJ/SZ+1NwTI/qohKyf074MI6eSuipVTYe8pvHmPH92ztH8GGxNs7CYZ+c4ZTh98C6bdoO2FVobhm5L321pt1vWF+f07QbXzKnmtzm9dQTrazZXvUysypKI+L7HNhN8CFwvV9xazFFZijFiCFhnUSrSt4lUzLdNKff33jN0A30X8NseFT2VUsy0RWcFPuKHSDRG0LoxsAO0tUQVyEhAmKMlegkclVFgJYwmKnIQH+EYC+cTTU6RfojoRvp/pDbGnEipCB8bRVaBIWUixTbMFf27gh4463DWYCuHriucmZByT0oJqxyVdlTGYW1N08zJ5gw3GGyCftMx9B1dt4Ua4eqlb2FhxzgZj4jPGKTt/zv+6wZe9kIQNaJGaveKssK9YSDwIv9mH7C92F4T9L0Uw70qYCzfMsMg5fcs1y3/4d/9GZ9cViwHR7QVfggSKiqF31zzd/5L9/kv//YtPnh3jrWG2azh6OSUy8tL7t+/z/vvv8/Hv/gl3/3gA6y1rLs1jWkAkdaZTqc3ECpTFrkUAvPFgulkQtu2wmlN0PeJP//FNX9tOuHYHVAwdtF2Pniw6zcJ7F5VwMIrr4O68edXoH2v795v1UY1BvkYVYTWRRMSTZFCMWSjyNqSjEHZCq17Cd6UQecofspKdCktQpuQOVEstELpk/HzRCZJgzbkgqAaazDW4lyDtg6tFZZIjlI5a82YAZMRGdDFckzSm9hcbBsTOhRVA52Lr3csmzXNECTDIoLIhhAiKSamTcNqmwjZk2OgdpWoR5AwGIwWzdST+YK7t+8RB0/ftkSfCCaKwHyMTJ3DlKKMpp5gbQWqVLimTBgGNt2ax8ue+xOhRwWfRNQ9KZHhUQqrZVxpNbqAHGSCckYZVaSXSiBr9kUTkG/cx/rg3y/y7wS1Oyi2GPGJLHSYMfCVgpjMm/LW33gmfO+dd2iDY9VperXh9O4tgkn8+Oc/5ic/+iOO53D39ozvvP0WTRUkmeNuo/Qp/Tby/NHA/eNIXAZoA4ujCiK0OrDtOj77/CdcPP+MX/68ZmrgaDZlMVswrSuymrBuA4+er/j0458ybwbu3Wo4PZ6QUk3bwtmdY7Ztz3xa8eDOHFtZNjHRBRFcdcqjlQiqWqtIUapWf/uHv83v/JXf54e/9ZeZTY8xuipcLhn4N9thFJ13u/SYMsY6ZrM57733DpcXz1Emc3T7FrNmQjuZMVsc8bu///s8/fIpy+tryApjHae3b/OdDz7kzp27HB8fUdcV508e8+mnn9HUNX/193+fqqrwfqDtAm2I4CpsMwXtJPhE48h0nWfwLc4HmolDh2OybmgaQ94aMBpTKXSnicFCyhgjchJGa2ZVzb/1b/+Ah595/uJHLX/xxTNQCVdZ5tUEpw3aAi5CGujbXgKOaS0Vj1VFDBrvy27tDe1PXtkK8jNWcGltOShAf+XrYyqk24QEGAWtU1EmtJjGCS6hkCpOii5TVkLu3+2cy67fOcO22/Ls/Jz3336HarEgpsT1k0dUjWazvuLJZw85m5qC7gXu3Tlh0/WkNEDsCH5BiJYYKx4+esx0PsNYyzAs6b3YMU2amqwyurJUdiqTaqpIoQat0XWNdjXGNQxDz9AFhk7cWTbLFkJkYg3T3BEV6JREQkArYiE9CxInO/pY7oWcIzF3ZANZayE/5yR0s1j4iV4J967SJC8uK0MI1CkzDJEUPSfVcUn7arQr6YjK0kwbiAOTSU1dG3LS2GzQ2tJUDW27oqobmukZJsCmvST4lth62tUSFTV+gKgT235NP6wIsWdoBVqcHd2hXtT0bUvN6puPt137dSzdBxIo37rdCCN/jS0TYuB/+X/+EV8877huAyFPwdYoI2n3VX8OiM9mzIb/xx8+4v/1Dz5Ho7jqErfuv83v/s5f4X/6P/+fsTg6onKVcEQNKJU5OjlCa5F7ss5KZWY5E1dVkv2IiS4N/Nv/1r/B3/t7/1/+s//8D+iGjmll8EHxf/pPn/D+u6ccz8tS9VK1yM1fVf71Bl8vd9uv7zqM1foppRuBg2wHdEGPjPCCnYVcga9AO5QWmZNeFcu/nDElWzEGGOPxlVL4lMVbPJVAqfSTKu4X1lpBTa0r6J1s4rSVIixlNNkY2SCiQOmSFcklayMC5ypGnIslMyLHFnRehKm7YSAm2aAnpcAY4jBwvbziaH5M7WtWbUfOUuWvVabRhrfvnHL7ZMGD26csTs5YrlZ03vN8ueSIGXMjPMG2banrmtl8hnKVbGJSYjapaSrD4AMfffoMn8AYh3U1Scna4KNkIPJYhUva2YwdbpA0UhSitZyVLlJgY2p1tBxL3NxY3ahmLsGdvIeCwI7buHHz881H8hsHdsbNcKaiQmHdwGw+wznH8mpDyhFtK6bzCR98+CGT+TGbtmO79WzWKy4uVqxXG7bXj1GxZV4rbt06ZTFbsFx3fPnkCt97NmlN6LfExmBNZjptOLt1ysXlltXqnM8++xkprCEbcoau84SYyTpRTRTtIJVZk8mU6XxKWG/pO0EAMBxE3pL/N1rxzjvvcevsDrPZAoU9SP/pMhGNHf5yO9yIxxjpupbHjx8xmVSkFJjPZqR+QA0JlzSzesbtW7do6hqrDGe3bnF0fMrJfEHqPWnwEAJRDRitih2MAWXJaEKCuw/e5ssvt3TdJcZUOFsjeyVJuymdMToRhppgDdYpss74qOhTJGZPFyMxBMiRySSgDNw5vc9f+91/kaPjp2gjg/wnTy4xGYwG44wsV6pIZ2RdeFn9bieitUVZU9LIacdr+CZtTHeTCyhycAFyzjdTLDsUYExtILvfRPExzKjIbkecOECfUKSyGxJfQblxx5swxEAIA94PKNWQlKX1ni8ffsGtO8d0feLyqqdJFcbVNJMp06MpNUrSJm7K5bYvqagjVmvhjfiQhZKQJZi01mBnM2II5DSQMqV4BkxVYyoJ7JSRjUeMIiYshteQA+QQ2bpItAprFEFpgpZq0jD2BaJ0nkoAl3Ii64DSFSprog9jx5Kh8PFKFW6WFK1ShqpyzKcTmkpTV4bJZMLUSTCujeNqHdkmaFVmcdQwnYquZN9FISbrilm9EOTbVWibpCI4Z1CRtA6sl5f0bcdqvZZ0ddgSQkuoI5U7xrkKrSypcPteg9F8g6Zu/vMVh30prQcUYs1uXN48ztckYl/5d7X/8cp0rPzc6fd+1Wfs0K7EH/544PlVz3Kz4ZfPAlfbzBAts8WRBBPGYF0N2pBCwKdIiJHKSMCxDgZrBY25vLzieLHgaLEQX19XkYIX8XiVGeWurKnQRrhY+WBeUFoKbL77ve8RQmAxmzA/WuC3K64vnvDH//A/lWO9JqAaN2A3r/3L/XCjSjGPXZr3AOCrunZ87wuf/VWk+F+lpZJBSOllJEYXhwitZfFX2uwq2JQuDxV3Y0MDtlAl1I204FgVXwKvIthudkYWRbNtJ5VS1khVRNxHgMMIChgzUtkaY6FfjsGjlb5QFPrTgYD3DmRNu7QwSMYqjN8L6LsNQ4AcM1VTo5TwNmujODmec3x8RDWdY6ylGwbOr652Kd1R17OqKqq6RjtHKGoAOQXECl3m+yeXLbaZYV0FaLowMHhP8ANZNeWcxfVDvfAYUc99ilboJKPWZ0rivR1TQpWgWt24rvrG+DmURjlE7A6vyTdpb5670BOMqWXwmCVVXWG0IaaAUmLuO5+f8ODt99BuxvnFJU+ePmW72XJxcUnXdsT+gqo2zCY1d05PmHXgdMfFc08OCp88KUScdgyhJpE4Pj3i6npD2y559vRjJlUGakKILFctmy7Thw5TiQm6sZaqmjCbzVj2gdz1wgEzMuoEpQGjDaaueOutdzk+PqWuJsL1UaNP31gmLh39qjqAvZ2IwLAhBLq+5fz5c2rneHDrjmjodR4dMrWtUbMjnLYMm5YHd+4xmy9oqpo0DMSux7cdIa2oa8d8NkEbcZbISpOV5Z33PuDi4iFDHzHKobUEfWYXuIoERewn5KZGKYd1hpBbfBzwqaOPgRwDVkVmE002ige3F/z2b/4e1+0f40PkzoMsti+AMgrrNDoVHbOUUGhijKQUSDkzmUjaV1uL9gL3pPTNRRZ3HrWZsmi/2Pk3g7pDXg15n9bYBXmqBHp5RLjVDr0aS+2zGie7kfeQ5PyiJ0WPwpHQ+JhYbzfUG0c/QNcNxKCoJ1Pq6ZR6foSpG5S2ZGW4aJ+jtaEu/os+yDH7riemKDwQGrHaynIPoDJGa+F2WvGR1dahjExGwssL+CESoiIFsZfrQgJrSEYCVp9zKRyR1LNSgtzFHCS4I6F0id4TxCi6fuOcnFIiRelbrTRVkRByleFoespsamgq4WOqqjBCKov3LaHPtETm85rpRCrgUurJWWON6CDaYVoy616Izg0oEuGyY+MjSq3R5oqUDN4PYmN0BHVzJMr4KRO9x3vxmvx27cU8Ji8FS1+N1xxEYG9Icn7tkQ4DEQ7nH3Uz0DgMTF7g0w1eUuZ90DukOgfPH/75hl9+ueX8asXzdSYph3aWajIV7qQSgXdZ3EX70afIpFLU1tLrClfSVNfLJc5oGT4poHUlnM4sOl+C/oh0idIyqJK6WTCgNdy/d4/KGh7cvcXbb73Fzz/6MT/58z/mj3xLGsVab5y2/LI3jHj5yryaJ/fqYG5fKVsQldchc7vg8Ntf51G/TedUshPlG5SAS4K6XOzAxIJtTKGOWlC7GtishBq0CxDUwdyYdq5JqsyrKefiWqEOgpR9cKduBHZSoTxq2MWUd+LCh92yq8gdU8yFKqPL/Joo0lOjvmaxKEtZgti2awlRkbNo0eZkyEjBxGzS0EwmaFcD0PY9V6s1R0eLAmjIul3XDa6uMM4RB1+C50hWCWulzy7XPfcXpzhbkTP0IYi7UAjYil1gN57PYaXxwdXbPT/u5wR5lWuYDtKuvPDOm/e22h9LjdSgF4otduvRa8b0K9qbB3bmCFdNaSwo/Zyrq0tyTiyX1/RDwNgT5vPvYu09XJVAbXj67DEPv3zMdtViK3jn7TnzozluMsNO5hhbMXjPtFZEvQbdovVWJFA1KKvwcUCbjLOeyq7RZNpNx9O+Q5tP2PjIsu1JeovSMiE19YSYYAiRfvD00UvHOnFwiBFmsyNOT+7zl374V7h1dh9rp1TVZCfkOGoHSTcLyvdil47B3GjmfHbrjP/Of/+/yz/5w3/A9bNn/Gj5j3n3/rsMbU9YbZnYhs31ina1QSdI7YCyHldNaJRiWK/48vqCf/qjv89//b/xd3jvO+/usuqubji9fZe/ce9vcfH8IV8+fAg50G562SlXFleJ4bNRCrYzZvaIW8fHvPXeWzx7+sds2jVd3LAZEpVKzKeGv/LDW5zNTjk7/g4Wi9bf4en5x/zJn39E5wN24qhqS20hh4AvosU5aUFOU2a98uSkqRtF01jhZxiDNd8csZOCjGKXNqINCG9Ol9tkt/Mr6JxCoZJCFW20sSq6ZAUoPuMkU3jDZOFdFLFgIR6XiT8FUtxSNTLBJj8w5IDKiuPjM/7qv/Kv8hd//ueo2vDed854+7ZGWwfakch0fSapRLYwP7pDuFpyfX0JVKSwxZd0aggtVTNBzRu2qyXd6pp+u6E5PiWmqsypemctpLSliz2bbcdm1RK6gd57cspUaAaj0BayE4eJUDTpUo6EPkm/GENMnUiUWEV2NYNW+JzI2eBbSWerJBWvRYWAmWmYLOZMJhVHM8viZI6tItZ6jArYkHBaU88nrJYDrQarM810gbVSsgFjajdRpV5sikJLDlfMjm5xNFUkY3iW12w86KxptOPug++xXW/YbFb0JhDNGp8H1peeocrEMJB8/43H20vtjdfrUlm62/1pvo1+46vbzUXgTdvPP7niH/3pM/7xoyldDAxDz+r5M9Ztj0/gs8PVEybTKZPJhBgzFxcyr5+d3S5V2b5s0BT9EEghoGyNrhqq2YLT01Oa2ZwYPZurC+bcxbmKqqol5VfI68poYuh21UwpxpLSU6iUGfzA7GjO9OT7aN/zk5/9kr//j/6EoWenFfbKtl/x3qwbf90Z7W/Rci7FgUVjclzEbZYsy5hhMiUVnpORzZ7RUkmO+GWP6DoMJCoyhqTYedDmnPEhiHSJAucVgw5gDDaL/aEUZGhG3by8SyZSNr6qpI1lg0hMOxH+3bkcXANThOOTAoqntSq75+wjqe/RWew+jbPYUGFUIJcgabtaopSicRVvnc1JUWgnxk3YrJe0bYfSFp2iWLNpQz94TtyEuppgtWU5bIlGk6xh8APOGaraEnJgXjVUxjBET9q2+F6KBL0JGMRqMeu9kMxo9VY0DEgqlooxIN4M4oRCaJAVZtxsZlB7+pZSSjjNxYNeZdlQZRH/3H0eUBA8Wct+7Ry7z5+umMzAJ3j85BnbbkAb8P2WfogsV4knzwInX6749LMvePr8c3waSMpTTTQniwU/+I2/jmssfQx88ugJl6trrpYDfd4Qwxale7QaICmW6xX1Zc2z50+4ur4Cet55cIyzDt9rhk5zedmx8h1b35PUwGw2wVrLZrtFB8Nm07Ld9qAzDovTDldVNPWU27fe5t13vsfZ6V0mzQxrrEwgRnpx7D41Duosuw2VEyLUWERcY0RrjXOOo8WCH/zg+/z//uAPOD9/TrWAbrJmeb3i+fNzLtuW55fP6LuWSVXTrzasQyL7wHe/9yGfPXzEZ48+53z5DOcss9m0yANUUtibEl23wRjH0eKYSSNVx+vNhi8efQHZoFLE6sTtu4l33lpw607D2fEdplVibTK6q0ghUE0tp8cLfuvDf4G3bt8hBctnX/wZf/rxx3zy6XMeP73g/Q/fZ348x1rF9vox3WZF5xUpZHQonLes6WNk6BNKRSo3phahqb75bjaGkSfCLlUhApHpBtduTMFm9oiczpTCiXFRGIUp1W5XTJZd6CiPolHF/zdLZa9tcHZKryTd2bYtAM/Pz+k3S959912OF0f0257t9pq2n2EioCNxuSHlilzVqMWCLmh0s2BxZ8pl/5h1u6TvWrTW1PUE6ypUSoTQ0262bFdbhqyYLuZUlcMYRVaGrCxZW8LgGboNfbum63wZk2UTb4XsHJUSlwYFUUE/DOSkZac8DBibKLV04gOZ9H5tHOWAYmJIiaSMbJgWC5Rr0K6imkypJlOUCqTUM2zPqbQlOwMeYnRU1ZyzswV3795Hq0DwHV2vcLHDapjNphxPpsTcEeKaoc1AIIaBGAcq7dAqY2xmNq+JsaMfIll7fLhAJYul5snlFUplmurb6Sa+3F5G63Yh1o4rs9dO2wvBchCNfBOO3Hjs8Z1jRVBZRNUBUvXiLVbSQf/Hf/8RXzzZcLnsOV/3+DAQomfwmUhNIDPkxHa1ZtP1VFXFbDrB+wGAtm3FMUgLF/N4CtFDSBpXTdh2HfenC/6lv/k3MdaiinSFteIGFJJ4+tpakHyjtKTECmUkFoRklHSoqrpUhA/ErFmuVpxfXLAatFQ7j+efJI+qXtWv6Sv6+CuLIko6O4/H/rr3HiJVb6A1+JomgZJw37QuMKJCMkdpb18VY0QZ2Ugr40QmSQcpivLynTWFzJ9Fi1PQsLwTYFc5EcMe0dbOFV3Prw4U9q498cZjROzGvxlzM/27ZwfoXYcqENQ4S4DY9z0+SEV7iOW8cyr894R1DmsVRmfqymKsouu3NE6Qy/Gz1ts12mSOjud4PwgVKCd86DAqoVLA4cULt3jTivKEp92u8dstqoxFnfcZnPG6pt2dP6KgN9HKMd26nxO0CDfvaFx7NHjkne8rY189Zl+ZAv4V+J1vHNh9+eyayVZIkZvNlgSicp89MWU2bc/T51dY9ymffvY518vHGCcCwFVdczQ/5dbt76NcYrm95nr9CRfLFcttT58HcurRWaycSLIQtX3HcnVN121RJI6PJpye3KFrDZs1XC4l8g8xkkm4yoGGbdfBQNFvC6AhBNHxAkPTzDk5vsXdu2/T1DOsrVHKFh5AQXVy3kk4jI+UhVeiX0glpJSKfpDh6OhI8ujFdyXHSBgGhraj22zxbS+Qbz0hD55sLNl7nj95wvmzp1xfX+Jqh3O2WK7oclMnIpbBR6ytWMwXVNWC6eyI5+cXfP7F53gfUDqg6sR3vlNz74FIbRAipA6VIiZLFWdlK6bNMbePf8DtkzPWmyXLzT/hybOHbFpPM51w+tbbLI7nGB25di3nuSW3iaD0TqRW5kItEH3Iu6KUcXB+05YEXLoxp+4m0VekUsZcai6p21z+LT6Wo4OG2q2WSo1inarshEo6QyusqbG2xhgLShNjYigLHoizxvLqmtpV6AYCA10/4BBOo1KQfS9VokbjsXicaB9mSW/tUhlKUqApeHLoyTFATuQYGbqWnCLNtC68l5IOCQlyRBHRKmNLjKqNAqPIWhFQREQ6ISKpNKJsDsgRVzmUklRHDJlcqsAUIpCagKyFSKy1E55f02BMg3E1xk1QqiIlTfCJ9XXLtJmQkiE7RYgapWqaqmI2O4HcMxhL3XSYwrmaTCY08xkxabrB47et2JaliFKZyhRqhAZlE1WtaCaGwcpCYpRC60S73WIcNJX7xuPtpXY4s7+2qYOfh3SBEmUjGpBvEtTdlCW5+Z59qdbNr/diwBGTZrUJfPak5Y9/vuJ8KTQU70vaPUcCgvRkLSLmq/U1ahiwXScBQSF0D8MgdIjyMUZnQhZh1kpp6mbC8ckp77z77k6CyFhDzrEI6GrG5T3vFkTK/XdYGbhf9BSQQ+KnP/sZXz7+kuvViqFsOFQ+6IGDIPermuIVqavDg4zfJx9c6q9YPF881qHQ8TdpO35a2pPlbxxLqT2St4NtSmq06NvtQiglKXo1bnRLOnRMCUowFQsVWyzCXk6n7tG3w9TfiCq++Dh8/at+MtJaxk5SiPNP+bv3vlh7ik2XBE5j8BdRSlxNumGgHjrBvkygsnM5rlbEmGjbLUoljjaL4gghEmRC1ZDAk9jLc0bjjLhqxOTph45hGHApY7URmafCO8wl0D8I614bYL0oM/SKq71bg17qp93lPky/skNwd5/wK6ynbxzY/fQXjzDOoLTCh0jabMr4kmL8q9UVm+6n/Pijjzg/f05MHfcf1FSVo2lOODp+j9nJbxHYQnjE06s1T6+esh22UrET806LJ6mMzxmfElerJTlEtIbZdMJf+q1/juAbliv49IsL/Bcf018l2tBiK0NMieV6Se8H1n3E+0TWmuCVeMUGTdMccXJ8j7u33sHoBq0qlB4XOhjNCVRRm949F8vuXO0JjyMZchjEH9VoOFoc0R2fUFe1aD05R20ttdLMm4YYHVNTYYFaa2ZVxR/85/8Z7mjK7OyY+x++R91MSUmJ5IWWwa6zIfiB6WTC3Tu3OTq6izaV+NPmxGa9JdrIrDL89X/xDk2zYLuBv/jRQzbLZ8Te4zhmUsGkbmiqW8zqH2Jx5PQpA8+IasPdB/f4/vGHNEf3qBqLYqA7jvx4uCATidrhcyYQiqCoBUQMeBgCkAkp7VC3b9JEuVxuKa3Ufr49+Jl3k9YoBCkoXVKKrIvti8olQ6bLmjLurszuBhYa7yikaairBZWbFWFQ4bP5QQK7s7Nb6Oj56U8+4oMPPuDs5JhKZx4//IKZsbjphOnJHTaXVwzbjuWTDXmhWXrFxSaKPRwybrZFmT+aiIoekyNOZczU4qYVl9crslbcbu4JolzcInKUoGc2ccRsSL5wWKxBVYVfRyZhiCSiygzk4mKRcZqiKSacmXWfyEZSABrxYkyuqNxbh7EzqnqObhopWqhrlLUMPhO8pm81Tx5ecnSWmS0MC6MYgiVGhTOOupqTkiOjOT4ZWLhjNBafItWkph96/JALyiNVfq5yTKuZ2P8ETx9b6glU9ZTLuGQ+nVDTEAapzLUqor5F6v81o7DAY/tn9v/UN3672fbInVKxHOlgl7h72Yu7969r43EPscP9t+o6w49+2fJ/+A8+Z8gDIUcGH8iAdhVGSXGM9x7nKo7ncx49frajk3jvOTs9wlpNP3T4YRB0JiR6n/ChkPBj5L3vfMj773/A4kgW2TFJ1PdrjJuijZNkXopiQZcTMfiSXhSdsjGVF2OE4AEYBs+/++/+u3z88cdcXUrx1ouL6Y2wN9/UDfsq/bkbwfKN7tvvIL8OFflVUJOvazFG8WlNEoAYPaJb48b4hc2xEt6s6Nntqh8Ye2TEjaFw4HYR1VjQkHapwHSAvo3nNQaaAkzsz/dlxO5VKdhXBTsyhe5bEYJXglyF4EWL0wdAE1La0WtIIuNCzjy6umbTtdR1QzNb0DgZW1VVMQwdfT+w2SxZr685O73DdDplOp1yenrM6J27Wa9Fu1YrJrUDMsEPbFtFHHpIQmFRKFyp/vUh7gO78WfOxXu7BL957Pmxv17ohLz/oRTEmMpwe2FM7wCHsul5oT+1FtmuNy2meOPAbu2BMKB1pq40IUV0BqcMWSmGuKZLS0JfkXRPJrFaR6b1HKcmtHPH89Vzsu65blf0PhCTJicHGUwWJEwhtiSCbGgGH4h9Dyni7ITT03vU1W1imnH3gZTcP3r6BZ88/BlNPaVbr7h+/oSKCpcSJif6IbDdStSeA7x7/4TF7C7Hx/fQqkZhUUhOfO9iwE7DTlKx+1vn5XbzIvwr/9V/lU9++nP+4d/7+/ybf+tf5tFnDzF//Gd0P/8FTXVCJqMT2JxxZXfxL/5Lf50f/fKnPL96zv/43/yfcHp6F+McY24gJ0gh0rcdOXli7Hn85We0feb58wtSiMQQGRK0bYXffId58xa1q4BzQpEhyTmJeHE9wzrL518+JLIg4Ll17y2Ov5iwOHrAgwcfoKsZMfbkuKWanjFxP+DzR+f86UePiToWP7/9jjIl6LsBikNJ/BbrbA6grBa+gZKdfDqcQMqkt9MG0mNBfUaPAV4ElC2E1mHvOaiVmNarEsjYIkqpFODwQ8ugHKE5hZSJwdN2iuUmcH61wQ+Rv/Sb3+Pv/t0/JPstZ3PN1XXk3fce8Nbb93DpLihHM6toTioeXXYoPWN6fMx2aLHZErzm6rJnUWuMzvSbSL2oqaYNxlUsVxtCEmeTaCqSboi5wkdLCF54HEYzmU7xvheRzMoBjuAjIQivJeqMzwqPgQCTpuHu6QJdJ4L3hOhxKYMuEkVa45opKWpBuXtP6iNDt2Wd17CwqDzgcibahN9EwjZw7+53mB/PMJVm6FpCzHLhdOLhZw8x2qN1QJvEZNaQs2V5ec22XdGFFZv+GuUsjTNkY7nqE19uVhhnmc8bfBLLqsXkhKNwj+hburbj2fqagMKaKdkdffMB98pByFfAMYep1q+abIuwLjtewFcd9IVjj+9PI/z0+q+aNP+7f/+n/OnHGwYs/ZCISZFNhTVGFs0EVT0j64H1esMXjx7RtbIh1Sbik2UyPWLSNKzWK7JN1BaqSWboPL0kU5jHJY+//FwqDSvL3/7X/3WcUVItGQa0qVHWkTIM3Vq4q0pwx5RKVboxggTFQBwGcQ7QBl1VvPvWWzx/9ozl9TXBB54uPRebwNnMvhw/vCqOOwyU81e/trzhK/v28Li/zhZyKlqcWSJNNXLaSsmBSuhxjk1AEv531mqnX6vVSNTPGEQaJcZEtKOd5hj2CWfZ5KK3lguWWtCxMb24C/BKt0g1rWQ+whAJIewKJndfG0hR5KLEVUKoSgqwiNab8MgkKNXK4AzYSjOsM9shlVMXJDHEvvCCAwrNxNb4wTP0a65WW6zRXF4tefL0nL5vmdQ1k6bCGsdqtST4nhQHzk5Odvp7Tx9/xr17d3FWYbVGZ0hZ4aMSWSwiMWdCv2FSV2g0PiT6ktg5DMZyEiR+TALBKMy+D6JvIKFjSvZgA3IYjAv48Ho0+LC45etS52N748CuD8ID0CR0EAsjig+cwO6pVBtmtDWoLBC894G261itlzx8+ktcDW2/whioXQ3Z4ENGZwkUtEoMJQcfQqRrO4bNFpU8zgX6TkqGtXLM5w23b90vsiSRdthA7FlbgaqrrKhyph86Qsh0OWFyYD4/Yz4/o6nmgCB1ihF+TfsLVnZEh5Inh5H2/ufuChJD4vjkjJNbt6nnMyYnR5xsO+7ev8vy4oIhRmKK5CHQWHEtmB7N+I2//M+RZzVPrp8zOzpGW7uzfVECT5FixA89OYu47PX1CnTN0fGC3/u93+PZxWPi4JnVDfP5P890egufNpwvf84QKhKRbCK3jqfcPjnh9OwUO1W0aUPnt2xaTU4VZEVKHt9tgUBl4PbZAs0Jm7ZnUosHqlJjYLfvgxgzxo7YwjefCMWBQH5qY8RLcqx0LZdnN2uVqqGshHDMXtuxxMWy+Gol2nXaKpQTYUlDorKGSius0jgmWJ2xpjhzILdfRtH7KNWpxrK8eMbJvGJoPb7vqLVCh4HYbvGbNZ3vSVqjJjMpfMiyQ++3G6FxGifVrzZJ0Y9TNLMabSsSBrEQGh+WlGSjAxpfJEmMMRCV6I4pUNrIRBwUMQg3JekyfaSKZrpgOplRT+Zo26PCUioZk+eonqGcJQCNEaK294qhvWDwkRgSvVpSW0eFIxlFzJqu7fAbz1tvnbGYN2ASQ7+EXOzbsmezabE2YG1EV5bNdkWKmuXVJWrS0/k1225DVU9IbU/c9oSY2XYDjdK4usKHgA+JEDUpW9p2y2rluVwF6tkdVF2xDvU3Hm/Sdlu6V4/Jg1fs3nFjs7EfnC9W0amxxPsVd8WLThHlyX1iTqm9pIm8aPc8KbNuA3/3T9b88nHPso1SfVxWI13y9FplSImuH1ht1mw2G7qu3x1qt16UhdlYQ4y6cM/GRV8Rc2bwgWoui/d22woJ3DmMsVLAUnhU+14oi1IMMtcqLRzYfIBtlsXLWsODB/f56c9+tjvV//efPiPFxH/t9+7euBaHV+xVadJ9enUEwl54zYvPvSbV+rrfv20b06WHP/dy36UdLPjjXxPj/LYPLhRCD9+T+w8+5yDNIddlLPB5+fxeXN/2iF3acetSHo3sx0wJBTl9VZryoMJzXBWycHhjzDtnDN97nCn2kCX4HBFd5xxhCIQQaH3Ps+fnbDYtwyBULyl6U5CSAB8xQkrkGOi2K4YcqJzGVRXW7dHBmBJuBAiQQNt6j3Ueqyy1U2x9FMcdfdg/SEoZyhpUAr2DPhhjhVdd8z2a/0Ll+2tSs2O18ot6h1/V3jiwG0LCuozNCJ9IA9ZiogJlhIuuRMPK1QIxGYN0frvEXic+/vwvmC9qtM44q5jWNVZV9Bmy6mXHoiM5iD5dPwTW6y39ekWOgcrC1VWH9z2u8kwXJ5wcnWGtpWkqPnv0M7rNFbbSqORwGFwGNpJ+ICR0zJwe3+VocYuqmqOU2002kprKsqMsfIZM2uXGx/RfSgcD/8ZFg+QTVdUwnS+Yn56Qa8v0dMGDtx+wPb9g03YMQyD2A4vphHraMDud85f+yl9mdu+Mx5fPSUoTiy6X0XIT5iy7Wz90QCATWK4uOT6+y4MH9/itH/42H/30L1herSAZ5ou/Rj1piOtPefjsEX2siDqhbOD+g2Punt7h7uldpieOzXDJ1eqap88G/BDouy3r9QWbTjOpLW7hOD2eo/IRV8stx4sZ16stqCIIrGDktEFCaws5ob+F/IQWXj/aKGpjUSQxnB5l1lCSOzVaFs2yGxozFFkX3thu0TRFLFULD67WIkBJZGIMThsMFptnVFVHVYHWsUygsjgOIVLVDWmr+ezjX/Deg3u0m4ZnT845OVJMrUINA367Ybu+wqNI84FU3yUl8EPH5vqa+XwishBJdFiMNdRzx/xkweAVXR/BWpSxRSzWEZPClMB2l/Y3RlwckMqqnDQpZoKHGDTKFgFUFDo3zI4eMGlmuLrB6jVJDQxxg06Bs+kE10xofWJSV+RsCANcPTnHD4HUJ/rhmlA5oqqJ1hBdQ9919NuO+fRtFvOKxMDVlSdnT46JPGS6TY9zgeAycXDYVBE9XJyfU99WdMOWrt3gHAxXa/yyJRSera0rXFXR+4FtZzEYfIblsme5HLheZWbvvYU3luWowfeN2zjp7gO8UesM2POFuBmAUcb9jbzLDSRvhI85GI/cDCRemORvHHvU4cnxpff2Q+LJ+cC/958+IeSMta5w46xsikYBVCWoy2q95vnFuXDoYimgUQU50HonSWGtxQ9iSi4ZlDL3ZUXXJ86aCc3OD1aJ5mLODGxIKaJjLBuOvQBuHwOUyu4UQ4lZFMYYMrKJtdby1oO3mM/nO5rLf/SHX+J95F/7vbs3UdSxv270246tVxCl/ab8TVG5V6XFX4eU5CyY6jdpOe/XkJykil5Sh6+HJVPZsCaldv+WwFV011KpB0uv/FJ5N2cjDNyDFPa4tu0Fk1/k1B0WTRxuWvZUmP0n7ZAmSa2Un+wQrxgTwceiNZpphx6sLnGS3qXoU4pYOyF60YZrW+HExSQSLnVd46zFailYCN4Th4HsI9kPbLs1KXScHh0xmc4wq0TOCh8DVRYit9Ja+Ic5E33AhB5rM9O6YtOLlt7hJZHvX2RoGLlz+kb/HfqOy//HmCGhdhQgdjHFi+1Q3P8wuPu1y51oU0HshSeRErFSAiFaIGtS0CRgVhmsrVAkuqHFh54h9azaLR99pJjPLFWV2Wwu0FpRaY2tFV0fCFkRIrQDmNZj2LC0ina5JPgA2fKnf/5LTs86Tk87PmhOmDZT6srR1DWfPvyImALGlI4IEvn3XSIPPbOm4a07D7h3911Oj+/QVDN2JPqxnw86zqjRz26shIGc93fMS9SCslPXxqCcQzUVyWRmR1PuvX2Xi88/F/PrIBYqWsHi9Ii77z1gMplwcuuUNDHMFgtykuqyru+Y1qUqNkb8sCGlAXIkhI7FYsLZyZyTo4aT4wlXV4948uwZ//5//B9y+8EZ6/4Jn5z/CUrdB2fJduDoOHP39oRbRzMeP13zi89+xuPHD/ns08+YnVyx8XOeX53w8ceBe3dv89b9I6b1gvv37vObv3mbk9s/5P/yf/u/4gfPSKQQ0VFB75pmQkiBlLs3HV4vNaMVyqjCcxTdwd2uO6rdzlV2+lpEiFXa84qNwji159EphVJSkDKf18xnNVYlDAGLGNRnr1g+iyzuNcWORtCyjBDHex/oL55x/eUjHn52wXB5BdrhVcPZBw8EDWumNEcLnl0/Z73tCb7i6MMjhkERNxvidkuoa1CaSiumR1Nms4rZUcPRyQM22w6f1qTU0kxn2HqCsVN8Mvhtx3rb0rVbwrAmhg0hFhFhpdCVUAfCAKHPqAqwol5fqQW+FUmT6AL98jHr63O67YrpzDI3msY5alOBzuSssVYxU46hb/Fbj20MabOkj5prnzGzu/TblqHdEoZeYhczQO7Ybq6xqmY2OcF3QoMY+i2fX1+wPdmiVMXltuMoLIhRk3xFt22JVy1pNdDYimbiybrnfPkcnyqunht0MGyHSDObol3F4v4ROl1iAGe+LsX5de1mUPdGLWfYSRqog+cSEF5RqZlvBne7NgYOBzvyGzof47HjmPcBEv/PP7zmP/mnK5QGq50gIEmRlEgxDcNAiJG2bWnblsvLK3wIBS2TxVIX/c9bt+6wWa3ptx3Hx8e0fU8eWmxqyaYR+Yek2PjIZrtlu93Q970EiDmhVSZ4T0obrAvMj05FpFxpdNY4Ny10CERmKAzkXEzbiw1UiiJRdffePT748EM++ugj4Vj6yA6uf33cs//lxQXwDdG3F1Gnr0JUblyXb9ByYreBH+e28TOMTgW9AlQkE0gqEiTfiQTEmugEVTVZEVSmshprNAaI41hSkrrVCvFzZ+8rm1W6gTYdFkYkMiFFQpHKCSkWpEvmB9lcS8AmFBd2ElNj6lUhPtO74UuCIlpd2YbkV/i+YyBio8JphTMWVE3bdsSh59179xm6rfhZD4lkomxalCL6gaAzPYk+TajygK4rqsoxeM/MwXw25we//dust56H158RtBQJqZTRMSOmD4aRn+jbFVXVcLywrCpDN0RCkAzl2NKIfub9M+MZjtQJVUqSUxo3nCPKKfJBI5cuM8YXxUWk4K7iFz5eQvXPJrCbUKNDj0mRqjJ0aDKOpC2NZWcK/v6dDoWhGzRfbBukEMeT8Ty/umI7aJoKyB3OOaypcPUE/IbgM13UmOktbt064s68YqG2dExYbz3Prwc+f/qcNjuCttzbPsMaUdqumoq+9/jg0S7hh46hy7SrSLsVocLFfMp33vkeZye3aepGxE61JRtJWY1q/EqJaXAYBmIQU3bvPaauCzp1WBk0yh3IRVVOEdNAjD1xaPn4Zz/l3skZ82mNqi0+9oTBc3Q8o542LO6ccvrgLufXl7TbFhJS3ZodovgPPkZCMSl+dvkZg2+L1Ukm+g3L6yd8/knCsuStuw3z6S1+/NM/5rNzy5BWDCHj8hqlwZB460Rz7/g508nAzz6/yxePP+f8+TkpwdH0hElzhM4LLi8+Z9s9pA8bfuevfgd39F3COtGuH+EwVMVwWVuF77Yl1QK5WHWpb7qVBSZWtJgySlTSU7l1tED3ZOGHjBXKWYnwpikon3V73pxWqmghGurKspg4plVFVXTWnEpoGnJlcHnLdGpxTuxsjMloq3Ha0LU9vtuQY08zX/DOd99jOp+h6xpSh6saqmbC9OyM+dU9Wq748nzNTFm0FXcJiCgl5PH56RHzoyl1YzHW4oF2u2VzdU1CSfVpPUXZBlJm6Hq6bYvSmd4nhkE0+Eahd6sTWolri64yENDKAZohekLY0mXPShm6qyvCMEiBRcoMfcDoREaTjPg3hiFRK8u8sVQqom2EmAhDBhOoqzUpbBl8y/PzJ5jmFFcrMharNGEIXPQrGgwhZ/ockP9B5RyLW45gkyCMyZD6jK0a0sSw3ASmRbcpRI82DX0CPySsnZOrBc1EcWe2pqk2bLziqvsWA67cv69dqMe014vpkl3utSBu425PXvz/p+2/gmzL0vtO7Lfcdselvf6W70IbNNANw0aDHJJBggMaKTQMzcyT+CbNy/BJilBoIhQxjzKh0MTMw4S8qAhqxKAoUkPRigTJAYcgQDbQvqu7fNW1mTfNsdstp4e1z8m8t7qBQjW0I25lVuZxuffaa33r//3NbiK+2qHHoS3LFd1j+Hl65Aty8BivoYLD4yLYXvB/+ocf8/bjQFuDI/lzhpgQNmsHTl+Ey/klbdslMUTw7CxjZRxSVlLRksyGNVp7wvwS4T1aSrTMaUPcKf9dU9M0DZv1hvV6Td3UKAVET9u2FKUaiOIORMoNllLhbYdnmxoTkpp2QNoJV+hR3/dcXlxydnZOjGmMv/Xhgv/s//kD/kd/6Q3GxRXX7sWr9eI1ut4S+/0WxU8IsV743YsI1R/F4YRLvELpicKk4iv1iFLBJbZfr/3TEaGH5lYQSA8MfC8lBGQQDES9w5uJMaC0SumuMokE1FZ/ISMpbF1zZUWSRANhKEp8cIToQHiSJ912Ax1Tx0bGgTK0LXy2G5s01rYoqhCg8RDSWI1KIrMM7Tx51IwnYySe6DqkD7ggUXJr+ZIcF54TwKWLkdqiPmBokDIwrnJeunPI0eEe0zJjUmXkecm68QPCKnZK3xQvmf5WHwPOO6JPjhiyz9kvxixiZOk9fhDjwZXI5PrY2X4fth5+4XlrlITCXT3uahxd3608P7CFGJD0kPxU/8gLu0xEjIzkEsajnIsgsTLFbik1BAATuLnnCB5WG8EDr4hRE/G40LNqaxwpAzA3AWREKMhNImX5CNaDLqdMJ4cczXKK7hmF1sTYcrb0XCyWCJPcpbv2kqAUShtkOcM5iyCS5yYJDGxPX1v61hNEROuC28d3mYynZFk27B5CatuJKzdtIVJB4J3De7ezYNjutp/f2QxckoHvJbVIu4pgCc7y8KOPKIVi/+5ddJnh8fS2pbUto3JKMa0oJiPeP32MDR6RGbxzV7eDTkqaMLQtLhdPUaFDKYlUka7bsJxHgttw5/Ye0+MDjg4P+NZ3vsOz1ZrOb4heQewhglSK48mYaWHRao42N2m7mqZrkUIxKmeMyxkijHD2fZr5kqyK9KKkZ0rd18wv14iQQqeD0ilH0CUTXTG0B6KI17gdf/ij0BLHYP2qthMeacKRKR0hxIgmkbGjhygEWoEaiiip0s5RDmM0M5LCKEpjKLUhU5JMCTIF0Y8SIjHpyDI9eCpFpILo02v0g/eWlJL9G8fcuPcyk+kInUtOHj9EDHYjqigYHR4xbyOXH15y1wWQGm2SeXP6PIpyPCKvSoyRREHKYLU9tmuJRbWLEhNKIUjjre86lFFYH7GeVGSJ1A4SIRGShQS0wFo/7DIl0Tucq+l8sldp6g2ClG7hYqDrPAJLkIEgXeLA2ICMgkKDJhKlw+ITQh8EIvaAJeKYr+aMNzllLIhBkylD73pW6zWyGBFUoCepXl2IGCkpxhlruRkWY03sfGrjm4jtW0qTooq8LhFZQe/AK4FSFVEZtLHcGK0ZVx3zRmJd9pnHG1wt4NdbceLF33+iFbgtvLaFWLj2Wj/xna69etw9V2x/J6751g2fyflI3VguVxbvBV0v+Ce/NycEjdYZQSceovMJYbE2EfNj9KzXG6y1g+l3TAuwEGnAiNT6izGy2dSURYEPkt52VDuTXE1wFqMHBD0my5+6abi4OOfZySkiHlJW+XCOBn5U8GnBH34WvL36WwfC/q5xmiTsABhj6LqeelOjlMJaeHha8/f+dctf+XOvMsoVv9/Uktp9DLZC6TzKa8jHT3zOc4vt1Wb9+qX8pJrx97nMf8ARVCBKDyIQhnb41dzJrphLcv201kQVk3ozDMXdjpqS5j9yEJqBhhJ3LValA5LEtZRbCu/Q5QA/UI4SajTUJYQYdu3QZD+SxBxSDRteFZFh+MqAUA2c6yF/cPj/7ZoqyWXiNceYWos60+QhxyjBeDwauN0B+j5lm0uxs0Txg9VY2pjsGu6psA2RGHuqWcX+bMyNoz1msymzUcG4zBCDhY/t+1TYbdfwQeG3TcQQPhWzSgCuY1xNsE7QWqjdIHS5JjTZjonrX18cT9c3DX+g+v36XMLVpvA5K5RPcXz6Vqx+xv5McDAuOL5xzJMNXG56zhZLiAolBbkx7E1yYtCpkvYNRg58AKmwSKTXCDeoZlTyjbGyIypHIGKdpJQ5pZkwKsZMMkPnPIg161Zw0Wy4PH9CtDXNy0dYKRBSI9oG4z37Q07sYr5i/uyM2K1xTUDlkjLLuXnzJuPxmLxIlg1pdKTK2setjclwjhl661tO3e6/14iSKe14EDpse+EKIRQxwLvvvs/BZI83XnmNgxvHXJyes1is+N7bP+TrN2+Azuh94Dvf+R6zwwNu3L497Ey2MGxMMDApVePs7Cn7Y4PRAWMC5xcnzOeaqhzx5S99hbt371OWEz56p+J3v/+7nFw2ZGqMcD4t5KXh1t4v4V0Scvy5X/sPeDZfEd2P2Fy2jMq7jKsDVCg5Pvw95k2Pcxs+ePeCevUhq8tz3vngm8kj0IO7dl7kdj0iCUSc++ycp0mpsDHgCEQjCDb55ImQBA0+poIuEyoVVDESnMAY0EaS5xJlrsjEOQzJGIpS5xTKUBhNkSkyPWa9bulsS3qGIiIHapMk4PDe45wlxIJq7xZf+MptRlIQg6XtGmxX03pH264ppvtM7tyl9Ib5b/2Q+dkFxeQAY0aYrEIqNWTHlqiswEVP19YEWaMETKYla1VSjMeYvEztYqnQKjn5d12P96kd4pygyCSZkuRGMx6P0TIhvV3X0rpAZyMdnrZd0Pg0Qcfo0FqRKYlGMt/06GZNjI7geoRMnMS+bxChxdDgZEz+cdmYojqgqNxgfAurTctq1RNjRrQl4wzW7oKTk2fUo5ZiGlFFWpTquiFimO5p8IJcZGipqC9XCDP4+TVLdAxU+0eMX/5lFv2GbKroOjg9WRC7pwS1YRzWzHJDCAUr/UdhUPwCancNKdv+IHmqXX9MeOExn/atIj+2QHlRTh4jq0XDv/7GA/7ab55zXgukKRC6JHhHdD2jScGmqRP/yHustfR9T9e2Q3rE1WvKIfMVdCoiBv+67XO1UlRVlqKfEAQhcCGgfIdEIY3GmIx6s+Fb3/4We9MpX//Vr/HLv/yLHB0esl4uaa2lqMZICcFbmnUHXYs0GShJvzpDmqQANzJxm8RQ0L755pt8+zvf5cHDh1hXcHbaUfeOJ8vh1LxQaF3/uv2+bQJ13dH1FilhPE5ZxcaY3WM+DfpxddquzHx/Gn/O64c3Hq18Qsxkas/FoW0aUAQZiCoFQUuZ2oYQIN+iZGLQXF+tPxRisC0aELs4tL5FQA3pEqiIVAGhPHL7/iRUznmJGrh3Cfnt8K4nRodUHm0gywQmA+3S3JowjeEzyoTqKiUGStTWmSV9joPKUOUpISnXhiorqEzBndkez5qWrpc4Ap1v0LlBAhcXF9SbFmcdKjlroQbOmfceJRVBSE42jj/2y29w9+Yhe3t7jGZTjFFEJWjbhkePHvH0yVMMmhjS5qS3FqOSy4cPDhcjwbWIaOkLzUiOOCgVhdJ8eNHiArt7//p4uELqhtGyA9yfR/M+TXF3vQh8Mebtj1w8oUvHeH/EzZsHfOUXfomVH/Phg2f8N//yt4k+oSO5Fkyn9/HO0fYblDhDySxB+LpiaS3WCmRUaJnhY8B6x6qpgQ6pArmWuPYx9caz1COsq4kiXcBxKVnUHa61bPBcnD1lbzJCKsNyuWB9ecZkknP3+BYmZjwsVhgFlQGdpYzayWQMxOTCHcHkWbLRCMkiQiqdiOgiFQ5h+N5v0dJrF2wrpNiqhEQYVLU+srd/yNd/9U/wT/7eP+TRk6e898GH3Lp7j8xUvPTGmt5Z7r38CkIrTs4uePz4lP2jmxwf3xycxre7oJTi4Kwd/Ho2+LZFC48yjqbtcM4gu4LzeaAcSapOc365ZLU+p+3mjEcjpBgT6QmhxxT3mFVvokyOdxlGQqYsm7jh6bML6jJSqpJSdshxAUbz7W9+h3H1I2KoaZoTep8ySrcmmFIKolQDKpB27NeMzv/Qx2wikyWCEFinWIYeS7KH0W4Ij44R6Unt+Ah98BitKDLFuFJIfUXq1SKQ54rMSBQiCWuExKAJ7QQlkiCn68Y4z+D8nsrWGCPWO+bzBbdnM2YjAwIWZ2cYGchzyd7R8SDeUGRG09meclTyla/9Aq2taRqNzKcIk2wgrHN0XYsps9QaySXONggtySczyGYc7lfkRmK7jvmyBdtDCCgVcCFNxnmuKDNDZjRFZijUCK0FUnqc7ZjkOeNMMTKCOSu6rqWzDlNUzMYjxqOKYpzR1R1dY1mvajKgKAuyaoxsG5xdEwNUVYEzGTqvqMoDVFHjAaEEcam4XPTUTc2oNAmxHtA8zBD07SSjag9Xe1q7JtYeKWfkUlEVjtPFObIIFBpu7heYySFM7rFxe4Q+4OsNdr3GrR4xOjLsFSPKag+vFOgMkxeffcDtjuscmOd/M+wBd0j0toX5/NO3rZSryTw9d1v8Df/5/Sb3IWmlbjr+1j/6gG897nk8d6xqx8UmEoVCB4/07RAiH1jXT9jUddpMRehtj3dbe4qrBUgPWdoxQm8tZW6GzGGLUGC0JjMq+ccJMcyNqUCwXiCH2LQQEgcoxshv/c5vMxoV3Ltzg2mloW+QQuK6Dtf3yRRWpxzk2A0+jvmYTGui79mcnaDKfZQ2CCTBdQNXS3A5X9C7hCz2UfGf/63f49d/6R5/4mfvJHNfsV1koessdetYbnqWqzbN71py+yglYkiphi7L73PqYSdIeLGxf31R9T+Jy/eHOIROCFxQkaA8KsqBWB9BDgbkRFTkSuWqBSJLkFsKqc/SHAxEKVG5QShNUBo50BIFpG7SkCWuVERkHqE9Qg4bPXzawAZ1pdUZXjkkN0wiSRCZGUmWgwspysuq1L3ZcpmlBiUCSgbU0PYVIm38Dw9nZDLQBws6cnM2IdOa2ahElhWbpmGzkRRhw+ky0HSe2VihVcofVjKJfVK3d7DL8YZMGV65sUdVVhTliPFkxigrUDKC8Dhbc3Z5weViQWFSGk8ckGerNJHkyJFrWNQ9jQ80WcdkbCmUQBnJNAssrKDz1ygYA7VgK4jY3u8JLY7P3Xu74zmqQHhhb7dtYz+P1KVkj08/1j49Yicj1XjC3uFNbt56iaovuZwHolPYIMnKxJsZjW5g+4Y8j2idPnYQkeAF0afMSo8geJWMDq1LEKqOFLlES8G63lBvLtjolrxIe48s0xyVM+ZNT9NarO1Yr1dkOjlNXyw2tPWGqpBIFEqqgVOlGI9TyHWRG7RRiUcQA4StyehQpPlU2DEM0OunOn0d2ovXkNIX4dYwEJszk3N84xZCas4vL3nn/ffJ8xJVFOwVJVIruhBYXi54cnrC5WKBc35QtQ3tYeIVwTImldBms8GKlkKnlrNzySS2tY4fvvMhi2VkNN7no8ePWbcbPI4+uiE3MCKlo4uK2eiYvBzzbH5O8A1a9WjTcbk6I/gGUVX8zOs3efBswcWq5/z8GaHNUMoNt/g1zhCerZddTMzoYfH77DvbKtdInV6vaSOZTpL2uOVcxGH3Piw2MYTBlw60FGRakhXyqpvlHVokUrEYFqvUVRR0jcWr5PMUokpF/yDE2KoEY4j0tgM1QSmNa2s280uMFsjZiHI0YbVa0bYbgheI2QyjNEc3bvDs4pIgUzNZmTS+vE98pNxWZFKl17Qt2mSovGB/NuFgr8TIwNpvWDqL8h49TPxeDKpfmRJgtFLkukQGlTwNcbRtR6bT7zOdk2WSgCRKSVZoyiqnGhWYwtD3qQ1rfUeRlejMkJUlfWawKEKQmKxAZiXaFBilU3i3Su2UzgU26xabJSNx59JNkpUaVNok+SjROkPliqgk1jsqNIokfslyRZSBoDTj4zusvcAGhe8s3aqm36zoN0uk6xlnE8blGJWPcHi8yBC6+szj7ZNH/APW7OvzwAv8mGvf7sxNXuB9Pd9+vfa0GHn7o3TPncxbfuuHc956ajlvIlJpsizH6AxlMoQUuNBjnWWzWdN13c5w1jo3GMmmQuZqgVCURZGschIex5o2CXDwKbtTKVIKRSpgok/nIjUnIkZng4I2oRaLxYJ33n2Hf/M7M/7017+SrKuEoF2eobMCoTU6U0Tb74xx62ZDqQVET9vVZFEmNFsq6uWSqig4PDzk7OKCLC8gRpQUfO+jNV9+rUu2PtbTu4BzyTy57x1N61g3lt56ykxT5posMzvUY2tXAZ9E3nZ797hrkl+pTp9bD4a5bVukf9ZDDlQgEQjCD8gaiMGnLsokERMqtUyFiMhho7rlmkmxTfgApESZRAeRShBV3IHCCe1haMVGlAlIHZBq6OmKZAMSYor1ijAY8SYHBkTyoFQatAGTQxYSv11YMXA1E2ooUigGSg7/9JYrFqkKgwgW2yXjea0kelACp02FwWUZMsuJWPoQ6KyjKA35IFyMPokZ3Jb2AGgtOdyfUo0qirKkKAogpnixkGLGXEhIe24ULQP/LXjiIJwQApRWKCUJMbBuOmZtQ5ZHtDJMC4kTAmyyf7taXK6KPIbxE4cuYHzh/k9Ftrh63K5dzu5xMW5LELEbp/9/E0/IoBmPb3B443X29l/GXXpUvMDWqfAY5YaqHDGZ3KdtLylrT1kqpJe0fWTT9oAfuAKJt2a7AM4ThGfvqKTIUqbg4nLOcv6MIhbcef0uzvWUZcnRjXssusjJ2SXrumaxXiFkIIbIo6fnbNYrilyxXqX8WqEk1ShDlyUuSsoqRykGHoFHCDVw5q6IopDvipRrvGa2U/T1STmhVeE5CDZxsJIJY1VNyMqKp8/OePL0FOvgpfsvcXB4SLU349vf/S4PHz3k4wcPuJgvqNsW6ywiDJYFwzXcvof3nvliQaUcoso4OJpRlLBeO05Oav7pb/xL9g7eYTI74Pd++B3KcYvJBSvfk6keJR1COc6bDYe5QY0r3v3ON2jbC7RuKEctH54ucF4wqkr+e3/uL/L//kf/lsdPPmQTGo6rA4zIaENBlJcQHAo/kD00HgEx7f4EScn6WY+qyBEyEqKnE5YyG5IQnKfIzWBgLSBq+tbibEyKseARMeUBjqucbdxZv+nSYuVDCqEWDh8l1gdWq3NUnixGrE+7OERAqbQx8SQPPe/s0J6wiHrB4uwkxdhpyWx2xAdPP+Dhg4+5c/uY4zc+hxkdMJnu4aKgcYLabhFicL1js9lQjUdoVZBJQ+s2yEygy4z7t2YczEqi6/DLE2Tfob2kUAYf22QvpNOOTwxtlkKPE7Jrexq7Yb5cUpQleV5QFhqlY1KhlznGCPJSkmUCEdLEHXEQO7JiSlmVjEYjfJnRCIWLGp1NkGaKkgUqetq2J4iEmvS1ZWN7ijJy585tuj4ilGK6P07G2CJ5awUExWxKVIIuNuRBpmuI5PDmAZt6hdUl49d+hY9/+C3adU2mzpifPCXUS+gb8ixjVu4zHR9APqWzC/qQ4/X0M4+3dEP/+EkzLfaDPfkLO+/nS4OfbB76iVeO1zY+u10+WBf4r/+bD/k376x563FPkJo4IOKZMeT5iDzPKYqCGCMrO6fvu4Tk73zG4jB/pNdVSu0WCqM102nFdDxiNqo4v5yTPBodwflhYVME2xNJcXrWp8UvIWSSqqrQ2mBdavcqJfjmt77Nj956i1/9+TcYVQVEz+Lxu0yO7iCzDNum+dI6R9tsePy9f81IO0xeIPfupjjCaorSJaePH7M3m/L6G2/w9GSwZgkeEQOPl5aLjaKzYJuei0XHurasa7c700LAdJRztF8ynRYYMxQExIFTmB75HH+SVMRd+ZMlxHJ7meLO+y39Uuxe4KdoyypHHNqIXghAD76VWwRuKAxESHmy0aOkJ6oADoQWCJ26SmooCFERRDKc3lpQRZKgbFe8qIDOAlp7lPZIZZOzgRhK/ZjQoTSeemLsQFiUCZgsYHwkr2KKpXMCbWXy0xzahgy8ZilSRKPU7Hh2o1zgWnA+YKTEWZfscfqAp0BKyHKD70dI3ROj5WJTc//WjNwoetsTNo66s2xIOdlSCkxmOLp1g9nejPFkTFnmtNbiug3etkSlUFlJMRozKpb0XZ94nsERY4YkWYsJU1IWHV3Xc7beMCoXTIKlKgsORwVKS7Iu8GRldzQsORRnL24TEm/R7/7vysfv2qPi1Wbixx0v8uuU+nRr6qcu7N585ecQjDk57Tm77PjmN77J22+/S+gu8UoRw5jIiPkmkJmK6f4BP/ezr/DOh+dcLh2d1Zho2d8v2JsWZFnk7HxNs/aEkPFzv/rnuXnzmFGRMZb/kmdPH7JZOYrRfer1GcgCIXNMOaaPSy7WPSermou6JnjPfN1ysd5QO0cdIr0PbPqObJyhhCIETTlSyWHd9uisIDNXQcK7dupgfxJhiBVTKGWSxidEpA9JKj74Qyl1FbkSfPJai/i027GerChpnefZySmL9b9C63+D1oayqgZvrpZNXfP6595g7+AIZXKEHm6IofcjhwLJOUe9aciKinx2wOff/BIPHn1E3y/QGRzfnuJDy+XqEVoH+qUgGsXsVkExCRghMCHjb/2dv86rn/s8B0c3eOeH73L/uODW8V2exobRtOfOnVt88XOvMq00pS7QFATbUZpTEJqmLmg6h4qWQnpmo4J5Y/E+YrIcfIsInzTA/MMctg9Jjew8Te8HL6GA8o4sz3Yy+xA8OgTaEBMyiUAKNTQntiapabfqXDJG7YVHxh4fFTJC4zZpsnfQe08XLW5I98hUgQoCFSXn5+fcPz4gQ8FmQTUeM5qMONyf8ei9d1C25c7hHvtVRTu/IHYBMbvH5XyOwxBkzma+QhsDIZAJRXA9zdqycZa9/RHTvQl7xzOODzWuX9HUG5qmwVpHDBqjJMG2FFlHJi1OJP5HJiXQEaKnaTcs1gvW60AUjig7UOBEh9MBKyNlbnChYdOkllXbdfhoMTmE2GDdhq4zLLoFvREgKiIFF+ctfb8B66iOc2QRiTIglRxaRpHzxQqdTyiUwimYX1iqUUkxyumC586NlxhVJWB59mCOc56OyLPaMTl+law65MF5z0WdxFBZ3hDbDUYIsmpCUZScXnasmmecnJzz0kt7nJ/P+d57H3/m8fb7HQOQMhxxuyX/w7/Qc895oQgM8K+/d87/6v/xNqcNODTleAJolIpooyjLitVySV3Pk41J3yc1XwiJH+euf7SB+6QSknDv9l0O9/cpsozOtVRFyfHhEffv3eOjh4/54OEjTp6dUlUjqkwTrae3kd5D5wNKRjKdkRcjJns3mC8uqOsNbdviB1GEEpHf/ea3+PLP/izHR4fsHSr6GNls1tDVFJN9bLPGri6ppCUrEofUYOnPP2b9THNZB/7a3/gbbLoe6wKbuqa3Aak0ZVlQzu7yz7+/4IOH3+Av/+Id4AqRPD4oGY9ypuOEKF7ZJcXnzvZ1oE2I52ep6yHtnxgHz8n8t24In72wkxqE8giV1MNeCoKIqEGtDCGhdjEiokPGgFFgdUJRhZUII3eiCykgDoIIJWVCfwaAIvHgBhGDAm0iRoExAqlcOk+D4M0Pfd8EXHRE0SFUj8kDWUipQk5IghZIK9FBIckQO1GDRAqNHPw1vYwIITFSUpkkRGgQiCGdJspEVBAiorQgE5pFcCS5Vce8ldzIZ+hxhQSK25axc4je0XUdy4sN42rMl372F7h/54iqKtC5QVtHv5rTrOY8W17SIxBaY7REthElSBxjrRPHTgi8MsjJGJM1RFZ8dHrKuKyYjsfM9vcYKUNmBN4ITn2bOJhRDuMjnegYIjt1C57rI+zFURWiJA55ZXGH/n3y0VsawBaR/4OOT13YTccj1k3Lk5OnvPv+25yevk/fnrA/CzRt8lRbrjZ8+PgxN49HlIXh7p17PHi6Qdct2kDo4WA25t7dQ6pKIeIpFxctq5Vif+9lDvcOKQyMzJi5LAmxJ8oSHyVd71ks15zPV6yantbB6cUSM7i21r0HbTDlmLzaY7NMEUWGCC6gQkTJgPM9bVejTYk2ceiobFVIYufmfc1NBsQ2sur5jfZ2F3w9uDcViTHJqWPglddfo24bzi4vOF/Msb0lhIjSSSWptCYrct743BvM9md47zEi22LtA1KQXlsIgTEZSmVIkRGjpO/TjqcqDVVZYR10Xc/63FOUCXUSsaFSIyalZpYrnp0+5cEDwenlBTLm5NkRWlZU5TmjQnOw9yq3bv4CmT7nKz//s4wPX+f7H55yd/oDNk3NWV1jXUTLQK4Ft4+mdKdL+iYp7woJwsihLfvZjroJ2D4FmHfOI4xCAipK8EMRriRSgFMyqbTENrooXai4lZ37ZJGS2uSBNvQEZxkXY8pqn9OTE2gjqMRfsdFifWpxaVGkFJUYoe/puw0b2bE+O+P47i200WxWS8pCpbjLGDFFMryM3qJEGMKqwfmI0hl4CG4grLcNOtOYLKPvOgQeY6CzazarJavFmvlqTe8kbpioo/SgUjaqkgqlI1J4Ykg+kz70+OjoXaAkiXCEEnjrCDKCkTgsyqWMqK7v6GPiRMao6F1AtB0urKjbDmFKdK6xQTJfLNlsWry13BpPKbMskdLLDN2mQmO5qRmNRgQMWlcUlacqSqo8ZywU03KfUVkhhWehVlRFTlZknDx9AmSEEDg7+4gi16lIty5FDmqNyAxB54h8jMg1UVuMMpTGMi5/ClIniVd5NZkKPjkNXxVNadH8A474/CR9/ZVha/uRXutv/+szfueHc042EZGPyLVBqTSmvO9x1rEJa+rNir7rB4Nhv8u1jj7FdSmtGI1KDvdnQ9EXef1zb1BkOXmWMSpLGMQKXduxHnztyjxLTvzGYDJD3Q6G4CESIsyKgigT6Xw92Jx0bbPLDpVD6su9WzcZ5xoZPFZojIjIskCNJ3QXD/H1kthuqMrkEymjxW/O8c4zX7R88HTBs9NTTFkhtKZpGgIK4ZMvX7AOaT3eKb75SPOVVxU395KwaFQa8kyhM3W1yJKoF9t0gfj8RXjuGl4nuV+1weRzT7hCbJ//+Wc5lE6cN2TyZpOD6kHI5J8a8WxNqUVMCtRt8TWwsBDqyqsu/e4aJyvuiAC7wiC19ER6b8Ugokit2B3baxj6IUIUFgYlrFSgDJggMFFQRIXTScS19awjlXVszbyFiITheyUkUaU1NnV0ks+sYEjRQSKFxBhN36fxtzvDShKzlOHqtQClkSYg84y894zHJTduHDOajDAmmXP7riU6i7U9z1aJ144Aoa4+H2KXS0QQAowiSAPCU4gC5xs623K59ASlmOQ5SmaMjWEeBTZlvaW/fHitIEjzMZEghwQeEk91+wcNo4mtRdIWKd7OEldFXkr88n577f6IxRO5gtN6xbJpKd9XLBePkCy5eSR5et7hfMdiseb9hw/Q+R1u5hMO9m+g1fsI2aG1QJBz4/CIV1++z3iS0Wwkrl+wWnRk2R5GjZGxQyPRyiQeFTIZF1sH6zUX8yWbxtJ7wbP5klylCSVEyWQ0ZXZ4k8Ob91m2brBoiPjYYXSytfDe0rYtWd6Th3ht+7Zj0HE9G3ZLppVSpwvBi7L4F4q7dFXSVwkvv/YK89WCxydP+PjBAzabFV3b42NgPOwCDmfHvPbG60wnEyLJ0VoMkO32wm9jeYq8QKsMoqSu+2Ti6B1laSjLCu88XRvpTiOH42Tzcd50zIp9jqaGW7PI6uKc83OPvaj53CtfIc+PMLKhLA6YlJrp+D6z2ecR7l/w5S9/npfe3MN84yEje8bJ+RM+vmzw0aGkYFxk3L91wLNVy7Lp8dZiysSdEOazt2I3dWrVO+txPqAKiVZp0vDWD4hmMpaWgyG1GEK0E18yEHwSyISQcixjSDtdHwJeBYoMpCnZNE1CRrXE6BwXbVJIuUT83673ylr6tqZGMt/U3JtMkMGxuThjf1ymydJFlI4om2B+FRzapJi2hDxkBNvhe3dlX6IleVli63lqOcnAullwOb9kebnmYrnC+yoJeaIg6mEnLyPKSLQBGRzBxaRi8ymr2Q+FrZDJ/T/6RIUWCPpgkQ5CH9jUa3ymAE30ifvpfItsPU3rqKoJKqto+4b5cs1qtcF7x0FnKLxEygxTVpgAnfWsmw6VVyilUaqiKDxlXjDOSop8RGVG5KpADg2ucZUzO9gnxoLoBa5vWS0ecHx4jBSGtu5RQmKKAlNVyeKjnJAVGqlWyCgojeBw+lO0xXb37fWCLfLJOG54vmB78XdcrfXbPguk3ftzv0u/9j5wue75f/2rp/zwUYtUOeMqnbuUx+mwNkUJOmdp6k3K+HX+6tpKiZEmJfpkhtlsxL37t9PrB8FXf+EXWa/WBO85PjogAvPLOR9++BEnz86IPqCUxvuA0hplDNaFREvwiYYwKnI6L+iCZz6fs96s8d4hSPOvVoqqKHjt3m3KIktdC5GTacjKEflon/rBtwnNEuF6iiLDW4frOvp2Sess82cLnj16RrQ1+WSMzDP6vkeojBBc8uHre4LL8aLkm48Fb75WMNnL2Bub5+by3eY7stsks/32Dzh287i4inW7fsnF0IL9KXQTQLLGSh/NEwdbIiFEMggXaRPKwFdMxWnafEQYxGoCEeQVZ2vInRNsyfZXn10puRt+1wu7hBj6HcARthELYhCSSJs+gwKhIsqkoiELAiEGP7pEFkvvHVN0I9t82F1LO+F5QaXXTpZPQ24tckD5JMoksWVvkxEyIv0WHcFEopR4kvesAGKekbc9k+mIg8N9iiIBACEEom1xtqHrGp6tN/TegYgIrV5Yr68KXmEEXiqUzsiyQNlZ6k3Lqm4TLWJUUWWeMtcUcriHZRhy7rfWPSB9KuxkSLITtrf/9lqx3TRsfRu2hd0wrmJC+tLDr2I7Py1W8qkLu/fe+T6ny5omeCYzwfE4MDkaMRnv8e0fPuLkrOfswjIPLaq0tO6Qg6Lk7HzFatMQdcEv/vwv8bU/9gt88QtvIkSktd+h9e/wg/d+l2++9W/YqyQmrFiu3kcZizaGy/YStKXQGdNqRGlytOpB9EnKbpIPXlEU/Oqv/ilefe1nuHfvNX7zv/3nfPTxW5yffUwuFDf3b3DzxhFSbT3pkugixIEgP0QCpYiXuNvdhZB2wlmeEb1LaNwLxZ1UaldNJ6sSubMKEFryxS9/ifuvvsw/+6e/weOPH7K4TK2Ul155mTfe/By/8Eu/xO07t8jLAqUVTtjhphjex6ebO8szxuMxwjqabsM777xL06wRIlIUmmkFzkImBD/z2j4//4WMqoQfvFdz696fZG/smGYPWC7PqD9K3DyTlxSjQ7RokXLE8f4hkZIPT57Q2R9y9NIe0/1Dvv71P0V8fMG0+IhnyzPef/gWN/YzfubujF/+uZ/hhx+d02zO6UPgRlWhjCL7KTh2lyuP61JGaYxggkdrQa4lsbMYD/mQGOJdmlRMluFcS9ulK4oUCBHY+hwZqYiRZAURHR8++oAfvPMhk0oyGhlMLul8CzEgI+QqVXUCMewAe/reke0f8It//E/hhEUBe8WEf/Ub/5QbR4ccHeyzPN+AyohWYjlhMr1Dv1pzsTplPr/ENQ2h78C3VJMp48mYw8M9ljIQyVhvPJvzcy7Ozlgv1qzOOvb3RmiSUeemd2gpMZlmVI3JVEm37jh5es5q3YOWoCVZliGEgZghKCj1GBU6urrFiZbF3OKXPQ0bxkdThBI0m4beBaRyKGMQMcP5gtAonp2cs2wbgoKsqtgrcuhhc9GhwpRRdUARYNE0BC/QUqGkZlmfkY1y9scFe9OXOJ8/46I/JWJ5dvIY6/YS565t6FeBPFMcVjlaNVhf00WPL2bcfuWL3Lh7n/fe+h0yE5ChYbk446P1OUgo5aec9X7S8eNsS57brEWezxa6Vrg99zq7J1/3y3ge24mAl3zwpOev/BffISDIs4yyHBF9oO9S8RxCoKnXtG1L3zRD9mtaqKtqzKv3b3D7xj7KaBablvm65eR8yYcPz7h37z73X3mJf/yP/hmzvT3uv3SfP/vVr/I3/sbf4vT0WRJ7YVg3a9brNUpmbBWvfijqkg1EBGmI3uL6jrPLJSbLhqgwxd54zI2jfV67f4tJXJMXt4nFBOUlqhgRg6dZPkOYKbnKUL6mPX2L7//oI+arBp2XPLhYced4yp/48l1iCPzgcc2D8w1VOaWua4JziBDw/YaaHuc7vt3X1P1NvvrGjL/6F/aSx9k2IHp7xsUnwz+e8yncdlrYFijXrpPYvcTusj/3Uqlb+pkPsbVdHN4w4GFQjwYCCI8cWr6KQJTgSR6eMQJBDZSX4TUkJPtdiRBhWNyHgktcWXlJKVDKDVYkw/sN4drJGXRoyeKIwicrlujSmixc4smT5p8QFCH4HUI4EPVScbctLMVWhGZBuVQgDh6ivk80pmZTMzoYo03OlucXhQKtMUagR2CqiBn+pBQikLiAk6MDjmdH5EVJVmiid8S2pq0vWS8vmS8WnNYbjMyIRqGVxsjEOZTb86eALKAqS3SCEBRN0IxuTtAL2CzWzOcX+K6jLUoOxp5pJjFKUCuBlldjybkrWpaQAeF8qiN8Gp8pB10MynIJYUi4em50XV3XBMYmj4744mD+Ccenz4qte9rGs7Gep4/PmN0uycqKg8keVXaB1h1RRazouFhd4GzLxyvPYtWAVlR7I8b7Nykn98mKV2manlX9Dss20quGtz74PfYKwX4euTvLMc7QBsnDp+fsVYJ8rKiyjMPpmItFsvwgG4osEdCx4M7Nexzv38aIEZkoyYQml4rJuOL1u3c4OLyJUoq8yDDGXHOO5qpYgwEyHaxLhgndmAwXfDLefLGwS4xUlJTEPJkeJgKzIivSwlhUBb/267/G+nxBVzcEHxjPpkxmUw6Oj1CZ2RFYpVKDvn3rXp3EHWmrFjB5wGhHbx1t0+x2y+1mSds5Aoqv/4m/SOY/JlOeX/mTb7Job1CoDYXKKfMTjNjge8uDp++wN8moskDrFrhO8vHH7/Lugx/y66+OGDWSRs75/tu/yZ26pl1KbCfZ3x9z59aIe7f26DdzXNsTbNpS3Do6xhSa7qeY9TqfbgYfBTEIRBgC7yMUeRK4OJt2pluH763oJYRIZyPrjUfKgJAkflamQZJilMQEIR3CdPSdpcwBrQg+jSdIO8WE+qWdse1a2s7RB0k2O6TE027WLJYX/PzXfgUVHTI6iCPOzi7xoaOYJtTXuZTkkBmDcD0RTWHGlNUIqRRts0aXJVFmdK2kbQLBCkQwZIOSWyow0pD1BmxMBt9h4GmEDC0r9qdT0IKgIl2zpKl7+j6huIUQaKERaHqh6XtL2wVsyjZKi2AvGRWTpM7VGttZYlvjrCX2K24ej8jyjLIsEbFBuBwtR8lPVF/REKq8wmgJ0TKqHDf3W+4eNNTBsJo/Y1NfgobVekXne1ZNw9604sbBIaNRxfFB5OnFY5CR/RsVKt6iLCf4zjJTFntxgfc9mUyK2HXbcrpafubxBgxtoWu9uiv55FD0Pde4Y9evunbE3X/FcwXBTnAVI3Xn+M/+Px+zbgOLOhCFIs/ylMSj5S6qq7fJLqTr+p09k0QgdeL4Otvx8cMTnpycE4MgyEjvPKtNz2S6x9nZOSC4dfs2vbU8fvyY/+q/+pu89+57rNdremvJsozRaMyNG2MuLy9pm5ro5NDGTcpYMeSGltUYVYx4drnEOYeQCpNVNM2GiZnw2qGiXV8gD19ClftIHxKhXoDIcrKqoD59Snf5hN4rTuvIk4uWy/UlT+c1H50u+fhkwf0bE07Xnsvac7asmc3GOGep12uIASUFRgmsh0dnS4Lv+D+y5r//K0ccTPQuhhCuKDLb67FD9HbXcPu4qyt4DVT95ENfGAH8FDF26rrSVoREI1ERVECGMCBww2caWsKKAH6I8xoSebYfMgzmvVd/z1XbLlGS0/yolBwM7od0IJGC7mOQxJByqJNozBNSxEUSbmg/dK8UUuUpmi4EIgYlBoQwpr3MLicWiCnzYrBRicnDLheIaOmaQNd3tH3D+PAmWhtA0Ick/jBKMz7QmFyhTUKnkXEXTSdDhmw9BI00BikVMXgInq5vWbU187YmZjEZQgfQWYGSi6HITRw3oQUyV5hCo60nBEH0hqAlmQCTGbSqaRrPsm5BaMaiIDcKWwrUkJKDAJMJrIt4l1THwgbSR0pz43ZMCj8gi0Gkc+Sv6hlxbRDGGBFq8Lb9sTvJTx6furCr8hyzCUQXuTyv2YwlblxRmhFaGJRMZodeWDbtBtf2dKctXkAxLpnNprio2DSCy2VkMbecPquZL2vQnvPFCb6TqErz5r3XEU7hmsCTsyWaikmWqtZxkZErAd6mAR5Dci0JjiIfQzSsFjV92yODoFA5k1xxvL/PdDrFieRsrpR8QWZ8tSlPyN32hokDlK2S1PlaEQhXu7/tVx11gmYHUrOQEhUDJjNM9mb4my2hTwHpKsuQJhl+Kq3ShCS5tqPcFpDsCskYJUEMZoou2RlIkUKQ+7al7Syokv2bbxI3FUbBjZe+zOpRSwj5oGI8ACzerblcPuHZ5YRJqQihoW0yztdrFt2a/s5X6PscJ2oePvkuhYfVomGz2VCWGft7Mw4O9untkPkoBEpIjo+OKcY5XfzshZ3JNHab7iEVQqTWlJICrQoQQ1i1lkMUHGxdz9Pwl/Q9Q/xNRBn1XIsthmw4n56+t7heYFSyd3ADLy4hfMmihxBxISTDXy/AFGmX1nR45zk8mEJwRG/xIaJXLUQx5A0Pm4UwTM5aI5VgVCqyskQZiY+pRZCUiBJvU5qHiAn18j7AEIGnoybGhGCIlPJF8Io8H1MWFSjoo+Ncbmi7nr7zEAQmT59ZwaCoTe6hmazQMvE2M51TZqkVuI2DCu0K328w0nGwt09eFRhtsPM1mdJkZkxbO0JM4dy275KBqBB475hUHftj2B8bNpc9mhUyrGj6LKkue0fXWY4PD9nb2yfPclabFpON0yI+mYAbQ3DUq0voN7h6gRGR8dEx4zyjcykp46c64paIsUV+tjDNj4FmthNHvD6qeGFCuSoOYgx8/Kxh0wXmtedf/mDBqg24AFplKJUQ+uAt0tWU0jIqPOVI8eg8srRhJ9ja8nyd97TrDd6lhUOZ1JYLURN8YLNeQ4zcvHWbzWZNXdd89OEH9G1L1/e0XUdVVWRZNlzvSN+3CUEYlLXE1DITUpDlefKxE2KX0qOUwnU9k1xw/6BMjSVTIvIx0vWIkHhiqZUYwDf07YKP55ani46ni5bTiyXz2rJqLOumY1ym5AkRHFUGUkusVPhM0XXD3BwjPsBi0w6efT2fuzPi5eOC/bFKBd5Vp41tfXd1jQQ7TtpwxcR1JcXQ1rz6/2Ge/6kG2PPH88hh2qgxFD/bN4pXHw45tEdFvCqakAyG2cBWAPjca285ZFvkTg6mt0PrcehqxJBQvjhEZm3dDRj+X8ik/tQCgpQIbRIXcKu8HoQlV2rsa64SgzdeFAKcQGkFWuLEEDYSA7bvhxSIJBZ0Q1dNK81olpNtCzu1tf5IgIqIKnF8s3yHhDF0uZy1NM5Shz6hoyoibFrLVepJp0IwCpSWqFyQZSpZjfnUCUreecnHLkZPZ3u6xlF3HZPSoJFkAyV+O8YkyfIliUYgSpEi0XZRSvFqCIah4BvmiDgUImJrW7e94eOw4fiU/f9PXdh94c1X8O+f0Tyac/pkwSO5ZGwy3nw9g2jQylEUkVpA07X0oaNrHAfHNzg6vsHtu/d4+OAx3r3Fg0c1z07mfPu73+Ji+YC8MihlkxGwzJgevoRoBRtf8/TpE0TQZDTcGs0Z5zDOoJCR6PxAXox0tmW1ijT1OScnT3n84CEiRmbVAZWRzEbJvHDee5TUECXOerS+htLF60hc6nFvETsx7EiSOvbKQHDH85Lp1kFKRPAJYRsifJRIO40I5OMR0nm6tkOaDKFUyhFVCqkGA6AhYib5CKWpJ8aIj+BsRt1dIqjJtWE8qqiKgrLMebZe0nQedM7JPOdnXvoLTKcz5r7h0r+NsiWOO2y6Y3p7gfc1Tb3io0cd+9OSGwc5pxePWDQeiyH4L9I1Y5rugrNn38K0U84vNzx68pjq1i3Ge4fs3bjH5ckJpswoRxm9k7z8+pscHO4TlPm0w+sTx53b+8zP5qxXAURBZjS5UYyKnLQ3SpYlYVuMhYAXDmUkSiiUyrB9MqL2gmT0m0B+RPS4oOi6QFPXxA4KKcAHWizd0GbIRkmg4p0nOEvUio0lLch9QGY5Rhv288iDt9/i1kuvsH98i9Viza17FS4Ems4N3noGI0s2TceozBiVGdNJidAgtUJpRV0vMXlASoWKJdFuiNYjAynvGIcQDeOiApkhpUN0jqZxCEr29/Y52t/DxcCmaXgonhFtnyKmiDhEmriFI6qQnN1VwWw8JcsDUmRM9X4SVNhUbFVFSdOe41mzN824cWOKUJrlakPdWsaTksODI1aXH7FqWhrbsW4bbt++hfeBZn3BF1+54Oa+Ylo19I8e8uqtDasW3no6ohxZytGIvcNDXrv3OZxvWCzn/PC9D7j78puU1QilFKenc5bnT2g2c/r5Y/YyODi+wed+/uscyMDZ+TOK6qezOxHewXZPHMPVKn7NAmN3DJPuc07y14u8QUQlQvreest/+Xd/xI9OPE83GVleYnIwMRCdJbqWvk+m0gdiwb3DjJdvVvzxL77M/+E3HvK7HyxxEdrGEoaouIS4JB8wFRw+RExWMNk7TMKcrmOzXvPhRx8mk/iBa5k4yakIaNuWhw8fJsSbQFev8HLYSHpSBrGSZGVOnmcon9rA1rqEnolAUIFbhyO+9NptzOw22eQQM5oimzXe9bi+pa+XdKs5hQ70heTv/+a3mG/6hIB3Hik01gWeXq74v/+TOQB5pnj55pST+YYYIrNRxql1WO+gi2SlobWWdQeXjed/8fcv+fmXJvy5z4/59V8skGrb6YhXK2+MAxi7ndcZFtM0n6e6dSiChmv+Cb+7F/jVn/WQ6kU4cEB9BupHMsrf8reHhs01FC49K6GpxJDWoqtPvvuTpZQ7WUNC7JLPXUpK2v4dYegWbRMUIlK43TmLMiZ+s1eEKNECjDZXhZ2UxBAH78Tnz02Mg33L8Hcpr1BWsJz3qSWbGWTjqOs1Umuq0TipP4cuy/5xyWiiMXkqSBMfLxWmQQX2zAH746Ph/DlCsHjbQRvovKNWgUxppJL4LqJCRIpkgK+NwYpIlmuKkWJcarxLlCprXULuCkFAUYwVRXdO8B22b4jeoIVgpDWYra1QEs5pI/EOtI0ol2yDwuDtmeL2Is4ldDP4iFOJ/6eChChB+i0hDCKDL+jWteMPPj51YXf7ToE396imR/zW4juM9jJ0JVm3F4ymggNtoBGczJsEMUYQpQIpkDaiNx47f8r3T09p/G/RNTWumSNDl+jcRUbv4TJ4PrqMbDZt8nbr1jy9qMn0iNduBg73j5iONJkEJ3ImexOUhnVzybffekTbtjx6+H1+7gvH3Lv5OpNyxPvvvM9lYwlZS5mPkVEm5AtJjC7x4SIQk1cTPnkipYGf1K3ObU90sh2RUgxmxnJ3ssP2bpMCEeKOe5eOhOBFEfFKoiu9I/wrpVKO32CYHGSE0O/69JPpCGsl1lX82p/9K3zru/+Mp6fvMNvPIKxxomPVe843G7ouFZ7/6O/+ddxf+Msc3LzDd374XZ6efIAKnkoqfvTRe6z6FflYk2dpF257wbopuNicsVo62hr+b//w/8znP3+Hw8OSKox4VDvON5G2H6FbOFnCj84U8/mYpZ/iZEsfWh7NezotKMvRpx1enzhu3ThmWo7YrGqeniwoR3niUmaavCip2w2btkfE5HXno9sp9ISW5JkmyxQuusRdGQ4hNFpPWa0uqOuOtgYRFHWbbrheOqLIkDHB8nbXIpH0naPtA50T6PEE6Ws2fc3luub1P/Yn2SwuODl/xvFrX+C7/+abrBeXVJWCm8fEmBbtcjSmKAx5mVFUVWpVKYEy0NsOITJC0ASnaTpB06aN/MW6SUkbSjPam4CP2I1jM3es6p4s6ygOdUKiQxqbhwcH5CajbRq6bkPrewIB6y23bt1mb3abUXGAr1ecXzxJoe6rU5ztk/F1ENy8dYPGQt8ptDKcXW4QCvquZ74WRHdBfdFxcnHJom7pnUNqxcnJCUb3SDknqEOEWFHlc7761Xc4f7Zg9SBy9lRQFhlFPiIzJd/74dspPUIrZi+9ih1VrBaXnH34Do6UFxlcQMQCV0Q66Vk9foic7tE5hcn2PvN4A56L8trSMl4s1j6xFF9rsW5/EyNcbAL/23+8oYsG6xzN5pKT5YQ2RowJyRMxdNw90Pyn/+EbGJm2jTEGFBEz8HasD5SlJBBoW5+UjMgrNBBQ2nB8dD9lYTrHpqnTvTDYMF33syuKgvU6CR8QgUKVKJlarzGGtFkQYYcobHnG92/f49nFJRcXF8kuQhqMkuB6Zlmk1AKhcsb3v4w2FfQtMnhk6BG+gW6FnP+Iv/2b3+c3fu9D5pueEBySSKEE09wxyQXjQmOPBH2Q1L3g/Scr/id/5d/l1dc/T5i9yf/4f/6fcnE5x4eAjn1KJSLZVv3Hf/U/5mA2Zn3xjLdHM27Ht9nzHyGj2xVwV9cLrrfWX4wk+/0KNinktWHx6RCUH3+Eq6JtyM1Nr5jMmOOuQEub/W3awfXweWCH5KZCdUDzYgSRTNvFNfuUnYmwiANNZSgPxVWSSPLNi8n0a0CRtsIMpdUOzY1a7SBFKbb5qSohvS/8nZEULUofMCg0ZrfZ2A605WqB0AqT6+QqQJrLJ/uGciQxWXI9QKjkOycUB9lt7ty5w/HsEJMpCBbve/p2xWK5og2eUCgylfzufAtWJLFPHNbtKCO6FBRjSVkJgtep2BoKPL/1kTUwvVehi8jZ2xuCzymiYiQNYZTvUN0oFM4GnANtAyZIvBcpEpNUzHkfcX26TN4HrPUpMzykdq3fjY30L/H24k4p/Acdn7qwyzJFNcoZjzUm08nheVTCELiudUDriBEaSAayPnY421DXcy7PYb28ZNEGajcMpNCkwsqJpIYL0IbIo8fPEgdvtaZrk8dO3TjWjSPP2dkyzPYOuXHzGIRn8cEFHz34EU3dcHH2iOwrh8z2DXvjnPffb1jVc4SGg8wQowUckaGFxxYXVewco2MyMN5WzXFoK4YQ6LqWsiyv+t8iSZmTomlIQhCJbHDd7XwHn28Rvutq2uEjEOMQaLyFYLl6vpC8+srn+ODBdzg5+2CwnaiRImCUwgbonafvGvrmEd/6zu9QTmZ8+OgDrJ1TKE2blTR9AwpynZOZRHq11rPZdHR9n/gBXnNRL7jcjDBVWiTmzYZF29Jai2xanjx9RtfD/LJmuWmTpUgMPHj0hIt5g1afPQkgM5pepZ1m8DE5mQ/5g1kmsV4i7ZYjEobAapdCnGWaZLRIwLiLyXw6oRQaqSoQKReWmFqjQunkARUjmTIUypBJDTJ56EUBWSbxIfGYIoputcD2HbIYk1UT+q4lxOQhNbt5l6hzNovH0La0bUPXd0n1lSX+2hZvj5GUzBLBuw4BeNvS9pam90gBfYgUZcb+/ojx1DA/j6xXjt5LeifRKiCDx1tLIE2A09EY4QIqRqJrEq9GamReMBsfsz+7SVXtsw6RKDS9C6zrmkwnvNpFicXjkdho6DsIiz61RLSksZroepraMl9t6LxHas3+/j5lWTKqApMq8XKSeVZKLx9PDHnec3lxhpuMqcYVWisCki5Z7qO0prMtvWsR0VMUGpEXRBSh7xFVR68i89UFPiqsC58gyf9hj+seUT8uEP76z56nS1x7jaD44Mzyw8c9P3pc44TBh0DbOIQyhNCiY8uf+bkZUpQcTRSvHucgFM8WPd//eIGQGYKIUZHP39F86aUpLghOF5bzdcAGiY8KF1OrVEqFyXJc8IgwhF2JITBdpnvAGE1mdLIFqUbD/eCvZ7UDCiX8VWtSJkPjUVkO58fiXOLl9TaR75Vw/OqXXuKV2wd4BMqURNsS+5roOnAN1Of4xWP+8W+/zW9//zHvPlqSZZpceUYZ7I0Ue6VknKd/QaSx0FpwvcBtLllenODDHuNqRF03tF0LA5ezGI05vnGDl19+melsyuV0HzWucMHQhSO87XftZKkgm38f4btd653tf3/MNU/X+vnC7+oXn2WUXR3yGg9tp34c5iSBuuJZ7aDDa925a1zA9HOJ4FouKVuV5rbQY/d3ioF7eK2hO7zM1qQY4kBrYWjtpjGldudJXOfzDb9jQKl3goSrv3T33jvUWOnE4+1t4hqKhDw619P39fC6yQ+vrEqyPENrxdZnVkiBlppbsxsczPYZj0fIYa0NMRAIdF2HyyLkGi0cEk0calGpVVIiy3SeslxTjjIyo/EyxbwpJYbNUcAHj4waOS2JnSeKOcE78B4VAyq7Uien9V0l1bGMqCASvUYHRAQfEp9OysSvC35AH3Va60KIqO34GPxsfVD4IWXl0xyfurCLSITSqCymwm48oRpNyIoRibDj04QsMhA5xIjrazq9ZLFc0zUnLFeB1ie5cjlKrSoXPC6AGebUvpd88O4HWBvpekdbJw5ZXcN8FZDKsq4DLiiOju9w/6X7ONfxzttv8eEH36BrHa53GO0ZjS3jWUOUF1wuwAfHZFISYkuIPZLBpFZIEDLtOkKSfCcVkXjxJBB8oK5r8iLZNWwXg12hNrRkt7u+6zv6tPOKuxsLuPaYracNibQatrvKZPXiXGoT3b1zl9lkDDFwcvKM0NdICcZoTDbCeUfTdWRG81u/9U9orSOKnsOjkul4AqN9kB6jdbq5lMAHT9dZbL9K0VLRIHSOMB4rJG2INK7jcnPOZd2zsRG/9jRtz8cPHrNedgNXId1U77z7DsFLuvbTjq5PHgqPtz1d22E7f9UekMkUWmmBNnLntB5wWGtRJMFDcB4pDTKmBkSKWBrQMTVC6yVa92gtKMuCLNdoFRCuZpwXlFlBpXOETi2u6AGl0oThLM4LNmenRCLl/g1iFJTlmNwYRNfw6he/zNn5Bd/5b0+x9ZrNZkPT1ORlhdIZQmmcj8kc1KdYuOADzq8JLLFdMn+tO4vUiqAUe4cTPvfGTYxwXJwJLtYejyLXKelEE+nahqg0Ikom1RisR0VPsAZlDDLLUeWEg/FNpqMDsmLERl0QoqD3gabvqEZl2mh1Ahs9Ho0nY9O0bDpLWWXs7Vd0HhqX1L2rdYMpDKOq5N79+1TllKO9c+4cnnN22eOFxIqK1abiYGYoqzkX5yd437F3MENrSTWZ0kiJjRH6jrZfgm/Zn+2Rjwu0KZEyY91c0os5Xew525zR9Aop9E+90CYLkSte69Vx9cKfLOzS4SPJQ9Irfuudlt/4QcPlqt45xbsA4yJlXmeq4a/++TfRUuID9L2jI+O7Dy3/u3/8FJlNUCKyX8H/7C/f5Gs/c8Trd/Z477Tn9z7oWHSKThT0NuXCxgDOWRJ5O6KkRMvE14wxYh0UmcYYjRSCyd4MIvR9T11f4kMqipXWqRUUQvLikjAqC27s79H1Hdb2EANlURBCk/KXjec/+NM/x2i6h0Ok57ZLQl+n4sk32MsnLB69zX/+d77Dct0hARcCRRWpDNzai+yNDIWSZCqpKZP4V3B7nPP2W9/lB+98QBj/iFGZM6oqrO3xHm7OZty9f5+v/vIvsrc3Zba/z8Hh0cAPvM0qBJqmwxiFNoqskOj6Eaq/GIqoMBQuQ/myu/ZyV/hFkTZ8z1//n1Ds/SGObVv1qrxKFKCE1ZlU2uw6b4PVFmHAv+JAA92CElfjNCFsaTcqokgZ5rtXueLaCSHTkBmKvG0xGLbWPAOaJ0hRjNu82hjjlYvfAEzEYf1KkWfhuXO1/dNkTG18IRJnWmcFuu1QUiGUAilx3tK0q8EXD5Q0jKoJJjOJqjS8Z5QRbTT3j+9SFkVq55LGVYxJQdzZDlcKqLLkYxsMQYMVEW1M8tdTEeElZZEzGqdMYaXcTrS4Q76jwDkQakzsItKkaDzvPHiPMWrnj5eSYhIQIWVy1UhK83RNvGfwsk1ijhAkymqCdcPvUkLPlgcph82Zc46+/yMu7D56dMa8Kblcgs5zVquWzaYkK2Ys1g0XlxsuNx7by+3GHC0EWQajsWB/qnjltQkfPal5dNriXcS5gPeaGDOmeyVVIchU5PHJCuskzgukMEg9YlXDt3/0mNl0w9nZiqbzdL2j3rRY2+K6iLMbMiHZm+R065rzZ49pG5ivThFB0jaRLPPsH7zBVBaMspLoUxzW9gbaWqEw3DPXd+nGGJy1rOsaZTKyLJl5ZjJLasNBQr3bVnG1JEgxBGoLsZsknh/8gTg4WPvguVI3CbquGzL7Iu++9y4fffwBT58+oa8bRBwKAtty89YYRI7JM7wfMRsLxrbj4vwpm7MNojOoEBCyIIQuBbVf83S1IaBkhlSazgvaxvLkfM2662ibDc8uPHUDHo2UBms9zvWEkBYQIQV5nnF5eYF34OxntzupL59RLxrqVctqWafoIWEoc7i4vKBxPa3tAWj7Fut7QoTCJCJ4DMk0OyVgRqLxqAgyCJyVHOQV06ygn0mKQpMrjVYSaQ4ozBijNMpISqOSQtB7Li8XGFlgZIaLlmazZjTZ5/jWa0zHisbX9F0HsaBvNmidc/+Lv8KTkzOEX2HbDTIGvJzghaAhcjCe4doN87NHjI9u0M6fsJ6f0MkAaLJcowrDm/cPePXlIz73xh26LjAeT7j9aM5b7z4BqzE6Q5gxISictfTOsXSgy4qqUGjToopEP+h9z8nD97g8e4wyGYvlhs1yg8JwdOMWZWnoOwdYVvMNznm8VEhVsdnU+OgZTTP2Dw1KCKSIHLQ1WhuKvESiWXYdeQd9mHA4XfPk1PPsPOcXvnYTX39Elhl+5stf5WfffJPppCTLFE1Ykk3GBClpT1b0FysKU3Dz1dd4+OAhWnTkVURPSjac432H3xRkB4pRMaXMJp95vAHDvTC0wOKW08IWzgJ+PKdKEvnNH9b8F/90jjI5bkDOm66nyAwxOOp6wf/6r/wxDl/7Gvbo6zy9/CF/5+/+fX77d7/N2cYltBs5xDlZqnJE7Ur+6v/lEcqke1IoRaRKisXeYvsOMxgZhwh7sxlx4OME39H3HV3fkQmH7ze0XcojbeurjABCj96aaItp6kwMPDQRoMgzDvan5FmKXcJ7ErIXubGX88e/dIPWG8Zmj72DO/jNCXH+AOoL8B297/lH/+oH/C//r7/BfGXT+RXJgPxolJD4eu0pTUQaUCSk0ZP4VLmBzOSAJcYP+WHtGWnI9goWm4b/wX/43+XrX/tllDGMqxIVI0TH2bMTMq3J85zxeERrO9q+o24iT5tf4kC8x03zASKGAWmJEDzxOgo1XHax5egRtosCEblTgP9RH2Jom27XABjarVyFyrPlX+3G5fPcq5SFzo/lCG6fvwUbrrpKV5sWpbYRckM8GGJoyz5vdrx97TCAEEKk6EV4vlUsiBAiUWl8Hwm2x9uOzkMXNULliBhpmoZFs04WOwyocS7JCjVw2ATE9LXQJdPpDZS0CWmOMal3Y0BH2OhANIo8k4iYo2KOkwmY0cYgtUIYTWxrqmrKZFoBkRC2TgsR7yXOB3xIojZBRITAjZcP6R736FXDZGrIC51ebxAWKSeGfG25a7d674dCjgGd3IqUIDiJtwyRgOlcKqUSUCQEkYD3apfB/Qcdn7qwe/D4GVbu0/RJTThf1FyMM5brDhfkABV6YhiSFaRgf7/k9Vfvsr9Xsjc1aG1Ztk95dnlBsCKBfC7Ze5R5znSsqXLB2dlymFcFfUwRUT5GNn3ELdbUvSWKyGJxAXTE4DBaU2UZo6JgbzLFtT1PHj4j0HL6dE1mapxVlGVguV6RjzoqROJIJ+lKar0ORVl4ro169U8qxd7ePpeXl5Rlwf7+PtudchoUfjcoUuUu2W4Gn4fWr+D3dIPJ59q9V3m1YigG089PT0/ZrFcEZ1Eip+89toe+j6xWHUJpAgrnBJlRGK3ZrwwWB8HT9g7v064+mS5HrE3XbAtze+/p+4YYPatlTd8pRADXZ+Ai0mu61iZLFx9x1mOMufJ6ioNj/U/RGzN2wYgOK3uCT1Fqo1HO8dE+7z9c0bQ2mVj6QO+HXY8S5HmOROEdNNamYDERcTYwKQQmk+RGYpwkCE1QqRBUUSBiwGOxoSUIjQ4aEfMhHs6xWTTkZY7tLU2zZHbnZUajKXlZ4N2GqAwUE0RWYvIRWghk3uLDM2JME5xWCq0ESpIUtNYSiejKYIoca0YIOcKzQSpHnsHeXsGrrxxx++aMqlAQAzdvjigKhcXx9GGL6yxPL+dkJi3yPsDGOfI8omUq0oQoUgvPaYzK2WwsnW/oHNiocQja3idSd1RoI1jNO5xzw0QrdyrWZtUwHg1+VUHhdZayQ92axntGBzPm65oPP15x5zCSmxxjKh4+6Llz01BOQOeGVbtEmsBIFqzXz3DNJSEI7KpmWk6QUjFfLQjBc3x4zJ3bRzx6+gTfChoXsNGx2CzZNB2S+Wceb2nc7r577mdbRMVGjUWTy57HF45HFz3f+GBJDJEPTluenG0SP0oppBKE4Hj57h3u3r3DS6+/ydGXblLs3Ubogn/xT9/mex8+49FlS+uGe0UMRtsopFRJWRcCGQrpIs4313KjA95brEyTv3MuLQQIfBiMql1KgiF4xI6+nsysxYDavHqj4PW7U+4ej7lce7713jkn85amj+xPKyqjaOuaGBV9b7E+qdQzo5mNKm4f7BNdT+yW6EYjTlbEfpXOZXGDv/Y3/wH/9nvv01rL8b7GqIiWkVxKjqaRl29U/Kmfv890JMm0TPcMDB6UAhmvCgnnPO+evs+HzyzzZaRtHf/sn/8mq4tT/vK/+3XsMiNWM3Q1oVDg+0R/mJQVwja4zYrN+SXdcsW4sMn0dtdoHDbdWyzq2o9fFF6k3ueWcvPZYeKfyOUTJH4ZkDTszz/n6hiEFkI+V2TFmFDGLUgR41Xs2nZNCyEi5VVx9xz1YLc+iedeVwh51aa/1nESYruOpt9tw+pffIwUgSAE0iSxGERcAB8lUmm8Tq1HEw1wRUlK9ixqJ/yJQZLrEZNsH0j0A0Qg4HB9j+t6XGcRoxxVSIRO51D4ZEkmRMpdVplBZBpZOIrKkGV6KMLEc+u3UB7pk+CEGJMn50t7PDl/QutaNmvFTCuUSWhzCENrdSgGQ0j5vDHqJMSLyd1CObH7O0MA36f7PYSYBFFDYReFHHh54Y8+UuxisSGanD5ErPes1i2Xi5rLeU3nIi5IQlREOoQIZJnhYG/KK6+8ztH+jNk4p3fnfPioIzNrepd2AXHoIxslKDLFuFLogeMTvUAmg3VCFGkB8j0uelCwqRd03RJJwGjJwWjKbDLmYH+K6zuenayo2w2rpaMoOoRQLJeOum7orRv8Y7YxYqnVvN2xvljQxWF3JqVkOp3x6NFDQvSMJ+NE2kyYdoqv8mF3M239doQYIlrSO75wQ197DyLBe6SSw/PSjRuEIIbAZrPB9j0xRKTKBk5CoOsE602PHuwBnA9gINcw3q/YuA4vFc47YoyDTYUhDGHuzvlBei6TqshZlJS0TYfvFZnOwWtkABUkfVfvgEnn4k4RtP0biIFdhMpnOLSvGSsLuSPXEJxDCTg4mPL+Q4Gzkb5LyiUHRJHMIrXKIEDvHZ212zk4nY8o0FKSFwHZCoTSkBUgNKF3BJdii1zokUGiyTACggv0Tc960SDkeIDEayaTA7LRCKUlvvNEaSDTRG2Qphgc0/MBgY1ordFaY7RCS5Hc/bsUsC1zRVFqXJ7TZiPwFqUblPFMK8nR4YTxuIDgUDKyt5czGhv64OnqM87PWs4Wi8EOQAGK1nVUfSQzHm08YphEfdQImdNsVqybBmEKEIYoktqxVwGtkpK77z3Wps9vMr2zCpDOUmSB6DUuZGgzou0WdK4jBoueGeKmYXPRUJiCWweKzCjOzmru3VbklUIVhnW/QbXJHqBtFrSdxTuQ0XC8fwMfIueX5wgF08mIm8cHXM7P2FhJH6GPlrqtsbama9zvP6j+gOMa2wpI61vvIpvOU3eeJkIXJYXoePtxz1uPWv7u754TQwq3b7pUBGdZRpEZ8kzz6st3+bmf/wq//O/8GiKk2CbfNvzeD97h4ekFm86lyXtAgkJM9iDWgpKJ8J0hk22Ob+ja5DMmJcnSwSXfRuvCDtGIAXqXOGhSwDjXFHnyyJMCZEjImZCSn31pzC/+zA0+/9IBT88WrDY1vXPYec+o0BgZ6ZoGqQqc91u7w4SGGcMoy7F9i2vmxMwirKftHE3M2ZT7/H9/520ePH7CpJK8fHfEKAsUOlJpxbiIvHZnxp/8hZcoTRgUj0MhvaWFheR7llrHjje//zhl4vrANJe899Z3YPOUP/Wzh4xFlQRteUFhBE3nkiq+3RC6mnZxztP3fpSsekSDGMXd+6R6Lb379WnrajOe/DS334utaOCn6Mamjby8KqSea/mHIbHhGqVna4OynWO341XwXNFGYvzs+OGp/RfRQzG1VQRfpwntCrsXpuwXbVMQz//uqngbHvNjEMJU1CX0LfFtxY5fGEhrvZAStEApg45697kYCnslt4WdJAqo8jF71REiDj2ZmJAub3ucTbQBWeXoXBD0toesUMOmJM9yZKYRRmGqnLzM0IPKd9tmT8WwT0u7BOHTOc1ExvTmlGfTZ/i6p+nbwdUiFZBSJpuUEAbBhmf39zjp2PrjpnSQbW0BTqaOYYgxdYwGcWUYEj4S/+6PuBVbTA55trCcL55xdrHgcCS5uKz50bsPeXB6SQgZUpVEv2E6g6Ojktdfe5M33vgKs9ExpalYLk8Y5YJM9njm7E0DCIeNG+qmpsgrimyc/lgsSEdRJBje+iRgmI5zpE9Zn1LWNHUKoJ6MSl556VX2ZyXlyPL08UOWy46mhf29u/Te0rua5SLS1oG+jdg+onKDdxKkTKgTPc57nHPkeZ6g0XDFGRBSUFQlSEnTtpydn3HTHCM1kOgp2L4jhlTsBBGT8ajRKUKq7QgxDBc/Ddw4IH1939NbSxBQVTqJUpRBSAh9Mgu9dfMGeVbgLQit6bqepo30Hci1RecemXeYwtJ5x9644Gtf+Rw9nifna979eE6IJbO9CZPRiL63rFcdXdNhbUtRFBgpKSVAJFeCzAgKExFZh8Xh8Fz06YYcWLX0fZ+ihUzyRVKfemT9hCM69sdwMM5oGHHRKg4PCo6Pco6m+/QtrFYr+n4w2BUCXeR0jSJ6S9e1Q+t7mHR8j3A5IhhgQ7AeRYZGo8uSul/RdR2rbkVPnWwetKYsS5qVZT3vOX3akFf7CBSxF7z/6H0ODma8+up9lK6IykLsaDaXaHNEpETlI6x1CCEoy5IiL8jzpCgMHhbzC8BhTOBo6siJCKUQ7TGqPoPoMFGxWrQ0TQ947t04oppoZrlmNN1H6xHvvHvKg3/7HtaBkgqtNQJL9GC1pBobbC3QmaYYFbgu0PQtdVuzn5eJZqACrulQwhNjjw8OaxtiTOfi8GiCCSXTMnLnUDK3z1isDX007N95nfDkEdquKI4VTXPKpreIPmPxtuKrn9swGa+Yzmq0v4lhj9ntfXLOaduG1emKUhesVhbrAveO99kf7+GCx/oWYk/dLfjoYY/MJFkxwviWVf2MTN+gWTU8eXr5Uw255Ak+FBcBEJKPz2r+9r9+yj/5wZzOCRCKxXqzM1JtuxYxtA+TwMcwqyr2J2Ome1N+5U/+Gp//whfQSrJerZB4Qt/y5MlHdG2dDFcRmCyZnGpCQsv7hs4KChG5Na04mCbroHcfrZiWhldvVjydRx7Pe85XyTfB+21gnMFI0MpT6chf+vpdvvYzN3np5jjZWQwcKc2WG5zukYPxPn2Eu8eX/L3fechyM6fMcvbGE/LJHqZu0U1HiBHnYbVpefD4EXmYoWxB5krGN/b41nff4xvvPOMffP/vsl+u+OrrmuOx5t//s2/y0nHF4SRLQfcyeaopLSCmyMaUhjHwxLaFh0jCIiUM/9G//4s7JC1G+Hv/4gd8//1T/vP//X/J//Q/+d8wm82wlw/R+ZjJeIS1PR+/9buU+0d8/OARf/Pv/C3+zK98hSOj4DC1lbcF3e7Y/v+2RZt+OGgABjbeZ9+z7g7rusTpIm3Erh9ieO8QB3T12vkI0W0/ESQm5ZYOlz7twHPb0nfSiFZErmC1XXeZrWjoqliMDCkS19q1Lx7PRXKRCAzDYvYct3z3uAE8URH6uqGdB2LMUh68gKBAYVB6hDTVUK/GoSDMUDpH6VTYBQI39m7wuYPPge0YKn0sIeUqu47ON8hpgU7OZoTokUJRZTk3DvYxJm1U+ugY3ZxSjSqMTuu9FHpANUMap6HDhYhSJCNnnYRIr/7iS6xOl6xPNpjMJMXwcD62dYOUCmP0MF4jXntCtMSYOHpEPVyugFPX7GPUtdazUFhr/1Cczk+9/I5nxzw4f8jlcoWQgq4PzOcNnhPWLhG8tXZ4YO9wj3v3b/H6629wfr7m9EkPXjMqYbHo2awdtu156dVD9vcKilHGhw8+REiBDRGTS1qfDHg9Hd7b5H9jxODl5dHKo0SgyCPBQ9t2HBxNOD4qKKo168bhBUMYvUXJBq0MSu7Rt56uTUTEbIg3SUTjQVC+g6sD13c1iVOR+Fa37t7m4uKcJ6dPWWzmTMYjqqpilCcrCiEEWut0Yw2D+3q1ba3dfR8GW4KuTzt+necMbzpU82kXYYxhOpmRmxKixvaBKCVSa1RIBQEDadY6IM/Q2YhitM/x3hgzqmn8GQ8fPePw4IBbN27y6NEJTe1YrdqBqOkp85LZaEa9WeJcj9HwhS/cpl0Hom8gBH77B4r1JtD1geAjQnqUEmiTYoaCZCB9frbjYiOZlIrZSHLfCvblPpPDfQpjuH/vkCCg84HzC4cJqaqOUdD0LdF7fABtUlEMgWAl1o5w7oCMAh+epvQGOpbrmqbe0LY1635JMAGhBMY5mrUjtAJfQxEDZW7Qecamj7zyuS9QZRD6lqwYIUIgOs/q5DFjfQSFQGU5601NiILJZI8YLUEIApFMQdSppRa6npwVtV9Av0Rzj71JAThs5/nG736Ajz1aBdzPvcErr95ByJwf/uAdfvT2M05ONygV0+6XpPo2xXjIEIUudKyXc6q854bQfPzRU5b1ikjkZ9/8PE9Pz4je88q9myx6UJkhK3JGxxn5aEw13ePmy68T2jm5dMxKQdk+Zr/P6PqKTTvj3s0vURSaWy/foPMN3vX43hK8R3eP+dg+obr8gNPLwOXmjKenDzguxuAk3kaUbNmbjFLMVStwwiAyQ1ZNkaKlsTXt+ZIsL9Fhn0qXuJHhtdk+T0TJYvFTwCewW9vTEiV537/ONzdLvvHoMb/+F/898iKnbmr++t/4m7RdNzzYD3OGojCGX//FQ77yyox7RzO+MX+JkWqxy8cYW7I6fUJeFChtODtfYvuWTHmkdfzyKwdMSon1lrqJ3L2xz0u3Drm1XzEqQKs0D9RN4jBXRaR1kraPWJ+QOXYtOUkUQ/Em4HiWMx1nFJkcNpSDEImrVtsWpfjiy1Pu3aj4+Tf2+Wv/4F1CEBztaU7qDV3bEHxPWVVY62l6x/uP55yePOOrnzviaO8W1ajiaz9/ky+8ecBf+DWHMaBFJBOR470R+WDjsiPnCzGAVsnCJewKgsGgY+cZCkoLVMx2VwgBf+brr/MrX7mPjZHR4ne4XL7DPB7y2ksvgRklC4XLh1iTYbsGGTvGueLk6SX/8tFj/vjXvpBabM8VKdcLu62vIWyJ2FvKDAMf87PPcoMrQ4gooXbegltE7fqg3IrHovTEwb5JiEHgsTM03mlsr2DIATiIgsFXMbCl423Xt9TqHqhEkSF5Z0DZtuclpjSKrfDvJxV8z6vJP9nOTeuqx3lH5ywokbhpISR+blYQpEnorZCD2AKkkjvEbmQnlKFCRUUcxCbEmCyv+hbvHChNkCKpXgeKQ4rAS/GjCkkMDoKjmkwxuXnOqWJLh/LeI6UmDZGQ+JYxIpVg7/iQoqgYT2ukkgPtakByh7QWITxSGOJApZJSpnMch/zubXhuTADQ9poYY3bnD7G1hfkkGvqTjk9d2Lmo6F2g7S1SKGKUWAfrTU+vI0qElCsXIcsqRqM9JpN93vnRYxaXHbYNHO4ZLubn9LaDGDjc3+fu7X2qScHlfE4UoEyJzjfEpsF5i48ejx9IhIrgA5kEbRSFkeRZCWjq2iENoB1Ii9QBnUWyIMnyItlGoDEmS6R/mzzPQvBImVqp4VpSwvVC7BMtWSKjasR6vcI5x7Nnp9R1yXg0QuyDEhqtkgLNO4HbDm6u6BrbG3fLl3HO0XcdvbVMioLtI9J7pvOqlKIsK/KsQsucphv2WFIMPILkoh29wAbohKRuJctNpJzmJAWYIbqIc57OWjabejgXW+hbMh6NOTo44PJSc3FxhiRycDClL1bICEZpivc8dWOHAZs2s4P2Y9dS4adYZ1edwmKQSqGlY29SMh5XGCXZm1Wsmp5l27NYrAkkbpFSEde5IdKNQWEHMUDXB9pekHUK0xfYXiCFQ1rP5aamtxbne/rBa1AMeZmijWRkVLnGzLJkuyEFTdtTTSYUKhDaRSK2B4u3NViXeEfeItFYa5P/3GhK362GlrcfJqz0+QIB5wO299jeI5UizyoQgdquOT15SNu3SBU5mD2jqCbMOnjvvcc8frJmtfZp4ooOKQJaCrROnMkgkpF2b3s00NUZZxeXoBSjyYRi7yalS9zZ2dEB3dImioSWjMsJWVmRVyOEKvCqoMOxcoIujgnaIMhYn63JtSAvDGU+TsEwQuBjJJqcjjs0oWB+9pTYpbYldo2XBTKq5J4fGDInFXUfcD61JPK8gNCz2nQ0myU3biq8KxChpCoPuD0zGJmzCT+lQfGWayQMXXkXK14hO6p5+WfW/OzP/RxlkbPZrLh785jzyyVd15PpjM/fqRjnmswovnC74NUDycEkUG4Cwq4JTU6UHtmvaJoVdedxfcskj0xLw83xmK++NmNcKax3tG3OneM97t864HivHBpWA0UkGIRMG6mI2qH+Sood0TqRrYe5BnbUD7Fth8Xt8v/iogt7o4zpyHC8V/DLbx5iPUxGIz7+bg1EiqJgNp2wXtdMysj9G4qRNhwdFIwnGToTHB5WHCJ4RUUSmTQVFFuPrvR1a5ZNqqGQw30ndq3FXYs0xmufcyhCRKK53LoxSy+hYD5fE2ygIJK5DEGJsJHMrwjNCWOx4EuvjLm9F5A24putKvWTxyemrut9yJ+CV/f8eyS6SgwyKVGH99jGwl9Hv9K6k1S828Lyykx5+xF/fODUlnN3zSDlGjoXr9Y9sS2sk8/dC3/81TPjkHE6/GqXXzoUPS9+iN1a8NwaGBFKo3VEx8SfNFIjpMJJSfLt8yjhB3WpTKbCAaZqQimLXUEntsVkFNi2p287XO8QFbubQMnkfqGkSkWT94gBZTfFIKQQkm0+uxyQ48QXVMOQ24pnkqdgXhbpNZVORfZuY8XAWWdIyJA7hFQIECHxoNMG7Eqss0X5Yky0HRg2OHJr85I+16c5PnVhd7moaVpH2Pa8VY7UAqQD3eFEwLvkkk8cI8Q+iIofvvtDHj18zGa1YjaRbNqIDZFKC+7dustrL90lLzKenmxovSNqRV5axMLjncWG7c3viT4QcompFKMyY29ccuvmfbJsxPlFTdPXXCwceVfT2SRoKErJvXs3uVyuUmxULLHOJsGAd1jXpcUV8L4lDrvIqz7781wEhoutjaYsC0bjkkdvf8TlpaAqS3KRURUj8ixHK4kNSb8shp1R4gqki5TMQwMhOJzrqesNTdsyOzjYFX8+hLSzEAGlJKPRiHE1oyqmbJpFQhElSJOSLRLHT7LpInETidbyo/fmdL5guWo4f1rTt54nj59ycXHJk0enOO8xOhsI2IbjG8d8/s03OD2d8b3vdXTthtFoj0xsyE3FpBLk+bPkwxP9kA3I7qZValiKfgrxxKIzrG3FxEtWqxNu3CgYj0qUhOk459atPaLRPHl4hh0mwKKAdWPTwkFESk30Ad8HLteWrGgIogVZ0m4shIaI5eTsHGkylNGIIkt/k3ME5zBWsbeXcbw3IpMZYZQTAixXixTzZQzCVEQpsf2Cvn7GuJqmHa6zRG+wPjAajTk4vkG9ELhmie8tiT+iiFoRMczrgkWdsW4UZR7J5QhpJD53NMuaVd3ihOIbv/cRrc05ONjjm9/8kHWXI1XBZFQhQ4/RMZldSk/XCxCSUZEhga6rObMtJxdr7r38GrdefhN38Hlu3K5QWqO1oDhbsLq8ZLm4YLxnWNZr3PyMh+99j872AwoNSlm0STv8B2+dokVkb29Cab/Ew+UDVt2Kdb8hH93i5p0vM67u8+6jbzENSwoJh1mJCmkyRXmCz2hrT2cbep/TNz1GKSZlid1c8HS+5vTZnNt3R6yXkRByZke3uXu44M6dO7z0xc9/5vEGDPe9wKsJ54d/gf3xmK+/pvnj/86fxvlUiPdty5/51T/G73zz+5w8O2NaZfwn/95r3Ds0dK7jo48uWF6uePxsyZN5y+fuzmCkCdowEZYPHjzgW2+9S656Pncz44v3p/z61z7HKJckjvdVCysh3mGofa7v2uXAo9xacwyTvbwq0tJzBFtF0+71djZOP95yCVIkUmYk/8O/9AYRw8Zq/utv/C5VWVGNjjm6scfJ4yd84abiP/rzN7l1e5r4WzuzWblrpaX0HTE0DGPyDttOFEO6DwwyACFTLBpXBcv1rslz6M+1r9v5+uDGmAMhiGxAvAMukveBcQ7d6i1eLSxf+++8jjE9Sk7Qau8nFGnXCpXdr7fqxPT9rhf8U+xeQ0h86ygCHg9asY0TC2G7iD/fMYLwXOdHKXW9ckrcuigSGHDteVE4hMiT2Im4a8uGOPjmDcK5lHQ0jIXrCLbYNoQTpw3kzg4lEtjVpcMpC9eef1WhSyQpBlIXipIJoa4T1856CjHw3QUEOlTsUCKQ5QKtJCIIRB+4PTtmakrAIVKfFULEo6nXNfVyRd2uEZVJoGUEKTRyUHNnJqMPG6RMXqeykLvCLrViRTJT3qKZAURM6v8UQ5dAIaUlolQokw2bLDmM28TnJqoBNBo2JjEJooSTKZoP2LKCpZQphtQnay+l9Q5plFIjVfI41J+S1PmpC7t33/uIje1QWuL6SOeH8FoFFIOkN3pin/HBexs2y8csF5pHJx+zbBegI0JPCcIRhWP/oKIsPFpuEH7F13/lazw6W/HuR09QZo31K5puk4LPTbaruLe+PBrFjYMbvHL/Ncpqn8m44bd/5zdomwWFDhzMNAeHI46OD3jp/qvsLxSrZc352WOWqzPG6zP26huMTUWWB5KqBqBAkNQo23bpdeg5xoR2WdtjTMbNG7fYbJYsVwtc3/P48WP2ZweDCWigazu00WRZhjHmua91XdM0DavViu9+97uUVcX+wcHOjTvNGyEVdwPNVCA52Dvm9s2XqLv32fTn+NCnos5tHelTgHcTFaLp+OjEQd6SqYy9mWG+6GkbgbMWkINJItg+0PeW84tnfPCR53K+YbnqkXKMVm9SjV5hUmoOJgXe/nVibFEqKSW1FokrA4Pnntx5eH2Wo1A5Z/PIYtnx9OMVH1+8xeHRIW+8eZfjuzepsoxZleH6DtsmqN4oTVFInJf4KNBCgVDoLOOll4+o65pl/RiTB+bzS5Ty5LlkMpuhsxK0wrKmMDmua1msLwg+UugJ09EhTx9/BztXZKMVhT5is9mgzRRVjhGdI89nZEcZEU0tMtp2w/nZCU3bc3xjxkuvvM7ZY8/yWUPjapxLi7bMCqhGnCxnnJytOXsKhTmFpxdIbVBFyezwi0yOEhF4sWn5+FnGyWVL7Q4ox/sIAV2/5Pb+EVqm23q1mdN3PdYLolP0bU2MEatHfP6X/gwHN+79/9r7sydJs/TMD/ud5dt8jz33zFqytl6wdGMdzAAgpOFQHJJjIkVdSCZKGpP+Ad3pWiajSTJdyDRXkplkNIm8oGZMHA2HAIUBiGmggUajt6ruriUr98zIWD18/daz6OK4e3hkJYBCVcskg8VrlhkR7t/un5/vOc/7vs9D1u7w3W/9S6QOXqGiKbj5zi+QOItkzIsPfkQ+GdFUOUQ2GLsLiZCard09dCdGp4LdPYGpCoyZ8v0/f0Hnym1MWVCePKPUn1A8/4Q46sPRiLGsGbsG35TcuWaRSYxRmrKEVpIhsTy5f5+zF4+489Y7/Opv/33+7FufYkXK7pVd5tMpO3vv0u3fYmf3TU7u/yek2T79nc4Xvt+W33O8x5iaoyf32Lq+R6vTJU7a2PEhvpyi6px//x/8Fn/vV36e08OnPP7ev2B//wWYDrduDnjz7g7OeYyHt6qUTx99l/c//R47O7s8LLa4//A+Tx9+wP/uf/YNui1JoiWtWF+AB2KldblgMPlsUbpfTGaWjQar94RYm9UvAJE8Xw7Uhe2sx8spNKUUpZHUpPyv/7f/exwS4R1aS4rS0K6f0Sv+JKjiL+qJhNQIYTkHjnLFDS1TjYHK8CgRrQEruwIXeLtyRFBymW4UXABhq9cW5+kgmHFeZNW08ty8ErrmIUIJff72y9s8P/vz7fCqZcTavr94LBVJQip6DVD5pVfDQnxlLY1p1kp4gJfG2AA6lqCOxbrLROuKp/1Lz3ttnTVQubhYq0vt19KxywmBvPDR+M9uT4ax+MWTA/p1Rj9p09MxTZVjpKeTpfRaUah51xGJUqS9Fv3dLnGcooUgIaZve5zsv8BbiFvtMFmRCmsN5fSY+fSMeT6laEq87y0aIwnamIXFlQ7FgmVckFRxHK8mTaHu7ZwVffUHJ1Y/lZIoGQVQhwhWYa5BoFef5eKyL65rtKjFt4vP73xyFdK+cvVvfSKTJjFKCBQ/Y2A3KyqsWEhieI/1FrMQf1Wo0CXkBM4apuMC6UbESqNp2OgIklQREdgLoTSbOzuk3W1E3KOsR1zd2WNmEvTzMdNpQVn6IMsgJHGkVqDBWqiNo2qgbByN1WijKIqCs0lBPivRUgARcSuha1o40aWoG+YVzOuG3IzpzseU9ZzMxkHQ1jmCKLELNk8e1MKsN9wEbpUWWMoHaAFJpNna2ARnmbkps+kMJaMgviw8k8kEiSfSiq2NDZIFqPOtFtPJkOPjYx4/fswHH7zPO++8y7XrV/ALli/Q0ZJVkYf3CA9ZltLvdei0UkZThWmCxAE+dNRYH4SUjfBUtWc8mzGbt0kjCzZ8QauqpCiDVI1xFoddNDwYprMJzw5q8ryhNDVaKh492Uday0anS7UXU9ce4QWRlljjyFoRcayQEoyBMO//4l6xSjfkZdDrmuaWspngjCPRjiRNqXDMZ3OqsglWS1KQFfFKlNgYjzd+wXRo4iQOiuauoW7GeNng8DQOkjQmTiVCSzCaWGcoF9HODP12i/5GlyiNsbahqM5orGU+T6mbBmvCdbPOolUEAsajMXrQIdKWqqpQOiJrdej1N6jzDerpkHo+pW5qDBaERqiIg+NTDo/OGJ1NubKdEqsYawWnJ0NanW1UnKLimEjWWK3IrSEvBcbXKBlkJGobai+tabB1iasFxglmSNLeFXTSIkr7jHPPeHRMOTshbiZMxsGGKlJQng0xVc50eMB8MqTOZzRVTtHkeOdJk4RBf8DR4QHJLCPrtmj3U2ppqUtJUQqiZECcbBCrDY6GPw4OBD6k7kUsQrV0nfDs8JQ4iojjjCzdQIpgyJ1mmpPxkO3G0t65Q1kZtFJ0+n2u371JpLdQREyHD9lrxWxtanavfglFbGDBMVFXFZ/ee59WdBfGMSfjKXs3biBdjmvGJGpClubsbBVs3d2kmyr63ZRIh0JpHwgE4lhSb2sa6+l2SlQ6YQPJ3f4uN3fbRCuyRay5Zvi1lFUQqF2lUVdaZUum6FVdiUs27vyclob2F15aZ1RYkE/nb6+wSy56HImb9DLNPC8xTUO/s0maZeRHEd/58IRf/kqfLFnvuFw85BYsDYvsxxJMLc/tAsAUimVtmxcqTDJW710Ea+tMml/7fyX/8RLiirTHLRxNLqy/YvwuXvuXa9Q/A4FWh/Mlkd3yfNfH98VxhCxIqKE+B1HnbN16g8JKiYBFxyXuMzVwapnKFOKcOFgHCYvPYwkoWAA7IcTCgmzhLOHOQeZqVSlWnshrm1tkXxc6eItjefzpkDu9bbY2BkRCoHVQxBAedBwTpzFea9pZRH83YetaHyEUs0lBU1tSl9JMJrQ7bVy1gZKOpi6pyxnz8RHFfBwaDSK1SmcDCOfxFbjCUuRzrAs+uJE+n1wsj9+YRdPJ6lrBeuYufG5yxaouLd2Wn1FVVcSxRIqFvZpYWLqtrOjWGd9X30fr+/Leh/Iu+f8FYFcZg1RB4R7fYAndOgqF8jqgWAve1eTzAlc5aBpa7WAh1N+Myacp1lu0cWxfuUZr4waqlTFvGjq9bbKxR8qYs7MpRWFxNkJoRRx7dBSaGUwjqBqHqi3jeclo3FCUJccnR0zzmnnukARhwLQX0clTJkXE8XjKaDRnOi9x7ozebExRzunZPtYGPTYRJEIRQfcZJYKUgHMBzC15XWFtsGkSoIVkqz/A1hW2bjganiKkorKG0hmOT4/xpkYLUDTBPFlpuv0ex4dHPHz0mB+9/yM+vf+A27ev0+umCy08s7IUWdYSeBe++K1WSr/fpt9JGQ41ttbUzmOoFzUTHm89VnjqxjGdO+Z5jYv8qtuvLHOqOhTYG29wwqITkNIxL+aM8ynWhtpj5wUf3/8IU3oG/S3GJZRVGGC0kigF3U5KlkU0jSEvPN4t++6+WAhdUEwcxcyQ1w5V55i6xtZzBhsDGi0YlWUAdkVorilySaeToRD42mCcQ0mN0hIlPZGSWAzGTIlScC7UgKlYoFOPVA7rFUrESK1pZz2uXB3QabdBhfRGU81onGM236BuTCjUbQwGh440QkbMxyO2t66TxDHWeeI4JWu16HS6mM1t5sNDivEZhW2wziGkQqM5Pt3n+PSY6WjC3tYeadKibBqOjo/YS/rEcRslUpJ2CgiMragbaExBFEE7hXnhcXWNLWfEskS4GCE0FYLNrdu0ultEWY+nP/gRs9MJCTXXNrtM5mfUVY1st5mfnDCbjzg9eRJcWUyFbWrGZ2MsgkFfcjVNePb0ADVr0yk9b213MDpCRBrlW0TJgCztM+injKsxsTgmIqeOW6iWRLoYXzqevNinpTRbrR69mwPwDqkkm7sDnp68oLCeqLtLYz1JktId7PHmV3+b0emY2ekLhvvf5603Uq7spVzb/RkAO++pq4qn93/MWzsSIS3P733EzSv/PbSocW6CnT8nEzWbseXa3T2EOh/gQ7ouPKy19Lx5sxuakoTktszxNzLg9QB8Fqk85+SC1zrnVWD5UDzHH0uR2uV768ud/x7OYwXeFttcPg7k2gM9bG+Jcy485hcPZk8h+hzKN4gmp5ydDimLmlakiVopZ+MZ//qDY37+7S5ZqlmmRJf80wpgrb1+/v8SQC6XWZ77YtmFR+lqwfWs6Llg5uJXv1rovFt1PZ24aHFYgkm/Uu5dY65CmvM8s33xs1jt+uLuvhRrJ1YP+qUt5bKWDvxSy1QAC+eC80a6czC6XN7hlirBLOU0luAKwnNq1YWsVDjTJeu7/HducwGE2rCwjiJovAZ5lpVSxAIsBucmv7qHXr43hZTB7ss5Hn4yZOP1LnpHY4FICRpJaPaJomC3GCnSRLG512X7xgCPYDqaU5QViUpoxkMG/Q622CCKoClzitmE+SgAO6M0sttbuz4eZT2u9Nh5zXw2RcUaFQWDgfXlAmMfAPJKPmhVw+fX7hcZSnYICeYlS2qtpShLtEoQKmjCSgV+MXEJn2OYZDi/6v9eTd6Wt/M5exeeU0qoIAb/swZ2fgl2EGgNzlqsEHgRzNHxCu8EUQx1mdM0FUp4+u0WgyzlxtaA/tt3eP+nD3j8/JT+5h12rv88ezt7SF8ghWRyts/Th0+ZTiZYa4JtVKxI4hgtLY0paKoGKwSNljx6csTTJ3+Gc4p5ccpwVuNduADT0nNwUpHXZxxNvs/JyRlFMaWpR/S727T721w9O2Vn5wbBuFgihcdjFgNu2E4AVcFW5LybLDBiWIfwjkRHbHQHSCeZjXKKqmLeNIgqZzweonBoLN/5zn0efPIxVT7n7t27PHz0mNlsTt003Ll+je1+GoytbYNtZDBijsLP85tL0u9usbd7g8nokMlkSCuNkKrDs8MRFkMsLKVrMK7BO0dTwWxqiPttNvs9xpOSqvaUjWNjp8N0lgcmsB2zOxgwrwxnswovG+LEoWTDLN9nXnhejF/w/v2frgFdR5rC3t4GrVbGcDhhNimp6gbzOcUUXxUGQ14ZZvMmeLMrQe1hNnf85P2f0tvbJe5vUFeeJG7Tamfs7ewQx5J5UZLnE5rGMDc11nlaZeiuSpKEfn9Avxcznc44PT0LhejeYSzkuWNy9gxnCryc0NRvk6Zz4thiVZe02wHVYl5rrIxD/WQ+p1AZVTFDupqdnQ2cKREi5eqtN7l/73HQpcNx/dp1mvExvpwhbIU2JVYpnILr124FJxRfMz49JFZQlCX7T+4znJ6RZTHdVsyV/jUaFeNUxLtvXidtdSnLgoNnTzg6PqQfCzZTwbQ0XHvra3R23yBuvc3x/g+YHj9nevRd3ItHGOnxSYrt9xj0t1F5weHRKSejv2DvxjXe/sY3ePThh+RVQWkNqDZbOze5fmWbd17fo339a1SmpKmnvLh/n8bmREpzpX2dyf1vcWwrGuv46i//hxwf/YDp8FOu7d3C1DXVbMpkts/V7jY7gy5XtvpMxlOs8LQHG3zj7/46xekYf/KM7/wX/0eMvkH/1h1u3r3JXTvk09FDxPSYQTtCJSMevTji+z8p+ce//oVvOSCAnY1uzH/0W3tk2RFKSm790g302V+EpgXhUKlfWP7J0Bwg3IVUqVo8eJ0MdZ5SLlNXYSQNsQBcXoT7e/mat59hs4LH7zkcWmYO/HI5wXkB+9rDHM4x0HrR9bJpAhYATrCsCT/fjwtalwMJN1QFB2O63qOw/OAvvsNXXrvBls/59a9dJ4kTll3p4XiW3YEXU7ufQUEXmApxzqIt//4rH2Kveu9llPUSDbkCkfIiU7fE1HyWEf1LU5ZfEtQtwy50RZMkDiDeC7xVAc/5heC/CA0WoelFrO4pKRX4iGXTnFuNtyFbIgiZpQBWXSjkX4A7b+tQe4ZECX3O9MIKZIBHCblGMGgMDuuDtpvy564UXoigwYhZ7F8iESgRfocAVvePhrxz9QpJFCGjDFcd0RQ1RAnj6ZS282StNj98+JTOWz3e2OiBgSRqQWWYjw6ZDoe0Ukk/tXhhUHGGsx7bSObTHNdvEW91ULKh8TWNNcgqYno0ZH4yY1Y0dNMWURSjowhra5Q4NxKom2DPF7Qio1XTD0IgFoyocwvgt7DtW9bGNbZhWhekaTuwnCwcbXyopVwu6707n2yEi4MgsIgOgW8avJT4pbfuwhWlET9jYAce70zosAsOtSCDr2kkI4LNR4QXHitrvHeMqzl32q/T2dxja+8qQkm07uNcyccfPWJ36xNMXXNlc5uPf/o+P3n/xzx79AQMRNrhhUNpR12mSKnoSsfNGwlJK0FGKcOR5HSUMy9qisbhfOhuXerCzWZTyjLn6OSUoiqDcKoyKDlnPJ4wPDvDmGZFb9uFXQ6cz2LWO2HPmylCN613ofDUORekSHpdXn/9NfaPj5kVBUVd00ozsDW+CXUn0+mM0fCULGtx68ZNWlmG1opPP7nHi8ePSKRi747l2vWbtNt6VefncXjncE6QJlv0u9dptx+QZQmDjQ1ef/PnmRWKe/fv8+mDe0R6jLcrdSEcgihJ6Q82efz02WIWZjl4cYJpDFIsjIlV8P11jUXrIEYrcDjb0DTBb1WIiHYnwxqFsXUQl61KjLHMZwVFaalri7FffOSbz2FeQNEodNKitiIoghtIKs92krGzu00SKXQSkaQJSZxQVjOqInRFmaohbXXoJAnN/AiVdlAqQSGpixJXGyKpwuxoYYtWzUqq2YR+t+H11+DThwdMpzFeSAyn9FNJkkjquaA2ntpYdFMSbW0gG4c0DTKJMLbEIYjjTSKlkT40sqTdlG4rIW8nTM40woSBT6uFmpVSCKUZn43BV1jv6GTBRqqjFDeyjIGccDK3FES0Nm+H2bp3JLEi0206nRatfp/uzh1qK8lLQVs+pacaonabdPsG/a0ezjq89RjrmRUFZVWQtiRf/Y1/yM7eHr1eC2szjvafMRmeQHLK9pUdelsb0Grx1TvXGU/PGI8UcvertHsDWlmHfrZBvdCc0krT2n6L3WvbFNNfpNXa4PinP6Cw+/R3tnAEF468KOl2Yu688w69rT1m4yH/rd/6LcaV43luKYdPyPKEjSbh03v7DIsS6xuydsrvf3ufs9GM+cTyj7/wHbcGzCT0OlGQwViCIGHCJG8JnMQSIC3ZINZIoPDU1/pincwyzjNWa+Bq9crS/P08nbrS+18xTgt1sjUwKdaKnC4M/cuHtF/7e7nccrOrc1+6HYQHuTMGEWt03Ga4/wCRpNQW7j98Skt5einI9g5CNit5iqWA7Oq6Lbe9ys2tTpt1TvE8LrJ0F07oQo50SeNd4P84Z+leiuWD+6Vrs9rB8vjWmkhW1zugrAvHfd448cXHOO8ETW1xzpIkCd4sauRYzBfkopvY+1CbhUCIc2mnJRANDSZgjTlnjb1HqDBxWJrKL59f1lqEUMgFeblK6y66Xc9r+papXxG67FWEswbjRNjXWqOLdQ3WhXNRSq7dU2G/1vvQgATB8Uen9Fodsk5MXkFRFvi6pvQO4Ru++pU79DdaVKYkkQqhPUILlNR85atfIVWOMh9zfPAcohSlYhKdMG9MYDv10sUhqE4084rRdEo+neNqw4vxKZu+x5X2BtgwCWMBvJbP+KZpkHFCYwxNEzQHpZB4Qk26Woi1uoXubdChrUNm0TZhMuXDvecXzVdujSAS6+oRi/tMrFjokB0VCHBiZV3nftbATgpWaT5EUESWMryWpAlatVGiReMKjB9TVQWFqXC6jU63ydo3mU4PMS7CWMXzZ/t8ev9jXFMi6js8uPchz588YjI8Q9hQiC+0RwhDUwUz6quDHtduGNJOF3QX5ypOx48p6pKiCS3HkjDTCVImJVVpKWsbqF4FaSwoy4LZfMZkMqFpqgDSFje2E0ublmXufEn7hxs00NDL7fuFHUigzeM4YWsrZV5VeCmx8zkIjW2Cz5vWGqUjlI6J4oRbN28y6PeIlOLRx58wPT3lhdTYeJOt7V1arQ7W2ZX2VOhwAq0yIt3FeYXWMRsbW7z77leobYvprOT+g4dIFVTR3YIgNy60X3gpF92qYao+mxfEsV7QxgTpE2Nx1hLpCC1VkG2xBoFDK4GONFev7FHXFUU5p6pG1HVNaRuKoqaqLI1ZdnF9sZgXkryWlNbTzhJq48CEAuyOFSgd0+m06fc7WIKjgxCSum6oqoqqqnHGkAFaaayHNGmRZR2SKGE+C4yeFBLnDE0TBoK6KHG2JlaejX5ELGvK0pMbQeXLkN5UEdQSYxxm4XmcKoUiQkqLikA0wXJILh6kzlpMU5PEPdrtjE47I0kiahdjpIIoRvoSJUBrQW1LmjrIjsRJjJGQKUFfayKbQ91gXUgjmCbYrsWxop8kRJ1NdH+X1u6bmOMjfDFGyBnKFmgcWseouEsaKSIhgv+raEgygXeaa3feYWNriyyWbFwtaGihW1sM6lO2dq7T67VRvYytvV2c9BRlyc7OLp3+FknSRosIr2OSOKYdJ4yNIEv7xDJjsLlB8+wBSTkGWeGFpqwLKlOwe+UKV67skbV77D/+lNdee4u49Dx9dIS0M1I7JWsmDI8PmHmHxdEoy6PDhrOhp5x98Wad8wgprDiSC7uo5UM+PMgv8GaLDka5AFnr8hMByywgmThnws6zOed/h/6AdUmPxT6W2zofgpZvXCCSXq7R+QzUkEtKjgvA7sKCy/2vAUAP1HXFtDzl+PgZnY0djIg4O3nBUcchNltc3UiQsgnnK5fpvMVDSiwf/MtjX2PlxEvntMRNa2OGX/tP+Av8H+cXRpx/Fp9RDV4DeeKz12ntAq72e77MOVhfLbPc9Rcf1i6E90Hmw1qDc4ALjQ8eh1cCpYIckrUWoaNF2nN5jwcR4VVt19pz6PyZtQR2iy7bwDUBrOznLhIW5yn6AEKWIDBcYyUUcrVfv9JlA1blQ0E+TK8uV9hmeFY21iB9cJeQUQJ4hJKLRsWaSVGQVg0923Dz9i7tdoK1FmMakAIvHFXdsLGxiTAzismUYjal8XOkTuhtblHaYMaaLoBkYw2maRCFJy9KiqYikoLaWoyzS5Icrzx+ZbEWzr1pGrSymCaANiHEohM4XNclWxd8Xw1VVVHWZSC8rMWK0PMqhWbZcLLEEUvguyZBuLgFw/dTCo2WgUn1y5vF+wsTuL8qPjewiyNomlBzBY44jYN9kXLcuHGDzcFVeu1dRrOCTx78mOPTfWoxZ1I2TEtFZQc8fn6Pk7MJ83LCdD7nj//kiJ+8n3Hn6ibUFfl0RkxD3WiUDhfYmIayrtm8cp1/8G/8GmnnGe3BNjLdovFPeH56ghmNyWtPokMOWnhHXdcoFfzwlHJEsUaqULOSz3PG4xHD4SnjyYRWq0scJ0Ek0trV2PKyzIn34QM3xiy+jEE9eknDelh0yu4yGAyYVRVPnz5GCIXWCUm3z9233kEJwa/88jfZ7HRC9+XpKZvtFtIairMhjx895I037tLvbyz88sSC/rWLGZejLGuePD7AmpR+9xpvvPZVRnlDmqXUdR4Eg7VA+PB1zvOCo5MhTWOo6oamsTgLraxDHMeAxVQ5R0fHNC50kTnryNoZSazI5zN8W6N1Sqe3yb/z7/7bzOYTDg6e873v/zmT8Ywir8nnnqIMo7D8Eu4TZ7M282pO4zxKpEyqHBpHWwo2WTZ8GN77+Xd59viQ+aykyEuqsmY+zxmPx7TbCbPpGfV8xqDb49rVG3T7PRIl+OP7HyGlp9fv4A1Mc0NVNZTljO12ilYR+4cx79zZ4cWw4NnJDJ+3KYoa6+f0koyqqqlMFBhrWxHFKbFqIaiJsi7UgoOz6cJ8PGeeT+l0Xsdu7yJNwXw2Iau6VAgKodB+QiIN7Rg6O5KNXovSCp4VM7qpJ5I1dT7nuM6Z+xQbZQxHHukmSBraqeb2jQ3m+gojv0v98BPazEhEja8kx0eHjCZjJrMJ9fCEr3/9Nne/coftq28gdEpVNZwcHXPv7ITCCAabG6juda5tvU3SanN7r0NeToLri2mYSHg2z3lxNuOtX/gaZ8Njnh4fcnh4SlpNaGnoxJqnxyNMU9GKI37n17/JtY2IQm4wPJih0y5RHZP5Nn/37/+7PN9/wr1Pf8ynD3/K8YsnqKTNIOuhb1yh3e1grKMrIw737/F0eMpHpzN+5Rd+nlt2wPg0++I3HOcPodWXfa0zDcIAvEgqLawCBU4Ej+JlWmX55F911bnlmiEuPHg5xwvL7YUBfp0dOl9whQf9+bbWt7s8yFfijjUw+HLjBItNygVAWq4vVcThJx/xp3/+RxyfHfH2u19jsLlNOv2Ik08fsPXaFr/41bfQSoJU5wznqjBwcUDntMRnd7o6vvXX7OolDxfA5sWVzgVhl4zcy+xnYLY+h9XcZ7a9flAvHfoSnH7JMMaFzIaxNLXB1vXis1eoSBJFCk0Yh2UUupStXTI9AVx4exGYLYGYlMHZZkk+WOPx0qCkWGikrdlZ+XMx/mUNHrByO/AeosijvEILTSQ0NfUFUGOa4FTjvEXKKHx3vFutb52jrEqUDx2w3U7Gw08/YTKtKU1M6R3f/ugRrSTitWubvP6rOySpRBqofYUWMXk55/DTp+xu9hn0UrJWh6vXbjAcjpibhhd2ysx5YuOxZcXMzCmb4KMdjxVFVVFJj95IuRUPSLSiMQZt9MKGJRAosY4onWU2z5EipSgLyjJfXTcpFEpFBPtNg7GhSW6e55RVSdKNQv2dE2sNFmLlJuEai/WWiCB8vCDlQq1i+AXlNa24hVSSxgatXaFUkJj7HPH5gZ3OME2DNQa7+LZFkaQVpbzz5uvs7Wwy6Pfp9t7jn/4XBeOzKU5WPHn4BJdLRJXxycOPmDVHGDXG4KhqSVHOKZoRf+eX3+NwX6DVlOFYM60slQmDWdap2dxLuP32XV4cVehsl87GLht7Ba3OBlqPacwUb9RCGdoirCBNPHG0mPU4QxQnbG70GZ0V5Pmcp0+fMJmM2NzYot3uoLQ+V+f+TPrEL1gzt7qhl6a8yzSE96FmQglBksSoOMJeuYprKnANkXPcvnkLISRZK2M6nfHs4X3e/95fMMgScB5hLDdv30YqRV3XJHF24YvrvEHoisZO+clPPuG9975Klu4wnxm+891vsX/wKVFWY6pAiUuh8MITJwlVXbG/vw/WIpUmkTGeKOiXKUE7S5kMj3BeoKQkVhG//Zu/xY3rV/jWt/6Ie/cfkaQxN69fYz6fUhQFTWMYj6fM5xV15TBmoYeE46/qGP/rwhqPaSx1VTGyDePRlEhI2r0eIomYFQ2HR1OuXL3O6dGY6WjC2dkZRTmjrhqUUkynM7a39uhv7tBtJ3T7Ea3MYKoZb97qEUUZrXTAo/0DmmpOnlfYGqJuiveaoyOB6c4YzYLVW5TGzIuKwnjirOTsbES/22bryg2s8hhrEKbCmgKRJFhikjThbDxma3ubKFXoNCPr9ugUA/rtFrULrhQ+TpjYBDrX2Wxf4UZrRCeqsE6xc7vP6ekhpqiYlg2TCnIjMM4g8gm+nqOUI2lFTGuJw6DcmCcPfsRmpkiUJK8Nzw9fcG2vz7/x67/AO29o+r2MVivD+RhUjPUx5W1N9NNDHj+7x4+/e8bJ2RQVJSRJytH2BrP6jExpdjub5KJmOD6mnE350Q9rXr/1Om/eusHtG3cYPX9Aq9Vma/cG3aNj8tOnUJxSjvbZu3ENt91CiYrD4zGzIqco5/yL3/tdnj7d5+R0yLSaEKkRqBiv21y/fZfHTx8zef4hA12hOn020owtfY+ygqxt2L1bffEbDhYq98s6q6Wm1hqbAyv5iSUIk0swt8AFSwZkyfova5UuYBghVqm9ZRZmudR6/dv5Ci+xTWJtO5+JNQD3ivdXPQpry63C+Yukl4R33hhw5UqXprlJlqQoZfnG7W8QaU2aJiRpayGYvGAzFlpe59s5dzU4P/h1xo2QD/TnE2i8RHDuOHGRnpTn7KN8NUANsVD0F8vfP8dg9IrrdfGTXxa7XyACv3CUdc2sqKlqQ9oylEWFdwYtPNmghTOKxik6cQcpYkAGsf5FQaS355MI7x3WligZkVcRz480d246ooWFnKs9QhqQDlFXJHEUUn1e43xIo1o8kU5Y1sTNqwJrDVmSoVWMTgzGh6yPVjp4cDuHdw21CTV7AoVxHmFNyFQ4hzFu8dPS35QIWTEZj5mMp8ynOVXZIKxllheMZyWNh1u2j7YeYUHIBO8b6qahLDy//4ff4vZenzevDuhEio8f3+ekKNl+43W2bt5AtiMqVwOa09MRpyfHbOabKARWOH548ohf2X4X5RMEjtRZppMpHkm33w5kivV4C/PJlMaaMEY7gmSPFMHRxwVRdek1rhEEL9qIhBjpFCDxFqx3eGlBSeI4w3mPRtOLe4s0tqVxFVJqCl9RW0OmWyRxO6g8uBq8CU2hKv5c99bn51S8CjSxl6Fw2Em0iui1u2z2u7RSjxITBp02ic7QPkF7TVPkTM6OefH8AaejE3yUI7RFx6CFCgNEHNNpJbDbo7zboJ8XuOOKZuoQTjDYiOlvZ0SDHj3eQiSaaeU4m4ypawE+RokwW1mKtDsdVNJ73QQVSYoiJ05iut0Os2lNWZWcnQ2Zz+cY01yoowvU8XmrM3BhZhOKHxezFWuRSyVwHz54t6BUFYI0jvFSIJxGeRdEcBHYpmEynXJyOuTg8JD0yh5YR+wVGxsbwY7Mn5v+uhVVHozCjavIizmTyYTDo0Puffoxn3z6Iadnhwhp12auHoFc6EEF+yrpxWrAtMZhqhojIY0CtaykJk4z9vZ22N7aZjAY0G632drawnlFnufM8zxcN6BpzKJ7GOp6Ic7Isuvni4XEEEfh2LXU1ElMJCVJKyZq9WnImOaCdMPhZfjyTKczqqqkMaFb1QuHI1jwZN1W8LGlwdqcQTvFWE1RGMrSLCRaFDqKg7WdcYwLQ2Nr8sZS2VDIXNY1UliauqKsCqzzRElg6bytcLZCyCikwIUINjHeorSk1W6h4oik3aUz2GLr6k0KDhZpVcFkViBUTKvVIelmKDFDOEtPJZimTRlnlIkgYYr0Aq8jst429UxgTc68qHj0/BCdFKAyytmUsxK0FFgk0ku2Njp85Z0rpOmMbi+l205Dh7MIg1cca2z+gnx4wvjwmHKeh1SJVohZC5lI4lYLtKa3uUnavsEulsbA+PQYX+YMepu4ckopBNN5wfHRAao6o+VzXJNgTRAbtwJGkwkOS9bKONg/ZP/giNFkSpxJZmWOjA1ZV+Gagtl8RFOfUXYi0riNjRSdfousG5O2HVF79oXvN1hrHDhHRmsE2TKFuexKFCvAtaolWo0fF2OZwL0Izs7r5ODVqdiX4zMp11eAvYvpx5eFP5YnulzyswDyQvrRQ6sVkWYxkAZAAWxtbCAWGpFCKoI6rVhjstZB48Wrt45w/csvXDgQuUbVrW9j4VixyN2ey6qsM25iAbLPj2e5nfNO25eu5TIXJlgDc684poBAL3x2XzQaY6maUCo0zyuqugZviYQntqEkKBDHAaQ6v1DBEqGY35kAqK01WGcCS+QlRQUvhpa97QiRepZSgNYYHItnigjjvPAea4P/r1vytj50X9d1hTGhVixJMiJl8EtJGSEX69gFUxekpYTUCNSi8SO4OYVat1D3fveNGyAcDx8/YzKvmVeGqrZUdY3zDuMFhbGLdcA6kM4t/GQ9IhU8ezKkLObMJhM22in3jkaMm5oyOaL9tavoSFLWBU5GC/suH1LYMtT6jYqcpmkwBFFitaizNtYgvMA0FuEErahDVTVB3FhGgTWcVwgcm/1Q/ycQxCImIiWWQXw5USlB50CTqAjjPMZZamOJpEU6RaQUqUoXDhThumoFpamD45YM0nJaapAeI8I1sJ+zIfFzAzvnQmdOAHgCKTStpMXVrR36rQTfnHE2G7HRneIrkCZGuwjpc0xxwtFhybwcEUtPnCiyTNBWHXrdDpsbG3iTsbPVY2dnB5F8SGlmzCtLU8K1q312rvRp0oSrb/0aB0dPePjoIz689wmjUYE3mkzD3HishaXX8cbGBtevbRHHmv0X+wgp6PV6HB6MyeclZT5kMplgTIMQ4cY7r1uwK3D1MkNnjFkVQobcu1to/QicM4Ei9wLnRXDiEBKkDjZTC/uosiw5OR0yPBuR5wVnkynKQ1tqNjYGJEmMINDhAWAujWbCF9lYQxTD0/0HTPIRJ6NjPnr0IZWZAQ6lFWaRblVq0aEkJTLSGBO+iM45qtqQ5yXCe7JEEitBkqR0un3eeecdlNZMJlNAcOvWbU7PJjx89ISdK9eDZhfBHWNpeVKWZlHgy5di7GJdEySGYtr06aQpQgk6nZSkdxWnMqZFhJ+NqLzFCjg7G1HXTUiDCUfcTWhsRVFMyHo3A4/Y1JimoKUTjqc1Tw6OGBU5Vmp0lJFEEic8VVMzmucczy1ChZRPU5SLugtNXeTUdYl3llhphHPgGryviLMrVAvGINIxcSxpd1IGG/3QidUdEEcRKu1T+fcRwyHlaI6RtKIAADDKSURBVMLZyUnwAm63yN0WpU9wZkpZnJFlkqg9IGGLOD0liiU6TbHtO0yOHzI9O+Dk+AmffvqELI3pddokdDiazMBZNgZdNvqb3Lqxxztv7fAv/5vHvHF7i6ydYnyNcBbnBcbHPH9+yPHxmLquiCOFszWuzpnnQ26+8Q6D3gadboub7/0S/b1dolbGD//0L7j34fd5nI/5yms3GR4dY3SbZwen/PB7f8zNTc/tnTa2t8noaExjKyb5jGeHL9jd3eb61Wsc/uge87Kk9pbtTp/p6QmxsvS3FaoZU02H5PMzJnaDdjQm7sYM+pvsXO2AKijd6Re/4WANoF18uC+bIc4f5Ocg5gKwexWoWwNZy334ZZp3tRCr2plXSpf8Jce5vv2LIO8V66z/Ii9ud1XDI8/Pi+VrfsFSSrUg+gRoiWfhSy1WG1xtc3VM/sIezhfx/rOvL89hjVE8P0S7fuDnGxfnnq0IdXG5hb3l+XaWQhHuM/sNyy+bK5brrTtgrLON8uL6XwLdNcbS2DAGjyb5wgfdESlomYUki2JhAxeATigLW9TUWYNAUpuGxjRIAUp55nXNyWTObLqFFoo0C/du0zTUTR2ICeHRKkL4COuq1cTCe4FbsHxlWS6ejYI4KYhkuhDUFcGcAIf1jqYJ3ZxSKpSKkVJjnMX4QIAEJbTgrvEbv/oNPv7gMX/2/ffZ275GUTmq2lIUJVEU6v5EJBf+FgLhBc44XAQkgnQ3ZvSp5fnpCT+4v8+gkzKJPFbCcPiIvbfeJMtSimKOTnskcUy/36Ov+0zyGdZ6ysaGRgeCXVcUpYjC4Gzo6K3LBk3EVnuDE3eCI9iGJTLhxekU2zh20h1m8xFJFLPZ6lEpgYgUDTXtJMN7RyQi+mmfyhlG8xmTfEYkKzKdkciUVGaUJtQdaumJlQAX9E1L0dCzHiUDsHdK0DgTwP/niL9RFdRSk0YjwNQkwnFls812q42nT65hPlVUuacpDVU159e+EXPr5oCr13b4v/yzMabWQEwct+l1d7h29SZff/fv8M6tm0RxReNOmVYZB0c/4vj4EJ1ucOfKr7I5uMswV0ybku9+/0P+7E9/n3sffYgre1gjkaExcGGqHGY2mxvbvHb7daQSvNg/YjaZcBQfMRpPyGcGvOHo6Ii8mAeUXNVEUWtFRb8sUBgKRN2iSDWk3gLbZ1cFp0sbdu8FuNAN412QRfHWYV2zoqc9ksYLZrXHRxlvv/seb771FlGcLNK64YuhtV4MYot6B92j19vj7ru3STLP2dmcb3/3j9FJQprFJGmbuhpjS4tpHCoB6Ty4GpvPsd5h0FivgIi6clgTmlTee3sX6yVFXbJ/eMj9B/coiyBfM5mWVLWjNp4//pM/Q6mQci0qS1N76joo7mNdAI5fosh4+1qbuiyRznNn0GVmYmTUot+/yicPhxA16HbG89mcelhRVQrd6pK0FUKGpoXalvTabQa9FicvDsl2NpHe8vwgpxzPyMuKeVlTG4nS4XN5OjxD39whjmM2NjzjPMilgKA72GJrQdU/fHy0oNsVhfH0I0uUZSgUjiqkIqxCmIqMnH4m2N4MtZzOlwinabVSrl6/hkgzxiLhl77xdVpxTKIUP3j/A06Phkxnc8ZFzu13foHOxi5Zb5vetS1kM0e4hsrP0FmPFpI9qdm6+ibt2NDWFQeHRyRZhvUJue6zvXuFJt7l3n7E8wcKUygOjwTv/3TIr/3iFoOuZjyc8+mjM+a1pLu9SzEbkY/mVHmBkZp/+A/+h2xcucbx6SH/1R9/C10N2U4L/ke/c42ts4L7jwt+8r3vsHntFk19xujoKbc2BdJMePz4kO/8xQOQmmt7u/zGN36OBydjRmhMq8/z0zH9QcJXbtzmv/+P/jHf/eFf8PDJYx48fsyNX3mNVN3E1SVPXkheHHxKazbh1998i+JQMByVHJ6O4X/xxe+51cOcFUQJ/wsRutUQrMx9XwZ0fvnIP2fz/rIIE8BFucfFN15e8pXrXkRpq4q48LtYByFw3nWwRsOtwNM6K7Z4bx2sKbEYCxdAaZV2/quaVF465gvgavnCejLTn1+v1aGIteUgpFJXF2Dt95cfX+ri7y8DxFXH6xLE+VfMPuUrPgdgoVj2s4yyrpBCE8eKyoZmL4+njDytpiHCooVgWk7RKl6wRgmNsThrcLbCekHdQF1LlBVMTIZ1Ed94Y5PhaIh3niuRxngZauEWag75LEfJCB2lNLZg6Zta1zVKRjjvKeo5bd3CWZiVc9KojZDBpcE6R2XKkJUR4XshVBQsEl2FdRUWRxSFbl+8wwmHsjGTWcODF0M+ejREJC2yTo/rN27zTjbgeHjE4ekLWm2LjpbXW6K8J+4l9H/+Bj96OCb3B1RnOS/GI2hJsm7C5s2riFYAkfXM8a1vfZdrN3pcv7JJGncpdEMsFS08UoPQHqRlVjSM8pK6btioobGWSMX0og7P50/Y2Npja3OPo/2n3P/0BU3t+Mprr9HSHbSMgYTNtuDFaE4xr0nLFK88USpI0phZ7Rai8RV10yYVYBrD/tkxWdoKKWtjcdrRmIKqnhHpjHkxwtYJIMhFw6yaM52PPte99TfQsTMgLVJ6pBRE0pHGjl4GkTBY0QPZ4eGTZ0zyEU4EHbub1wZc22uRpYZuu8141lCVBq19QKZVTVkZGqtxtaOsY0zdA9dGEOOMIxJtpE3Ix3MePb3Hxx9+wIv959SlxVYltvFYG4p25bIeDSjLislkhtaKLGmxlCk5HwAdVVWeD7JiSbMvOoxkuKGkPBdlXDZMCOGxxgT2y5kFCBQIufSJE3incGYpixLkSjznaZes3WHv2g3e+znDazdvcvO1O2xevX7efSs9arHvwNjZRQMHCDS9bp/N7QzECdY+xzUKR7gxkzgwRt6BN5Y6L0i0p5NGWCWZFYamtninFwOeRynIWinWSQyW5y/2FwxXzXxe4pDUjScvDEVlFsXSjroOAs/OsSiB8avj/KJRO4tUEWmkSdMUYwQNcDYbczY6QyUxqavwcURVGhrraHX6JHGyaC4piTzklcWOpvQHcDLWSOeZ5w0nw9HieDXWGiovMN4hlGc0naNXtgChU9jUBjtviBeMRl6XlMWUqphj6govS6QWKBXj6gIpssCUeoWShELoJGX5wJJSkKYxg41NausYjMbURY4tCiolaUeCU1MgTMmgleDnQ4wMdjXx1jaTmSGfTpnMH7G5uUGiBU2c0ulu0ssiujFE6TbGJziZIeMNsjRh3Hj+5IcnPHl6zNHpjChNeXo0odMRDDox09GU3Y0tRvOaSdWQRhF3fuU3Sbtb3P/0U+59+AHupz/g+Ogp9XxGixxX1UxOFZvtiuYKlKbN6ekJ8yqkrr/6zus0psV0OqR4fswsnyGE4uB0TLcVLNCmsyGzKifNEoSFfH7KfDKizguUlwxPT3j9xh5Xr14lSuZkWQ6yxtSaqY8pfYRwX6JbBy4wWUuiJtTALUESrIDIK2KFn156+5W1bi+lZRHnk0khzoWIQzr1ZfC2WvE8xblMG69YvhVVSEgvrgG+Vbp0Ddgtz23t/cAqnoO589Tmct+L/YX/zsWCF3prF4gxv7b/5TXmPE198a0wrnxm1SXYW1tltdV1FnN1PlxcUKzve3mu8qWF16/3+obW1v0ZNE4AFLUFJ9AC6sVEHuGJvQrk46I00C8bJnyoxwoSXQbvPJVpwMdEKsLYmtE4wiO4tjVGorDGUpU1SulFutYAHusFOIe0Fm/CfpfNA1YYrPc0BryOEahAmiz2jQj1d86FbmilVXi2ifDPGbvo/AzPLytCRkerGHyHVqdPf6vHw/tH2LJmWpTklWVrq4MxJd6NETikX2jgObGyp/V4tq9usdmNievtkDFLBVkn5fata/Q6XY5OT/nwk494/bU79PsxqYqQDsq6obaObtrCKo9IFXGWMjwdUVUGhELUkmZusLFFdRQf/OQjrt0osT7i8GxIu9PDW8XBaEQnS2nqhhejAwrTBKF/p3g6PmO33yNDYZwDE+oihVBkUQIisG9LEWIBeGuo8ppZXlKUNe2kJq8mWBmTRB1mbsa8nFEV+ee6tz4/sBMWhEOohbODcqSRp52CcCXWe4pK8+DZp0yKIU5WaC3Z2dpioxdhXM3WYIuiOqOoZngsVVUym085Gx9zNhmglKCuK8YjqCq56MyscNZQFwXj41N+/MNv8/jxfSbDU1yjqKsS07jAkMUKIR3CW4SUzOcFx8dDkigiTVpILajsZNGhEkBPs9CdEYKFrIhfFKKG7iL0QqtmVSfgzoHdIi1rbbPSD1I6WG3hz0FVAHR+5Q8nFoNqq91h76qi0x9w7cpVtra2SHs9isYtumH9YnYfBA2DVQxBasV7oiij0+nTmlYoqaiNpbGWqhGkabpIj4b9V0VB2orpdVr4RFPbGbOyXHXvSQk6WlwXERT3T09PibXEO894nJO22zTGk+cNOgogH+kWzh0sGwkXgqtfgq4DysbSi1uhM0jHYBuqumGYnzDPp2gT44UhYiMU53rIWh3SNA3SK7VFSUdR5szKhqgVczadEQGRgNpUeK/QxDSmppQCIz1xLJmVFaKSaCXJ2glKOBrjmJcFWoNQEuMN83xKWcwwdYXVFUQpUmpwU2QkkV6DU8RJTJQkqDjG++BFKHHoWNPp9ijKnF7kqOuCpmxQHhIl0MIQK0u328cUI7wwSO3xvsWsLDgdTzl+8Yhu6kjSVmiSSdtEaQsdR+ymbXS7h07atNJNDs+GnJ6e8OEnz6kOj7FWYFAUQPtBSjuNqWYT9jY3MW7G2XzI9tY2b3ztm3Sv3GFSCz758fcYnTxjPHzO3TtvoFOBlJ6nzw7Z3Ei4dTWmcH0OP9ynLBqcV2xt7lI2EV5CkuZUdSgpmFcVV3ev4USOqScYLB6NqR37zz4JPrv5nFhpxmdD5LVdtno9KtPgxS5lXTKdNEy8xKqYVtb6Uvcc8NJzfAFe/BIcBUDwqm5Uv2DTX/3QP2eo1rXdljWz4e+w0yCpIDlXoFwW7b8CvKyK5c6PIYwucg1XLY59qZ21Yt0WjSHL41lu9wLgVJ8Fsa8AVSuEtwCJF0hDf4GKW9/TObsJiwn1+dsXucz1Da41VbBm/7WWRn91XICQa8fx14G05Wf3MvP3edb9q6OoDDFypTFnnUUKQSSCRaf0AuUlwoNa2LRVTRXkjXwYcKumJtExkYxoKJnOQoqU7TFabOCsJS9r0sjjMKxrFXqCPpq3S+cLFp9DsKY0BhARCLmoxTOrWvSiLha6mxKNXgj4OrywOBvu3VXZpQcpFFIqJC2yTpfeRofK7NOYBpPPORlNUfEdvK9oZ8H5QXpCLfGiSNDbAC63r2zQFwO2tA/ALpYkrZSr164wRfLk2T6Pnj3mN/7ObyGw1POCMs+Z5iVVY+mkHSpRY6MOcafFcHxMjSKONDSSpmioTYMpDR998imVk7S6m5xMRsTZAClijkZj0jimLCqOjo85ykt6nRZJpHl2dkK/3cI5KKuapg5WfNZDrBTWNRhnkd5TlcXie+KYVXMm85KyNLiOIa8rGhkjZUzezCmrUP7zeeJvMMUNQrQSUNIjXGB4em1JmZ/w6fEB7z+b82j/p0ybElJPIvvQvIFq+qSR4Je/mZF+/F3uP/kJSdrQVGOG4zkfPjjk+ZFjZ/s6O5t3+Paff4+DwxfkxZRuFw6O/zWj6QZF1ePTT75LXhi0FeSFoCgbGuNQQuNkAGdxpOj3upyNxkzHZ6RRyutv3qHfTjmbVSRJtBC7VSv9maqq0CpesHp+lQYFEFq/pF8XBkljLHXdLGYuAqkWtRoehBNg/WL2smD7cKHOTQpQmjhrEWUt+tt7xGlKLTXz2gYvWRat6+v+fwS2sTFz8nzM4f6Y06MZ0/mcNOpRliNqY8MYK4Pwsdehy3c2qhmkLa7u7qDaGUWzz3ASaigCC6tIEsmTZ09prKA0is3NTZqqpG4qVCRoGoe1Eh1nVGWOjoLm2mqCL4L+V/PSsPyFwkVcHeyx1+nz4vSEh/vPmNUFJoro9rewRlDOa8pyGAyj0xZN3XA0HoVziTPKerxQW08Ylp5WZLi91eE37t7ho/0eHz885Sf3ThlXDUknox0r0syidEZZGM5O57z92h1irbHWcv/Z/dBEgWWn1+Z4OuUkz7FKUltP5ATCKwQRSTrAozDNhJ1b77C1c5OOjsHOcNUUbytElqGlRNQ11fCAN996jWcPnvLi8TMm8wJLRJRpHILGSLb6m7zxzhscjX5AYhUtL+jEKQcvHqGUR8sEVZwxLEuqquTOrW3+rd95l7feUFg55P/0f/4XPLx3wP5BTr+fYZ3H4+j2dnj27DnOFlhTcTYuaZxE6S7/3v/8f8kf/N4/5yf/z/8HuprS1DmdVDJ44zbT8YhEZKRRzD/93Y/4n/yH7/DVd9rcud3wxt1vMJy0OJsojmenjIczqsLzK7/0Td598y12Nnr0WoJZOePp/gn3n7zA2hatLMNJwx/90T/Hp7cQcZtO1ufO1ZjTw+f8wdMHjCMYtHqYynD/Rw/o33qPvc0+t+6+96VuufP2iJfj/EH+V7E1681Wn113bT8XtrEG+FaM0jqbBKBWrPo6W7eGgpZH8JdiDSHWUowL0Lr25uJFycrD7GcRa8B17cW145ar3eKXKW3/l34KIV5Klf6MDvXVsahzW+7Erxeuf/m07Hyc00QapSSVqamamlYc009apCohkoJYSjKd0UpbNNby7OQpURTEv/GKvKiI2x10omm8w8gc7xym8sTxHOMqhuM5LanIopg4ikl0hJEa54PHtQTwhGYMAqFgnKUxNTIKDWBaOqqqQKkFs2wMaLWqMU1k0NR0xuC9WjihWBwNkZVoIYmjiI1eg7xXMTsqmZclaSxJlERIOMtf8O4bW/zS136b2viwrnZE6QBrG4R3KOF57WaPNprO4jPIsowkyYijAcqX7G2OuPv6Ha4MdpBeM7QT/u/f+c9QdGi1MnYGfR4Nn0EsuLp3jR19hccnh8zLEe7KVQpTUFQ5rvYYX1Nbw7yuyL3kk8f3cMbza1+9wydH99FOE6mIp8f76GFwnTkdTrjS6kPjOWxOmPopszqYKGQqwmCQArbSFuPJBE/QWv3pi+dI5chiRTM1zOoJKE9FjbcWUzVMq89HmHxuYNcYidYRsVDUdbD18tEA2bvLVEbMzTFNNUQj8MaiZcTN19/g6t1fYHdnAyksf/Qv/9/kkxEtpdjttzl5MUKUNYmNmQ5LqvyAo2PDaDYi0Q2DDcl7dzJu7BoaCp6OY3b3rjI8m3E2moMKDMZyxhNbhUYRq2C54gSUXpDPGrqHB8SpZG5q8tLSNCF1MJmGYsVQcO+xpgndONYuUqxLwUK7aIwICvRVFQrpl2UaHh8sXRaz9jAceMwyCSIFHoVHBBrchJTt0iBZIZAehHdBAxGHcw2Nt+dCvyKAKFN7imnFkwcPuHolQwvLZqfGWYXVMUhFPZshbYwSCYaCysCwqHk+mqBzhfMddrczruzt8snHD5nO5lgrmBUNUiniOFjVlJWlKi0qyqgqj7FhxpckS489wIuF1cpyMhtSB+pLjLqxgLoomIuIOG1z/eY7jKY5zw9OkbGmsqGL7Oa1Pr7MKcuS48mMpnYLAeYzWrFjd3eTbr/DyXDIu7s97mwlRL7itZ2EDtvsRB2++3Qfm6SoKKYbDWisQdiSOrVgPV5b0JadrR4nR2OKvKaWnqNnB5xdvUnjBVl3i0R5pKsYDg9JvMbHHVpxwtbmtVBQW02RoqKcvMCUE+JUcXpS8OzZAc9fnOJVlyf7L/je+z/FA7ubG7SyjFoIOltd5uWcH/3gh3R3RpycWI4PLdZXzIspWRaxt9vl3TffoixqzsZjhqNjHj96gHJHtPpdvnq3ixIN88IRRS2ixWQkyyJ67R2kbyjHE2bCcOX1r/D629/gO3/4n2GOnnM980zxdDqbNA7yukbGnkoa5r7izp33OJh2cQ81qdBMDx8zGWtORgn39k/Q0tDvZvzcO3ex8zEyP+HdO4JyPmH/k+f85P0XXHvzXXb2ruBszfd++mNuvNnG4xgNH7LTvsOvvHeH21cH/G/+2R+y/d5tdq9tsnP9Lfo7W8Fy53OmKf6yOO9eDX8tH+ovd3iGZcQa67YO1s5lTpayJ+vrLBYJ3+VlqneNFFqyaef4Taz9XAK+c1bMr29ztc5nXxDrNNpSZHXNRmmVZl1jFF+OwF6uPVjWU6QXGkfW0rGvUMr3L19PtzinNTbvwn7/CqAplsf918TnGonCh8byM7hwfi+DvAuM6hcLrUQQRW8seZGjowiNRgGN87jG4ayj31qwXR6kihG+wRmLNQ3WQllVeCeYznKUDw0IRRHSf8Y4amMw1KhUkDhFJFNQAQha22Dtks31aCmIUCgkhRccHz6j126xMxiQxC20CJ7AJJ5Ia5QOwvvehOtmnefh/iGb/ZROFqOtRjeBscNptC149+4eWRrx4/sfMcsLdBJz7eYOUerREmbznP1pzq3dAd2tDlq3aEUOfBDOn4xyWp0B3f5WqL1r99A6QThFP9rgrTttBv0d5qOCzf4OVzbaXLtynaK0aKnRJsFYhTEC0zj2R8e004yOaPH9Dz/gdHiIEHCYjSicZzQfc3DyLEiVOEdVNZyeTJhT0M067Pa6bPQ2aZoK6wwbW5s8PXnK4UiRxinTskEogYwEP/7kY5J2QpJGnGm10vozxjKfztBa4BLFsRDkdoRSoHWMtwVVZbGfU9HpcwM76wQahZQ6MFdIvGyh21eZFnMmuWU2m2EbG+wz0Kg4piQm9xHSw8nwmKYqiaVGOYe0wdKpk2WcnFWcFXOKk4a8Lulo6LU1u/2YzUxQeEGr0bTaHWa5QaoKRINUBH89FxoEpF8kL6zBSofxHtvAeDohqgQVkroO6UMlBEVZLtKxwQ7FebkQdVwwbEqdp0K9W81C7bIpAi6kG6w9n/QKQkfQKiWyVtzr/VKzSi6TJ0jvV9sPpsLBeD50vofGFYHE2dChHGlFEjmkCjWG46kIX2yp8U0DLiWoY4cOo7w2nExmKBmTtTrsDrq88fotXjw/Yj4vaEzofo6FQIvgrFA3lrpxpKlaNH6EL/DSlSMcrlx0ErsLA/+XqUVJohiPorIeoyzWSbzXwd0BiJREJjF7gw6+NBR5Q+0SZnnoNHPOkqaKXrfF1mYPS81WP6EVC0aTGe1eTL8TsbuZsV2U+DhBRTEJEWVTIWWOqS2CIHKtIxEaJwrDyEmG85rZtKAsaryXSJ0iRL3wDza4pgAVoaOMLE1RGOrpEaPjA6ajY0w1o5VJHj0dsn8843Bck3TqUJ6Q5zTWkqYJSEUrasPCtm0+y1FtS5HXlGWDUA0Oi3Hh81EKhFx2PFv2D2d439AeVHjriJVER5IsbdHYBmMNTV2SbHTIooiWh/bGHldu3GB7a8CH3/sxbe/pZX7xvRPgBY1RZK0BnY5h0BHc3H6NcTFlvl8zSCOePj1mnkuKqkusY9JI00kjlKto6QkbccEgdpyMS+r5mNl0xpVrd4giwXxSoWQP6yCKBVuDNmnS48rOHm/c3EHU0G5t0+7tcTac4q0LNTn6S7IoF0is5UP85Xv4oizJK+/xFRYTr3h9LaUnlinX8/c+KzS8tvwFkPjSS2vb8qt1zpf36wBxrUHCL399CVz+tcnGl+rnPrPcMrX3qo28hN/Eov7N81kQ+NeGEF9krVdu58LP8Mfi54WL89JrXzy0FFRNECmuygYlYrwNElTSOoQDJ1fTi+A8ISWSRd21swgDRoRGinnuka5G4CnLhlSlwVXBCBocGFBaorxa1dgZ01A3BilBqeADLxzgPE1dU1UVMQLd7RM7hfY6lB0pH1QRvKcuKpyBWCdEMuL0ZMQg3SVtJ0QiRitBUTQMJ2Ne29wjjiRR4tnopyRJRJSm7Gz3mOYT8rzk6MhznJfs9rp4q2mMItZBXk1FGuEtUdQmaw+wRU0Ud4jiFF9borRHaSzWSqajMYP2Nt1WO2hUSgPCI5ygsZ6yNszyguPRkKtbu0RK8/TgKfN5gRCCSZljPNS2IS/n5HlNYxqcd0zzglJUKJkyrx3OBsFp4wwqUkyrAlU7uhhGM4eOJGmiaIqcBENsI+YIMh3qGmvTIBaWoVXjOTmbYlVBFEuypMKbgqb2uPrzjXGfX+7EgjGLGjPjaRxYkdLuX+Hx85/ydP+Yp0+egiixjcWJhucH+3z3hz9iMOgSKcvxyQFaWZI44fToCGs8rV6Xa9duMp4L9k+HPD44RrczOq0I3VIYq/AkxEmbrXSL58MCIRVS6aBnJARSgkBiGoO3BldClHms9jglUbLDeDZHCEcjJE3jECikhLzIqaqKuq4Jo2m06PJf6Mc5i7XNeUPDK8CLUmrttXObl8BcSV4eKuRi2ZWFy/oMfVHLEAb28JV2Pvwu/ALECkG73eZXf/WbaDWiKKZwcoo/KLHOI71H6WAlEyxLPFpJ6tpwdDTGmpqv/9zP89Zbb/PmG9f5/vfexzlHWRrQbnFOlrqeUhY1xjiiiEWnawCfSwwa7GwCa7m0mlmOtK+Sf/i80e5to6MBjZCcTH/CvXsjnJHsbPWDtlu3Q7/T5+3re2QqAtdnWnT46GhEaTxCaprylK2NDv1ei+7dd4irEcPxlEePTnn37hvMmogTAXuvvc1mJyLRkuGswSKYTCZ4YVHakCUJ7VZCJlP2Oj2OznL+4AeP6NgIZyOk19RGoIQh9obtrQGlh8YbrPMoX2BmLxg9OeV3/9nvUTZBRXyrq/mL+/uMfAvTu8UtkbPRSbl7e4/7z444ODlhOs95t/0m49MTpIzJ0g6TswLvJEkqqZqcrNVCacfx8JD3P/k+k9Gc0XDGlRu3uX+Q8uhYYynIyhOms4KqbLh1c8CLg0NOT08Q3tJv36SzNWBrb4uv/zv/Y/LxEfsff5tiMqO9oVEJlI9L6rqitpq8jLl5423uXm/z+l6Lnf5Vvv2Db3N0csLGoMt/86NjOjLi7Ssxf++//W8h8lOq0SHf/86/4j/6N6/y+vUIU8744NMOx9M2mzvb/Ppv/g7f/r1/ysEnP+brd7/Go+E+W9e3+e1f/w1m84zBxhZKRNzpdLm2dZfSp/zL3/tXDLIZb7yxy9d+7rUvfL/BUsduycB9dgB9pZH9+ZsX/vQrPPYqcHe+PS8WNXQrhiss5HGL3+ViLLpY2/fZdOR5Ld6FjtD1ZcSSrQu/L5mzv+xbumIUl+f00vf5L4U1F1jEl8//5eNe/Fw6dAgW18KdM3tL4PiZGsO/bP9rvy/H2guM4PnrF/7+DBB/CT2vUss/mw5Z6cHUhqqoMSUY6amEYaIKtqLgPiH0soPCIZxDYUl0hhMWbIWqAtvTCMFwlJFEFUqUzCYz2r0M5SWSBCMEsUhISMEluIVEynw+o3QNWsckcUKSxVjrKOqa09EZ3mf0I01aRaQCEq1QMg4uFEozm894+vApWqXsbe+yOehwvH/K3Z09NqMeqU6BhJODp/zJH/2Av/vzv8a9B9/lD//sT3nzjavsbu6SpCmlgD/+3gtGZwVH+x69HTOZbjHtOKrC4luOQafD9sYNZmWP/sYG7cEmB6NjEpWikxSJQyQtHn34Af/JP/tP+YW3v8LOxhU6aY+nR/sIBK0ko9dqkZclp5MRT6MXPDt4gbaCVhxzMjmgqlrhoy4mOB9EgRWSP//gA7L2Nq1Wh5EpSFPNrDSMpyOODl/gfI2QlijStHodolQRR5ayNPhSUFWWK1tdauEpao+rJLUqEcphtWCj16KwllnVcPbilFZP0urEaF1ja4O0HvU5/df/Rm1kQdqjoa6Dp929e/f4J//kn1AUOdZVCCFRWlFVjrpumE/3+fa3/wStwLuKVmLptGMQitHM4htBzyl0a0DSrWFoyd2M2CkmtSOaO56PJK3BFkJ2mVQNR0dDprMipErtYkAUEqEURI5WS7PdSsjaE4yA2numxbklRyQlRVHhjMFIT1nk1HWNc54oSjH1eW2dtcsu1NDFGFi9BRhbWK+s27GEAli3NusWKHGxLkcuWT7OB8r1tI61Ft9wAdiFAU+C9AgJRZ1T+5or1++gxZzh8JD9oxm18Xhxrru2XFtLeP2N2/Q6ilar4uN7E4p8zLPnj3FuSlnNkQoCEA2KDkJAUZSr87HWLBpFFmrb8hzALnpCVuezLCT/MjPa48OCMrbEypEXM25sbaBQpNLRbm9yNh5yenzIb/wP3qMYS86Oj5n8+Ee809vl+dTy44MZg51dGq8oZwXzSUk+MLSyPle++g6PJveoncf2M+6+9os0uURY+IU7MD14zkfDA7790QPGOx1uXNtA6y7PD4ZsbnWRiWBjo02WCYSwFEWFNB4VC5RO8NMSYypc5EjTPlJ5nrx4xrMHRzw9yVFRivENP3x0yLPjhrgt2Ws1fPzhAcPjA2ST82/+1jd5/vyAyazkZD6hpRSmqSlmOcX8ORs7W+xeafP8oKabpLTSLnHW4+njA3avv84vvPcmx08/5nR4gmksvVaPnU7E5vVtbr9znWpyyu6gSyfLyAY3+do3v06cJDx99Jj9n/wrbDGnGg5pcsfQV2RZzN133yFO25yeTfjpR/eZHj/nh/szflRNiRPPnTt79Lop//r9D/mVt94lSlOaRPIb38zomIaorImTv0smR7w4GfEn358RdVq8/dX3ePObu/zg9/9Tmrpm9/ZttjsZX//lv0Ov06abtnjy7AmTUY7o9PjvfPMG//kf/ucc15bf+c0bfPyDU4r5Kftnn0+V/a+OV9fRvZp8Xn7D1oDHhfTdcpF1Wuzl9dc3tc4MncuXrADly+Bjue3l/vyiw3O9Tm4JYOQaKJXyIhH4UqwA3EvM2peLv4rhelV692cTfy3zeGHhV4C7V8YSRP8VQP9zRFPVeGOQHiKR4poA7qwBaQ1Ch3rtxtbUVYUQgo1+B20TMALnDT/60Z+yd22Pzb09ZCPZzIJt1cQ7WqpLt9WlnbUR1HSTLlLA07MHbGzsEkcdXOUxRU4kBLFTRGjG0zPKomCr1UNEEUlLcyYMURzREhGp1wuvV0thgcbzC7/ym7R0jC1Kfu6dn+Pa7nV63R0GnQ3+y9/9PfZPh3Tu3ODxRPPuz/09br35df4P/7f/K6/vXefWtev0ex0+/vQphydHTJuat67cYmprPnhyj5/eH/M//Uf/Njvbt+i1d0laDT/48AP+y4PH/MPf/u9y7/GHTM6OyE9yrr31LodHj0mlZ//sgH/2J/81jfXkec6g0yaKBTNVYoqCI2OY1zXGVOxPh8RxQivbRnmIs5jebsYnP/6UfNZwzJh2L8V4S1WXeAXRoI1pJOU0B13iaoE3GhGBrT2lcZQ0yFhRV4bptEKTg9REUcLe9oAXL05RKmJzc5tJVRBFMZ0k4cSf0hhDVUtmtSG1wRauNj9jYLcEHm7RramUxDQNBwcHOGdR2qNjR6IcUgqk8FTGMBqNUdKjZEOiE6raY0LpEkgo6prHz485HeXMCoNxEumDHlpp4XRm6I9BasNpWTIYDNBRhtI5Lw4nWLdIa0pAS3qDNq9d22azF5E3DZPS8WBfgPKhFXuRTdVakSbJwic1mLSLJdsGK6C29GfVWq1StOuAbNkNuzJRXk/RLOrnXu6iE7BgvS6OYqvra+0FYCeWXSsOBJbaVOR1wWweik8bG+NJcSKosrul1bMQKBnMmwfdNtsbMYOeoaq65I3lxYt9yuKMssyDp64WCM3CIo2VDAyI0DRiHc6eg7YVsHO88mH4ZRg7LePV423Q2iWNE2IlyaRApz2k9OS5ZjIakUhFu9Ont7WB9hokGJHgNga0vCMTnkF3C6cLYq1pJ22c2COvLUWtOTsbcnoyp84bpqOMK5sDejvX2NzdpTMITTmj+YyjySw4MKiIrJMGhw/p0ToCFzqWnQj8iqvLwHInU7RKeHE258Wjh5Q5ZC2N0BqZtSntCOkcrcgyERGWmNJFzIoCD0Q6yNZc2duhrhuG4zNmZ4ZIxnTSLlt9y2t7O3RaHWTS4+hwRqe7wd6V6+xm8PDxI05Oz5jN5ui+pteJyXZ6TJop13Zvk3a6lCJD+5JqNkXYKdtRi3kxZ1SN6LcSUB7hFUVeMTqbUVQVm92UQTfDFg4rHHu7MTdu3CRKMozoBjbTOZR1fPKTe6TihFSXXL2ecvQs5+hoxrMxfOXuXaYFPN8/4NGjx1y5eYdur0t59hwlN/FCMCmhMRWzySmnR1Py6RH57AVlLWnKPU5PppSuIt3+cnIny07Li3+/vMwrGLh1zLJoBTzHRMvv8DpbtA7yFpO2z+x0fQuvYpp8mNAuj3nB5K8fw/n2139yfsCvOkH/EjPoFxO1tYKSzx7LzyY+L0R65V4Xl/QCtub8ufU3jQvb+MyOlj+/OKgDsE2w3dJKIoUkkZpMSrR3eGfwXuPw1E1DJkEphSYhkjFlU3J2dkar3aXX6dPPenQiw2YrQcqSosrJsh6trEcra5NpRZYlON/gxwlp1EMrSapT4rwBX6FlQxL3OPNTHJrt/g5FnYPTVBWkm12iOAYhmI5ndHsZWdxlc/MqzoQGMmEkb7/5Ht00ReoWrXafVm+brtNkWkMUYRcpnhvX7nD16i3SVod7Dx7xq7/2Gzzf3+fT+/fQKmUw6KCkY/7+Mz7++CHzs5Je+5DWTgedKESseHr4hOn0iHw2oigaDt//PrN8Sr/Tp6kbZvWcvKxxkacWIKzF1jmJTlBJTJzFuHlJUZUYa+n1uxzMh1SNozFJqG1UGiEjIh2DCWoJaadNWVQ0VUnTBDzkFDgrSHRCHCXgPfm8RkUiuFvIYFVmjMeYhskwxzqPUFAbD1IjCTqsfmlsICTOCYoyrOt+1s4TUkqMsSuLqziKALlIYYYuGuMsOg5gItKSWgQtuUgLWllIa5ZlEPQVQduVeVXw448eMC8Vo2mJdXLR/ymoPZxODdlpAxLGxYw7r7/GbF4RnYz48ONnWBOK9pXwSC3pb3Z56+4Nbm8njPKCF8OSF8OC0gW/WGsawBOnEb1em1a7hRQCY8/P7WUWDtwK6CwdG5agbsVSvQLYrf9bhhBB7HR9vWX4QA+GuojVIH0+uw4fuKE2NfMi5/DojH43oygEXmQgBQ6P8RbpBJJQHK+1opslbHZb7G5DlA54/6N7PHn6lKMjjzNhQh/FErQ/t1YiTO7xYk3uZTHEu/V07PkDb1l393Lh+N802q0WETWJgo32HlmrIIsFvSSl8jFp2qYoah5+8ozbd27R6m6ydesN/OmQXrvh9rWUR2ob8orYeK6/9R752Sm2LhHS0917nbOp4+C45OGDD3n85Bnj0Yx+usW/9+//BwxuDnjt3VOSOCefDjkdn3E0m5N7RZxmxO0UX1QIEazroAhlCt7ipcI3JbZqqESG1i1Oz0re//Axe4M9fJSQJTFbV67A0xlCOjqpo9PtcTKeMfc5D569IEKihaaTxLx++yZVOSeRNUf7EbFo04m3SHYGvPvaNXrdDkam3Ht0RL8/YHtzwFt3r5BGER/5+/z46B5Kd2m3BIO+JpqmvPuVt7j1+ms8ONzngw/eZzQcEuN4bfM9DhrP0+qMnc02jRGUteXw8IRnz56QpQl333iNK1e2sHUPbMMvfvUqm7vXabf6fOMdze/+wR+Qj8ZkjeVf/Vd/Dpkm7Xf5ys8ZfvwXp+TzMYOtlO03vsGzH3zAn/zZd6mmBa+/81X29rb44OH3GI0HlEZjVIrDMJ3MeVEWPHh8D+lrEplx+GzKk2cjeoUi2/hyQONz14QugJJY+31FUIvzSR1wnmL8TEHGGghbB13L8WP1/yK96uFc8dtz3pSgWHbif4aGWwN4FwCO82HGJF91vi8LjfhVU9QX/ja/jIXXxorPtdqyqWGtieyVy4iXXlvf38tq6V+k/vfCOupLA1trLFKFWmkk9KOYVAkSb7HehNpdD3XVILRAixjpLJHSnBVnPNl/yu61G+zuXKHfHdA/nbLb3QIKTkYHtDsD0rSLVjHdTpe0LWh8QRT3SKIe3VaLLL1Kt4ioqyG2PoOkA+IQoTzbG7c5PHyCqz01kl7cJ0ojattwcrJPK+vSStvsXe0yHk6JULSimPfe/Trj0QhnDUna4fqdt4nPhsyLGUkrZnyyz/Bon6/e/Rp3br3BaDzmD/74O/yv/uP/mOePn2L+X/+c3E65tnebXi/j9//oL/jud3/AB9FHdLob/KP/4O+zuTXAxYaPP/0xUeKQ0lNHmj/7sz8lS8L4dHh2jKgqRF1AKiiAprEk5ZxW1qHVa9PebHNU5VRlhdOGazc2efL0OXVlkWnoPo7ihDhro+sZCEmqY9qbA54/fYwpG6TX6DjGqQaBJ9MdWllCYwzNsCDREq8UPo5QQGlKatNQjUa0NwVCCYomOFZIsRADd8EGTUgFTjIrFrjj896q/svQKpdxGZdxGZdxGZdxGZfx/zfxs/VIuYzLuIzLuIzLuIzLuIz/n8UlsLuMy7iMy7iMy7iMy/hbEpfA7jIu4zIu4zIu4zIu429JXAK7y7iMy7iMy7iMy7iMvyVxCewu4zIu4zIu4zIu4zL+lsQlsLuMy7iMy7iMy7iMy/hbEpfA7jIu4zIu4zIu4zIu429JXAK7y7iMy7iMy7iMy7iMvyVxCewu4zIu4zIu4zIu4zL+lsT/B3GK2NSAJmU+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create customized dataset\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        self.imgs_info = pd.read_csv(annotations_file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = self.imgs_info.iloc[idx, 0]\n",
    "        image_raw = cv.imread(img_path)\n",
    "        image_rgb = cv.cvtColor(image_raw, cv.COLOR_BGR2RGB)\n",
    "        image = cv.resize(image_rgb, (100, 100))\n",
    "        category = 1. if self.imgs_info.iloc[idx, 1] == 'dog' else 0.\n",
    "        sample = {'image': image, 'category': category}\n",
    "        return sample\n",
    "\n",
    "# Loop training dataset\n",
    "dataset_train = CatsDogsDataset(annotations_file='annotation_train.csv')\n",
    "for i, sample in enumerate(dataset_train):\n",
    "    image = sample['image']\n",
    "    category = sample['category']\n",
    "    if not i%100:\n",
    "        print(i, image.shape, category)\n",
    "print(i, image.shape, category)\n",
    "    \n",
    "dataset_test = CatsDogsDataset(annotations_file='annotation_test.csv')\n",
    "\n",
    "# Create shuffled data loader \n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1000, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1000, shuffle=True)\n",
    "samples = next(iter(dataloader_train))\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "for i in range(4):\n",
    "    image = samples['image'][i]\n",
    "    category = samples['category'][i]\n",
    "    axs[i] = plt.subplot(1, 4, i + 1)\n",
    "    axs[i].set_title(f'Sample #{i+1}: {category}')\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(image)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can re-run the above coding cell to get different samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocess the Data\n",
    "A typical binary classification dataset is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a target vector $\\mathbf{y} = [^{(1)}y, ^{(2)}y, ..., ^{(M)}y]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x}$ is a normalized and flattened image array, and $^{(m)}y \\in \\{0, 1\\}$.\n",
    "\n",
    "- A colored image is usually represented by a **3-dimensional array with shape $(width, height, 3)$**. Where, $width$ indicates number of pixel columns, $height$ indicates number of pixel rows, and 3 indicates 3 color channels (red, green , blue).\n",
    "- When a digital image is loaded, each pixel bears an integer value ranged **0~255** to represent the color intensity.\n",
    "\n",
    "![](https://miro.medium.com/v2/format:webp/1*pFywKuWmz7Xk07OXxPiX2Q.png)\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(20\\%) Exercise 1: Data Preprocessing}}$\n",
    "1. Separate raw feature array and target array.\n",
    "2. Reshape feature array and target array.\n",
    "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(557, 30000) (140, 30000) (557, 1) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract features/images and targets/labels\n",
    "data_train = next(iter(dataloader_train))\n",
    "data_test = next(iter(dataloader_test))\n",
    "\n",
    "\n",
    "# Separate features from targets \n",
    "raw_features_train = data_train['image'].numpy()\n",
    "raw_features_test = data_test['image'].numpy()\n",
    "raw_labels_train = data_train['category'].numpy()\n",
    "raw_labels_test = data_test['category'].numpy()\n",
    "\n",
    "### START CODE HERE ### ( 6 lines of code)\n",
    "# Reshape feature matrix to (M, width*height*3), target vector to (M, 1)\n",
    "reshaped_features_train = raw_features_train.reshape(raw_features_train.shape[0], -1)\n",
    "reshaped_features_test = raw_features_test.reshape(raw_features_test.shape[0], -1)\n",
    "reshaped_labels_train = raw_labels_train.reshape(-1 ,1)\n",
    "reshaped_labels_test = raw_labels_test.reshape(-1, 1)\n",
    "\n",
    "# Rescale features within range: 0~1\n",
    "rescaled_features_train = reshaped_features_train / 255\n",
    "rescaled_features_test = reshaped_features_test / 255\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Finalize data to be used later\n",
    "features_train = rescaled_features_train\n",
    "features_test = rescaled_features_test\n",
    "labels_train = reshaped_labels_train\n",
    "labels_test = reshaped_labels_test\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "print(features_train.shape, features_test.shape, labels_train.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "(557, 30000) (140, 30000) (557, 1) (140, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model\n",
    "Apply a sigmoid function to transform the output of a linear model within range 0~1. The new model is also called **Logistic Regression Model**.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 2: Logistic Regression Model}}$\n",
    "1. Let the linear model: $\\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{w}^T + b$ to take feature matrix $\\mathbf{X}$ as the input and output a transformed/intermediate feature matrix $\\mathbf{Z}$.\n",
    "2. Apply sigmoid function on $\\mathbf{Z}$, so that the prediction will be: $\\mathbf{\\hat{y}} = \\sigma(\\mathbf{Z}) = 1 / (1 + e^{-\\mathbf{Z}})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78035352]\n",
      " [0.68058492]\n",
      " [0.60273538]\n",
      " [0.41968119]]\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 2 lines of code)\n",
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function\n",
    "    Args:\n",
    "        x: independent variable, could be an arrary of any shape or a scalar.\n",
    "    Returns:\n",
    "        y: dependent variable, could be an arrary of any shape or a scalar.\n",
    "    \"\"\"\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def forward(in_features, weight, bias):\n",
    "    \"\"\" Logistic model function\n",
    "    Args:\n",
    "        in_features: feature matrix, 2d array with shape (# samples, # pixels)\n",
    "        weight: a row vector with shape (1, # pixels)\n",
    "        biase: a scalar\n",
    "    Returns:\n",
    "        predictions: model predicted values, a column vector or 2d array with shape (# samples, 1)\n",
    "    \"\"\"\n",
    "    predictions = sigmoid(in_features @ weight.T + bias)\n",
    "    return predictions\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "print(forward(np.random.normal(size=(4, features_test.shape[1])), np.random.normal(0, 0.01, (1, features_test.shape[1])), np.random.normal(0, 0.01)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "[[0.78035352]\n",
    " [0.68058492]\n",
    " [0.60273538]\n",
    " [0.41968119]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary Cross Entropy\n",
    "It is OK to use a Mean Squared Error (MSE) function to compute the model loss. It is better to use a Binary Cross Entropy (BCE) function to assess the model for a binary classification problem. \n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y}) = \\frac{1}{M} \\Sigma [-\\mathbf{y} \\log \\hat{\\mathbf{y}} - (1 - \\mathbf{y}) \\log(1 - \\hat{\\mathbf{y}})]$$\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 3: Cross Entropy Loss}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5250353081044535\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 1 line of code)\n",
    "def bce_loss(predictions, labels):\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy function\n",
    "        Args:\n",
    "            predictions: model predicted values, a 2d array with shape (# samples, 1)\n",
    "            labels: labeled values from data set, a 2d array with shape (# samples, 1)\n",
    "        Returns:\n",
    "            loss_value: averaged CE error, a scalar\n",
    "    \"\"\"\n",
    "    loss_value = np.mean(-labels * np.log(predictions) - (1 - labels) * np.log(1 - predictions))\n",
    "    return loss_value\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "print(bce_loss(forward(np.random.normal(size=(4, features_test.shape[1])), np.random.normal(0, 0.01, (1, features_test.shape[1])), np.random.normal(0, 0.01)), np.random.randint(0, 2, (4, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "0.5250353081044535\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Optimization\n",
    "### 4.1 Gradient of the Loss\n",
    "By computing the gradient of the loss, we can figure out what would be the best directions to change the model parameters (weight and bias) so that the loss of the model can be reduced.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial w_1} & \\frac{\\partial \\mathcal{L}}{\\partial w_2} & \\dots & \\frac{\\partial \\mathcal{L}}{\\partial w_N} \\end{bmatrix} = \\frac{1}{M} (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\cdot \\mathbf{X}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b}  = \\overline{\\hat{\\mathbf{y}} - \\mathbf{y}} $$\n",
    "\n",
    "### 4.2 Iterative Gradient Descent\n",
    "Then by tweaking the model parameters along the gradient a small step (learning rate, $\\alpha$) iteratively, we expect to bring the model loss down to a reasonable scale.\n",
    "\n",
    "- $\\text{Initialize } \\mathbf{w} \\text{ and } b$\n",
    "- $\\text{Repeat until converge}$\n",
    "    - $\\mathbf{w} = \\mathbf{w} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}$\n",
    "    - $b = b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
    "### $\\color{violet}{\\textbf{(40\\%) Exercise 4: Gradient Descent Optimization}}$\n",
    "1. Define a function to compute gradient of loss\n",
    "2. Perform gradient descent optimization using appropriate iterations and learning rate.\n",
    "    1. Initialize weights and bias\n",
    "    2. Make predictions\n",
    "    3. Log training loss and test loss\n",
    "    4. Update weights and bias\n",
    "    5. Repeat 2 to 5 until converge.\n",
    "    \n",
    "**$\\color{red}{\\textbf{Note: bring training loss below 0.2}}$**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 training loss: 0.6931511791528576, test loss: 0.6929915736842316\n",
      "Iteration 2 training loss: 0.6917171728896355, test loss: 0.6933645705211164\n",
      "Iteration 3 training loss: 0.6903702936740851, test loss: 0.6930923422170504\n",
      "Iteration 4 training loss: 0.689147278427895, test loss: 0.6939444879792748\n",
      "Iteration 5 training loss: 0.6881347443480272, test loss: 0.6934252016674373\n",
      "Iteration 6 training loss: 0.6875304600280003, test loss: 0.6956000503371261\n",
      "Iteration 7 training loss: 0.6877711581207288, test loss: 0.6951491400440579\n",
      "Iteration 8 training loss: 0.6898139561236327, test loss: 0.7016478830427538\n",
      "Iteration 9 training loss: 0.6956334393853851, test loss: 0.7043190233522495\n",
      "Iteration 10 training loss: 0.7090788549891107, test loss: 0.7260165346125399\n",
      "Iteration 11 training loss: 0.7360338947531456, test loss: 0.7450029818701858\n",
      "Iteration 12 training loss: 0.7819959286288555, test loss: 0.806670116346269\n",
      "Iteration 13 training loss: 0.8414283815863693, test loss: 0.8503586496921747\n",
      "Iteration 14 training loss: 0.895501848679217, test loss: 0.9287013436661308\n",
      "Iteration 15 training loss: 0.924916547086509, test loss: 0.9350986493897009\n",
      "Iteration 16 training loss: 0.9376244370027758, test loss: 0.9754107351343841\n",
      "Iteration 17 training loss: 0.9389517152110111, test loss: 0.9513850497578548\n",
      "Iteration 18 training loss: 0.939891708202981, test loss: 0.9803433778126589\n",
      "Iteration 19 training loss: 0.9378926710829539, test loss: 0.9527026894282908\n",
      "Iteration 20 training loss: 0.9372318768441441, test loss: 0.9800537782884825\n",
      "Iteration 21 training loss: 0.9354296770280541, test loss: 0.9525705947627499\n",
      "Iteration 22 training loss: 0.9341918882787309, test loss: 0.9792980993777218\n",
      "Iteration 23 training loss: 0.9328536832558806, test loss: 0.9522664030464619\n",
      "Iteration 24 training loss: 0.9311553323104187, test loss: 0.9784802192443038\n",
      "Iteration 25 training loss: 0.9302735583597981, test loss: 0.9519003952853481\n",
      "Iteration 26 training loss: 0.9281546008801865, test loss: 0.9776367646639496\n",
      "Iteration 27 training loss: 0.9277009910927451, test loss: 0.9514867559846985\n",
      "Iteration 28 training loss: 0.9251932535852708, test loss: 0.9767740855177063\n",
      "Iteration 29 training loss: 0.9251393537612597, test loss: 0.9510313090758363\n",
      "Iteration 30 training loss: 0.9222721958604988, test loss: 0.9758956291790712\n",
      "Iteration 31 training loss: 0.9225910188337586, test loss: 0.9505387501261282\n",
      "Iteration 32 training loss: 0.9193918719995957, test loss: 0.9750042411811145\n",
      "Iteration 33 training loss: 0.9200579988871481, test loss: 0.950013287979097\n",
      "Iteration 34 training loss: 0.9165524853431608, test loss: 0.9741024026214737\n",
      "Iteration 35 training loss: 0.9175420281605262, test loss: 0.9494587343636155\n",
      "Iteration 36 training loss: 0.9137540484811587, test loss: 0.9731922891478348\n",
      "Iteration 37 training loss: 0.9150445942707253, test loss: 0.9488785434076484\n",
      "Iteration 38 training loss: 0.9109964145694038, test loss: 0.9722758096004487\n",
      "Iteration 39 training loss: 0.9125669633844086, test loss: 0.9482758438998624\n",
      "Iteration 40 training loss: 0.9082793032535502, test loss: 0.9713546385063712\n",
      "Iteration 41 training loss: 0.9101102027954155, test loss: 0.9476534683076869\n",
      "Iteration 42 training loss: 0.9056023229112947, test loss: 0.9704302442858616\n",
      "Iteration 43 training loss: 0.9076752014385555, test loss: 0.9470139791707753\n",
      "Iteration 44 training loss: 0.9029649898179097, test loss: 0.9695039138631969\n",
      "Iteration 45 training loss: 0.905262688548731, test loss: 0.9463596931549004\n",
      "Iteration 46 training loss: 0.9003667446723334, test loss: 0.9685767741885631\n",
      "Iteration 47 training loss: 0.9028732506277705, test loss: 0.9456927029921185\n",
      "Iteration 48 training loss: 0.8978069668479569, test loss: 0.9676498110929426\n",
      "Iteration 49 training loss: 0.9005073468627153, test loss: 0.9450148975036651\n",
      "Iteration 50 training loss: 0.8952849866774251, test loss: 0.9667238858337674\n",
      "Iteration 51 training loss: 0.8981653231256123, test loss: 0.9443279798794711\n",
      "Iteration 52 training loss: 0.8928000960355579, test loss: 0.9657997496361455\n",
      "Iteration 53 training loss: 0.8958474246733864, test loss: 0.943633484369423\n",
      "Iteration 54 training loss: 0.8903515574466526, test loss: 0.9648780564902191\n",
      "Iteration 55 training loss: 0.8935538076564614, test loss: 0.9429327915256014\n",
      "Iteration 56 training loss: 0.8879386119105732, test loss: 0.9639593744280636\n",
      "Iteration 57 training loss: 0.8912845495360544, test loss: 0.942227142121077\n",
      "Iteration 58 training loss: 0.8855604856151307, test loss: 0.9630441954721904\n",
      "Iteration 59 training loss: 0.8890396585022549, test loss: 0.9415176498590327\n",
      "Iteration 60 training loss: 0.8832163956794479, test loss: 0.9621329444212675\n",
      "Iteration 61 training loss: 0.8868190819779079, test loss: 0.9408053129755931\n",
      "Iteration 62 training loss: 0.8809055550535922, test loss: 0.9612259866161886\n",
      "Iteration 63 training loss: 0.884622714286832, test loss: 0.9400910248305759\n",
      "Iteration 64 training loss: 0.8786271766831933, test loss: 0.9603236348105294\n",
      "Iteration 65 training loss: 0.8824504035589059, test loss: 0.9393755835722053\n",
      "Iteration 66 training loss: 0.8763804770335759, test loss: 0.9594261552531277\n",
      "Iteration 67 training loss: 0.8803019579390143, test loss: 0.9386597009544673\n",
      "Iteration 68 training loss: 0.8741646790557523, test loss: 0.9585337730765673\n",
      "Iteration 69 training loss: 0.8781771511616574, test loss: 0.9379440103791525\n",
      "Iteration 70 training loss: 0.871979014666124, test loss: 0.9576466770733894\n",
      "Iteration 71 training loss: 0.8760757275482117, test loss: 0.9372290742286067\n",
      "Iteration 72 training loss: 0.8698227268026706, test loss: 0.9567650239315494\n",
      "Iteration 73 training loss: 0.8739974064793227, test loss: 0.9365153905497055\n",
      "Iteration 74 training loss: 0.8676950711125597, test loss: 0.9558889419917652\n",
      "Iteration 75 training loss: 0.8719418863907024, test loss: 0.935803399144552\n",
      "Iteration 76 training loss: 0.8655953173192882, test loss: 0.9550185345817161\n",
      "Iteration 77 training loss: 0.8699088483366832, test loss: 0.9350934871187883\n",
      "Iteration 78 training loss: 0.8635227503115408, test loss: 0.9541538829754046\n",
      "Iteration 79 training loss: 0.8678979591622146, test loss: 0.934385993934194\n",
      "Iteration 80 training loss: 0.8614766709907743, test loss: 0.953295049020205\n",
      "Iteration 81 training loss: 0.8659088743205897, test loss: 0.933681216008346\n",
      "Iteration 82 training loss: 0.8594563969100226, test loss: 0.9524420774690897\n",
      "Iteration 83 training loss: 0.8639412403710199, test loss: 0.9329794109005689\n",
      "Iteration 84 training loss: 0.8574612627324601, test loss: 0.9515949980511347\n",
      "Iteration 85 training loss: 0.8619946971872379, test loss: 0.9322808011200939\n",
      "Iteration 86 training loss: 0.8554906205347922, test loss: 0.9507538273095587\n",
      "Iteration 87 training loss: 0.8600688799055877, test loss: 0.9315855775893541\n",
      "Iteration 88 training loss: 0.8535438399774966, test loss: 0.9499185702331859\n",
      "Iteration 89 training loss: 0.8581634206385508, test loss: 0.930893902792539\n",
      "Iteration 90 training loss: 0.8516203083612646, test loss: 0.949089221704278\n",
      "Iteration 91 training loss: 0.8562779499773243, test loss: 0.9302059136369991\n",
      "Iteration 92 training loss: 0.8497194305866248, test loss: 0.9482657677830669\n",
      "Iteration 93 training loss: 0.8544120983049365, test loss: 0.9295217240527304\n",
      "Iteration 94 training loss: 0.8478406290316652, test loss: 0.9474481868470499\n",
      "Iteration 95 training loss: 0.8525654969394129, test loss: 0.9288414273530237\n",
      "Iteration 96 training loss: 0.8459833433609237, test loss: 0.946636450601078\n",
      "Iteration 97 training loss: 0.8507377791246955, test loss: 0.9281650983773773\n",
      "Iteration 98 training loss: 0.8441470302769079, test loss: 0.9458305249724954\n",
      "Iteration 99 training loss: 0.8489285808853658, test loss: 0.9274927954359561\n",
      "Iteration 100 training loss: 0.84233116322427, test loss: 0.9450303709040054\n",
      "Iteration 101 training loss: 0.8471375417596947, test loss: 0.9268245620732198\n",
      "Iteration 102 training loss: 0.8405352320554023, test loss: 0.9442359450555478\n",
      "Iteration 103 training loss: 0.8453643054241643, test loss: 0.9261604286668123\n",
      "Iteration 104 training loss: 0.8387587426650928, test loss: 0.9434472004252259\n",
      "Iteration 105 training loss: 0.843608520221328, test loss: 0.9255004138764048\n",
      "Iteration 106 training loss: 0.8370012166009085, test loss: 0.9426640868982369\n",
      "Iteration 107 training loss: 0.8418698396017273, test loss: 0.9248445259559073\n",
      "Iteration 108 training loss: 0.8352621906550827, test loss: 0.9418865517317682\n",
      "Iteration 109 training loss: 0.8401479224895253, test loss: 0.9241927639412933\n",
      "Iteration 110 training loss: 0.8335412164429221, test loss: 0.9411145399829672\n",
      "Iteration 111 training loss: 0.8384424335805565, test loss: 0.9235451187252052\n",
      "Iteration 112 training loss: 0.8318378599720627, test loss: 0.9403479948863077\n",
      "Iteration 113 training loss: 0.8367530435806286, test loss: 0.9229015740285279\n",
      "Iteration 114 training loss: 0.8301517012063018, test loss: 0.9395868581860025\n",
      "Iteration 115 training loss: 0.8350794293911106, test loss: 0.9222621072782355\n",
      "Iteration 116 training loss: 0.8284823336271978, test loss: 0.9388310704284875\n",
      "Iteration 117 training loss: 0.8334212742481316, test loss: 0.9216266903999736\n",
      "Iteration 118 training loss: 0.8268293637961638, test loss: 0.9380805712194586\n",
      "Iteration 119 training loss: 0.8317782678210598, test loss: 0.9209952905331166\n",
      "Iteration 120 training loss: 0.8251924109193678, test loss: 0.9373352994494647\n",
      "Iteration 121 training loss: 0.8301501062753442, test loss: 0.92036787067534\n",
      "Iteration 122 training loss: 0.8235711064173785, test loss: 0.9365951934916085\n",
      "Iteration 123 training loss: 0.8285364923042677, test loss: 0.9197443902631325\n",
      "Iteration 124 training loss: 0.8219650935011888, test loss: 0.9358601913745392\n",
      "Iteration 125 training loss: 0.8269371351336775, test loss: 0.9191248056941017\n",
      "Iteration 126 training loss: 0.8203740267559572, test loss: 0.9351302309335523\n",
      "Iteration 127 training loss: 0.8253517505033278, test loss: 0.918509070796409\n",
      "Iteration 128 training loss: 0.8187975717335632, test loss: 0.934405249942326\n",
      "Iteration 129 training loss: 0.8237800606280723, test loss: 0.9178971372501928\n",
      "Iteration 130 training loss: 0.8172354045548598, test loss: 0.9336851862275265\n",
      "Iteration 131 training loss: 0.8222217941417925, test loss: 0.9172889549654127\n",
      "Iteration 132 training loss: 0.8156872115223146, test loss: 0.9329699777682815\n",
      "Iteration 133 training loss: 0.8206766860266315, test loss: 0.9166844724201468\n",
      "Iteration 134 training loss: 0.8141526887435658, test loss: 0.9322595627822937\n",
      "Iteration 135 training loss: 0.8191444775298081, test loss: 0.9160836369630181\n",
      "Iteration 136 training loss: 0.8126315417662834, test loss: 0.9315538798001719\n",
      "Iteration 137 training loss: 0.8176249160700391, test loss: 0.9154863950831035\n",
      "Iteration 138 training loss: 0.8111234852245911, test loss: 0.9308528677293842\n",
      "Iteration 139 training loss: 0.8161177551353549, test loss: 0.9148926926503622\n",
      "Iteration 140 training loss: 0.8096282424972129, test loss: 0.9301564659090699\n",
      "Iteration 141 training loss: 0.8146227541738925, test loss: 0.9143024751293735\n",
      "Iteration 142 training loss: 0.808145545377403, test loss: 0.9294646141568228\n",
      "Iteration 143 training loss: 0.8131396784790589, test loss: 0.9137156877689024\n",
      "Iteration 144 training loss: 0.8066751337546475, test loss: 0.9287772528084182\n",
      "Iteration 145 training loss: 0.8116682990702928, test loss: 0.9131322757695938\n",
      "Iteration 146 training loss: 0.8052167553080607, test loss: 0.9280943227513551\n",
      "Iteration 147 training loss: 0.8102083925705005, test loss: 0.9125521844318953\n",
      "Iteration 148 training loss: 0.8037701652113378, test loss: 0.9274157654529757\n",
      "Iteration 149 training loss: 0.8087597410811087, test loss: 0.9119753592861065\n",
      "Iteration 150 training loss: 0.8023351258490841, test loss: 0.9267415229838497\n",
      "Iteration 151 training loss: 0.8073221320555577, test loss: 0.9114017462062948\n",
      "Iteration 152 training loss: 0.8009114065443005, test loss: 0.9260715380370126\n",
      "Iteration 153 training loss: 0.8058953581719471, test loss: 0.9108312915096453\n",
      "Iteration 154 training loss: 0.7994987832967724, test loss: 0.9254057539435941\n",
      "Iteration 155 training loss: 0.8044792172054568, test loss: 0.9102639420426897\n",
      "Iteration 156 training loss: 0.7980970385320869, test loss: 0.924744114685302\n",
      "Iteration 157 training loss: 0.8030735119010758, test loss: 0.909699645255708\n",
      "Iteration 158 training loss: 0.7967059608609803, test loss: 0.9240865649041681\n",
      "Iteration 159 training loss: 0.8016780498470965, test loss: 0.9091383492664902\n",
      "Iteration 160 training loss: 0.795325344848705, test loss: 0.9234330499099226\n",
      "Iteration 161 training loss: 0.8002926433497684, test loss: 0.9085800029145357\n",
      "Iteration 162 training loss: 0.7939549907940925, test loss: 0.9227835156853118\n",
      "Iteration 163 training loss: 0.7989171093094373, test loss: 0.9080245558066659\n",
      "Iteration 164 training loss: 0.7925947045179843, test loss: 0.922137908889634\n",
      "Iteration 165 training loss: 0.7975512690984549, test loss: 0.9074719583549368\n",
      "Iteration 166 training loss: 0.7912442971606944, test loss: 0.9214961768607416\n",
      "Iteration 167 training loss: 0.7961949484410848, test loss: 0.9069221618076594\n",
      "Iteration 168 training loss: 0.7899035849881694, test loss: 0.9208582676157206\n",
      "Iteration 169 training loss: 0.7948479772955968, test loss: 0.9063751182742523\n",
      "Iteration 170 training loss: 0.7885723892065103, test loss: 0.9202241298504321\n",
      "Iteration 171 training loss: 0.7935101897387031, test loss: 0.9058307807445978\n",
      "Iteration 172 training loss: 0.7872505357845244, test loss: 0.9195937129380775\n",
      "Iteration 173 training loss: 0.7921814238524595, test loss: 0.905289103103492\n",
      "Iteration 174 training loss: 0.7859378552839772, test loss: 0.9189669669269303\n",
      "Iteration 175 training loss: 0.7908615216137216, test loss: 0.9047500401407403\n",
      "Iteration 176 training loss: 0.7846341826972247, test loss: 0.9183438425373543\n",
      "Iteration 177 training loss: 0.7895503287862294, test loss: 0.9042135475573864\n",
      "Iteration 178 training loss: 0.7833393572919073, test loss: 0.9177242911582157\n",
      "Iteration 179 training loss: 0.7882476948153643, test loss: 0.9036795819685199\n",
      "Iteration 180 training loss: 0.7820532224623985, test loss: 0.9171082648427781\n",
      "Iteration 181 training loss: 0.7869534727256086, test loss: 0.9031481009030682\n",
      "Iteration 182 training loss: 0.7807756255877075, test loss: 0.9164957163041625\n",
      "Iteration 183 training loss: 0.7856675190207221, test loss: 0.9026190628009323\n",
      "Iteration 184 training loss: 0.7795064178955461, test loss: 0.9158865989104401\n",
      "Iteration 185 training loss: 0.7843896935866331, test loss: 0.9020924270078\n",
      "Iteration 186 training loss: 0.7782454543322748, test loss: 0.9152808666794106\n",
      "Iteration 187 training loss: 0.7831198595970312, test loss: 0.9015681537679266\n",
      "Iteration 188 training loss: 0.7769925934384606, test loss: 0.9146784742731242\n",
      "Iteration 189 training loss: 0.7818578834216404, test loss: 0.9010462042151565\n",
      "Iteration 190 training loss: 0.77574769722978, test loss: 0.914079376992186\n",
      "Iteration 191 training loss: 0.7806036345371393, test loss: 0.9005265403624211\n",
      "Iteration 192 training loss: 0.774510631083016, test loss: 0.9134835307698767\n",
      "Iteration 193 training loss: 0.7793569854406921, test loss: 0.900009125089932\n",
      "Iteration 194 training loss: 0.7732812636269085, test loss: 0.9128908921661266\n",
      "Iteration 195 training loss: 0.7781178115660438, test loss: 0.899493922132265\n",
      "Iteration 196 training loss: 0.7720594666376224, test loss: 0.912301418361365\n",
      "Iteration 197 training loss: 0.7768859912021312, test loss: 0.8989808960645046\n",
      "Iteration 198 training loss: 0.7708451149386134, test loss: 0.9117150671502694\n",
      "Iteration 199 training loss: 0.7756614054141581, test loss: 0.8984700122876111\n",
      "Iteration 200 training loss: 0.769638086304676, test loss: 0.9111317969354293\n",
      "Iteration 201 training loss: 0.7744439379670742, test loss: 0.8979612370131469\n",
      "Iteration 202 training loss: 0.7684382613699742, test loss: 0.9105515667209494\n",
      "Iteration 203 training loss: 0.7732334752514036, test loss: 0.8974545372474868\n",
      "Iteration 204 training loss: 0.7672455235398546, test loss: 0.9099743361059952\n",
      "Iteration 205 training loss: 0.7720299062113598, test loss: 0.8969498807756271\n",
      "Iteration 206 training loss: 0.7660597589062643, test loss: 0.9094000652782994\n",
      "Iteration 207 training loss: 0.7708331222751862, test loss: 0.8964472361446898\n",
      "Iteration 208 training loss: 0.7648808561665891, test loss: 0.908828715007637\n",
      "Iteration 209 training loss: 0.7696430172876585, test loss: 0.8959465726472112\n",
      "Iteration 210 training loss: 0.7637087065457526, test loss: 0.9082602466392738\n",
      "Iteration 211 training loss: 0.7684594874446911, test loss: 0.8954478603042929\n",
      "Iteration 212 training loss: 0.7625432037214103, test loss: 0.9076946220874028\n",
      "Iteration 213 training loss: 0.7672824312299762, test loss: 0.8949510698486868\n",
      "Iteration 214 training loss: 0.7613842437520897, test loss: 0.9071318038285641\n",
      "Iteration 215 training loss: 0.766111749353604, test loss: 0.8944561727078707\n",
      "Iteration 216 training loss: 0.7602317250081333, test loss: 0.9065717548950614\n",
      "Iteration 217 training loss: 0.7649473446925921, test loss: 0.8939631409871746\n",
      "Iteration 218 training loss: 0.759085548105306, test loss: 0.9060144388683775\n",
      "Iteration 219 training loss: 0.7637891222332709, test loss: 0.8934719474529966\n",
      "Iteration 220 training loss: 0.7579456158409391, test loss: 0.9054598198725866\n",
      "Iteration 221 training loss: 0.7626369890154611, test loss: 0.8929825655161567\n",
      "Iteration 222 training loss: 0.756811833132487, test loss: 0.9049078625677774\n",
      "Iteration 223 training loss: 0.7614908540783859, test loss: 0.8924949692154223\n",
      "Iteration 224 training loss: 0.7556841069583806, test loss: 0.9043585321434782\n",
      "Iteration 225 training loss: 0.7603506284082605, test loss: 0.8920091332012311\n",
      "Iteration 226 training loss: 0.7545623463010688, test loss: 0.9038117943120959\n",
      "Iteration 227 training loss: 0.759216224887501, test loss: 0.8915250327196467\n",
      "Iteration 228 training loss: 0.7534464620921386, test loss: 0.9032676153023647\n",
      "Iteration 229 training loss: 0.758087558245502, test loss: 0.8910426435965604\n",
      "Iteration 230 training loss: 0.7523363671594243, test loss: 0.9027259618528108\n",
      "Iteration 231 training loss: 0.7569645450109246, test loss: 0.8905619422221659\n",
      "Iteration 232 training loss: 0.7512319761760009, test loss: 0.9021868012052333\n",
      "Iteration 233 training loss: 0.7558471034654475, test loss: 0.8900829055357153\n",
      "Iteration 234 training loss: 0.7501332056109841, test loss: 0.9016501010982021\n",
      "Iteration 235 training loss: 0.7547351535989281, test loss: 0.8896055110105762\n",
      "Iteration 236 training loss: 0.7490399736820443, test loss: 0.901115829760578\n",
      "Iteration 237 training loss: 0.7536286170659258, test loss: 0.8891297366395948\n",
      "Iteration 238 training loss: 0.7479522003095621, test loss: 0.9005839559050528\n",
      "Iteration 239 training loss: 0.7525274171435429, test loss: 0.8886555609207774\n",
      "Iteration 240 training loss: 0.7468698070723473, test loss: 0.9000544487217154\n",
      "Iteration 241 training loss: 0.7514314786905318, test loss: 0.8881829628432923\n",
      "Iteration 242 training loss: 0.7457927171648494, test loss: 0.8995272778716406\n",
      "Iteration 243 training loss: 0.7503407281076315, test loss: 0.8877119218738009\n",
      "Iteration 244 training loss: 0.7447208553557968, test loss: 0.8990024134805075\n",
      "Iteration 245 training loss: 0.7492550932990854, test loss: 0.8872424179431163\n",
      "Iteration 246 training loss: 0.7436541479481968, test loss: 0.8984798261322459\n",
      "Iteration 247 training loss: 0.7481745036353026, test loss: 0.8867744314331956\n",
      "Iteration 248 training loss: 0.74259252274064, test loss: 0.8979594868627135\n",
      "Iteration 249 training loss: 0.7470988899166223, test loss: 0.8863079431644613\n",
      "Iteration 250 training loss: 0.7415359089898501, test loss: 0.8974413671534052\n",
      "Iteration 251 training loss: 0.7460281843381418, test loss: 0.885842934383453\n",
      "Iteration 252 training loss: 0.7404842373744266, test loss: 0.8969254389251968\n",
      "Iteration 253 training loss: 0.7449623204555751, test loss: 0.88537938675081\n",
      "Iteration 254 training loss: 0.7394374399597303, test loss: 0.896411674532125\n",
      "Iteration 255 training loss: 0.7439012331521022, test loss: 0.8849172823295767\n",
      "Iteration 256 training loss: 0.738395450163861, test loss: 0.8959000467552041\n",
      "Iteration 257 training loss: 0.7428448586061787, test loss: 0.8844566035738329\n",
      "Iteration 258 training loss: 0.7373582027246844, test loss: 0.8953905287962822\n",
      "Iteration 259 training loss: 0.7417931342602706, test loss: 0.8839973333176404\n",
      "Iteration 260 training loss: 0.7363256336678659, test loss: 0.894883094271939\n",
      "Iteration 261 training loss: 0.7407459987904854, test loss: 0.8835394547643044\n",
      "Iteration 262 training loss: 0.7352976802758657, test loss: 0.8943777172074234\n",
      "Iteration 263 training loss: 0.7397033920770664, test loss: 0.8830829514759443\n",
      "Iteration 264 training loss: 0.7342742810578632, test loss: 0.8938743720306409\n",
      "Iteration 265 training loss: 0.7386652551757222, test loss: 0.8826278073633663\n",
      "Iteration 266 training loss: 0.7332553757205678, test loss: 0.8933730335661784\n",
      "Iteration 267 training loss: 0.737631530289765, test loss: 0.8821740066762349\n",
      "Iteration 268 training loss: 0.7322409051398859, test loss: 0.8928736770293835\n",
      "Iteration 269 training loss: 0.7366021607430272, test loss: 0.8817215339935361\n",
      "Iteration 270 training loss: 0.7312308113334098, test loss: 0.8923762780204891\n",
      "Iteration 271 training loss: 0.7355770909535344, test loss: 0.8812703742143256\n",
      "Iteration 272 training loss: 0.730225037433694, test loss: 0.8918808125187879\n",
      "Iteration 273 training loss: 0.7345562664079069, test loss: 0.8808205125487564\n",
      "Iteration 274 training loss: 0.729223527662296, test loss: 0.8913872568768597\n",
      "Iteration 275 training loss: 0.7335396336364669, test loss: 0.8803719345093797\n",
      "Iteration 276 training loss: 0.7282262273045456, test loss: 0.8908955878148495\n",
      "Iteration 277 training loss: 0.7325271401890286, test loss: 0.8799246259027113\n",
      "Iteration 278 training loss: 0.7272330826850224, test loss: 0.8904057824148017\n",
      "Iteration 279 training loss: 0.731518734611348, test loss: 0.8794785728210579\n",
      "Iteration 280 training loss: 0.7262440411437124, test loss: 0.8899178181150482\n",
      "Iteration 281 training loss: 0.730514366422213, test loss: 0.8790337616345976\n",
      "Iteration 282 training loss: 0.7252590510128183, test loss: 0.8894316727046537\n",
      "Iteration 283 training loss: 0.7295139860911506, test loss: 0.8785901789837043\n",
      "Iteration 284 training loss: 0.7242780615942059, test loss: 0.8889473243179167\n",
      "Iteration 285 training loss: 0.7285175450167338, test loss: 0.878147811771514\n",
      "Iteration 286 training loss: 0.7233010231374578, test loss: 0.8884647514289309\n",
      "Iteration 287 training loss: 0.7275249955054686, test loss: 0.8777066471567236\n",
      "Iteration 288 training loss: 0.7223278868185183, test loss: 0.8879839328462046\n",
      "Iteration 289 training loss: 0.7265362907512407, test loss: 0.8772666725466166\n",
      "Iteration 290 training loss: 0.7213586047189072, test loss: 0.8875048477073418\n",
      "Iteration 291 training loss: 0.7255513848153097, test loss: 0.8768278755903088\n",
      "Iteration 292 training loss: 0.7203931298054845, test loss: 0.8870274754737818\n",
      "Iteration 293 training loss: 0.7245702326068268, test loss: 0.8763902441722081\n",
      "Iteration 294 training loss: 0.7194314159107464, test loss: 0.8865517959256027\n",
      "Iteration 295 training loss: 0.7235927898638647, test loss: 0.8759537664056818\n",
      "Iteration 296 training loss: 0.7184734177136364, test loss: 0.8860777891563865\n",
      "Iteration 297 training loss: 0.7226190131349423, test loss: 0.875518430626925\n",
      "Iteration 298 training loss: 0.7175190907208554, test loss: 0.8856054355681469\n",
      "Iteration 299 training loss: 0.7216488597610308, test loss: 0.8750842253890259\n",
      "Iteration 300 training loss: 0.7165683912486521, test loss: 0.8851347158663211\n",
      "Iteration 301 training loss: 0.7206822878580224, test loss: 0.8746511394562181\n",
      "Iteration 302 training loss: 0.7156212764050823, test loss: 0.8846656110548251\n",
      "Iteration 303 training loss: 0.7197192562996517, test loss: 0.8742191617983194\n",
      "Iteration 304 training loss: 0.7146777040727187, test loss: 0.8841981024311737\n",
      "Iteration 305 training loss: 0.718759724700856, test loss: 0.8737882815853449\n",
      "Iteration 306 training loss: 0.7137376328918001, test loss: 0.8837321715816641\n",
      "Iteration 307 training loss: 0.7178036534015575, test loss: 0.8733584881822966\n",
      "Iteration 308 training loss: 0.7128010222438061, test loss: 0.8832678003766263\n",
      "Iteration 309 training loss: 0.7168510034508597, test loss: 0.8729297711441162\n",
      "Iteration 310 training loss: 0.7118678322354409, test loss: 0.8828049709657368\n",
      "Iteration 311 training loss: 0.7159017365916435, test loss: 0.8725021202108019\n",
      "Iteration 312 training loss: 0.7109380236830215, test loss: 0.8823436657733981\n",
      "Iteration 313 training loss: 0.7149558152455503, test loss: 0.8720755253026803\n",
      "Iteration 314 training loss: 0.7100115580972484, test loss: 0.8818838674941838\n",
      "Iteration 315 training loss: 0.714013202498343, test loss: 0.8716499765158298\n",
      "Iteration 316 training loss: 0.7090883976683566, test loss: 0.8814255590883486\n",
      "Iteration 317 training loss: 0.7130738620856326, test loss: 0.8712254641176513\n",
      "Iteration 318 training loss: 0.7081685052516308, test loss: 0.8809687237774045\n",
      "Iteration 319 training loss: 0.7121377583789602, test loss: 0.8708019785425787\n",
      "Iteration 320 training loss: 0.7072518443532742, test loss: 0.8805133450397601\n",
      "Iteration 321 training loss: 0.7112048563722242, test loss: 0.8703795103879285\n",
      "Iteration 322 training loss: 0.7063383791166246, test loss: 0.8800594066064276\n",
      "Iteration 323 training loss: 0.7102751216684425, test loss: 0.86995805040988\n",
      "Iteration 324 training loss: 0.7054280743087018, test loss: 0.8796068924567929\n",
      "Iteration 325 training loss: 0.7093485204668409, test loss: 0.8695375895195843\n",
      "Iteration 326 training loss: 0.7045208953070851, test loss: 0.8791557868144514\n",
      "Iteration 327 training loss: 0.7084250195502586, test loss: 0.8691181187793977\n",
      "Iteration 328 training loss: 0.7036168080871013, test loss: 0.8787060741431064\n",
      "Iteration 329 training loss: 0.7075045862728593, test loss: 0.8686996293992336\n",
      "Iteration 330 training loss: 0.7027157792093263, test loss: 0.8782577391425346\n",
      "Iteration 331 training loss: 0.7065871885481446, test loss: 0.8682821127330298\n",
      "Iteration 332 training loss: 0.701817775807381, test loss: 0.8778107667446121\n",
      "Iteration 333 training loss: 0.705672794837255, test loss: 0.8678655602753296\n",
      "Iteration 334 training loss: 0.7009227655760214, test loss: 0.8773651421094063\n",
      "Iteration 335 training loss: 0.7047613741375566, test loss: 0.8674499636579676\n",
      "Iteration 336 training loss: 0.7000307167595091, test loss: 0.8769208506213287\n",
      "Iteration 337 training loss: 0.7038528959714996, test loss: 0.8670353146468631\n",
      "Iteration 338 training loss: 0.6991415981402592, test loss: 0.8764778778853521\n",
      "Iteration 339 training loss: 0.7029473303757473, test loss: 0.8666216051389118\n",
      "Iteration 340 training loss: 0.698255379027755, test loss: 0.8760362097232878\n",
      "Iteration 341 training loss: 0.7020446478905625, test loss: 0.8662088271589757\n",
      "Iteration 342 training loss: 0.6973720292477232, test loss: 0.875595832170125\n",
      "Iteration 343 training loss: 0.7011448195494489, test loss: 0.8657969728569668\n",
      "Iteration 344 training loss: 0.6964915191315646, test loss: 0.8751567314704325\n",
      "Iteration 345 training loss: 0.7002478168690375, test loss: 0.8653860345050216\n",
      "Iteration 346 training loss: 0.6956138195060303, test loss: 0.8747188940748186\n",
      "Iteration 347 training loss: 0.6993536118392132, test loss: 0.8649760044947639\n",
      "Iteration 348 training loss: 0.694738901683139, test loss: 0.8742823066364515\n",
      "Iteration 349 training loss: 0.698462176913474, test loss: 0.86456687533465\n",
      "Iteration 350 training loss: 0.6938667374503295, test loss: 0.873846956007639\n",
      "Iteration 351 training loss: 0.6975734849995173, test loss: 0.864158639647399\n",
      "Iteration 352 training loss: 0.6929972990608404, test loss: 0.8734128292364671\n",
      "Iteration 353 training loss: 0.6966875094500469, test loss: 0.8637512901674983\n",
      "Iteration 354 training loss: 0.6921305592243128, test loss: 0.8729799135634958\n",
      "Iteration 355 training loss: 0.6958042240537947, test loss: 0.8633448197387884\n",
      "Iteration 356 training loss: 0.6912664910976098, test loss: 0.8725481964185114\n",
      "Iteration 357 training loss: 0.6949236030267529, test loss: 0.8629392213121193\n",
      "Iteration 358 training loss: 0.6904050682758466, test loss: 0.8721176654173374\n",
      "Iteration 359 training loss: 0.6940456210036082, test loss: 0.8625344879430782\n",
      "Iteration 360 training loss: 0.6895462647836268, test loss: 0.8716883083586979\n",
      "Iteration 361 training loss: 0.6931702530293755, test loss: 0.8621306127897869\n",
      "Iteration 362 training loss: 0.6886900550664783, test loss: 0.8712601132211408\n",
      "Iteration 363 training loss: 0.6922974745512247, test loss: 0.8617275891107635\n",
      "Iteration 364 training loss: 0.6878364139824863, test loss: 0.8708330681600097\n",
      "Iteration 365 training loss: 0.691427261410495, test loss: 0.8613254102628493\n",
      "Iteration 366 training loss: 0.6869853167941151, test loss: 0.870407161504474\n",
      "Iteration 367 training loss: 0.6905595898348945, test loss: 0.8609240696991965\n",
      "Iteration 368 training loss: 0.6861367391602159, test loss: 0.8699823817546098\n",
      "Iteration 369 training loss: 0.6896944364308746, test loss: 0.8605235609673169\n",
      "Iteration 370 training loss: 0.6852906571282161, test loss: 0.8695587175785332\n",
      "Iteration 371 training loss: 0.6888317781761828, test loss: 0.860123877707185\n",
      "Iteration 372 training loss: 0.6844470471264853, test loss: 0.8691361578095844\n",
      "Iteration 373 training loss: 0.6879715924125819, test loss: 0.8597250136494011\n",
      "Iteration 374 training loss: 0.6836058859568719, test loss: 0.8687146914435652\n",
      "Iteration 375 training loss: 0.6871138568387352, test loss: 0.8593269626134025\n",
      "Iteration 376 training loss: 0.6827671507874096, test loss: 0.8682943076360216\n",
      "Iteration 377 training loss: 0.6862585495032528, test loss: 0.8589297185057314\n",
      "Iteration 378 training loss: 0.6819308191451853, test loss: 0.8678749956995795\n",
      "Iteration 379 training loss: 0.6854056487978948, test loss: 0.858533275318349\n",
      "Iteration 380 training loss: 0.6810968689093689, test loss: 0.8674567451013284\n",
      "Iteration 381 training loss: 0.6845551334509271, test loss: 0.8581376271269998\n",
      "Iteration 382 training loss: 0.6802652783043963, test loss: 0.8670395454602507\n",
      "Iteration 383 training loss: 0.6837069825206258, test loss: 0.8577427680896216\n",
      "Iteration 384 training loss: 0.6794360258933068, test loss: 0.8666233865446981\n",
      "Iteration 385 training loss: 0.6828611753889278, test loss: 0.8573486924448\n",
      "Iteration 386 training loss: 0.6786090905712254, test loss: 0.8662082582699169\n",
      "Iteration 387 training loss: 0.6820176917552216, test loss: 0.8569553945102668\n",
      "Iteration 388 training loss: 0.6777844515589937, test loss: 0.8657941506956155\n",
      "Iteration 389 training loss: 0.681176511630278, test loss: 0.8565628686814406\n",
      "Iteration 390 training loss: 0.6769620883969379, test loss: 0.8653810540235771\n",
      "Iteration 391 training loss: 0.6803376153303128, test loss: 0.8561711094300067\n",
      "Iteration 392 training loss: 0.676141980938776, test loss: 0.864968958595318\n",
      "Iteration 393 training loss: 0.6795009834711833, test loss: 0.8557801113025358\n",
      "Iteration 394 training loss: 0.6753241093456598, test loss: 0.8645578548897864\n",
      "Iteration 395 training loss: 0.6786665969627105, test loss: 0.855389868919142\n",
      "Iteration 396 training loss: 0.674508454080346, test loss: 0.864147733521106\n",
      "Iteration 397 training loss: 0.677834437003128, test loss: 0.8550003769721755\n",
      "Iteration 398 training loss: 0.673694995901497, test loss: 0.8637385852363585\n",
      "Iteration 399 training loss: 0.677004485073651, test loss: 0.8546116302249502\n",
      "Iteration 400 training loss: 0.6728837158581047, test loss: 0.8633304009134107\n",
      "Iteration 401 training loss: 0.6761767229331649, test loss: 0.8542236235105071\n",
      "Iteration 402 training loss: 0.6720745952840369, test loss: 0.8629231715587766\n",
      "Iteration 403 training loss: 0.6753511326130287, test loss: 0.8538363517304086\n",
      "Iteration 404 training loss: 0.6712676157927021, test loss: 0.8625168883055246\n",
      "Iteration 405 training loss: 0.6745276964119926, test loss: 0.853449809853564\n",
      "Iteration 406 training loss: 0.67046275927183, test loss: 0.8621115424112186\n",
      "Iteration 407 training loss: 0.6737063968912251, test loss: 0.8530639929150874\n",
      "Iteration 408 training loss: 0.669660007878366, test loss: 0.8617071252558999\n",
      "Iteration 409 training loss: 0.6728872168694465, test loss: 0.8526788960151838\n",
      "Iteration 410 training loss: 0.6688593440334739, test loss: 0.8613036283401052\n",
      "Iteration 411 training loss: 0.6720701394181693, test loss: 0.8522945143180627\n",
      "Iteration 412 training loss: 0.6680607504176491, test loss: 0.8609010432829228\n",
      "Iteration 413 training loss: 0.6712551478570387, test loss: 0.8519108430508824\n",
      "Iteration 414 training loss: 0.6672642099659349, test loss: 0.8604993618200815\n",
      "Iteration 415 training loss: 0.670442225749275, test loss: 0.8515278775027162\n",
      "Iteration 416 training loss: 0.6664697058632423, test loss: 0.8600985758020779\n",
      "Iteration 417 training loss: 0.6696313568972116, test loss: 0.8511456130235487\n",
      "Iteration 418 training loss: 0.6656772215397695, test loss: 0.8596986771923372\n",
      "Iteration 419 training loss: 0.668822525337929, test loss: 0.8507640450232935\n",
      "Iteration 420 training loss: 0.6648867406665206, test loss: 0.8592996580654081\n",
      "Iteration 421 training loss: 0.668015715338981, test loss: 0.8503831689708373\n",
      "Iteration 422 training loss: 0.6640982471509175, test loss: 0.858901510605189\n",
      "Iteration 423 training loss: 0.6672109113942124, test loss: 0.8500029803931053\n",
      "Iteration 424 training loss: 0.6633117251325069, test loss: 0.8585042271031903\n",
      "Iteration 425 training loss: 0.6664080982196634, test loss: 0.8496234748741496\n",
      "Iteration 426 training loss: 0.6625271589787581, test loss: 0.858107799956827\n",
      "Iteration 427 training loss: 0.6656072607495622, test loss: 0.8492446480542599\n",
      "Iteration 428 training loss: 0.6617445332809491, test loss: 0.8577122216677402\n",
      "Iteration 429 training loss: 0.6648083841323997, test loss: 0.8488664956290934\n",
      "Iteration 430 training loss: 0.6609638328501388, test loss: 0.8573174848401554\n",
      "Iteration 431 training loss: 0.6640114537270888, test loss: 0.8484890133488272\n",
      "Iteration 432 training loss: 0.6601850427132271, test loss: 0.8569235821792641\n",
      "Iteration 433 training loss: 0.6632164550992008, test loss: 0.8481121970173279\n",
      "Iteration 434 training loss: 0.6594081481090938, test loss: 0.8565305064896401\n",
      "Iteration 435 training loss: 0.6624233740172837, test loss: 0.8477360424913412\n",
      "Iteration 436 training loss: 0.6586331344848204, test loss: 0.856138250673682\n",
      "Iteration 437 training loss: 0.6616321964492543, test loss: 0.8473605456796999\n",
      "Iteration 438 training loss: 0.6578599874919914, test loss: 0.8557468077300854\n",
      "Iteration 439 training loss: 0.6608429085588657, test loss: 0.8469857025425493\n",
      "Iteration 440 training loss: 0.6570886929830709, test loss: 0.855356170752342\n",
      "Iteration 441 training loss: 0.660055496702249, test loss: 0.8466115090905894\n",
      "Iteration 442 training loss: 0.6563192370078563, test loss: 0.8549663329272678\n",
      "Iteration 443 training loss: 0.6592699474245234, test loss: 0.8462379613843339\n",
      "Iteration 444 training loss: 0.6555516058100042, test loss: 0.8545772875335551\n",
      "Iteration 445 training loss: 0.6584862474564798, test loss: 0.8458650555333853\n",
      "Iteration 446 training loss: 0.6547857858236287, test loss: 0.8541890279403532\n",
      "Iteration 447 training loss: 0.6577043837113284, test loss: 0.8454927876957249\n",
      "Iteration 448 training loss: 0.6540217636699702, test loss: 0.8538015476058746\n",
      "Iteration 449 training loss: 0.6569243432815148, test loss: 0.8451211540770183\n",
      "Iteration 450 training loss: 0.6532595261541319, test loss: 0.8534148400760239\n",
      "Iteration 451 training loss: 0.6561461134356011, test loss: 0.8447501509299358\n",
      "Iteration 452 training loss: 0.6524990602618845, test loss: 0.8530288989830555\n",
      "Iteration 453 training loss: 0.6553696816152097, test loss: 0.8443797745534851\n",
      "Iteration 454 training loss: 0.6517403531565342, test loss: 0.8526437180442512\n",
      "Iteration 455 training loss: 0.6545950354320282, test loss: 0.8440100212923605\n",
      "Iteration 456 training loss: 0.6509833921758577, test loss: 0.8522592910606244\n",
      "Iteration 457 training loss: 0.6538221626648764, test loss: 0.8436408875363008\n",
      "Iteration 458 training loss: 0.6502281648290963, test loss: 0.8518756119156461\n",
      "Iteration 459 training loss: 0.6530510512568312, test loss: 0.8432723697194663\n",
      "Iteration 460 training loss: 0.6494746587940138, test loss: 0.8514926745739945\n",
      "Iteration 461 training loss: 0.6522816893124089, test loss: 0.8429044643198207\n",
      "Iteration 462 training loss: 0.6487228619140115, test loss: 0.8511104730803256\n",
      "Iteration 463 training loss: 0.651514065094806, test loss: 0.8425371678585317\n",
      "Iteration 464 training loss: 0.6479727621953048, test loss: 0.8507290015580671\n",
      "Iteration 465 training loss: 0.6507481670231918, test loss: 0.8421704768993791\n",
      "Iteration 466 training loss: 0.6472243478041525, test loss: 0.8503482542082323\n",
      "Iteration 467 training loss: 0.6499839836700582, test loss: 0.8418043880481748\n",
      "Iteration 468 training loss: 0.646477607064145, test loss: 0.8499682253082562\n",
      "Iteration 469 training loss: 0.6492215037586186, test loss: 0.8414388979521954\n",
      "Iteration 470 training loss: 0.6457325284535457, test loss: 0.8495889092108514\n",
      "Iteration 471 training loss: 0.6484607161602622, test loss: 0.8410740032996236\n",
      "Iteration 472 training loss: 0.6449891006026849, test loss: 0.8492103003428828\n",
      "Iteration 473 training loss: 0.6477016098920554, test loss: 0.8407097008190005\n",
      "Iteration 474 training loss: 0.6442473122914071, test loss: 0.8488323932042655\n",
      "Iteration 475 training loss: 0.6469441741142931, test loss: 0.8403459872786877\n",
      "Iteration 476 training loss: 0.6435071524465674, test loss: 0.8484551823668783\n",
      "Iteration 477 training loss: 0.6461883981280995, test loss: 0.8399828594863401\n",
      "Iteration 478 training loss: 0.642768610139579, test loss: 0.8480786624734981\n",
      "Iteration 479 training loss: 0.6454342713730735, test loss: 0.8396203142883855\n",
      "Iteration 480 training loss: 0.6420316745840075, test loss: 0.8477028282367511\n",
      "Iteration 481 training loss: 0.6446817834249824, test loss: 0.8392583485695172\n",
      "Iteration 482 training loss: 0.6412963351332145, test loss: 0.8473276744380858\n",
      "Iteration 483 training loss: 0.6439309239934986, test loss: 0.8388969592521913\n",
      "Iteration 484 training loss: 0.6405625812780459, test loss: 0.8469531959267582\n",
      "Iteration 485 training loss: 0.6431816829199809, test loss: 0.8385361432961358\n",
      "Iteration 486 training loss: 0.6398304026445664, test loss: 0.8465793876188406\n",
      "Iteration 487 training loss: 0.6424340501752993, test loss: 0.8381758976978667\n",
      "Iteration 488 training loss: 0.6390997889918376, test loss: 0.8462062444962407\n",
      "Iteration 489 training loss: 0.6416880158577007, test loss: 0.8378162194902117\n",
      "Iteration 490 training loss: 0.6383707302097404, test loss: 0.8458337616057441\n",
      "Iteration 491 training loss: 0.6409435701907161, test loss: 0.8374571057418434\n",
      "Iteration 492 training loss: 0.6376432163168383, test loss: 0.8454619340580681\n",
      "Iteration 493 training loss: 0.6402007035211086, test loss: 0.8370985535568183\n",
      "Iteration 494 training loss: 0.6369172374582834, test loss: 0.845090757026934\n",
      "Iteration 495 training loss: 0.6394594063168593, test loss: 0.836740560074125\n",
      "Iteration 496 training loss: 0.6361927839037622, test loss: 0.844720225748155\n",
      "Iteration 497 training loss: 0.6387196691651928, test loss: 0.8363831224672393\n",
      "Iteration 498 training loss: 0.635469846045481, test loss: 0.8443503355187378\n",
      "Iteration 499 training loss: 0.6379814827706403, test loss: 0.8360262379436845\n",
      "Iteration 500 training loss: 0.6347484143961899, test loss: 0.8439810816960023\n",
      "Iteration 501 training loss: 0.6372448379531376, test loss: 0.8356699037446015\n",
      "Iteration 502 training loss: 0.6340284795872453, test loss: 0.8436124596967115\n",
      "Iteration 503 training loss: 0.636509725646162, test loss: 0.8353141171443242\n",
      "Iteration 504 training loss: 0.6333100323667084, test loss: 0.843244464996222\n",
      "Iteration 505 training loss: 0.6357761368949019, test loss: 0.8349588754499606\n",
      "Iteration 506 training loss: 0.6325930635974806, test loss: 0.8428770931276424\n",
      "Iteration 507 training loss: 0.6350440628544617, test loss: 0.8346041760009821\n",
      "Iteration 508 training loss: 0.6318775642554743, test loss: 0.8425103396810103\n",
      "Iteration 509 training loss: 0.6343134947881012, test loss: 0.8342500161688178\n",
      "Iteration 510 training loss: 0.6311635254278177, test loss: 0.8421442003024808\n",
      "Iteration 511 training loss: 0.6335844240655063, test loss: 0.8338963933564547\n",
      "Iteration 512 training loss: 0.6304509383110946, test loss: 0.8417786706935272\n",
      "Iteration 513 training loss: 0.6328568421610938, test loss: 0.8335433049980457\n",
      "Iteration 514 training loss: 0.6297397942096159, test loss: 0.8414137466101594\n",
      "Iteration 515 training loss: 0.6321307406523456, test loss: 0.8331907485585206\n",
      "Iteration 516 training loss: 0.6290300845337249, test loss: 0.8410494238621483\n",
      "Iteration 517 training loss: 0.6314061112181764, test loss: 0.8328387215332052\n",
      "Iteration 518 training loss: 0.6283218007981336, test loss: 0.8406856983122701\n",
      "Iteration 519 training loss: 0.6306829456373287, test loss: 0.8324872214474444\n",
      "Iteration 520 training loss: 0.6276149346202902, test loss: 0.8403225658755565\n",
      "Iteration 521 training loss: 0.6299612357868009, test loss: 0.832136245856231\n",
      "Iteration 522 training loss: 0.6269094777187764, test loss: 0.8399600225185614\n",
      "Iteration 523 training loss: 0.6292409736403005, test loss: 0.8317857923438409\n",
      "Iteration 524 training loss: 0.626205421911736, test loss: 0.8395980642586374\n",
      "Iteration 525 training loss: 0.6285221512667291, test loss: 0.8314358585234711\n",
      "Iteration 526 training loss: 0.6255027591153309, test loss: 0.8392366871632246\n",
      "Iteration 527 training loss: 0.6278047608286927, test loss: 0.8310864420368861\n",
      "Iteration 528 training loss: 0.6248014813422262, test loss: 0.838875887349151\n",
      "Iteration 529 training loss: 0.6270887945810412, test loss: 0.8307375405540656\n",
      "Iteration 530 training loss: 0.6241015807001039, test loss: 0.8385156609819425\n",
      "Iteration 531 training loss: 0.6263742448694325, test loss: 0.8303891517728595\n",
      "Iteration 532 training loss: 0.6234030493902022, test loss: 0.8381560042751468\n",
      "Iteration 533 training loss: 0.6256611041289251, test loss: 0.830041273418647\n",
      "Iteration 534 training loss: 0.622705879705883, test loss: 0.8377969134896646\n",
      "Iteration 535 training loss: 0.6249493648825931, test loss: 0.8296939032439998\n",
      "Iteration 536 training loss: 0.6220100640312258, test loss: 0.8374383849330949\n",
      "Iteration 537 training loss: 0.6242390197401702, test loss: 0.8293470390283507\n",
      "Iteration 538 training loss: 0.6213155948396453, test loss: 0.8370804149590874\n",
      "Iteration 539 training loss: 0.6235300613967136, test loss: 0.829000678577666\n",
      "Iteration 540 training loss: 0.6206224646925351, test loss: 0.8367229999667074\n",
      "Iteration 541 training loss: 0.6228224826312964, test loss: 0.8286548197241225\n",
      "Iteration 542 training loss: 0.6199306662379368, test loss: 0.8363661363998095\n",
      "Iteration 543 training loss: 0.6221162763057205, test loss: 0.828309460325789\n",
      "Iteration 544 training loss: 0.6192401922092325, test loss: 0.8360098207464213\n",
      "Iteration 545 training loss: 0.621411435363254, test loss: 0.8279645982663109\n",
      "Iteration 546 training loss: 0.6185510354238594, test loss: 0.8356540495381374\n",
      "Iteration 547 training loss: 0.6207079528273898, test loss: 0.8276202314546011\n",
      "Iteration 548 training loss: 0.61786318878205, test loss: 0.8352988193495218\n",
      "Iteration 549 training loss: 0.6200058218006279, test loss: 0.827276357824533\n",
      "Iteration 550 training loss: 0.617176645265593, test loss: 0.8349441267975191\n",
      "Iteration 551 training loss: 0.619305035463279, test loss: 0.8269329753346379\n",
      "Iteration 552 training loss: 0.6164913979366163, test loss: 0.8345899685408777\n",
      "Iteration 553 training loss: 0.6186055870722876, test loss: 0.8265900819678074\n",
      "Iteration 554 training loss: 0.6158074399363935, test loss: 0.8342363412795782\n",
      "Iteration 555 training loss: 0.6179074699600796, test loss: 0.8262476757309977\n",
      "Iteration 556 training loss: 0.6151247644841691, test loss: 0.8338832417542733\n",
      "Iteration 557 training loss: 0.6172106775334255, test loss: 0.8259057546549395\n",
      "Iteration 558 training loss: 0.6144433648760056, test loss: 0.8335306667457338\n",
      "Iteration 559 training loss: 0.6165152032723279, test loss: 0.8255643167938511\n",
      "Iteration 560 training loss: 0.6137632344836526, test loss: 0.8331786130743065\n",
      "Iteration 561 training loss: 0.6158210407289262, test loss: 0.8252233602251543\n",
      "Iteration 562 training loss: 0.6130843667534323, test loss: 0.8328270775993766\n",
      "Iteration 563 training loss: 0.6151281835264212, test loss: 0.8248828830491951\n",
      "Iteration 564 training loss: 0.6124067552051475, test loss: 0.8324760572188415\n",
      "Iteration 565 training loss: 0.6144366253580186, test loss: 0.8245428833889675\n",
      "Iteration 566 training loss: 0.6117303934310078, test loss: 0.83212554886859\n",
      "Iteration 567 training loss: 0.6137463599858904, test loss: 0.8242033593898407\n",
      "Iteration 568 training loss: 0.6110552750945738, test loss: 0.831775549521991\n",
      "Iteration 569 training loss: 0.613057381240156, test loss: 0.8238643092192889\n",
      "Iteration 570 training loss: 0.6103813939297208, test loss: 0.8314260561893895\n",
      "Iteration 571 training loss: 0.6123696830178782, test loss: 0.8235257310666273\n",
      "Iteration 572 training loss: 0.6097087437396203, test loss: 0.8310770659176098\n",
      "Iteration 573 training loss: 0.6116832592820802, test loss: 0.8231876231427481\n",
      "Iteration 574 training loss: 0.6090373183957386, test loss: 0.8307285757894676\n",
      "Iteration 575 training loss: 0.6109981040607764, test loss: 0.8228499836798616\n",
      "Iteration 576 training loss: 0.6083671118368532, test loss: 0.8303805829232862\n",
      "Iteration 577 training loss: 0.6103142114460218, test loss: 0.8225128109312404\n",
      "Iteration 578 training loss: 0.6076981180680859, test loss: 0.8300330844724243\n",
      "Iteration 579 training loss: 0.6096315755929784, test loss: 0.8221761031709667\n",
      "Iteration 580 training loss: 0.607030331159953, test loss: 0.8296860776248077\n",
      "Iteration 581 training loss: 0.6089501907189947, test loss: 0.8218398586936827\n",
      "Iteration 582 training loss: 0.606363745247431, test loss: 0.8293395596024686\n",
      "Iteration 583 training loss: 0.6082700511027052, test loss: 0.8215040758143443\n",
      "Iteration 584 training loss: 0.6056983545290393, test loss: 0.8289935276610925\n",
      "Iteration 585 training loss: 0.6075911510831418, test loss: 0.8211687528679779\n",
      "Iteration 586 training loss: 0.605034153265938, test loss: 0.8286479790895713\n",
      "Iteration 587 training loss: 0.6069134850588624, test loss: 0.8208338882094403\n",
      "Iteration 588 training loss: 0.6043711357810412, test loss: 0.8283029112095643\n",
      "Iteration 589 training loss: 0.6062370474870944, test loss: 0.820499480213181\n",
      "Iteration 590 training loss: 0.6037092964581449, test loss: 0.8279583213750623\n",
      "Iteration 591 training loss: 0.6055618328828912, test loss: 0.8201655272730083\n",
      "Iteration 592 training loss: 0.6030486297410711, test loss: 0.827614206971962\n",
      "Iteration 593 training loss: 0.6048878358183057, test loss: 0.8198320278018593\n",
      "Iteration 594 training loss: 0.6023891301328244, test loss: 0.8272705654176454\n",
      "Iteration 595 training loss: 0.6042150509215751, test loss: 0.8194989802315698\n",
      "Iteration 596 training loss: 0.6017307921947654, test loss: 0.8269273941605647\n",
      "Iteration 597 training loss: 0.6035434728763226, test loss: 0.8191663830126502\n",
      "Iteration 598 training loss: 0.6010736105457953, test loss: 0.8265846906798332\n",
      "Iteration 599 training loss: 0.602873096420769, test loss: 0.8188342346140633\n",
      "Iteration 600 training loss: 0.6004175798615566, test loss: 0.8262424524848242\n",
      "Iteration 601 training loss: 0.6022039163469618, test loss: 0.8185025335230036\n",
      "Iteration 602 training loss: 0.5997626948736454, test loss: 0.8259006771147733\n",
      "Iteration 603 training loss: 0.6015359275000143, test loss: 0.818171278244682\n",
      "Iteration 604 training loss: 0.5991089503688385, test loss: 0.8255593621383885\n",
      "Iteration 605 training loss: 0.6008691247773582, test loss: 0.8178404673021109\n",
      "Iteration 606 training loss: 0.5984563411883321, test loss: 0.8252185051534648\n",
      "Iteration 607 training loss: 0.6002035031280104, test loss: 0.8175100992358936\n",
      "Iteration 608 training loss: 0.5978048622269947, test loss: 0.8248781037865057\n",
      "Iteration 609 training loss: 0.5995390575518507, test loss: 0.8171801726040158\n",
      "Iteration 610 training loss: 0.5971545084326306, test loss: 0.8245381556923485\n",
      "Iteration 611 training loss: 0.5988757830989111, test loss: 0.81685068598164\n",
      "Iteration 612 training loss: 0.596505274805258, test loss: 0.824198658553797\n",
      "Iteration 613 training loss: 0.5982136748686802, test loss: 0.816521637960903\n",
      "Iteration 614 training loss: 0.5958571563963961, test loss: 0.823859610081259\n",
      "Iteration 615 training loss: 0.5975527280094145, test loss: 0.8161930271507152\n",
      "Iteration 616 training loss: 0.5952101483083683, test loss: 0.8235210080123881\n",
      "Iteration 617 training loss: 0.5968929377174668, test loss: 0.8158648521765629\n",
      "Iteration 618 training loss: 0.5945642456936121, test loss: 0.8231828501117322\n",
      "Iteration 619 training loss: 0.5962342992366211, test loss: 0.8155371116803147\n",
      "Iteration 620 training loss: 0.5939194437540044, test loss: 0.8228451341703866\n",
      "Iteration 621 training loss: 0.5955768078574419, test loss: 0.8152098043200285\n",
      "Iteration 622 training loss: 0.5932757377401957, test loss: 0.8225078580056527\n",
      "Iteration 623 training loss: 0.5949204589166325, test loss: 0.8148829287697609\n",
      "Iteration 624 training loss: 0.5926331229509565, test loss: 0.8221710194607001\n",
      "Iteration 625 training loss: 0.5942652477964062, test loss: 0.8145564837193827\n",
      "Iteration 626 training loss: 0.5919915947325342, test loss: 0.8218346164042365\n",
      "Iteration 627 training loss: 0.5936111699238649, test loss: 0.814230467874392\n",
      "Iteration 628 training loss: 0.5913511484780196, test loss: 0.8214986467301795\n",
      "Iteration 629 training loss: 0.5929582207703913, test loss: 0.8139048799557345\n",
      "Iteration 630 training loss: 0.590711779626726, test loss: 0.8211631083573357\n",
      "Iteration 631 training loss: 0.5923063958510505, test loss: 0.8135797186996223\n",
      "Iteration 632 training loss: 0.5900734836635764, test loss: 0.8208279992290837\n",
      "Iteration 633 training loss: 0.5916556907239992, test loss: 0.8132549828573592\n",
      "Iteration 634 training loss: 0.5894362561185021, test loss: 0.8204933173130605\n",
      "Iteration 635 training loss: 0.5910061009899089, test loss: 0.8129306711951648\n",
      "Iteration 636 training loss: 0.588800092565851, test loss: 0.8201590606008554\n",
      "Iteration 637 training loss: 0.5903576222913961, test loss: 0.8126067824940043\n",
      "Iteration 638 training loss: 0.588164988623805, test loss: 0.8198252271077057\n",
      "Iteration 639 training loss: 0.5897102503124616, test loss: 0.8122833155494183\n",
      "Iteration 640 training loss: 0.587530939953808, test loss: 0.8194918148721984\n",
      "Iteration 641 training loss: 0.5890639807779418, test loss: 0.8119602691713564\n",
      "Iteration 642 training loss: 0.5868979422600019, test loss: 0.8191588219559769\n",
      "Iteration 643 training loss: 0.5884188094529673, test loss: 0.811637642184014\n",
      "Iteration 644 training loss: 0.5862659912886741, test loss: 0.8188262464434507\n",
      "Iteration 645 training loss: 0.587774732142431, test loss: 0.8113154334256691\n",
      "Iteration 646 training loss: 0.5856350828277119, test loss: 0.8184940864415108\n",
      "Iteration 647 training loss: 0.5871317446904647, test loss: 0.8109936417485242\n",
      "Iteration 648 training loss: 0.5850052127060663, test loss: 0.8181623400792475\n",
      "Iteration 649 training loss: 0.586489842979926, test loss: 0.8106722660185492\n",
      "Iteration 650 training loss: 0.584376376793226, test loss: 0.8178310055076756\n",
      "Iteration 651 training loss: 0.5858490229318926, test loss: 0.8103513051153268\n",
      "Iteration 652 training loss: 0.5837485709986988, test loss: 0.8175000808994608\n",
      "Iteration 653 training loss: 0.585209280505165, test loss: 0.8100307579319013\n",
      "Iteration 654 training loss: 0.5831217912715017, test loss: 0.8171695644486523\n",
      "Iteration 655 training loss: 0.5845706116957791, test loss: 0.8097106233746284\n",
      "Iteration 656 training loss: 0.5824960335996594, test loss: 0.8168394543704184\n",
      "Iteration 657 training loss: 0.5839330125365247, test loss: 0.8093909003630287\n",
      "Iteration 658 training loss: 0.581871294009712, test loss: 0.8165097489007865\n",
      "Iteration 659 training loss: 0.5832964790964749, test loss: 0.8090715878296422\n",
      "Iteration 660 training loss: 0.581247568566229, test loss: 0.8161804462963874\n",
      "Iteration 661 training loss: 0.5826610074805209, test loss: 0.8087526847198877\n",
      "Iteration 662 training loss: 0.5806248533713332, test loss: 0.8158515448342037\n",
      "Iteration 663 training loss: 0.5820265938289165, test loss: 0.8084341899919213\n",
      "Iteration 664 training loss: 0.5800031445642311, test loss: 0.8155230428113222\n",
      "Iteration 665 training loss: 0.5813932343168301, test loss: 0.8081161026165001\n",
      "Iteration 666 training loss: 0.5793824383207525, test loss: 0.8151949385446882\n",
      "Iteration 667 training loss: 0.5807609251539032, test loss: 0.8077984215768462\n",
      "Iteration 668 training loss: 0.578762730852896, test loss: 0.8148672303708676\n",
      "Iteration 669 training loss: 0.5801296625838179, test loss: 0.8074811458685149\n",
      "Iteration 670 training loss: 0.5781440184083831, test loss: 0.8145399166458102\n",
      "Iteration 671 training loss: 0.5794994428838713, test loss: 0.8071642744992639\n",
      "Iteration 672 training loss: 0.5775262972702199, test loss: 0.8142129957446158\n",
      "Iteration 673 training loss: 0.5788702623645564, test loss: 0.8068478064889254\n",
      "Iteration 674 training loss: 0.576909563756265, test loss: 0.8138864660613081\n",
      "Iteration 675 training loss: 0.5782421173691515, test loss: 0.8065317408692809\n",
      "Iteration 676 training loss: 0.5762938142188069, test loss: 0.8135603260086067\n",
      "Iteration 677 training loss: 0.5776150042733166, test loss: 0.8062160766839378\n",
      "Iteration 678 training loss: 0.5756790450441449, test loss: 0.8132345740177096\n",
      "Iteration 679 training loss: 0.5769889194846951, test loss: 0.8059008129882077\n",
      "Iteration 680 training loss: 0.5750652526521808, test loss: 0.8129092085380724\n",
      "Iteration 681 training loss: 0.5763638594425247, test loss: 0.8055859488489899\n",
      "Iteration 682 training loss: 0.5744524334960147, test loss: 0.8125842280371961\n",
      "Iteration 683 training loss: 0.5757398206172543, test loss: 0.8052714833446536\n",
      "Iteration 684 training loss: 0.5738405840615495, test loss: 0.8122596310004178\n",
      "Iteration 685 training loss: 0.5751167995101661, test loss: 0.8049574155649243\n",
      "Iteration 686 training loss: 0.5732297008671, test loss: 0.8119354159307008\n",
      "Iteration 687 training loss: 0.574494792653007, test loss: 0.8046437446107734\n",
      "Iteration 688 training loss: 0.5726197804630118, test loss: 0.811611581348435\n",
      "Iteration 689 training loss: 0.5738737966076245, test loss: 0.8043304695943082\n",
      "Iteration 690 training loss: 0.5720108194312831, test loss: 0.8112881257912361\n",
      "Iteration 691 training loss: 0.5732538079656099, test loss: 0.8040175896386654\n",
      "Iteration 692 training loss: 0.5714028143851952, test loss: 0.8109650478137486\n",
      "Iteration 693 training loss: 0.5726348233479475, test loss: 0.8037051038779073\n",
      "Iteration 694 training loss: 0.5707957619689493, test loss: 0.8106423459874539\n",
      "Iteration 695 training loss: 0.5720168394046704, test loss: 0.8033930114569188\n",
      "Iteration 696 training loss: 0.5701896588573087, test loss: 0.810320018900482\n",
      "Iteration 697 training loss: 0.5713998528145229, test loss: 0.8030813115313089\n",
      "Iteration 698 training loss: 0.5695845017552472, test loss: 0.809998065157424\n",
      "Iteration 699 training loss: 0.5707838602846267, test loss: 0.8027700032673124\n",
      "Iteration 700 training loss: 0.5689802873976049, test loss: 0.8096764833791513\n",
      "Iteration 701 training loss: 0.5701688585501573, test loss: 0.8024590858416958\n",
      "Iteration 702 training loss: 0.5683770125487482, test loss: 0.8093552722026364\n",
      "Iteration 703 training loss: 0.5695548443740223, test loss: 0.8021485584416637\n",
      "Iteration 704 training loss: 0.5677746740022375, test loss: 0.8090344302807763\n",
      "Iteration 705 training loss: 0.5689418145465478, test loss: 0.8018384202647705\n",
      "Iteration 706 training loss: 0.5671732685804992, test loss: 0.8087139562822214\n",
      "Iteration 707 training loss: 0.5683297658851701, test loss: 0.8015286705188299\n",
      "Iteration 708 training loss: 0.5665727931345054, test loss: 0.8083938488912061\n",
      "Iteration 709 training loss: 0.5677186952341333, test loss: 0.8012193084218321\n",
      "Iteration 710 training loss: 0.5659732445434567, test loss: 0.8080741068073835\n",
      "Iteration 711 training loss: 0.5671085994641918, test loss: 0.8009103332018586\n",
      "Iteration 712 training loss: 0.5653746197144737, test loss: 0.8077547287456623\n",
      "Iteration 713 training loss: 0.5664994754723199, test loss: 0.8006017440970032\n",
      "Iteration 714 training loss: 0.5647769155822916, test loss: 0.8074357134360489\n",
      "Iteration 715 training loss: 0.5658913201814251, test loss: 0.8002935403552918\n",
      "Iteration 716 training loss: 0.5641801291089621, test loss: 0.8071170596234899\n",
      "Iteration 717 training loss: 0.5652841305400692, test loss: 0.7999857212346086\n",
      "Iteration 718 training loss: 0.56358425728356, test loss: 0.8067987660677215\n",
      "Iteration 719 training loss: 0.5646779035221918, test loss: 0.7996782860026209\n",
      "Iteration 720 training loss: 0.5629892971218948, test loss: 0.8064808315431186\n",
      "Iteration 721 training loss: 0.5640726361268426, test loss: 0.799371233936709\n",
      "Iteration 722 training loss: 0.562395245666229, test loss: 0.8061632548385497\n",
      "Iteration 723 training loss: 0.5634683253779157, test loss: 0.7990645643238963\n",
      "Iteration 724 training loss: 0.5618020999850001, test loss: 0.8058460347572328\n",
      "Iteration 725 training loss: 0.5628649683238923, test loss: 0.7987582764607845\n",
      "Iteration 726 training loss: 0.5612098571725491, test loss: 0.805529170116597\n",
      "Iteration 727 training loss: 0.5622625620375858, test loss: 0.7984523696534888\n",
      "Iteration 728 training loss: 0.5606185143488547, test loss: 0.8052126597481448\n",
      "Iteration 729 training loss: 0.5616611036158942, test loss: 0.7981468432175763\n",
      "Iteration 730 training loss: 0.5600280686592698, test loss: 0.8048965024973191\n",
      "Iteration 731 training loss: 0.5610605901795558, test loss: 0.7978416964780082\n",
      "Iteration 732 training loss: 0.559438517274267, test loss: 0.804580697223373\n",
      "Iteration 733 training loss: 0.5604610188729117, test loss: 0.7975369287690812\n",
      "Iteration 734 training loss: 0.5588498573891861, test loss: 0.8042652427992428\n",
      "Iteration 735 training loss: 0.559862386863672, test loss: 0.797232539434376\n",
      "Iteration 736 training loss: 0.5582620862239887, test loss: 0.8039501381114237\n",
      "Iteration 737 training loss: 0.5592646913426876, test loss: 0.7969285278267032\n",
      "Iteration 738 training loss: 0.5576752010230159, test loss: 0.8036353820598492\n",
      "Iteration 739 training loss: 0.5586679295237271, test loss: 0.7966248933080563\n",
      "Iteration 740 training loss: 0.5570891990547527, test loss: 0.8033209735577737\n",
      "Iteration 741 training loss: 0.5580720986432575, test loss: 0.7963216352495641\n",
      "Iteration 742 training loss: 0.5565040776115963, test loss: 0.803006911531657\n",
      "Iteration 743 training loss: 0.5574771959602309, test loss: 0.7960187530314463\n",
      "Iteration 744 training loss: 0.5559198340096291, test loss: 0.8026931949210544\n",
      "Iteration 745 training loss: 0.5568832187558757, test loss: 0.7957162460429729\n",
      "Iteration 746 training loss: 0.5553364655883977, test loss: 0.8023798226785072\n",
      "Iteration 747 training loss: 0.5562901643334923, test loss: 0.7954141136824243\n",
      "Iteration 748 training loss: 0.5547539697106952, test loss: 0.8020667937694372\n",
      "Iteration 749 training loss: 0.5556980300182537, test loss: 0.7951123553570559\n",
      "Iteration 750 training loss: 0.5541723437623493, test loss: 0.8017541071720468\n",
      "Iteration 751 training loss: 0.5551068131570106, test loss: 0.7948109704830636\n",
      "Iteration 752 training loss: 0.5535915851520146, test loss: 0.8014417618772179\n",
      "Iteration 753 training loss: 0.5545165111181017, test loss: 0.7945099584855524\n",
      "Iteration 754 training loss: 0.5530116913109702, test loss: 0.801129756888417\n",
      "Iteration 755 training loss: 0.5539271212911678, test loss: 0.7942093187985076\n",
      "Iteration 756 training loss: 0.5524326596929205, test loss: 0.800818091221603\n",
      "Iteration 757 training loss: 0.5533386410869717, test loss: 0.7939090508647708\n",
      "Iteration 758 training loss: 0.5518544877738033, test loss: 0.8005067639051368\n",
      "Iteration 759 training loss: 0.5527510679372211, test loss: 0.7936091541360132\n",
      "Iteration 760 training loss: 0.5512771730515996, test loss: 0.8001957739796955\n",
      "Iteration 761 training loss: 0.5521643992943973, test loss: 0.7933096280727179\n",
      "Iteration 762 training loss: 0.5507007130461494, test loss: 0.7998851204981899\n",
      "Iteration 763 training loss: 0.551578632631588, test loss: 0.7930104721441604\n",
      "Iteration 764 training loss: 0.5501251052989725, test loss: 0.7995748025256834\n",
      "Iteration 765 training loss: 0.5509937654423254, test loss: 0.792711685828394\n",
      "Iteration 766 training loss: 0.5495503473730933, test loss: 0.7992648191393152\n",
      "Iteration 767 training loss: 0.5504097952404264, test loss: 0.7924132686122368\n",
      "Iteration 768 training loss: 0.5489764368528693, test loss: 0.7989551694282281\n",
      "Iteration 769 training loss: 0.5498267195598403, test loss: 0.7921152199912624\n",
      "Iteration 770 training loss: 0.5484033713438261, test loss: 0.7986458524934962\n",
      "Iteration 771 training loss: 0.549244535954499, test loss: 0.7918175394697927\n",
      "Iteration 772 training loss: 0.5478311484724947, test loss: 0.7983368674480589\n",
      "Iteration 773 training loss: 0.548663241998172, test loss: 0.7915202265608936\n",
      "Iteration 774 training loss: 0.5472597658862548, test loss: 0.7980282134166564\n",
      "Iteration 775 training loss: 0.5480828352843268, test loss: 0.791223280786375\n",
      "Iteration 776 training loss: 0.5466892212531822, test loss: 0.7977198895357691\n",
      "Iteration 777 training loss: 0.5475033134259919, test loss: 0.7909267016767899\n",
      "Iteration 778 training loss: 0.5461195122619003, test loss: 0.7974118949535602\n",
      "Iteration 779 training loss: 0.546924674055626, test loss: 0.7906304887714417\n",
      "Iteration 780 training loss: 0.5455506366214363, test loss: 0.7971042288298207\n",
      "Iteration 781 training loss: 0.5463469148249902, test loss: 0.7903346416183896\n",
      "Iteration 782 training loss: 0.5449825920610818, test loss: 0.7967968903359198\n",
      "Iteration 783 training loss: 0.5457700334050261, test loss: 0.7900391597744604\n",
      "Iteration 784 training loss: 0.5444153763302583, test loss: 0.7964898786547562\n",
      "Iteration 785 training loss: 0.5451940274857361, test loss: 0.7897440428052602\n",
      "Iteration 786 training loss: 0.5438489871983859, test loss: 0.7961831929807137\n",
      "Iteration 787 training loss: 0.5446188947760701, test loss: 0.7894492902851928\n",
      "Iteration 788 training loss: 0.5432834224547581, test loss: 0.7958768325196214\n",
      "Iteration 789 training loss: 0.5440446330038162, test loss: 0.7891549017974786\n",
      "Iteration 790 training loss: 0.5427186799084195, test loss: 0.7955707964887145\n",
      "Iteration 791 training loss: 0.5434712399154944, test loss: 0.7888608769341762\n",
      "Iteration 792 training loss: 0.5421547573880487, test loss: 0.795265084116602\n",
      "Iteration 793 training loss: 0.5428987132762563, test loss: 0.7885672152962089\n",
      "Iteration 794 training loss: 0.5415916527418462, test loss: 0.7949596946432339\n",
      "Iteration 795 training loss: 0.5423270508697885, test loss: 0.7882739164933942\n",
      "Iteration 796 training loss: 0.5410293638374248, test loss: 0.7946546273198761\n",
      "Iteration 797 training loss: 0.5417562504982202, test loss: 0.7879809801444752\n",
      "Iteration 798 training loss: 0.5404678885617069, test loss: 0.7943498814090848\n",
      "Iteration 799 training loss: 0.5411863099820357, test loss: 0.7876884058771564\n",
      "Iteration 800 training loss: 0.5399072248208245, test loss: 0.7940454561846874\n",
      "Iteration 801 training loss: 0.5406172271599904, test loss: 0.7873961933281421\n",
      "Iteration 802 training loss: 0.5393473705400246, test loss: 0.7937413509317651\n",
      "Iteration 803 training loss: 0.5400489998890329, test loss: 0.7871043421431798\n",
      "Iteration 804 training loss: 0.5387883236635778, test loss: 0.7934375649466412\n",
      "Iteration 805 training loss: 0.5394816260442292, test loss: 0.7868128519771034\n",
      "Iteration 806 training loss: 0.5382300821546935, test loss: 0.7931340975368693\n",
      "Iteration 807 training loss: 0.5389151035186934, test loss: 0.7865217224938843\n",
      "Iteration 808 training loss: 0.5376726439954376, test loss: 0.7928309480212296\n",
      "Iteration 809 training loss: 0.5383494302235218, test loss: 0.7862309533666831\n",
      "Iteration 810 training loss: 0.537116007186656, test loss: 0.7925281157297257\n",
      "Iteration 811 training loss: 0.5377846040877315, test loss: 0.7859405442779058\n",
      "Iteration 812 training loss: 0.5365601697479021, test loss: 0.7922256000035879\n",
      "Iteration 813 training loss: 0.5372206230582045, test loss: 0.7856504949192634\n",
      "Iteration 814 training loss: 0.5360051297173685, test loss: 0.7919234001952762\n",
      "Iteration 815 training loss: 0.5366574850996345, test loss: 0.7853608049918351\n",
      "Iteration 816 training loss: 0.5354508851518247, test loss: 0.7916215156684917\n",
      "Iteration 817 training loss: 0.5360951881944799, test loss: 0.7850714742061359\n",
      "Iteration 818 training loss: 0.5348974341265575, test loss: 0.7913199457981891\n",
      "Iteration 819 training loss: 0.53553373034292, test loss: 0.7847825022821864\n",
      "Iteration 820 training loss: 0.5343447747353177, test loss: 0.7910186899705943\n",
      "Iteration 821 training loss: 0.534973109562818, test loss: 0.7844938889495892\n",
      "Iteration 822 training loss: 0.5337929050902706, test loss: 0.790717747583225\n",
      "Iteration 823 training loss: 0.5344133238896859, test loss: 0.7842056339476059\n",
      "Iteration 824 training loss: 0.5332418233219514, test loss: 0.7904171180449164\n",
      "Iteration 825 training loss: 0.5338543713766557, test loss: 0.7839177370252411\n",
      "Iteration 826 training loss: 0.5326915275792258, test loss: 0.7901168007758509\n",
      "Iteration 827 training loss: 0.5332962500944556, test loss: 0.783630197941328\n",
      "Iteration 828 training loss: 0.5321420160292544, test loss: 0.7898167952075911\n",
      "Iteration 829 training loss: 0.5327389581313889, test loss: 0.7833430164646197\n",
      "Iteration 830 training loss: 0.531593286857463, test loss: 0.7895171007831182\n",
      "Iteration 831 training loss: 0.5321824935933206, test loss: 0.7830561923738845\n",
      "Iteration 832 training loss: 0.5310453382675173, test loss: 0.7892177169568736\n",
      "Iteration 833 training loss: 0.5316268546036668, test loss: 0.7827697254580043\n",
      "Iteration 834 training loss: 0.5304981684813023, test loss: 0.7889186431948056\n",
      "Iteration 835 training loss: 0.5310720393033889, test loss: 0.7824836155160794\n",
      "Iteration 836 training loss: 0.5299517757389071, test loss: 0.7886198789744211\n",
      "Iteration 837 training loss: 0.5305180458509946, test loss: 0.7821978623575357\n",
      "Iteration 838 training loss: 0.5294061582986146, test loss: 0.7883214237848399\n",
      "Iteration 839 training loss: 0.5299648724225414, test loss: 0.7819124658022382\n",
      "Iteration 840 training loss: 0.5288613144368962, test loss: 0.7880232771268557\n",
      "Iteration 841 training loss: 0.5294125172116478, test loss: 0.781627425680607\n",
      "Iteration 842 training loss: 0.5283172424484119, test loss: 0.7877254385130006\n",
      "Iteration 843 training loss: 0.5288609784295065, test loss: 0.7813427418337398\n",
      "Iteration 844 training loss: 0.5277739406460146, test loss: 0.787427907467614\n",
      "Iteration 845 training loss: 0.528310254304906, test loss: 0.7810584141135393\n",
      "Iteration 846 training loss: 0.5272314073607618, test loss: 0.7871306835269188\n",
      "Iteration 847 training loss: 0.5277603430842551, test loss: 0.7807744423828442\n",
      "Iteration 848 training loss: 0.5266896409419303, test loss: 0.7868337662390987\n",
      "Iteration 849 training loss: 0.5272112430316137, test loss: 0.780490826515565\n",
      "Iteration 850 training loss: 0.5261486397570371, test loss: 0.7865371551643833\n",
      "Iteration 851 training loss: 0.5266629524287287, test loss: 0.7802075663968272\n",
      "Iteration 852 training loss: 0.5256084021918672, test loss: 0.7862408498751382\n",
      "Iteration 853 training loss: 0.5261154695750754, test loss: 0.7799246619231176\n",
      "Iteration 854 training loss: 0.5250689266505045, test loss: 0.7859448499559587\n",
      "Iteration 855 training loss: 0.5255687927879049, test loss: 0.7796421130024357\n",
      "Iteration 856 training loss: 0.5245302115553702, test loss: 0.7856491550037706\n",
      "Iteration 857 training loss: 0.5250229204022961, test loss: 0.7793599195544528\n",
      "Iteration 858 training loss: 0.5239922553472661, test loss: 0.7853537646279355\n",
      "Iteration 859 training loss: 0.5244778507712139, test loss: 0.7790780815106735\n",
      "Iteration 860 training loss: 0.5234550564854239, test loss: 0.7850586784503611\n",
      "Iteration 861 training loss: 0.5239335822655736, test loss: 0.7787965988146062\n",
      "Iteration 862 training loss: 0.5229186134475606, test loss: 0.7847638961056189\n",
      "Iteration 863 training loss: 0.5233901132743111, test loss: 0.778515471421936\n",
      "Iteration 864 training loss: 0.5223829247299392, test loss: 0.7844694172410661\n",
      "Iteration 865 training loss: 0.5228474422044573, test loss: 0.7782346993007069\n",
      "Iteration 866 training loss: 0.5218479888474364, test loss: 0.7841752415169736\n",
      "Iteration 867 training loss: 0.5223055674812224, test loss: 0.7779542824315069\n",
      "Iteration 868 training loss: 0.5213138043336162, test loss: 0.7838813686066614\n",
      "Iteration 869 training loss: 0.5217644875480814, test loss: 0.777674220807662\n",
      "Iteration 870 training loss: 0.520780369740809, test loss: 0.7835877981966377\n",
      "Iteration 871 training loss: 0.5212242008668708, test loss: 0.7773945144354344\n",
      "Iteration 872 training loss: 0.5202476836401986, test loss: 0.7832945299867472\n",
      "Iteration 873 training loss: 0.520684705917888, test loss: 0.7771151633342281\n",
      "Iteration 874 training loss: 0.5197157446219139, test loss: 0.7830015636903233\n",
      "Iteration 875 training loss: 0.5201460011999992, test loss: 0.776836167536802\n",
      "Iteration 876 training loss: 0.5191845512951299, test loss: 0.7827088990343484\n",
      "Iteration 877 training loss: 0.5196080852307533, test loss: 0.7765575270894876\n",
      "Iteration 878 training loss: 0.518654102288172, test loss: 0.7824165357596199\n",
      "Iteration 879 training loss: 0.5190709565465026, test loss: 0.7762792420524156\n",
      "Iteration 880 training loss: 0.5181243962486302, test loss: 0.7821244736209249\n",
      "Iteration 881 training loss: 0.5185346137025305, test loss: 0.7760013124997496\n",
      "Iteration 882 training loss: 0.5175954318434786, test loss: 0.7818327123872192\n",
      "Iteration 883 training loss: 0.5179990552731865, test loss: 0.7757237385199248\n",
      "Iteration 884 training loss: 0.5170672077592032, test loss: 0.781541251841816\n",
      "Iteration 885 training loss: 0.5174642798520271, test loss: 0.7754465202158973\n",
      "Iteration 886 training loss: 0.5165397227019366, test loss: 0.7812500917825813\n",
      "Iteration 887 training loss: 0.5169302860519669, test loss: 0.7751696577053989\n",
      "Iteration 888 training loss: 0.5160129753975992, test loss: 0.780959232022135\n",
      "Iteration 889 training loss: 0.5163970725054335, test loss: 0.7748931511212005\n",
      "Iteration 890 training loss: 0.5154869645920499, test loss: 0.7806686723880637\n",
      "Iteration 891 training loss: 0.5158646378645326, test loss: 0.7746170006113833\n",
      "Iteration 892 training loss: 0.5149616890512428, test loss: 0.780378412723137\n",
      "Iteration 893 training loss: 0.5153329808012207, test loss: 0.7743412063396182\n",
      "Iteration 894 training loss: 0.5144371475613937, test loss: 0.7800884528855359\n",
      "Iteration 895 training loss: 0.5148021000074847, test loss: 0.7740657684854552\n",
      "Iteration 896 training loss: 0.513913338929153, test loss: 0.7797987927490861\n",
      "Iteration 897 training loss: 0.5142719941955304, test loss: 0.7737906872446183\n",
      "Iteration 898 training loss: 0.513390261981788, test loss: 0.779509432203503\n",
      "Iteration 899 training loss: 0.5137426620979797, test loss: 0.7735159628293118\n",
      "Iteration 900 training loss: 0.5128679155673734, test loss: 0.7792203711546429\n",
      "Iteration 901 training loss: 0.5132141024680751, test loss: 0.7732415954685357\n",
      "Iteration 902 training loss: 0.5123462985549903, test loss: 0.7789316095247655\n",
      "Iteration 903 training loss: 0.5126863140798946, test loss: 0.7729675854084088\n",
      "Iteration 904 training loss: 0.5118254098349346, test loss: 0.778643147252803\n",
      "Iteration 905 training loss: 0.5121592957285747, test loss: 0.7726939329125037\n",
      "Iteration 906 training loss: 0.5113052483189349, test loss: 0.7783549842946426\n",
      "Iteration 907 training loss: 0.5116330462305414, test loss: 0.7724206382621885\n",
      "Iteration 908 training loss: 0.5107858129403773, test loss: 0.778067120623414\n",
      "Iteration 909 training loss: 0.5111075644237537, test loss: 0.7721477017569831\n",
      "Iteration 910 training loss: 0.510267102654544, test loss: 0.7777795562297908\n",
      "Iteration 911 training loss: 0.5105828491679536, test loss: 0.7718751237149206\n",
      "Iteration 912 training loss: 0.5097491164388578, test loss: 0.7774922911223002\n",
      "Iteration 913 training loss: 0.5100588993449275, test loss: 0.7716029044729248\n",
      "Iteration 914 training loss: 0.5092318532931382, test loss: 0.7772053253276446\n",
      "Iteration 915 training loss: 0.5095357138587775, test loss: 0.7713310443871939\n",
      "Iteration 916 training loss: 0.5087153122398688, test loss: 0.7769186588910326\n",
      "Iteration 917 training loss: 0.509013291636203, test loss: 0.771059543833599\n",
      "Iteration 918 training loss: 0.5081994923244734, test loss: 0.776632291876522\n",
      "Iteration 919 training loss: 0.5084916316267931, test loss: 0.7707884032080923\n",
      "Iteration 920 training loss: 0.5076843926156036, test loss: 0.7763462243673751\n",
      "Iteration 921 training loss: 0.5079707328033297, test loss: 0.7705176229271282\n",
      "Iteration 922 training loss: 0.5071700122054389, test loss: 0.7760604564664234\n",
      "Iteration 923 training loss: 0.5074505941621019, test loss: 0.7702472034280953\n",
      "Iteration 924 training loss: 0.5066563502099962, test loss: 0.7757749882964479\n",
      "Iteration 925 training loss: 0.506931214723232, test loss: 0.7699771451697629\n",
      "Iteration 926 training loss: 0.5061434057694516, test loss: 0.7754898200005682\n",
      "Iteration 927 training loss: 0.5064125935310126, test loss: 0.7697074486327391\n",
      "Iteration 928 training loss: 0.5056311780484752, test loss: 0.7752049517426483\n",
      "Iteration 929 training loss: 0.5058947296542577, test loss: 0.7694381143199424\n",
      "Iteration 930 training loss: 0.5051196662365767, test loss: 0.7749203837077108\n",
      "Iteration 931 training loss: 0.5053776221866635, test loss: 0.7691691427570876\n",
      "Iteration 932 training loss: 0.5046088695484652, test loss: 0.7746361161023703\n",
      "Iteration 933 training loss: 0.5048612702471845, test loss: 0.7689005344931845\n",
      "Iteration 934 training loss: 0.5040987872244209, test loss: 0.774352149155277\n",
      "Iteration 935 training loss: 0.5043456729804211, test loss: 0.7686322901010534\n",
      "Iteration 936 training loss: 0.5035894185306806, test loss: 0.7740684831175749\n",
      "Iteration 937 training loss: 0.5038308295570219, test loss: 0.7683644101778528\n",
      "Iteration 938 training loss: 0.5030807627598367, test loss: 0.7737851182633774\n",
      "Iteration 939 training loss: 0.5033167391740986, test loss: 0.7680968953456244\n",
      "Iteration 940 training loss: 0.5025728192312514, test loss: 0.7735020548902553\n",
      "Iteration 941 training loss: 0.5028034010556564, test loss: 0.7678297462518527\n",
      "Iteration 942 training loss: 0.5020655872914829, test loss: 0.773219293319741\n",
      "Iteration 943 training loss: 0.5022908144530379, test loss: 0.7675629635700421\n",
      "Iteration 944 training loss: 0.5015590663147298, test loss: 0.7729368338978523\n",
      "Iteration 945 training loss: 0.5017789786453837, test loss: 0.7672965480003086\n",
      "Iteration 946 training loss: 0.5010532557032881, test loss: 0.7726546769956281\n",
      "Iteration 947 training loss: 0.5012678929401061, test loss: 0.7670305002699923\n",
      "Iteration 948 training loss: 0.5005481548880258, test loss: 0.7723728230096832\n",
      "Iteration 949 training loss: 0.5007575566733816, test loss: 0.7667648211342829\n",
      "Iteration 950 training loss: 0.500043763328873, test loss: 0.7720912723627837\n",
      "Iteration 951 training loss: 0.5002479692106575, test loss: 0.7664995113768682\n",
      "Iteration 952 training loss: 0.4995400805153278, test loss: 0.7718100255044347\n",
      "Iteration 953 training loss: 0.4997391299471765, test loss: 0.7662345718105974\n",
      "Iteration 954 training loss: 0.4990371059669813, test loss: 0.7715290829114928\n",
      "Iteration 955 training loss: 0.49923103830851895, test loss: 0.765970003278167\n",
      "Iteration 956 training loss: 0.4985348392340597, test loss: 0.7712484450887942\n",
      "Iteration 957 training loss: 0.4987236937511617, test loss: 0.7657058066528236\n",
      "Iteration 958 training loss: 0.49803327989798274, test loss: 0.7709681125698046\n",
      "Iteration 959 training loss: 0.49821709576305734, test loss: 0.7654419828390902\n",
      "Iteration 960 training loss: 0.4975324275719441, test loss: 0.7706880859172889\n",
      "Iteration 961 training loss: 0.4977112438642304, test loss: 0.7651785327735116\n",
      "Iteration 962 training loss: 0.4970322819015081, test loss: 0.7704083657240017\n",
      "Iteration 963 training loss: 0.4972061376073942, test loss: 0.7649154574254219\n",
      "Iteration 964 training loss: 0.49653284256522834, test loss: 0.7701289526134023\n",
      "Iteration 965 training loss: 0.49670177657858844, test loss: 0.7646527577977354\n",
      "Iteration 966 training loss: 0.4960341092752861, test loss: 0.769849847240389\n",
      "Iteration 967 training loss: 0.49619816039783576, test loss: 0.7643904349277599\n",
      "Iteration 968 training loss: 0.4955360817781492, test loss: 0.7695710502920582\n",
      "Iteration 969 training loss: 0.49569528871982105, test loss: 0.7641284898880335\n",
      "Iteration 970 training loss: 0.49503875985525414, test loss: 0.7692925624884889\n",
      "Iteration 971 training loss: 0.495193161234593, test loss: 0.7638669237871868\n",
      "Iteration 972 training loss: 0.49454214332370877, test loss: 0.7690143845835491\n",
      "Iteration 973 training loss: 0.49469177766828654, test loss: 0.7636057377708295\n",
      "Iteration 974 training loss: 0.49404623203701953, test loss: 0.7687365173657303\n",
      "Iteration 975 training loss: 0.49419113778387097, test loss: 0.7633449330224644\n",
      "Iteration 976 training loss: 0.4935510258858422, test loss: 0.7684589616590075\n",
      "Iteration 977 training loss: 0.4936912413819204, test loss: 0.7630845107644273\n",
      "Iteration 978 training loss: 0.493056524798756, test loss: 0.768181718323727\n",
      "Iteration 979 training loss: 0.49319208830141004, test loss: 0.7628244722588542\n",
      "Iteration 980 training loss: 0.4925627287430643, test loss: 0.7679047882575202\n",
      "Iteration 981 training loss: 0.4926936784205374, test loss: 0.7625648188086781\n",
      "Iteration 982 training loss: 0.4920696377256213, test loss: 0.76762817239625\n",
      "Iteration 983 training loss: 0.4921960116575709, test loss: 0.7623055517586552\n",
      "Iteration 984 training loss: 0.49157725179368417, test loss: 0.7673518717149846\n",
      "Iteration 985 training loss: 0.4916990879717241, test loss: 0.7620466724964209\n",
      "Iteration 986 training loss: 0.4910855710357946, test loss: 0.7670758872290026\n",
      "Iteration 987 training loss: 0.4912029073640596, test loss: 0.7617881824535772\n",
      "Iteration 988 training loss: 0.49059459558268786, test loss: 0.7668002199948303\n",
      "Iteration 989 training loss: 0.49070746987842095, test loss: 0.7615300831068124\n",
      "Iteration 990 training loss: 0.4901043256082315, test loss: 0.7665248711113126\n",
      "Iteration 991 training loss: 0.4902127756023948, test loss: 0.7612723759790555\n",
      "Iteration 992 training loss: 0.48961476133039433, test loss: 0.7662498417207172\n",
      "Iteration 993 training loss: 0.4897188246683034, test loss: 0.7610150626406618\n",
      "Iteration 994 training loss: 0.4891259030122473, test loss: 0.7659751330098737\n",
      "Iteration 995 training loss: 0.48922561725422925, test loss: 0.7607581447106373\n",
      "Iteration 996 training loss: 0.4886377509629955, test loss: 0.7657007462113499\n",
      "Iteration 997 training loss: 0.4887331535850723, test loss: 0.7605016238578981\n",
      "Iteration 998 training loss: 0.48815030553904437, test loss: 0.7654266826046644\n",
      "Iteration 999 training loss: 0.488241433933641, test loss: 0.7602455018025661\n",
      "Iteration 1000 training loss: 0.4876635671451003, test loss: 0.7651529435175395\n",
      "Iteration 1001 training loss: 0.4877504586217782, test loss: 0.7599897803173068\n",
      "Iteration 1002 training loss: 0.4871775362353058, test loss: 0.7648795303271919\n",
      "Iteration 1003 training loss: 0.48726022802152263, test loss: 0.7597344612287037\n",
      "Iteration 1004 training loss: 0.48669221331441226, test loss: 0.7646064444616669\n",
      "Iteration 1005 training loss: 0.486770742556308, test loss: 0.759479546418677\n",
      "Iteration 1006 training loss: 0.4862075989389892, test loss: 0.764333687401213\n",
      "Iteration 1007 training loss: 0.48628200270219996, test loss: 0.7592250378259421\n",
      "Iteration 1008 training loss: 0.48572369371867424, test loss: 0.764061260679703\n",
      "Iteration 1009 training loss: 0.4857940089891728, test loss: 0.7589709374475149\n",
      "Iteration 1010 training loss: 0.4852404983174609, test loss: 0.7637891658860974\n",
      "Iteration 1011 training loss: 0.48530676200242606, test loss: 0.7587172473402599\n",
      "Iteration 1012 training loss: 0.48475801345502967, test loss: 0.7635174046659584\n",
      "Iteration 1013 training loss: 0.484820262383745, test loss: 0.7584639696224863\n",
      "Iteration 1014 training loss: 0.4842762399081208, test loss: 0.7632459787230079\n",
      "Iteration 1015 training loss: 0.4843345108329016, test loss: 0.7582111064755924\n",
      "Iteration 1016 training loss: 0.48379517851195186, test loss: 0.7629748898207411\n",
      "Iteration 1017 training loss: 0.4838495081091031, test loss: 0.7579586601457594\n",
      "Iteration 1018 training loss: 0.4833148301616795, test loss: 0.7627041397840857\n",
      "Iteration 1019 training loss: 0.48336525503248423, test loss: 0.7577066329456962\n",
      "Iteration 1020 training loss: 0.4828351958139096, test loss: 0.7624337305011197\n",
      "Iteration 1021 training loss: 0.4828817524856495, test loss: 0.7574550272564377\n",
      "Iteration 1022 training loss: 0.4823562764882555, test loss: 0.7621636639248409\n",
      "Iteration 1023 training loss: 0.4823990014152621, test loss: 0.7572038455291966\n",
      "Iteration 1024 training loss: 0.4818780732689451, test loss: 0.7618939420749953\n",
      "Iteration 1025 training loss: 0.48191700283368466, test loss: 0.7569530902872734\n",
      "Iteration 1026 training loss: 0.4814005873064803, test loss: 0.7616245670399623\n",
      "Iteration 1027 training loss: 0.48143575782067316, test loss: 0.7567027641280214\n",
      "Iteration 1028 training loss: 0.48092381981934973, test loss: 0.7613555409787032\n",
      "Iteration 1029 training loss: 0.4809552675251221, test loss: 0.7564528697248749\n",
      "Iteration 1030 training loss: 0.48044777209579637, test loss: 0.7610868661227709\n",
      "Iteration 1031 training loss: 0.4804755331668688, test loss: 0.7562034098294353\n",
      "Iteration 1032 training loss: 0.47997244549564055, test loss: 0.7608185447783828\n",
      "Iteration 1033 training loss: 0.47999655603855135, test loss: 0.7559543872736251\n",
      "Iteration 1034 training loss: 0.4794978414521628, test loss: 0.7605505793285647\n",
      "Iteration 1035 training loss: 0.4795183375075287, test loss: 0.7557058049719021\n",
      "Iteration 1036 training loss: 0.4790239614740459, test loss: 0.7602829722353591\n",
      "Iteration 1037 training loss: 0.4790408790178603, test loss: 0.7554576659235468\n",
      "Iteration 1038 training loss: 0.47855080714737935, test loss: 0.7600157260421087\n",
      "Iteration 1039 training loss: 0.47856418209234863, test loss: 0.7552099732150153\n",
      "Iteration 1040 training loss: 0.47807838013772813, test loss: 0.7597488433758096\n",
      "Iteration 1041 training loss: 0.478088248334647, test loss: 0.7549627300223646\n",
      "Iteration 1042 training loss: 0.47760668219226604, test loss: 0.759482326949545\n",
      "Iteration 1043 training loss: 0.47761307943143405, test loss: 0.7547159396137534\n",
      "Iteration 1044 training loss: 0.4771357151419795, test loss: 0.7592161795649927\n",
      "Iteration 1045 training loss: 0.4771386771546578, test loss: 0.754469605352018\n",
      "Iteration 1046 training loss: 0.4766654809039388, test loss: 0.7589504041150166\n",
      "Iteration 1047 training loss: 0.4766650433638496, test loss: 0.7542237306973263\n",
      "Iteration 1048 training loss: 0.4761959814836437, test loss: 0.7586850035863408\n",
      "Iteration 1049 training loss: 0.47619218000851227, test loss: 0.7539783192099124\n",
      "Iteration 1050 training loss: 0.47572721897744313, test loss: 0.75841998106231\n",
      "Iteration 1051 training loss: 0.47572008913058356, test loss: 0.7537333745528967\n",
      "Iteration 1052 training loss: 0.4752591955750306, test loss: 0.7581553397257377\n",
      "Iteration 1053 training loss: 0.4752487728669776, test loss: 0.7534889004951882\n",
      "Iteration 1054 training loss: 0.4747919135620201, test loss: 0.7578910828618479\n",
      "Iteration 1055 training loss: 0.47477823345220604, test loss: 0.7532449009144776\n",
      "Iteration 1056 training loss: 0.47432537532260294, test loss: 0.7576272138613103\n",
      "Iteration 1057 training loss: 0.4743084732210818, test loss: 0.75300137980032\n",
      "Iteration 1058 training loss: 0.47385958334228867, test loss: 0.7573637362233737\n",
      "Iteration 1059 training loss: 0.47383949461150904, test loss: 0.752758341257312\n",
      "Iteration 1060 training loss: 0.47339454021073235, test loss: 0.7571006535590988\n",
      "Iteration 1061 training loss: 0.47337130016735834, test loss: 0.7525157895083638\n",
      "Iteration 1062 training loss: 0.4729302486246515, test loss: 0.7568379695946965\n",
      "Iteration 1063 training loss: 0.472903892541435, test loss: 0.7522737288980705\n",
      "Iteration 1064 training loss: 0.4724667113908337, test loss: 0.7565756881749721\n",
      "Iteration 1065 training loss: 0.4724372744985373, test loss: 0.752032163896186\n",
      "Iteration 1066 training loss: 0.47200393142923935, test loss: 0.7563138132668789\n",
      "Iteration 1067 training loss: 0.4719714489186126, test loss: 0.7517910991012006\n",
      "Iteration 1068 training loss: 0.47154191177620164, test loss: 0.7560523489631875\n",
      "Iteration 1069 training loss: 0.4715064188000097, test loss: 0.7515505392440256\n",
      "Iteration 1070 training loss: 0.4710806555877257, test loss: 0.7557912994862699\n",
      "Iteration 1071 training loss: 0.47104218726283337, test loss: 0.7513104891917897\n",
      "Iteration 1072 training loss: 0.47062016614289054, test loss: 0.7555306691920055\n",
      "Iteration 1073 training loss: 0.47057875755240275, test loss: 0.7510709539517472\n",
      "Iteration 1074 training loss: 0.4701604468473575, test loss: 0.7552704625738094\n",
      "Iteration 1075 training loss: 0.47011613304281546, test loss: 0.7508319386753046\n",
      "Iteration 1076 training loss: 0.469701501236986, test loss: 0.7550106842667913\n",
      "Iteration 1077 training loss: 0.46965431724062223, test loss: 0.7505934486621645\n",
      "Iteration 1078 training loss: 0.46924333298156146, test loss: 0.754751339052042\n",
      "Iteration 1079 training loss: 0.46919331378861406, test loss: 0.7503554893645934\n",
      "Iteration 1080 training loss: 0.4687859458886369, test loss: 0.7544924318610577\n",
      "Iteration 1081 training loss: 0.468733126469724, test loss: 0.7501180663918148\n",
      "Iteration 1082 training loss: 0.46832934390749337, test loss: 0.7542339677803015\n",
      "Iteration 1083 training loss: 0.4682737592110489, test loss: 0.7498811855145315\n",
      "Iteration 1084 training loss: 0.4678735311332191, test loss: 0.753975952055909\n",
      "Iteration 1085 training loss: 0.46781521608799137, test loss: 0.7496448526695803\n",
      "Iteration 1086 training loss: 0.4674185118109137, test loss: 0.7537183900985394\n",
      "Iteration 1087 training loss: 0.46735750132852744, test loss: 0.7494090739647193\n",
      "Iteration 1088 training loss: 0.4669642903400181, test loss: 0.7534612874883772\n",
      "Iteration 1089 training loss: 0.46690061931760096, test loss: 0.7491738556835571\n",
      "Iteration 1090 training loss: 0.46651087127877444, test loss: 0.7532046499802888\n",
      "Iteration 1091 training loss: 0.46644457460164934, test loss: 0.7489392042906213\n",
      "Iteration 1092 training loss: 0.46605825934881895, test loss: 0.7529484835091383\n",
      "Iteration 1093 training loss: 0.46598937189326367, test loss: 0.7487051264365718\n",
      "Iteration 1094 training loss: 0.4656064594399112, test loss: 0.7526927941952639\n",
      "Iteration 1095 training loss: 0.46553501607598397, test loss: 0.7484716289635616\n",
      "Iteration 1096 training loss: 0.46515547661480067, test loss: 0.7524375883501223\n",
      "Iteration 1097 training loss: 0.4650815122092353, test loss: 0.748238718910749\n",
      "Iteration 1098 training loss: 0.4647053161142366, test loss: 0.7521828724821038\n",
      "Iteration 1099 training loss: 0.4646288655334072, test loss: 0.7480064035199631\n",
      "Iteration 1100 training loss: 0.46425598336212287, test loss: 0.7519286533025207\n",
      "Iteration 1101 training loss: 0.4641770814750773, test loss: 0.7477746902415249\n",
      "Iteration 1102 training loss: 0.46380748397081983, test loss: 0.751674937731776\n",
      "Iteration 1103 training loss: 0.46372616565238467, test loss: 0.747543586740229\n",
      "Iteration 1104 training loss: 0.46335982374659707, test loss: 0.7514217329057122\n",
      "Iteration 1105 training loss: 0.46327612388055345, test loss: 0.7473131009014866\n",
      "Iteration 1106 training loss: 0.46291300869524, test loss: 0.7511690461821487\n",
      "Iteration 1107 training loss: 0.4628269621775713, test loss: 0.7470832408376322\n",
      "Iteration 1108 training loss: 0.46246704502781155, test loss: 0.750916885147608\n",
      "Iteration 1109 training loss: 0.46237868677002275, test loss: 0.7468540148943992\n",
      "Iteration 1110 training loss: 0.46202193916657386, test loss: 0.7506652576242357\n",
      "Iteration 1111 training loss: 0.46193130409908273, test loss: 0.7466254316575617\n",
      "Iteration 1112 training loss: 0.4615776977510683, test loss: 0.7504141716769179\n",
      "Iteration 1113 training loss: 0.46148482082667036, test loss: 0.7463974999597505\n",
      "Iteration 1114 training loss: 0.4611343276443617, test loss: 0.7501636356205992\n",
      "Iteration 1115 training loss: 0.46103924384176526, test loss: 0.746170228887439\n",
      "Iteration 1116 training loss: 0.46069183593945373, test loss: 0.7499136580278046\n",
      "Iteration 1117 training loss: 0.4605945802668887, test loss: 0.7459436277881045\n",
      "Iteration 1118 training loss: 0.4602502299658547, test loss: 0.7496642477363674\n",
      "Iteration 1119 training loss: 0.46015083746475127, test loss: 0.7457177062775661\n",
      "Iteration 1120 training loss: 0.4598095172963282, test loss: 0.7494154138573681\n",
      "Iteration 1121 training loss: 0.45970802304506747, test loss: 0.7454924742474953\n",
      "Iteration 1122 training loss: 0.4593697057538048, test loss: 0.7491671657832844\n",
      "Iteration 1123 training loss: 0.4592661448715382, test loss: 0.7452679418731077\n",
      "Iteration 1124 training loss: 0.4589308034184653, test loss: 0.7489195131963537\n",
      "Iteration 1125 training loss: 0.45882521106900326, test loss: 0.7450441196210269\n",
      "Iteration 1126 training loss: 0.4584928186349937, test loss: 0.7486724660771559\n",
      "Iteration 1127 training loss: 0.4583852300307609, test loss: 0.7448210182573279\n",
      "Iteration 1128 training loss: 0.4580557600200009, test loss: 0.7484260347134072\n",
      "Iteration 1129 training loss: 0.45794621042605765, test loss: 0.7445986488557533\n",
      "Iteration 1130 training loss: 0.45761963646961923, test loss: 0.7481802297089776\n",
      "Iteration 1131 training loss: 0.4575081612077448, test loss: 0.7443770228061043\n",
      "Iteration 1132 training loss: 0.45718445716726364, test loss: 0.7479350619931234\n",
      "Iteration 1133 training loss: 0.4570710916201009, test loss: 0.7441561518228019\n",
      "Iteration 1134 training loss: 0.45675023159156247, test loss: 0.7476905428299397\n",
      "Iteration 1135 training loss: 0.4566350112068202, test loss: 0.7439360479536171\n",
      "Iteration 1136 training loss: 0.4563169695244511, test loss: 0.7474466838280287\n",
      "Iteration 1137 training loss: 0.4561999298191606, test loss: 0.7437167235885654\n",
      "Iteration 1138 training loss: 0.45588468105942964, test loss: 0.7472034969503856\n",
      "Iteration 1139 training loss: 0.45576585762425137, test loss: 0.7434981914689611\n",
      "Iteration 1140 training loss: 0.45545337660997726, test loss: 0.746960994524496\n",
      "Iteration 1141 training loss: 0.45533280511355395, test loss: 0.7432804646966259\n",
      "Iteration 1142 training loss: 0.4550230669181216, test loss: 0.7467191892526461\n",
      "Iteration 1143 training loss: 0.45490078311147214, test loss: 0.7430635567432454\n",
      "Iteration 1144 training loss: 0.4545937630631556, test loss: 0.7464780942224364\n",
      "Iteration 1145 training loss: 0.4544698027841045, test loss: 0.7428474814598665\n",
      "Iteration 1146 training loss: 0.45416547647049677, test loss: 0.746237722917497\n",
      "Iteration 1147 training loss: 0.4540398756481329, test loss: 0.7426322530865254\n",
      "Iteration 1148 training loss: 0.4537382189206805, test loss: 0.7459980892283999\n",
      "Iteration 1149 training loss: 0.4536110135798384, test loss: 0.7424178862619966\n",
      "Iteration 1150 training loss: 0.4533120025584791, test loss: 0.7457592074637548\n",
      "Iteration 1151 training loss: 0.4531832288242344, test loss: 0.7422043960336531\n",
      "Iteration 1152 training loss: 0.4528868399021357, test loss: 0.7455210923614851\n",
      "Iteration 1153 training loss: 0.45275653400430715, test loss: 0.7419917978674221\n",
      "Iteration 1154 training loss: 0.4524627438527033, test loss: 0.745283759100271\n",
      "Iteration 1155 training loss: 0.4523309421303511, test loss: 0.7417801076578225\n",
      "Iteration 1156 training loss: 0.45203972770347384, test loss: 0.7450472233111496\n",
      "Iteration 1157 training loss: 0.45190646660938444, test loss: 0.741569341738069\n",
      "Iteration 1158 training loss: 0.4516178051494843, test loss: 0.7448115010892543\n",
      "Iteration 1159 training loss: 0.4514831212546305, test loss: 0.7413595168902223\n",
      "Iteration 1160 training loss: 0.451196990297083, test loss: 0.7445766090056848\n",
      "Iteration 1161 training loss: 0.45106092029504624, test loss: 0.7411506503553668\n",
      "Iteration 1162 training loss: 0.45077729767353797, test loss: 0.7443425641194811\n",
      "Iteration 1163 training loss: 0.4506398783848794, test loss: 0.7409427598437913\n",
      "Iteration 1164 training loss: 0.4503587422366667, test loss: 0.7441093839896921\n",
      "Iteration 1165 training loss: 0.45022001061323286, test loss: 0.7407358635451515\n",
      "Iteration 1166 training loss: 0.4499413393844658, test loss: 0.7438770866875061\n",
      "Iteration 1167 training loss: 0.449801332513612, test loss: 0.7405299801385805\n",
      "Iteration 1168 training loss: 0.44952510496471465, test loss: 0.7436456908084285\n",
      "Iteration 1169 training loss: 0.4493838600734292, test loss: 0.7403251288027232\n",
      "Iteration 1170 training loss: 0.4491100552845274, test loss: 0.7434152154844716\n",
      "Iteration 1171 training loss: 0.448967609743437, test loss: 0.7401213292256598\n",
      "Iteration 1172 training loss: 0.4486962071198219, test loss: 0.7431856803963316\n",
      "Iteration 1173 training loss: 0.44855259844705847, test loss: 0.7399186016146807\n",
      "Iteration 1174 training loss: 0.4482835777246739, test loss: 0.7429571057855154\n",
      "Iteration 1175 training loss: 0.4481388435895803, test loss: 0.7397169667058747\n",
      "Iteration 1176 training loss: 0.4478721848405211, test loss: 0.7427295124663873\n",
      "Iteration 1177 training loss: 0.4477263630671716, test loss: 0.7395164457734895\n",
      "Iteration 1178 training loss: 0.4474620467051772, test loss: 0.7425029218380855\n",
      "Iteration 1179 training loss: 0.4473151752756877, test loss: 0.7393170606390157\n",
      "Iteration 1180 training loss: 0.4470531820616152, test loss: 0.7422773558962766\n",
      "Iteration 1181 training loss: 0.4469052991192156, test loss: 0.7391188336799451\n",
      "Iteration 1182 training loss: 0.44664561016647314, test loss: 0.7420528372446907\n",
      "Iteration 1183 training loss: 0.44649675401831107, test loss: 0.7389217878381518\n",
      "Iteration 1184 training loss: 0.4462393507982341, test loss: 0.7418293891063925\n",
      "Iteration 1185 training loss: 0.4460895599178805, test loss: 0.7387259466278338\n",
      "Iteration 1186 training loss: 0.44583442426502645, test loss: 0.7416070353347279\n",
      "Iteration 1187 training loss: 0.4456837372946471, test loss: 0.7385313341429556\n",
      "Iteration 1188 training loss: 0.4454308514119873, test loss: 0.7413858004238881\n",
      "Iteration 1189 training loss: 0.4452793071641459, test loss: 0.7383379750641247\n",
      "Iteration 1190 training loss: 0.44502865362812755, test loss: 0.7411657095190266\n",
      "Iteration 1191 training loss: 0.444876291087181, test loss: 0.7381458946648286\n",
      "Iteration 1192 training loss: 0.4446278528526334, test loss: 0.7409467884258559\n",
      "Iteration 1193 training loss: 0.44447471117567927, test loss: 0.7379551188169572\n",
      "Iteration 1194 training loss: 0.44422847158053247, test loss: 0.7407290636196517\n",
      "Iteration 1195 training loss: 0.4440745900978648, test loss: 0.7377656739955271\n",
      "Iteration 1196 training loss: 0.4438305328676501, test loss: 0.7405125622535826\n",
      "Iteration 1197 training loss: 0.4436759510826787, test loss: 0.7375775872825234\n",
      "Iteration 1198 training loss: 0.443434060334776, test loss: 0.7402973121662781\n",
      "Iteration 1199 training loss: 0.4432788179233597, test loss: 0.7373908863697648\n",
      "Iteration 1200 training loss: 0.4430390781709546, test loss: 0.7400833418885451\n",
      "Iteration 1201 training loss: 0.4428832149800993, test loss: 0.7372055995606928\n",
      "Iteration 1202 training loss: 0.442645611135811, test loss: 0.7398706806491324\n",
      "Iteration 1203 training loss: 0.44248916718167763, test loss: 0.7370217557709866\n",
      "Iteration 1204 training loss: 0.4422536845608147, test loss: 0.7396593583794407\n",
      "Iteration 1205 training loss: 0.4420967000259819, test loss: 0.736839384527889\n",
      "Iteration 1206 training loss: 0.441863324349382, test loss: 0.739449405717068\n",
      "Iteration 1207 training loss: 0.4417058395793043, test loss: 0.7366585159681317\n",
      "Iteration 1208 training loss: 0.44147455697571036, test loss: 0.7392408540080723\n",
      "Iteration 1209 training loss: 0.44131661247431136, test loss: 0.7364791808343398\n",
      "Iteration 1210 training loss: 0.44108740948223374, test loss: 0.7390337353078329\n",
      "Iteration 1211 training loss: 0.4409290459065693, test loss: 0.7363014104697896\n",
      "Iteration 1212 training loss: 0.44070190947558163, test loss: 0.7388280823803758\n",
      "Iteration 1213 training loss: 0.4405431676295087, test loss: 0.7361252368113865\n",
      "Iteration 1214 training loss: 0.4403180851209213, test loss: 0.7386239286960331\n",
      "Iteration 1215 training loss: 0.44015900594770246, test loss: 0.7359506923807304\n",
      "Iteration 1216 training loss: 0.4399359651345559, test loss: 0.7384213084272925\n",
      "Iteration 1217 training loss: 0.43977658970833006, test loss: 0.7357778102731233\n",
      "Iteration 1218 training loss: 0.43955557877464807, test loss: 0.7382202564426927\n",
      "Iteration 1219 training loss: 0.4393959482906939, test loss: 0.7356066241443762\n",
      "Iteration 1220 training loss: 0.43917695582993194, test loss: 0.7380208082986085\n",
      "Iteration 1221 training loss: 0.43901711159365153, test loss: 0.7354371681952632\n",
      "Iteration 1222 training loss: 0.4388001266062768, test loss: 0.737823000228771\n",
      "Iteration 1223 training loss: 0.4386401100208223, test loss: 0.7352694771534699\n",
      "Iteration 1224 training loss: 0.4384251219109561, test loss: 0.7376268691313596\n",
      "Iteration 1225 training loss: 0.43826497446342383, test loss: 0.7351035862528783\n",
      "Iteration 1226 training loss: 0.4380519730344789, test loss: 0.7374324525534985\n",
      "Iteration 1227 training loss: 0.4378917362805936, test loss: 0.7349395312100299\n",
      "Iteration 1228 training loss: 0.4376807117298331, test loss: 0.7372397886729886\n",
      "Iteration 1229 training loss: 0.4375204272770443, test loss: 0.7347773481976034\n",
      "Iteration 1230 training loss: 0.4373113701889926, test loss: 0.7370489162771002\n",
      "Iteration 1231 training loss: 0.4371510796779074, test loss: 0.734617073814751\n",
      "Iteration 1232 training loss: 0.4369439810165371, test loss: 0.7368598747382524\n",
      "Iteration 1233 training loss: 0.43678372610061134, test loss: 0.7344587450541258\n",
      "Iteration 1234 training loss: 0.43657857720023574, test loss: 0.7366727039864047\n",
      "Iteration 1235 training loss: 0.43641839952365014, test loss: 0.7343023992654439\n",
      "Iteration 1236 training loss: 0.4362151920784448, test loss: 0.7364874444779831\n",
      "Iteration 1237 training loss: 0.43605513325209394, test loss: 0.7341480741154284\n",
      "Iteration 1238 training loss: 0.43585385930417625, test loss: 0.7363041371611659\n",
      "Iteration 1239 training loss: 0.43569396087969986, test loss: 0.7339958075439773\n",
      "Iteration 1240 training loss: 0.435494612805695, test loss: 0.7361228234373649\n",
      "Iteration 1241 training loss: 0.4353349162474876, test loss: 0.7338456377164169\n",
      "Iteration 1242 training loss: 0.43513748674351044, test loss: 0.7359435451187291\n",
      "Iteration 1243 training loss: 0.4349780333986477, test loss: 0.7336976029717003\n",
      "Iteration 1244 training loss: 0.4347825154636348, test loss: 0.7357663443815128\n",
      "Iteration 1245 training loss: 0.4346233465296635, test loss: 0.7335517417664248\n",
      "Iteration 1246 training loss: 0.434429733446992, test loss: 0.7355912637151641\n",
      "Iteration 1247 training loss: 0.43427088993753443, test loss: 0.7334080926145585\n",
      "Iteration 1248 training loss: 0.43407917525486944, test loss: 0.7354183458669872\n",
      "Iteration 1249 training loss: 0.433920697963005, test loss: 0.733266694022771\n",
      "Iteration 1250 training loss: 0.4337308754703225, test loss: 0.7352476337822531\n",
      "Iteration 1251 training loss: 0.4335728049297149, test loss: 0.7331275844212943\n",
      "Iteration 1252 training loss: 0.4333848686354549, test loss: 0.7350791705396558\n",
      "Iteration 1253 training loss: 0.4332272450792076, test loss: 0.7329908020902469\n",
      "Iteration 1254 training loss: 0.4330411891845189, test loss: 0.7349129992820099\n",
      "Iteration 1255 training loss: 0.4328840525017508, test loss: 0.7328563850813848\n",
      "Iteration 1256 training loss: 0.4326998713727991, test loss: 0.7347491631421296\n",
      "Iteration 1257 training loss: 0.432543261062948, test loss: 0.7327243711352698\n",
      "Iteration 1258 training loss: 0.4323609492012682, test loss: 0.7345877051638381\n",
      "Iteration 1259 training loss: 0.4322049043261431, test loss: 0.7325947975938659\n",
      "Iteration 1260 training loss: 0.4320244563370303, test loss: 0.734428668218086\n",
      "Iteration 1261 training loss: 0.431869015470652, test loss: 0.7324677013086186\n",
      "Iteration 1262 training loss: 0.4316904260295957, test loss: 0.7342720949141959\n",
      "Iteration 1263 training loss: 0.43153562720588007, test loss: 0.7323431185440944\n",
      "Iteration 1264 training loss: 0.43135889102306485, test loss: 0.734118027506273\n",
      "Iteration 1265 training loss: 0.43120477168142635, test loss: 0.7322210848773064\n",
      "Iteration 1266 training loss: 0.4310298834643333, test loss: 0.7339665077948642\n",
      "Iteration 1267 training loss: 0.43087648039330306, test loss: 0.732101635092883\n",
      "Iteration 1268 training loss: 0.43070343480746787, test loss: 0.7338175770239898\n",
      "Iteration 1269 training loss: 0.43055078408644637, test loss: 0.7319848030742943\n",
      "Iteration 1270 training loss: 0.4303795757144444, test loss: 0.7336712757737089\n",
      "Iteration 1271 training loss: 0.4302277126537294, test loss: 0.7318706216913767\n",
      "Iteration 1272 training loss: 0.4300583359524822, test loss: 0.733527643848431\n",
      "Iteration 1273 training loss: 0.4299072950317383, test loss: 0.7317591226844712\n",
      "Iteration 1274 training loss: 0.4297397442882521, test loss: 0.7333867201612307\n",
      "Iteration 1275 training loss: 0.42958955909361424, test loss: 0.7316503365455161\n",
      "Iteration 1276 training loss: 0.42942382837928633, test loss: 0.7332485426144786\n",
      "Iteration 1277 training loss: 0.42927453153931405, test loss: 0.7315442923965098\n",
      "Iteration 1278 training loss: 0.4291106146629643, test loss: 0.733113147977147\n",
      "Iteration 1279 training loss: 0.4289622377836919, test loss: 0.7314410178657992\n",
      "Iteration 1280 training loss: 0.42880012824350044, test loss: 0.7329805717592129\n",
      "Iteration 1281 training loss: 0.42865270184285365, test loss: 0.7313405389627136\n",
      "Iteration 1282 training loss: 0.4284923927774094, test loss: 0.7328508480836287\n",
      "Iteration 1283 training loss: 0.42834594621928807, test loss: 0.7312428799511137\n",
      "Iteration 1284 training loss: 0.4281874303579763, test loss: 0.7327240095563927\n",
      "Iteration 1285 training loss: 0.42804199178632685, test loss: 0.7311480632224853\n",
      "Iteration 1286 training loss: 0.4278852613993094, test loss: 0.7326000871353063\n",
      "Iteration 1287 training loss: 0.42774085767253933, test loss: 0.7310561091692608\n",
      "Iteration 1288 training loss: 0.4275859045206028, test loss: 0.7324791099980619\n",
      "Iteration 1289 training loss: 0.42744256114671125, test loss: 0.7309670360591007\n",
      "Iteration 1290 training loss: 0.4272893764312829, test loss: 0.7323611054103578\n",
      "Iteration 1291 training loss: 0.4271471175041072, test loss: 0.7308808599109236\n",
      "Iteration 1292 training loss: 0.4269956918177585, test loss: 0.7322460985947922\n",
      "Iteration 1293 training loss: 0.42685453995475575, test loss: 0.7307975943735101\n",
      "Iteration 1294 training loss: 0.4267048632325348, test loss: 0.7321341126013302\n",
      "Iteration 1295 training loss: 0.42656483951453733, test loss: 0.7307172506075599\n",
      "Iteration 1296 training loss: 0.4264169009864884, test loss: 0.7320251681801947\n",
      "Iteration 1297 training loss: 0.42627802489988786, test loss: 0.7306398371721005\n",
      "Iteration 1298 training loss: 0.426131813045132, test loss: 0.7319192836580557\n",
      "Iteration 1299 training loss: 0.42599410242695684, test loss: 0.7305653599161958\n",
      "Iteration 1300 training loss: 0.4258496049297218, test loss: 0.7318164748184379\n",
      "Iteration 1301 training loss: 0.42571307591608515, test loss: 0.7304938218769018\n",
      "Iteration 1302 training loss: 0.4255702796240796, test loss: 0.7317167547872897\n",
      "Iteration 1303 training loss: 0.42543494660247333, test loss: 0.7304252231844446\n",
      "Iteration 1304 training loss: 0.42529383748800853, test loss: 0.7316201339246658\n",
      "Iteration 1305 training loss: 0.42515971305392386, test loss: 0.7303595609755972\n",
      "Iteration 1306 training loss: 0.4250202761781859, test loss: 0.7315266197235013\n",
      "Iteration 1307 training loss: 0.42488737109652974, test loss: 0.7302968293162142\n",
      "Iteration 1308 training loss: 0.42474959057740364, test loss: 0.7314362167164323\n",
      "Iteration 1309 training loss: 0.4246179137491714, test loss: 0.7302370191338771\n",
      "Iteration 1310 training loss: 0.42448177273300963, test loss: 0.7313489263916247\n",
      "Iteration 1311 training loss: 0.4243513311676562, test loss: 0.7301801181615618\n",
      "Iteration 1312 training loss: 0.4242168118053715, test loss: 0.731264747118536\n",
      "Iteration 1313 training loss: 0.4240876105992988, test loss: 0.7301261108932036\n",
      "Iteration 1314 training loss: 0.42395469402714353, test loss: 0.7311836740845049\n",
      "Iteration 1315 training loss: 0.4238267363486922, test loss: 0.7300749785519729\n",
      "Iteration 1316 training loss: 0.4236954026740621, test loss: 0.7311056992430153\n",
      "Iteration 1317 training loss: 0.4235686897553617, test loss: 0.7300266990720137\n",
      "Iteration 1318 training loss: 0.423438918047933, test loss: 0.7310308112744145\n",
      "Iteration 1319 training loss: 0.4233134491839187, test loss: 0.7299812470943035\n",
      "Iteration 1320 training loss: 0.4231852174723927, test loss: 0.7309589955598013\n",
      "Iteration 1321 training loss: 0.42306099002725217, test loss: 0.7299385939772104\n",
      "Iteration 1322 training loss: 0.4229342753019428, test loss: 0.7308902341687045\n",
      "Iteration 1323 training loss: 0.4228112847232016, test loss: 0.7298987078222106\n",
      "Iteration 1324 training loss: 0.42268606294465416, test loss: 0.7308245058610823\n",
      "Iteration 1325 training loss: 0.42256430278505, test loss: 0.7298615535151098\n",
      "Iteration 1326 training loss: 0.4224405488988334, test loss: 0.7307617861040587\n",
      "Iteration 1327 training loss: 0.42232001084606496, test loss: 0.7298270927829951\n",
      "Iteration 1328 training loss: 0.4221976988038219, test loss: 0.730702047103702\n",
      "Iteration 1329 training loss: 0.4220783727181936, test loss: 0.7297952842669931\n",
      "Iteration 1330 training loss: 0.4219574755049813, test loss: 0.7306452578520145\n",
      "Iteration 1331 training loss: 0.4218393494648925, test loss: 0.7297660836107787\n",
      "Iteration 1332 training loss: 0.421719839132781, test loss: 0.7305913841891764\n",
      "Iteration 1333 training loss: 0.42160289948793733, test loss: 0.7297394435646272\n",
      "Iteration 1334 training loss: 0.4214847471957763, test loss: 0.7305403888809425\n",
      "Iteration 1335 training loss: 0.4213689786279268, test loss: 0.7297153141046459\n",
      "Iteration 1336 training loss: 0.42125215468712407, test loss: 0.7304922317109489\n",
      "Iteration 1337 training loss: 0.4211375402780553, test loss: 0.7296936425666768\n",
      "Iteration 1338 training loss: 0.42102201420414964, test loss: 0.7304468695875409\n",
      "Iteration 1339 training loss: 0.4209085355105935, test loss: 0.729674373794203\n",
      "Iteration 1340 training loss: 0.420794276080342, test loss: 0.7304042566645909\n",
      "Iteration 1341 training loss: 0.42068191321538545, test loss: 0.7296574502994482\n",
      "Iteration 1342 training loss: 0.42056888852902263, test loss: 0.7303643444756294\n",
      "Iteration 1343 training loss: 0.4204576202495397, test loss: 0.7296428124367144\n",
      "Iteration 1344 training loss: 0.4203457977978111, test loss: 0.730327082080484\n",
      "Iteration 1345 training loss: 0.42023560159737294, test loss: 0.7296303985868693\n",
      "Iteration 1346 training loss: 0.42012494833288927, test loss: 0.7302924162234806\n",
      "Iteration 1347 training loss: 0.42001580053955473, test loss: 0.7296201453517716\n",
      "Iteration 1348 training loss: 0.4199062829519635, test loss: 0.7302602915021515\n",
      "Iteration 1349 training loss: 0.41979815883029764, test loss: 0.7296119877573124\n",
      "Iteration 1350 training loss: 0.4196897430247272, test loss: 0.7302306505452822\n",
      "Iteration 1351 training loss: 0.4195826168813528, test loss: 0.7296058594636561\n",
      "Iteration 1352 training loss: 0.41947526865954443, test loss: 0.7302034341990339\n",
      "Iteration 1353 training loss: 0.4193691139514983, test loss: 0.7296016929811799\n",
      "Iteration 1354 training loss: 0.41926279889501195, test loss: 0.7301785817198001\n",
      "Iteration 1355 training loss: 0.41915758834014727, test loss: 0.7295994198905592\n",
      "Iteration 1356 training loss: 0.4190522718950049, test loss: 0.7301560309723925\n",
      "Iteration 1357 training loss: 0.4189479775836662, test loss: 0.729598971065391\n",
      "Iteration 1358 training loss: 0.4188436251457811, test loss: 0.7301357186321024\n",
      "Iteration 1359 training loss: 0.4187402186529693, test loss: 0.7296002768957369\n",
      "Iteration 1360 training loss: 0.418636795653704, test loss: 0.73011758038916\n",
      "Iteration 1361 training loss: 0.4185342481509497, test loss: 0.7296032675109578\n",
      "Iteration 1362 training loss: 0.41843172014214697, test loss: 0.7301015511541059\n",
      "Iteration 1363 training loss: 0.418330002508324, test loss: 0.7296078730002324\n",
      "Iteration 1364 training loss: 0.41822833524616554, test loss: 0.730087565262588\n",
      "Iteration 1365 training loss: 0.41812741817649396, test loss: 0.7296140236291903\n",
      "Iteration 1366 training loss: 0.4180265777035619, test loss: 0.7300755566781435\n",
      "Iteration 1367 training loss: 0.4179264318160822, test loss: 0.7296216500511452\n",
      "Iteration 1368 training loss: 0.41782638454102233, test loss: 0.7300654591915516\n",
      "Iteration 1369 training loss: 0.41772698047985896, test loss: 0.7296306835114955\n",
      "Iteration 1370 training loss: 0.4176276932540813, test loss: 0.7300572066154211\n",
      "Iteration 1371 training loss: 0.4175290017888596, test loss: 0.7296410560439405\n",
      "Iteration 1372 training loss: 0.41743044197975004, test loss: 0.730050732972744\n",
      "Iteration 1373 training loss: 0.4173324341005833, test loss: 0.72965270065728\n",
      "Iteration 1374 training loss: 0.4172345696607483, test loss: 0.7300459726782461\n",
      "Iteration 1375 training loss: 0.4171372166682705, test loss: 0.7296655515116787\n",
      "Iteration 1376 training loss: 0.41704001620038655, test loss: 0.730042860711464\n",
      "Iteration 1377 training loss: 0.4169432897903686, test loss: 0.7296795440834061\n",
      "Iteration 1378 training loss: 0.41684672260726596, test loss: 0.7300413327805958\n",
      "Iteration 1379 training loss: 0.4167505949494213, test loss: 0.7296946153172109\n",
      "Iteration 1380 training loss: 0.4166546311290865, test loss: 0.7300413254762946\n",
      "Iteration 1381 training loss: 0.416559074939741, test loss: 0.7297107037656265\n",
      "Iteration 1382 training loss: 0.41646368537498835, test loss: 0.7300427764147029\n",
      "Iteration 1383 training loss: 0.41636867398336, test loss: 0.7297277497146589\n",
      "Iteration 1384 training loss: 0.41627383042597904, test loss: 0.7300456243691565\n",
      "Iteration 1385 training loss: 0.4161793378338853, test loss: 0.7297456952954617\n",
      "Iteration 1386 training loss: 0.4160850129331368, test loss: 0.7300498093901234\n",
      "Iteration 1387 training loss: 0.41599101386801735, test loss: 0.729764484581746\n",
      "Iteration 1388 training loss: 0.4158971812034107, test loss: 0.7300552729130737\n",
      "Iteration 1389 training loss: 0.41580365116462126, test loss: 0.7297840636728308\n",
      "Iteration 1390 training loss: 0.415710285272962, test loss: 0.7300619578541118\n",
      "Iteration 1391 training loss: 0.4156172005713644, test loss: 0.7298043807623725\n",
      "Iteration 1392 training loss: 0.41552427696812394, test loss: 0.7300698086933216\n",
      "Iteration 1393 training loss: 0.41543161475905344, test loss: 0.7298253861929549\n",
      "Iteration 1394 training loss: 0.4153391099541602, test loss: 0.7300787715459047\n",
      "Iteration 1395 training loss: 0.4152468482639163, test loss: 0.729847032496839\n",
      "Iteration 1396 training loss: 0.4151547397721197, test loss: 0.730088794221296\n",
      "Iteration 1397 training loss: 0.41506285751817507, test loss: 0.7298692744232991\n",
      "Iteration 1398 training loss: 0.41497112386417884, test loss: 0.7300998262705509\n",
      "Iteration 1399 training loss: 0.4148796008693503, test loss: 0.7298920689530632\n",
      "Iteration 1400 training loss: 0.41478822158795026, test loss: 0.7301118190223893\n",
      "Iteration 1401 training loss: 0.4146970385888166, test loss: 0.7299153753004832\n",
      "Iteration 1402 training loss: 0.4146059942203135, test loss: 0.7301247256083696\n",
      "Iteration 1403 training loss: 0.41451513287020286, test loss: 0.7299391549041252\n",
      "Iteration 1404 training loss: 0.4144244049513918, test loss: 0.7301385009777269\n",
      "Iteration 1405 training loss: 0.41433384781828664, test loss: 0.7299633714065493\n",
      "Iteration 1406 training loss: 0.4142434188693443, test loss: 0.7301531019024875\n",
      "Iteration 1407 training loss: 0.4141531494290813, test loss: 0.7299879906240926\n",
      "Iteration 1408 training loss: 0.4140630029366951, test loss: 0.7301684869735058\n",
      "Iteration 1409 training loss: 0.41397300556184896, test loss: 0.7300129805075163\n",
      "Iteration 1410 training loss: 0.41388312595894283, test loss: 0.7301846165881145\n",
      "Iteration 1411 training loss: 0.41379338590379766, test loss: 0.730038311094399\n",
      "Iteration 1412 training loss: 0.4137037585462174, test loss: 0.7302014529301063\n",
      "Iteration 1413 training loss: 0.41361426192823486, test loss: 0.7300639544541789\n",
      "Iteration 1414 training loss: 0.4135248730687594, test loss: 0.7302189599427786\n",
      "Iteration 1415 training loss: 0.41343560684695246, test loss: 0.7300898846267462\n",
      "Iteration 1416 training loss: 0.41334644360699613, test loss: 0.7302371032957796\n",
      "Iteration 1417 training loss: 0.41325739555761404, test loss: 0.7301160775554855\n",
      "Iteration 1418 training loss: 0.4131684458969775, test loss: 0.7302558503464941\n",
      "Iteration 1419 training loss: 0.41307960458689896, test loss: 0.7301425110156462\n",
      "Iteration 1420 training loss: 0.41299085727192014, test loss: 0.7302751700966891\n",
      "Iteration 1421 training loss: 0.4129022120301391, test loss: 0.730169164538898\n",
      "Iteration 1422 training loss: 0.4128136566005791, test loss: 0.7302950331451298\n",
      "Iteration 1423 training loss: 0.412725197488153, test loss: 0.7301960193348894\n",
      "Iteration 1424 training loss: 0.4126368242231405, test loss: 0.7303154116368414\n",
      "Iteration 1425 training loss: 0.41254854200194907, test loss: 0.7302230582105981\n",
      "Iteration 1426 training loss: 0.41246034188528446, test loss: 0.7303362792096683\n",
      "Iteration 1427 training loss: 0.4123722279859328, test loss: 0.7302502654882048\n",
      "Iteration 1428 training loss: 0.41228419267103733, test loss: 0.7303576109387437\n",
      "Iteration 1429 training loss: 0.41219623916020637, test loss: 0.7302776269221821\n",
      "Iteration 1430 training loss: 0.41210836093497855, test loss: 0.7303793832794427\n",
      "Iteration 1431 training loss: 0.4120205604825098, test loss: 0.7303051296162347\n",
      "Iteration 1432 training loss: 0.4119328322343282, test loss: 0.7304015740093521\n",
      "Iteration 1433 training loss: 0.41184517808029913, test loss: 0.7303327619406711\n",
      "Iteration 1434 training loss: 0.41175759326139055, test loss: 0.7304241621697443\n",
      "Iteration 1435 training loss: 0.41167007918341514, test loss: 0.7303605134507336\n",
      "Iteration 1436 training loss: 0.41158263177678145, test loss: 0.7304471280069988\n",
      "Iteration 1437 training loss: 0.4114952520577447, test loss: 0.7303883748063565\n",
      "Iteration 1438 training loss: 0.41140793654381913, test loss: 0.7304704529143702\n",
      "Iteration 1439 training loss: 0.41132068594022747, test loss: 0.7304163376937644\n",
      "Iteration 1440 training loss: 0.4112334972644106, test loss: 0.730494119374452\n",
      "Iteration 1441 training loss: 0.4111463709755188, test loss: 0.7304443947492705\n",
      "Iteration 1442 training loss: 0.41105930451671846, test loss: 0.7305181109026506\n",
      "Iteration 1443 training loss: 0.4109722981545676, test loss: 0.7304725394855788\n",
      "Iteration 1444 training loss: 0.41088534969485024, test loss: 0.7305424119919309\n",
      "Iteration 1445 training loss: 0.4107984592553296, test loss: 0.7305007662208464\n",
      "Iteration 1446 training loss: 0.410711624950767, test loss: 0.7305670080590589\n",
      "Iteration 1447 training loss: 0.41062484678579225, test loss: 0.7305290700107087\n",
      "Iteration 1448 training loss: 0.41053812313857047, test loss: 0.730591885392531\n",
      "Iteration 1449 training loss: 0.4104514539294498, test loss: 0.7305574465834287\n",
      "Iteration 1450 training loss: 0.41036483776128946, test loss: 0.7306170311023349\n",
      "Iteration 1451 training loss: 0.4102782744933302, test loss: 0.7305858922782873\n",
      "Iteration 1452 training loss: 0.41019176292024967, test loss: 0.730642433071663\n",
      "Iteration 1453 training loss: 0.41010530285864405, test loss: 0.7306144039872924\n",
      "Iteration 1454 training loss: 0.41001889326708424, test loss: 0.7306680799106582\n",
      "Iteration 1455 training loss: 0.4099325339340925, test loss: 0.7306429791002481\n",
      "Iteration 1456 training loss: 0.4098462239584078, test loss: 0.7306939609122554\n",
      "Iteration 1457 training loss: 0.40975996311184865, test loss: 0.7306716154531957\n",
      "Iteration 1458 training loss: 0.409673750613156, test loss: 0.7307200660101417\n",
      "Iteration 1459 training loss: 0.40958758622619823, test loss: 0.730700311280204\n",
      "Iteration 1460 training loss: 0.4095014692725665, test loss: 0.7307463857388533\n",
      "Iteration 1461 training loss: 0.4094153995148066, test loss: 0.7307290651684687\n",
      "Iteration 1462 training loss: 0.40932937636276057, test loss: 0.7307729111959899\n",
      "Iteration 1463 training loss: 0.40924339958256156, test loss: 0.7307578760166505\n",
      "Iteration 1464 training loss: 0.40915746865986447, test loss: 0.7307996340065257\n",
      "Iteration 1465 training loss: 0.4090715833679238, test loss: 0.7307867429963676\n",
      "Iteration 1466 training loss: 0.4089857432575984, test loss: 0.7308265462891701\n",
      "Iteration 1467 training loss: 0.4088999481117052, test loss: 0.7308156655167419\n",
      "Iteration 1468 training loss: 0.40881419753724735, test loss: 0.7308536406247226\n",
      "Iteration 1469 training loss: 0.40872849132818456, test loss: 0.7308446431918838\n",
      "Iteration 1470 training loss: 0.4086428291399183, test loss: 0.7308809100263627\n",
      "Iteration 1471 training loss: 0.408557210778462, test loss: 0.7308736758111913\n",
      "Iteration 1472 training loss: 0.40847163594098335, test loss: 0.7309083479117957\n",
      "Iteration 1473 training loss: 0.40838610444594736, test loss: 0.7309027633123278\n",
      "Iteration 1474 training loss: 0.40830061602660045, test loss: 0.7309359480771801\n",
      "Iteration 1475 training loss: 0.4082151705138712, test loss: 0.7309319057567464\n",
      "Iteration 1476 training loss: 0.408129767672201, test loss: 0.7309637046727534\n",
      "Iteration 1477 training loss: 0.4080444073447093, test loss: 0.7309611033076111\n",
      "Iteration 1478 training loss: 0.4079590893228325, test loss: 0.7309916121800675\n",
      "Iteration 1479 training loss: 0.40787381346140494, test loss: 0.7309903562099759\n",
      "Iteration 1480 training loss: 0.4077885795752408, test loss: 0.7310196653907509\n",
      "Iteration 1481 training loss: 0.4077033875302761, test loss: 0.7310196647730762\n",
      "Iteration 1482 training loss: 0.4076182371615828, test loss: 0.7310478593867082\n",
      "Iteration 1483 training loss: 0.4075331283454968, test loss: 0.7310490293545896\n",
      "Iteration 1484 training loss: 0.4074480609346542, test loss: 0.731076189521669\n",
      "Iteration 1485 training loss: 0.4073630348150409, test loss: 0.7310784503467226\n",
      "Iteration 1486 training loss: 0.4072780498545275, test loss: 0.7311046514040045\n",
      "Iteration 1487 training loss: 0.407193105947982, test loss: 0.7311079281639912\n",
      "Iteration 1488 training loss: 0.40710820297649325, test loss: 0.7311332408807258\n",
      "Iteration 1489 training loss: 0.40702334084304737, test loss: 0.731137463232555\n",
      "Iteration 1490 training loss: 0.40693851944020376, test loss: 0.7311619540225844\n",
      "Iteration 1491 training loss: 0.4068537386783235, test loss: 0.7311670559809836\n",
      "Iteration 1492 training loss: 0.4067689984599218, test loss: 0.7311907871102025\n",
      "Iteration 1493 training loss: 0.40668429870202305, test loss: 0.7311967068323247\n",
      "Iteration 1494 training loss: 0.4065996393157832, test loss: 0.7312197366211497\n",
      "Iteration 1495 training loss: 0.4065150202242183, test loss: 0.7312264161973638\n",
      "Iteration 1496 training loss: 0.4064304413459839, test loss: 0.7312487992179106\n",
      "Iteration 1497 training loss: 0.4063459026094604, test loss: 0.731256184468958\n",
      "Iteration 1498 training loss: 0.40626140393981136, test loss: 0.7312779717366656\n",
      "Iteration 1499 training loss: 0.4061769452702018, test loss: 0.7312860120173428\n",
      "Iteration 1500 training loss: 0.4060925265314415, test loss: 0.7313072511768305\n",
      "Iteration 1501 training loss: 0.40600814766094834, test loss: 0.7313158991863131\n",
      "Iteration 1502 training loss: 0.40592380859442984, test loss: 0.7313366346912945\n",
      "Iteration 1503 training loss: 0.4058395092730718, test loss: 0.7313458462901833\n",
      "Iteration 1504 training loss: 0.4057552496368291, test loss: 0.7313661195773055\n",
      "Iteration 1505 training loss: 0.4056710296302167, test loss: 0.7313758536114445\n",
      "Iteration 1506 training loss: 0.4055868491968722, test loss: 0.7313957032679514\n",
      "Iteration 1507 training loss: 0.40550270828424423, test loss: 0.7314059213990362\n",
      "Iteration 1508 training loss: 0.40541860683916253, test loss: 0.7314253833241926\n",
      "Iteration 1509 training loss: 0.4053345448116553, test loss: 0.7314360498671603\n",
      "Iteration 1510 training loss: 0.40525052215131896, test loss: 0.7314551574274024\n",
      "Iteration 1511 training loss: 0.40516653881044534, test loss: 0.7314662391945711\n",
      "Iteration 1512 training loss: 0.40508259474102787, test loss: 0.7314850233723803\n",
      "Iteration 1513 training loss: 0.4049986898973417, test loss: 0.731496489524275\n",
      "Iteration 1514 training loss: 0.40491482423345765, test loss: 0.7315149790607962\n",
      "Iteration 1515 training loss: 0.4048309977053852, test loss: 0.7315268009635897\n",
      "Iteration 1516 training loss: 0.4047472102689956, test loss: 0.7315450224950415\n",
      "Iteration 1517 training loss: 0.404663461881814, test loss: 0.7315571735845036\n",
      "Iteration 1518 training loss: 0.4045797525012718, test loss: 0.7315751517724489\n",
      "Iteration 1519 training loss: 0.40449608208621757, test loss: 0.731587607424298\n",
      "Iteration 1520 training loss: 0.4044124505954358, test loss: 0.7316053650798618\n",
      "Iteration 1521 training loss: 0.40432885798892937, test loss: 0.7316181024863809\n",
      "Iteration 1522 training loss: 0.40424530422665694, test loss: 0.7316356606885226\n",
      "Iteration 1523 training loss: 0.404161789269629, test loss: 0.7316486587413022\n",
      "Iteration 1524 training loss: 0.4040783130788236, test loss: 0.731666036949262\n",
      "Iteration 1525 training loss: 0.40399487561613007, test loss: 0.7316792761279102\n",
      "Iteration 1526 training loss: 0.4039114768434132, test loss: 0.7316964922879655\n",
      "Iteration 1527 training loss: 0.4038281167233301, test loss: 0.7317099545546247\n",
      "Iteration 1528 training loss: 0.4037447952185173, test loss: 0.7317270252013018\n",
      "Iteration 1529 training loss: 0.40366151229230274, test loss: 0.7317406939007972\n",
      "Iteration 1530 training loss: 0.40357826790799706, test loss: 0.7317576342526946\n",
      "Iteration 1531 training loss: 0.40349506202951524, test loss: 0.731771494018134\n",
      "Iteration 1532 training loss: 0.40341189462075716, test loss: 0.7317883180685204\n",
      "Iteration 1533 training loss: 0.4033287656461533, test loss: 0.731802354732164\n",
      "Iteration 1534 training loss: 0.40324567507012, test loss: 0.7318190753345248\n",
      "Iteration 1535 training loss: 0.40316262285754123, test loss: 0.7318332758437314\n",
      "Iteration 1536 training loss: 0.40307960897328815, test loss: 0.7318499047924366\n",
      "Iteration 1537 training loss: 0.40299663338264463, test loss: 0.7318642571304981\n",
      "Iteration 1538 training loss: 0.40291369605088323, test loss: 0.7318808052367718\n",
      "Iteration 1539 training loss: 0.4028307969436426, test loss: 0.7318952983484436\n",
      "Iteration 1540 training loss: 0.4027479360265513, test loss: 0.7319117755118193\n",
      "Iteration 1541 training loss: 0.4026651132655635, test loss: 0.7319263992333476\n",
      "Iteration 1542 training loss: 0.4025823286266249, test loss: 0.7319428145087926\n",
      "Iteration 1543 training loss: 0.40249958207597214, test loss: 0.731957559502251\n",
      "Iteration 1544 training loss: 0.40241687357983513, test loss: 0.7319739211631427\n",
      "Iteration 1545 training loss: 0.40233420310470397, test loss: 0.7319887788548823\n",
      "Iteration 1546 training loss: 0.40225157061706507, test loss: 0.7320050944520214\n",
      "Iteration 1547 training loss: 0.402168976083638, test loss: 0.7320200569750457\n",
      "Iteration 1548 training loss: 0.40208641947114104, test loss: 0.7320363333918889\n",
      "Iteration 1549 training loss: 0.40200390074650305, test loss: 0.7320513935319632\n",
      "Iteration 1550 training loss: 0.401921419876654, test loss: 0.732067637036255\n",
      "Iteration 1551 training loss: 0.4018389768287141, test loss: 0.7320827881815702\n",
      "Iteration 1552 training loss: 0.40175657156980776, test loss: 0.7320990044735521\n",
      "Iteration 1553 training loss: 0.4016742040672321, test loss: 0.7321142405677576\n",
      "Iteration 1554 training loss: 0.40159187428829085, test loss: 0.7321304348251273\n",
      "Iteration 1555 training loss: 0.40150958220044564, test loss: 0.7321457503235594\n",
      "Iteration 1556 training loss: 0.4014273277711669, test loss: 0.7321619272433523\n",
      "Iteration 1557 training loss: 0.40134511096807, test loss: 0.732177317072286\n",
      "Iteration 1558 training loss: 0.4012629317587817, test loss: 0.7321934809098428\n",
      "Iteration 1559 training loss: 0.401180790111062, test loss: 0.7322089404285985\n",
      "Iteration 1560 training loss: 0.4010986859926849, test loss: 0.7322250950337831\n",
      "Iteration 1561 training loss: 0.4010166193715472, test loss: 0.7322406199995285\n",
      "Iteration 1562 training loss: 0.4009345902155626, test loss: 0.7322567688503477\n",
      "Iteration 1563 training loss: 0.4008525984927584, test loss: 0.732272355385439\n",
      "Iteration 1564 training loss: 0.4007706441711812, test loss: 0.732288501619219\n",
      "Iteration 1565 training loss: 0.4006887272189833, test loss: 0.7323041461809291\n",
      "Iteration 1566 training loss: 0.4006068476043386, test loss: 0.7323202926231949\n",
      "Iteration 1567 training loss: 0.4005250052955197, test loss: 0.7323359919756852\n",
      "Iteration 1568 training loss: 0.40044320026082314, test loss: 0.7323521411668767\n",
      "Iteration 1569 training loss: 0.4003614324686379, test loss: 0.7323678923552757\n",
      "Iteration 1570 training loss: 0.40027970188737877, test loss: 0.7323840465754433\n",
      "Iteration 1571 training loss: 0.4001980084855477, test loss: 0.7323998469018942\n",
      "Iteration 1572 training loss: 0.40011635223167447, test loss: 0.7324160081934958\n",
      "Iteration 1573 training loss: 0.4000347330943711, test loss: 0.7324318551950522\n",
      "Iteration 1574 training loss: 0.3999531510422791, test loss: 0.7324480253839775\n",
      "Iteration 1575 training loss: 0.3998716060441179, test loss: 0.7324639168122212\n",
      "Iteration 1576 training loss: 0.3997900980686383, test loss: 0.7324800975271584\n",
      "Iteration 1577 training loss: 0.39970862708466526, test loss: 0.7324960313294285\n",
      "Iteration 1578 training loss: 0.3996271930610561, test loss: 0.7325122240196887\n",
      "Iteration 1579 training loss: 0.39954579596673934, test loss: 0.7325281983218083\n",
      "Iteration 1580 training loss: 0.3994644357706775, test loss: 0.7325444042737067\n",
      "Iteration 1581 training loss: 0.3993831124419005, test loss: 0.7325604173641079\n",
      "Iteration 1582 training loss: 0.3993018259494739, test loss: 0.7325766377160093\n",
      "Iteration 1583 training loss: 0.3992205762625283, test loss: 0.7325926880311547\n",
      "Iteration 1584 training loss: 0.39913936335023054, test loss: 0.7326089237872736\n",
      "Iteration 1585 training loss: 0.39905818718181024, test loss: 0.7326250098982828\n",
      "Iteration 1586 training loss: 0.39897704772653436, test loss: 0.732641261941328\n",
      "Iteration 1587 training loss: 0.3988959449537303, test loss: 0.7326573825417241\n",
      "Iteration 1588 training loss: 0.3988148788327639, test loss: 0.7326736516444736\n",
      "Iteration 1589 training loss: 0.39873384933305944, test loss: 0.7326898055389631\n",
      "Iteration 1590 training loss: 0.39865285642408027, test loss: 0.7327060923748488\n",
      "Iteration 1591 training loss: 0.39857190007534665, test loss: 0.7327222784690605\n",
      "Iteration 1592 training loss: 0.3984909802564183, test loss: 0.7327385836218363\n",
      "Iteration 1593 training loss: 0.3984100969369107, test loss: 0.7327548009129448\n",
      "Iteration 1594 training loss: 0.3983292500864792, test loss: 0.732771124885509\n",
      "Iteration 1595 training loss: 0.39824843967483314, test loss: 0.7327873724536766\n",
      "Iteration 1596 training loss: 0.3981676656717225, test loss: 0.7328037156761125\n",
      "Iteration 1597 training loss: 0.39808692804695045, test loss: 0.7328199926766836\n",
      "Iteration 1598 training loss: 0.3980062267703606, test loss: 0.7328363555135842\n",
      "Iteration 1599 training loss: 0.3979255618118488, test loss: 0.7328526611699744\n",
      "Iteration 1600 training loss: 0.39784493314135155, test loss: 0.7328690439271016\n",
      "Iteration 1601 training loss: 0.39776434072885675, test loss: 0.7328853775243238\n",
      "Iteration 1602 training loss: 0.3976837845443934, test loss: 0.732901780454663\n",
      "Iteration 1603 training loss: 0.3976032645580404, test loss: 0.7329181413334417\n",
      "Iteration 1604 training loss: 0.3975227807399184, test loss: 0.732934564642696\n",
      "Iteration 1605 training loss: 0.39744233306019716, test loss: 0.732950952194118\n",
      "Iteration 1606 training loss: 0.39736192148908805, test loss: 0.7329673960456907\n",
      "Iteration 1607 training loss: 0.397281545996851, test loss: 0.7329838097063507\n",
      "Iteration 1608 training loss: 0.39720120655378743, test loss: 0.7330002742258592\n",
      "Iteration 1609 training loss: 0.39712090313024684, test loss: 0.733016713473457\n",
      "Iteration 1610 training loss: 0.39704063569662035, test loss: 0.7330331987528176\n",
      "Iteration 1611 training loss: 0.3969604042233463, test loss: 0.7330496631021683\n",
      "Iteration 1612 training loss: 0.3968802086809046, test loss: 0.7330661692032869\n",
      "Iteration 1613 training loss: 0.3968000490398222, test loss: 0.7330826582027096\n",
      "Iteration 1614 training loss: 0.39671992527066724, test loss: 0.7330991851608175\n",
      "Iteration 1615 training loss: 0.39663983734405434, test loss: 0.7331156983888677\n",
      "Iteration 1616 training loss: 0.39655978523063967, test loss: 0.7331322462155285\n",
      "Iteration 1617 training loss: 0.3964797689011249, test loss: 0.7331487832780459\n",
      "Iteration 1618 training loss: 0.3963997883262535, test loss: 0.7331653519638651\n",
      "Iteration 1619 training loss: 0.39631984347681387, test loss: 0.7331819124913078\n",
      "Iteration 1620 training loss: 0.3962399343236361, test loss: 0.7331985020083722\n",
      "Iteration 1621 training loss: 0.3961600608375948, test loss: 0.7332150856534113\n",
      "Iteration 1622 training loss: 0.3960802229896062, test loss: 0.7332316959574829\n",
      "Iteration 1623 training loss: 0.3960004207506307, test loss: 0.7332483023928332\n",
      "Iteration 1624 training loss: 0.3959206540916698, test loss: 0.7332649334253181\n",
      "Iteration 1625 training loss: 0.3958409229837692, test loss: 0.7332815623417855\n",
      "Iteration 1626 training loss: 0.3957612273980156, test loss: 0.7332982140315011\n",
      "Iteration 1627 training loss: 0.3956815673055394, test loss: 0.7333148651362246\n",
      "Iteration 1628 training loss: 0.3956019426775116, test loss: 0.7333315374009839\n",
      "Iteration 1629 training loss: 0.3955223534851468, test loss: 0.7333482104158531\n",
      "Iteration 1630 training loss: 0.3954427996997004, test loss: 0.7333649031638809\n",
      "Iteration 1631 training loss: 0.3953632812924702, test loss: 0.7333815978241152\n",
      "Iteration 1632 training loss: 0.3952837982347954, test loss: 0.7333983109553168\n",
      "Iteration 1633 training loss: 0.3952043504980572, test loss: 0.7334150270081896\n",
      "Iteration 1634 training loss: 0.39512493805367743, test loss: 0.7334317604152806\n",
      "Iteration 1635 training loss: 0.39504556087312015, test loss: 0.7334484976189722\n",
      "Iteration 1636 training loss: 0.39496621892788997, test loss: 0.733465251188489\n",
      "Iteration 1637 training loss: 0.39488691218953303, test loss: 0.7334820093110604\n",
      "Iteration 1638 training loss: 0.3948076406296361, test loss: 0.7334987829242579\n",
      "Iteration 1639 training loss: 0.39472840421982697, test loss: 0.7335155617427281\n",
      "Iteration 1640 training loss: 0.3946492029317741, test loss: 0.7335323552763803\n",
      "Iteration 1641 training loss: 0.3945700367371867, test loss: 0.7335491545759024\n",
      "Iteration 1642 training loss: 0.3944909056078143, test loss: 0.7335659679030123\n",
      "Iteration 1643 training loss: 0.3944118095154471, test loss: 0.7335827874761339\n",
      "Iteration 1644 training loss: 0.3943327484319151, test loss: 0.7335996204665628\n",
      "Iteration 1645 training loss: 0.39425372232908906, test loss: 0.7336164601125644\n",
      "Iteration 1646 training loss: 0.39417473117887936, test loss: 0.7336333126335918\n",
      "Iteration 1647 training loss: 0.3940957749532365, test loss: 0.733650172157896\n",
      "Iteration 1648 training loss: 0.3940168536241508, test loss: 0.7336670440747112\n",
      "Iteration 1649 training loss: 0.39393796716365215, test loss: 0.7336839232883534\n",
      "Iteration 1650 training loss: 0.3938591155438103, test loss: 0.7337008144644926\n",
      "Iteration 1651 training loss: 0.39378029873673437, test loss: 0.7337177131836483\n",
      "Iteration 1652 training loss: 0.3937015167145728, test loss: 0.7337346234813791\n",
      "Iteration 1653 training loss: 0.3936227694495136, test loss: 0.7337515415269406\n",
      "Iteration 1654 training loss: 0.39354405691378347, test loss: 0.7337684708075991\n",
      "Iteration 1655 training loss: 0.3934653790796488, test loss: 0.733785408004799\n",
      "Iteration 1656 training loss: 0.39338673591941437, test loss: 0.7338023561290874\n",
      "Iteration 1657 training loss: 0.3933081274054243, test loss: 0.7338193123071607\n",
      "Iteration 1658 training loss: 0.39322955351006106, test loss: 0.7338362791354078\n",
      "Iteration 1659 training loss: 0.3931510142057461, test loss: 0.7338532541272906\n",
      "Iteration 1660 training loss: 0.39307250946493916, test loss: 0.7338702395196784\n",
      "Iteration 1661 training loss: 0.39299403926013865, test loss: 0.7338872331617394\n",
      "Iteration 1662 training loss: 0.39291560356388117, test loss: 0.7339042369785024\n",
      "Iteration 1663 training loss: 0.39283720234874164, test loss: 0.7339212491103012\n",
      "Iteration 1664 training loss: 0.39275883558733304, test loss: 0.733938271211899\n",
      "Iteration 1665 training loss: 0.3926805032523065, test loss: 0.7339553016759723\n",
      "Iteration 1666 training loss: 0.392602205316351, test loss: 0.7339723419232389\n",
      "Iteration 1667 training loss: 0.39252394175219335, test loss: 0.7339893905649073\n",
      "Iteration 1668 training loss: 0.39244571253259813, test loss: 0.7340064488191812\n",
      "Iteration 1669 training loss: 0.39236751763036776, test loss: 0.7340235154863781\n",
      "Iteration 1670 training loss: 0.3922893570183417, test loss: 0.7340405916096137\n",
      "Iteration 1671 training loss: 0.3922112306693974, test loss: 0.7340576761527305\n",
      "Iteration 1672 training loss: 0.3921331385564493, test loss: 0.7340747700075928\n",
      "Iteration 1673 training loss: 0.39205508065244926, test loss: 0.7340918722793423\n",
      "Iteration 1674 training loss: 0.3919770569303862, test loss: 0.7341089837292897\n",
      "Iteration 1675 training loss: 0.39189906736328617, test loss: 0.7341261035845807\n",
      "Iteration 1676 training loss: 0.3918211119242121, test loss: 0.7341432324939339\n",
      "Iteration 1677 training loss: 0.39174319058626383, test loss: 0.7341603697897616\n",
      "Iteration 1678 training loss: 0.391665303322578, test loss: 0.7341775160237617\n",
      "Iteration 1679 training loss: 0.39158745010632784, test loss: 0.7341946706191075\n",
      "Iteration 1680 training loss: 0.3915096309107233, test loss: 0.7342118340439648\n",
      "Iteration 1681 training loss: 0.39143184570901074, test loss: 0.734229005799706\n",
      "Iteration 1682 training loss: 0.39135409447447284, test loss: 0.7342461862826407\n",
      "Iteration 1683 training loss: 0.3912763771804287, test loss: 0.7342633750614699\n",
      "Iteration 1684 training loss: 0.39119869380023364, test loss: 0.7342805724707454\n",
      "Iteration 1685 training loss: 0.39112104430727895, test loss: 0.734297778137097\n",
      "Iteration 1686 training loss: 0.3910434286749921, test loss: 0.7343149923420449\n",
      "Iteration 1687 training loss: 0.3909658468768365, test loss: 0.734332214762029\n",
      "Iteration 1688 training loss: 0.3908882988863113, test loss: 0.7343494456330727\n",
      "Iteration 1689 training loss: 0.39081078467695163, test loss: 0.7343666846744152\n",
      "Iteration 1690 training loss: 0.39073330422232794, test loss: 0.7343839320830833\n",
      "Iteration 1691 training loss: 0.3906558574960466, test loss: 0.7344011876150703\n",
      "Iteration 1692 training loss: 0.39057844447174916, test loss: 0.7344184514340104\n",
      "Iteration 1693 training loss: 0.39050106512311294, test loss: 0.7344357233274396\n",
      "Iteration 1694 training loss: 0.39042371942385035, test loss: 0.734453003430425\n",
      "Iteration 1695 training loss: 0.390346407347709, test loss: 0.7344702915575587\n",
      "Iteration 1696 training loss: 0.39026912886847165, test loss: 0.7344875878194946\n",
      "Iteration 1697 training loss: 0.3901918839599564, test loss: 0.7345048920540186\n",
      "Iteration 1698 training loss: 0.3901146725960159, test loss: 0.7345222043509434\n",
      "Iteration 1699 training loss: 0.39003749475053795, test loss: 0.7345395245679278\n",
      "Iteration 1700 training loss: 0.3899603503974451, test loss: 0.734556852777013\n",
      "Iteration 1701 training loss: 0.38988323951069465, test loss: 0.7345741888528778\n",
      "Iteration 1702 training loss: 0.38980616206427837, test loss: 0.7345915328524256\n",
      "Iteration 1703 training loss: 0.38972911803222277, test loss: 0.7346088846649067\n",
      "Iteration 1704 training loss: 0.38965210738858874, test loss: 0.7346262443343464\n",
      "Iteration 1705 training loss: 0.38957513010747163, test loss: 0.7346436117624656\n",
      "Iteration 1706 training loss: 0.3894981861630009, test loss: 0.7346609869823477\n",
      "Iteration 1707 training loss: 0.38942127552934047, test loss: 0.7346783699063831\n",
      "Iteration 1708 training loss: 0.38934439818068817, test loss: 0.734695760558373\n",
      "Iteration 1709 training loss: 0.38926755409127595, test loss: 0.734713158859834\n",
      "Iteration 1710 training loss: 0.38919074323536984, test loss: 0.7347305648267031\n",
      "Iteration 1711 training loss: 0.38911396558726963, test loss: 0.7347479783883037\n",
      "Iteration 1712 training loss: 0.38903722112130906, test loss: 0.734765399553922\n",
      "Iteration 1713 training loss: 0.3889605098118554, test loss: 0.7347828282595584\n",
      "Iteration 1714 training loss: 0.38888383163330975, test loss: 0.7348002645088842\n",
      "Iteration 1715 training loss: 0.3888071865601067, test loss: 0.7348177082436119\n",
      "Iteration 1716 training loss: 0.3887305745667143, test loss: 0.7348351594626816\n",
      "Iteration 1717 training loss: 0.388653995627634, test loss: 0.7348526181126946\n",
      "Iteration 1718 training loss: 0.3885774497174007, test loss: 0.734870084188613\n",
      "Iteration 1719 training loss: 0.3885009368105825, test loss: 0.7348875576412232\n",
      "Iteration 1720 training loss: 0.38842445688178057, test loss: 0.7349050384621516\n",
      "Iteration 1721 training loss: 0.3883480099056294, test loss: 0.73492252660577\n",
      "Iteration 1722 training loss: 0.3882715958567962, test loss: 0.7349400220609157\n",
      "Iteration 1723 training loss: 0.3881952147099815, test loss: 0.7349575247850341\n",
      "Iteration 1724 training loss: 0.38811886643991833, test loss: 0.7349750347646378\n",
      "Iteration 1725 training loss: 0.38804255102137275, test loss: 0.7349925519598116\n",
      "Iteration 1726 training loss: 0.3879662684291434, test loss: 0.7350100763551376\n",
      "Iteration 1727 training loss: 0.3878900186380616, test loss: 0.7350276079129681\n",
      "Iteration 1728 training loss: 0.38781380162299134, test loss: 0.7350451466162903\n",
      "Iteration 1729 training loss: 0.38773761735882906, test loss: 0.7350626924294095\n",
      "Iteration 1730 training loss: 0.38766146582050337, test loss: 0.7350802453340007\n",
      "Iteration 1731 training loss: 0.3875853469829757, test loss: 0.7350978052960551\n",
      "Iteration 1732 training loss: 0.38750926082123915, test loss: 0.7351153722961754\n",
      "Iteration 1733 training loss: 0.38743320731031966, test loss: 0.7351329463018106\n",
      "Iteration 1734 training loss: 0.3873571864252747, test loss: 0.7351505272926951\n",
      "Iteration 1735 training loss: 0.3872811981411942, test loss: 0.7351681152375417\n",
      "Iteration 1736 training loss: 0.3872052424331999, test loss: 0.7351857101153882\n",
      "Iteration 1737 training loss: 0.38712931927644545, test loss: 0.7352033118960476\n",
      "Iteration 1738 training loss: 0.38705342864611636, test loss: 0.7352209205580059\n",
      "Iteration 1739 training loss: 0.38697757051742987, test loss: 0.7352385360720358\n",
      "Iteration 1740 training loss: 0.3869017448656348, test loss: 0.7352561584161946\n",
      "Iteration 1741 training loss: 0.38682595166601186, test loss: 0.7352737875620974\n",
      "Iteration 1742 training loss: 0.3867501908938731, test loss: 0.7352914234874733\n",
      "Iteration 1743 training loss: 0.3866744625245621, test loss: 0.7353090661646806\n",
      "Iteration 1744 training loss: 0.3865987665334537, test loss: 0.7353267155712075\n",
      "Iteration 1745 training loss: 0.3865231028959545, test loss: 0.7353443716800689\n",
      "Iteration 1746 training loss: 0.3864474715875017, test loss: 0.7353620344685851\n",
      "Iteration 1747 training loss: 0.38637187258356437, test loss: 0.7353797039103557\n",
      "Iteration 1748 training loss: 0.38629630585964225, test loss: 0.7353973799825934\n",
      "Iteration 1749 training loss: 0.3862207713912663, test loss: 0.7354150626594212\n",
      "Iteration 1750 training loss: 0.38614526915399855, test loss: 0.7354327519179951\n",
      "Iteration 1751 training loss: 0.3860697991234318, test loss: 0.7354504477329101\n",
      "Iteration 1752 training loss: 0.38599436127518966, test loss: 0.7354681500813065\n",
      "Iteration 1753 training loss: 0.38591895558492667, test loss: 0.7354858589382076\n",
      "Iteration 1754 training loss: 0.3858435820283281, test loss: 0.7355035742807737\n",
      "Iteration 1755 training loss: 0.3857682405811097, test loss: 0.7355212960844186\n",
      "Iteration 1756 training loss: 0.38569293121901804, test loss: 0.7355390243263519\n",
      "Iteration 1757 training loss: 0.3856176539178299, test loss: 0.7355567589823455\n",
      "Iteration 1758 training loss: 0.38554240865335276, test loss: 0.7355745000296826\n",
      "Iteration 1759 training loss: 0.38546719540142443, test loss: 0.7355922474444666\n",
      "Iteration 1760 training loss: 0.3853920141379129, test loss: 0.7356100012040735\n",
      "Iteration 1761 training loss: 0.3853168648387166, test loss: 0.7356277612849151\n",
      "Iteration 1762 training loss: 0.38524174747976403, test loss: 0.7356455276644762\n",
      "Iteration 1763 training loss: 0.38516666203701366, test loss: 0.735663300319458\n",
      "Iteration 1764 training loss: 0.3850916084864545, test loss: 0.7356810792274672\n",
      "Iteration 1765 training loss: 0.38501658680410505, test loss: 0.7356988643654772\n",
      "Iteration 1766 training loss: 0.3849415969660138, test loss: 0.7357166557112269\n",
      "Iteration 1767 training loss: 0.38486663894825945, test loss: 0.7357344532419471\n",
      "Iteration 1768 training loss: 0.38479171272695023, test loss: 0.7357522569355192\n",
      "Iteration 1769 training loss: 0.384716818278224, test loss: 0.7357700667694177\n",
      "Iteration 1770 training loss: 0.3846419555782486, test loss: 0.735787882721673\n",
      "Iteration 1771 training loss: 0.38456712460322107, test loss: 0.7358057047699937\n",
      "Iteration 1772 training loss: 0.38449232532936845, test loss: 0.7358235328925631\n",
      "Iteration 1773 training loss: 0.384417557732947, test loss: 0.7358413670673151\n",
      "Iteration 1774 training loss: 0.38434282179024226, test loss: 0.7358592072725906\n",
      "Iteration 1775 training loss: 0.38426811747756945, test loss: 0.7358770534865398\n",
      "Iteration 1776 training loss: 0.3841934447712729, test loss: 0.7358949056876646\n",
      "Iteration 1777 training loss: 0.38411880364772627, test loss: 0.7359127638543244\n",
      "Iteration 1778 training loss: 0.3840441940833321, test loss: 0.7359306279651845\n",
      "Iteration 1779 training loss: 0.38396961605452257, test loss: 0.7359484979988068\n",
      "Iteration 1780 training loss: 0.3838950695377583, test loss: 0.7359663739340212\n",
      "Iteration 1781 training loss: 0.38382055450952945, test loss: 0.7359842557495876\n",
      "Iteration 1782 training loss: 0.3837460709463546, test loss: 0.7360021434245007\n",
      "Iteration 1783 training loss: 0.38367161882478173, test loss: 0.7360200369377131\n",
      "Iteration 1784 training loss: 0.3835971981213871, test loss: 0.7360379362683858\n",
      "Iteration 1785 training loss: 0.383522808812776, test loss: 0.7360558413956587\n",
      "Iteration 1786 training loss: 0.38344845087558244, test loss: 0.736073752298859\n",
      "Iteration 1787 training loss: 0.38337412428646883, test loss: 0.736091668957311\n",
      "Iteration 1788 training loss: 0.38329982902212645, test loss: 0.7361095913505067\n",
      "Iteration 1789 training loss: 0.3832255650592748, test loss: 0.7361275194579514\n",
      "Iteration 1790 training loss: 0.3831513323746621, test loss: 0.7361454532593017\n",
      "Iteration 1791 training loss: 0.38307713094506457, test loss: 0.7361633927342403\n",
      "Iteration 1792 training loss: 0.38300296074728724, test loss: 0.7361813378625879\n",
      "Iteration 1793 training loss: 0.382928821758163, test loss: 0.7361992886242006\n",
      "Iteration 1794 training loss: 0.3828547139545533, test loss: 0.7362172449990629\n",
      "Iteration 1795 training loss: 0.3827806373133475, test loss: 0.7362352069672021\n",
      "Iteration 1796 training loss: 0.382706591811463, test loss: 0.7362531745087646\n",
      "Iteration 1797 training loss: 0.38263257742584555, test loss: 0.7362711476039459\n",
      "Iteration 1798 training loss: 0.38255859413346865, test loss: 0.7362891262330532\n",
      "Iteration 1799 training loss: 0.3824846419113338, test loss: 0.7363071103764482\n",
      "Iteration 1800 training loss: 0.3824107207364703, test loss: 0.7363251000145979\n",
      "Iteration 1801 training loss: 0.38233683058593537, test loss: 0.7363430951280273\n",
      "Iteration 1802 training loss: 0.3822629714368139, test loss: 0.7363610956973612\n",
      "Iteration 1803 training loss: 0.3821891432662186, test loss: 0.7363791017032861\n",
      "Iteration 1804 training loss: 0.38211534605128966, test loss: 0.736397113126584\n",
      "Iteration 1805 training loss: 0.3820415797691949, test loss: 0.7364151299480999\n",
      "Iteration 1806 training loss: 0.38196784439712983, test loss: 0.7364331521487709\n",
      "Iteration 1807 training loss: 0.3818941399123172, test loss: 0.7364511797096001\n",
      "Iteration 1808 training loss: 0.38182046629200744, test loss: 0.7364692126116777\n",
      "Iteration 1809 training loss: 0.38174682351347816, test loss: 0.7364872508361614\n",
      "Iteration 1810 training loss: 0.3816732115540342, test loss: 0.7365052943642945\n",
      "Iteration 1811 training loss: 0.38159963039100797, test loss: 0.7365233431773877\n",
      "Iteration 1812 training loss: 0.3815260800017588, test loss: 0.7365413972568345\n",
      "Iteration 1813 training loss: 0.3814525603636733, test loss: 0.7365594565840969\n",
      "Iteration 1814 training loss: 0.38137907145416516, test loss: 0.7365775211407183\n",
      "Iteration 1815 training loss: 0.3813056132506751, test loss: 0.7365955909083098\n",
      "Iteration 1816 training loss: 0.38123218573067086, test loss: 0.7366136658685621\n",
      "Iteration 1817 training loss: 0.3811587888716469, test loss: 0.7366317460032349\n",
      "Iteration 1818 training loss: 0.381085422651125, test loss: 0.736649831294164\n",
      "Iteration 1819 training loss: 0.38101208704665335, test loss: 0.7366679217232555\n",
      "Iteration 1820 training loss: 0.38093878203580694, test loss: 0.7366860172724903\n",
      "Iteration 1821 training loss: 0.380865507596188, test loss: 0.736704117923918\n",
      "Iteration 1822 training loss: 0.38079226370542474, test loss: 0.7367222236596631\n",
      "Iteration 1823 training loss: 0.38071905034117226, test loss: 0.7367403344619181\n",
      "Iteration 1824 training loss: 0.3806458674811125, test loss: 0.7367584503129485\n",
      "Iteration 1825 training loss: 0.38057271510295343, test loss: 0.7367765711950884\n",
      "Iteration 1826 training loss: 0.3804995931844298, test loss: 0.7367946970907432\n",
      "Iteration 1827 training loss: 0.38042650170330267, test loss: 0.7368128279823866\n",
      "Iteration 1828 training loss: 0.3803534406373595, test loss: 0.7368309638525622\n",
      "Iteration 1829 training loss: 0.380280409964414, test loss: 0.7368491046838823\n",
      "Iteration 1830 training loss: 0.3802074096623061, test loss: 0.7368672504590275\n",
      "Iteration 1831 training loss: 0.38013443970890204, test loss: 0.7368854011607465\n",
      "Iteration 1832 training loss: 0.38006150008209416, test loss: 0.7369035567718558\n",
      "Iteration 1833 training loss: 0.3799885907598009, test loss: 0.7369217172752388\n",
      "Iteration 1834 training loss: 0.3799157117199668, test loss: 0.7369398826538464\n",
      "Iteration 1835 training loss: 0.37984286294056224, test loss: 0.7369580528906958\n",
      "Iteration 1836 training loss: 0.37977004439958373, test loss: 0.7369762279688703\n",
      "Iteration 1837 training loss: 0.3796972560750536, test loss: 0.7369944078715199\n",
      "Iteration 1838 training loss: 0.37962449794502007, test loss: 0.7370125925818588\n",
      "Iteration 1839 training loss: 0.37955176998755713, test loss: 0.7370307820831677\n",
      "Iteration 1840 training loss: 0.3794790721807644, test loss: 0.7370489763587919\n",
      "Iteration 1841 training loss: 0.37940640450276747, test loss: 0.7370671753921402\n",
      "Iteration 1842 training loss: 0.3793337669317174, test loss: 0.737085379166687\n",
      "Iteration 1843 training loss: 0.3792611594457909, test loss: 0.7371035876659696\n",
      "Iteration 1844 training loss: 0.37918858202319017, test loss: 0.7371218008735895\n",
      "Iteration 1845 training loss: 0.37911603464214294, test loss: 0.7371400187732102\n",
      "Iteration 1846 training loss: 0.3790435172809026, test loss: 0.7371582413485599\n",
      "Iteration 1847 training loss: 0.3789710299177476, test loss: 0.7371764685834274\n",
      "Iteration 1848 training loss: 0.3788985725309819, test loss: 0.7371947004616652\n",
      "Iteration 1849 training loss: 0.3788261450989349, test loss: 0.7372129369671866\n",
      "Iteration 1850 training loss: 0.37875374759996117, test loss: 0.7372311780839672\n",
      "Iteration 1851 training loss: 0.37868138001244045, test loss: 0.7372494237960435\n",
      "Iteration 1852 training loss: 0.37860904231477766, test loss: 0.7372676740875131\n",
      "Iteration 1853 training loss: 0.3785367344854027, test loss: 0.737285928942534\n",
      "Iteration 1854 training loss: 0.37846445650277116, test loss: 0.7373041883453244\n",
      "Iteration 1855 training loss: 0.3783922083453628, test loss: 0.7373224522801631\n",
      "Iteration 1856 training loss: 0.3783199899916828, test loss: 0.7373407207313878\n",
      "Iteration 1857 training loss: 0.3782478014202614, test loss: 0.7373589936833962\n",
      "Iteration 1858 training loss: 0.3781756426096534, test loss: 0.737377271120645\n",
      "Iteration 1859 training loss: 0.37810351353843874, test loss: 0.737395553027649\n",
      "Iteration 1860 training loss: 0.3780314141852219, test loss: 0.7374138393889824\n",
      "Iteration 1861 training loss: 0.37795934452863233, test loss: 0.7374321301892773\n",
      "Iteration 1862 training loss: 0.3778873045473239, test loss: 0.7374504254132231\n",
      "Iteration 1863 training loss: 0.37781529421997545, test loss: 0.7374687250455682\n",
      "Iteration 1864 training loss: 0.37774331352529017, test loss: 0.7374870290711161\n",
      "Iteration 1865 training loss: 0.377671362441996, test loss: 0.7375053374747293\n",
      "Iteration 1866 training loss: 0.37759944094884523, test loss: 0.7375236502413267\n",
      "Iteration 1867 training loss: 0.3775275490246148, test loss: 0.7375419673558827\n",
      "Iteration 1868 training loss: 0.3774556866481059, test loss: 0.7375602888034288\n",
      "Iteration 1869 training loss: 0.37738385379814426, test loss: 0.7375786145690518\n",
      "Iteration 1870 training loss: 0.3773120504535798, test loss: 0.7375969446378945\n",
      "Iteration 1871 training loss: 0.3772402765932869, test loss: 0.7376152789951546\n",
      "Iteration 1872 training loss: 0.377168532196164, test loss: 0.7376336176260858\n",
      "Iteration 1873 training loss: 0.37709681724113375, test loss: 0.7376519605159952\n",
      "Iteration 1874 training loss: 0.37702513170714314, test loss: 0.7376703076502455\n",
      "Iteration 1875 training loss: 0.3769534755731632, test loss: 0.7376886590142531\n",
      "Iteration 1876 training loss: 0.376881848818189, test loss: 0.7377070145934889\n",
      "Iteration 1877 training loss: 0.3768102514212394, test loss: 0.737725374373477\n",
      "Iteration 1878 training loss: 0.3767386833613578, test loss: 0.7377437383397949\n",
      "Iteration 1879 training loss: 0.3766671446176109, test loss: 0.7377621064780736\n",
      "Iteration 1880 training loss: 0.3765956351690898, test loss: 0.7377804787739972\n",
      "Iteration 1881 training loss: 0.3765241549949092, test loss: 0.7377988552133017\n",
      "Iteration 1882 training loss: 0.37645270407420756, test loss: 0.7378172357817766\n",
      "Iteration 1883 training loss: 0.3763812823861474, test loss: 0.7378356204652624\n",
      "Iteration 1884 training loss: 0.3763098899099145, test loss: 0.7378540092496524\n",
      "Iteration 1885 training loss: 0.37623852662471874, test loss: 0.737872402120891\n",
      "Iteration 1886 training loss: 0.37616719250979336, test loss: 0.7378907990649742\n",
      "Iteration 1887 training loss: 0.3760958875443953, test loss: 0.7379092000679496\n",
      "Iteration 1888 training loss: 0.3760246117078052, test loss: 0.7379276051159146\n",
      "Iteration 1889 training loss: 0.37595336497932696, test loss: 0.7379460141950182\n",
      "Iteration 1890 training loss: 0.3758821473382879, test loss: 0.7379644272914596\n",
      "Iteration 1891 training loss: 0.3758109587640391, test loss: 0.7379828443914879\n",
      "Iteration 1892 training loss: 0.3757397992359546, test loss: 0.7380012654814024\n",
      "Iteration 1893 training loss: 0.37566866873343213, test loss: 0.7380196905475521\n",
      "Iteration 1894 training loss: 0.37559756723589244, test loss: 0.7380381195763351\n",
      "Iteration 1895 training loss: 0.3755264947227799, test loss: 0.7380565525541994\n",
      "Iteration 1896 training loss: 0.3754554511735616, test loss: 0.7380749894676413\n",
      "Iteration 1897 training loss: 0.37538443656772813, test loss: 0.738093430303206\n",
      "Iteration 1898 training loss: 0.37531345088479334, test loss: 0.7381118750474874\n",
      "Iteration 1899 training loss: 0.3752424941042936, test loss: 0.7381303236871279\n",
      "Iteration 1900 training loss: 0.37517156620578906, test loss: 0.7381487762088174\n",
      "Iteration 1901 training loss: 0.3751006671688622, test loss: 0.7381672325992941\n",
      "Iteration 1902 training loss: 0.3750297969731189, test loss: 0.7381856928453434\n",
      "Iteration 1903 training loss: 0.37495895559818787, test loss: 0.7382041569337984\n",
      "Iteration 1904 training loss: 0.3748881430237206, test loss: 0.7382226248515393\n",
      "Iteration 1905 training loss: 0.3748173592293914, test loss: 0.7382410965854931\n",
      "Iteration 1906 training loss: 0.3747466041948977, test loss: 0.7382595721226337\n",
      "Iteration 1907 training loss: 0.37467587789995926, test loss: 0.7382780514499816\n",
      "Iteration 1908 training loss: 0.37460518032431883, test loss: 0.7382965345546032\n",
      "Iteration 1909 training loss: 0.3745345114477418, test loss: 0.738315021423611\n",
      "Iteration 1910 training loss: 0.3744638712500161, test loss: 0.7383335120441638\n",
      "Iteration 1911 training loss: 0.37439325971095244, test loss: 0.7383520064034657\n",
      "Iteration 1912 training loss: 0.37432267681038395, test loss: 0.7383705044887662\n",
      "Iteration 1913 training loss: 0.3742521225281662, test loss: 0.7383890062873602\n",
      "Iteration 1914 training loss: 0.3741815968441775, test loss: 0.7384075117865873\n",
      "Iteration 1915 training loss: 0.3741110997383185, test loss: 0.7384260209738325\n",
      "Iteration 1916 training loss: 0.37404063119051206, test loss: 0.7384445338365244\n",
      "Iteration 1917 training loss: 0.37397019118070374, test loss: 0.7384630503621374\n",
      "Iteration 1918 training loss: 0.37389977968886107, test loss: 0.7384815705381886\n",
      "Iteration 1919 training loss: 0.37382939669497417, test loss: 0.7385000943522402\n",
      "Iteration 1920 training loss: 0.3737590421790553, test loss: 0.7385186217918973\n",
      "Iteration 1921 training loss: 0.37368871612113885, test loss: 0.7385371528448094\n",
      "Iteration 1922 training loss: 0.3736184185012815, test loss: 0.7385556874986685\n",
      "Iteration 1923 training loss: 0.3735481492995619, test loss: 0.7385742257412107\n",
      "Iteration 1924 training loss: 0.37347790849608115, test loss: 0.7385927675602146\n",
      "Iteration 1925 training loss: 0.3734076960709618, test loss: 0.7386113129435015\n",
      "Iteration 1926 training loss: 0.3733375120043491, test loss: 0.7386298618789354\n",
      "Iteration 1927 training loss: 0.37326735627640983, test loss: 0.7386484143544229\n",
      "Iteration 1928 training loss: 0.3731972288673328, test loss: 0.7386669703579124\n",
      "Iteration 1929 training loss: 0.37312712975732876, test loss: 0.7386855298773948\n",
      "Iteration 1930 training loss: 0.3730570589266304, test loss: 0.7387040929009026\n",
      "Iteration 1931 training loss: 0.3729870163554921, test loss: 0.7387226594165097\n",
      "Iteration 1932 training loss: 0.37291700202419026, test loss: 0.7387412294123317\n",
      "Iteration 1933 training loss: 0.3728470159130226, test loss: 0.7387598028765254\n",
      "Iteration 1934 training loss: 0.3727770580023089, test loss: 0.738778379797289\n",
      "Iteration 1935 training loss: 0.3727071282723908, test loss: 0.7387969601628611\n",
      "Iteration 1936 training loss: 0.37263722670363103, test loss: 0.7388155439615213\n",
      "Iteration 1937 training loss: 0.3725673532764144, test loss: 0.7388341311815895\n",
      "Iteration 1938 training loss: 0.37249750797114695, test loss: 0.7388527218114262\n",
      "Iteration 1939 training loss: 0.3724276907682565, test loss: 0.738871315839432\n",
      "Iteration 1940 training loss: 0.37235790164819227, test loss: 0.7388899132540475\n",
      "Iteration 1941 training loss: 0.372288140591425, test loss: 0.7389085140437529\n",
      "Iteration 1942 training loss: 0.37221840757844665, test loss: 0.7389271181970685\n",
      "Iteration 1943 training loss: 0.37214870258977084, test loss: 0.7389457257025532\n",
      "Iteration 1944 training loss: 0.37207902560593237, test loss: 0.7389643365488068\n",
      "Iteration 1945 training loss: 0.37200937660748745, test loss: 0.7389829507244661\n",
      "Iteration 1946 training loss: 0.37193975557501335, test loss: 0.7390015682182082\n",
      "Iteration 1947 training loss: 0.3718701624891088, test loss: 0.739020189018749\n",
      "Iteration 1948 training loss: 0.3718005973303937, test loss: 0.7390388131148425\n",
      "Iteration 1949 training loss: 0.3717310600795091, test loss: 0.7390574404952811\n",
      "Iteration 1950 training loss: 0.37166155071711715, test loss: 0.739076071148896\n",
      "Iteration 1951 training loss: 0.3715920692239012, test loss: 0.7390947050645559\n",
      "Iteration 1952 training loss: 0.37152261558056543, test loss: 0.7391133422311676\n",
      "Iteration 1953 training loss: 0.3714531897678352, test loss: 0.739131982637676\n",
      "Iteration 1954 training loss: 0.371383791766457, test loss: 0.7391506262730632\n",
      "Iteration 1955 training loss: 0.371314421557198, test loss: 0.739169273126349\n",
      "Iteration 1956 training loss: 0.3712450791208466, test loss: 0.73918792318659\n",
      "Iteration 1957 training loss: 0.3711757644382117, test loss: 0.7392065764428803\n",
      "Iteration 1958 training loss: 0.3711064774901233, test loss: 0.7392252328843514\n",
      "Iteration 1959 training loss: 0.3710372182574323, test loss: 0.7392438925001703\n",
      "Iteration 1960 training loss: 0.3709679867210101, test loss: 0.7392625552795419\n",
      "Iteration 1961 training loss: 0.3708987828617491, test loss: 0.7392812212117067\n",
      "Iteration 1962 training loss: 0.3708296066605623, test loss: 0.739299890285942\n",
      "Iteration 1963 training loss: 0.3707604580983834, test loss: 0.7393185624915611\n",
      "Iteration 1964 training loss: 0.3706913371561667, test loss: 0.7393372378179135\n",
      "Iteration 1965 training loss: 0.37062224381488723, test loss: 0.7393559162543841\n",
      "Iteration 1966 training loss: 0.3705531780555403, test loss: 0.7393745977903939\n",
      "Iteration 1967 training loss: 0.3704841398591422, test loss: 0.739393282415399\n",
      "Iteration 1968 training loss: 0.37041512920672925, test loss: 0.7394119701188915\n",
      "Iteration 1969 training loss: 0.37034614607935873, test loss: 0.7394306608903982\n",
      "Iteration 1970 training loss: 0.37027719045810803, test loss: 0.739449354719481\n",
      "Iteration 1971 training loss: 0.3702082623240749, test loss: 0.7394680515957374\n",
      "Iteration 1972 training loss: 0.3701393616583778, test loss: 0.7394867515087985\n",
      "Iteration 1973 training loss: 0.3700704884421551, test loss: 0.7395054544483312\n",
      "Iteration 1974 training loss: 0.3700016426565658, test loss: 0.7395241604040361\n",
      "Iteration 1975 training loss: 0.3699328242827891, test loss: 0.7395428693656487\n",
      "Iteration 1976 training loss: 0.36986403330202433, test loss: 0.7395615813229384\n",
      "Iteration 1977 training loss: 0.36979526969549115, test loss: 0.7395802962657085\n",
      "Iteration 1978 training loss: 0.36972653344442924, test loss: 0.7395990141837964\n",
      "Iteration 1979 training loss: 0.36965782453009854, test loss: 0.7396177350670736\n",
      "Iteration 1980 training loss: 0.3695891429337791, test loss: 0.7396364589054445\n",
      "Iteration 1981 training loss: 0.36952048863677095, test loss: 0.7396551856888475\n",
      "Iteration 1982 training loss: 0.36945186162039423, test loss: 0.739673915407254\n",
      "Iteration 1983 training loss: 0.36938326186598913, test loss: 0.7396926480506693\n",
      "Iteration 1984 training loss: 0.3693146893549157, test loss: 0.7397113836091307\n",
      "Iteration 1985 training loss: 0.36924614406855394, test loss: 0.739730122072709\n",
      "Iteration 1986 training loss: 0.3691776259883039, test loss: 0.7397488634315083\n",
      "Iteration 1987 training loss: 0.3691091350955853, test loss: 0.7397676076756642\n",
      "Iteration 1988 training loss: 0.36904067137183794, test loss: 0.7397863547953458\n",
      "Iteration 1989 training loss: 0.36897223479852115, test loss: 0.739805104780754\n",
      "Iteration 1990 training loss: 0.36890382535711425, test loss: 0.739823857622122\n",
      "Iteration 1991 training loss: 0.36883544302911636, test loss: 0.7398426133097155\n",
      "Iteration 1992 training loss: 0.368767087796046, test loss: 0.7398613718338316\n",
      "Iteration 1993 training loss: 0.3686987596394418, test loss: 0.7398801331848\n",
      "Iteration 1994 training loss: 0.3686304585408616, test loss: 0.739898897352981\n",
      "Iteration 1995 training loss: 0.3685621844818833, test loss: 0.7399176643287678\n",
      "Iteration 1996 training loss: 0.3684939374441041, test loss: 0.7399364341025838\n",
      "Iteration 1997 training loss: 0.3684257174091407, test loss: 0.7399552066648848\n",
      "Iteration 1998 training loss: 0.36835752435862956, test loss: 0.7399739820061567\n",
      "Iteration 1999 training loss: 0.3682893582742265, test loss: 0.7399927601169174\n",
      "Iteration 2000 training loss: 0.3682212191376068, test loss: 0.7400115409877154\n",
      "Iteration 2001 training loss: 0.36815310693046527, test loss: 0.7400303246091298\n",
      "Iteration 2002 training loss: 0.36808502163451595, test loss: 0.7400491109717706\n",
      "Iteration 2003 training loss: 0.36801696323149236, test loss: 0.740067900066278\n",
      "Iteration 2004 training loss: 0.36794893170314746, test loss: 0.7400866918833232\n",
      "Iteration 2005 training loss: 0.36788092703125325, test loss: 0.7401054864136076\n",
      "Iteration 2006 training loss: 0.3678129491976012, test loss: 0.7401242836478624\n",
      "Iteration 2007 training loss: 0.36774499818400214, test loss: 0.7401430835768487\n",
      "Iteration 2008 training loss: 0.3676770739722857, test loss: 0.7401618861913585\n",
      "Iteration 2009 training loss: 0.3676091765443012, test loss: 0.7401806914822125\n",
      "Iteration 2010 training loss: 0.3675413058819166, test loss: 0.7401994994402618\n",
      "Iteration 2011 training loss: 0.3674734619670194, test loss: 0.7402183100563868\n",
      "Iteration 2012 training loss: 0.3674056447815161, test loss: 0.7402371233214975\n",
      "Iteration 2013 training loss: 0.36733785430733207, test loss: 0.740255939226533\n",
      "Iteration 2014 training loss: 0.3672700905264119, test loss: 0.7402747577624615\n",
      "Iteration 2015 training loss: 0.36720235342071916, test loss: 0.7402935789202809\n",
      "Iteration 2016 training loss: 0.3671346429722363, test loss: 0.7403124026910175\n",
      "Iteration 2017 training loss: 0.3670669591629648, test loss: 0.740331229065727\n",
      "Iteration 2018 training loss: 0.3669993019749249, test loss: 0.740350058035493\n",
      "Iteration 2019 training loss: 0.366931671390156, test loss: 0.7403688895914284\n",
      "Iteration 2020 training loss: 0.3668640673907162, test loss: 0.7403877237246748\n",
      "Iteration 2021 training loss: 0.3667964899586823, test loss: 0.7404065604264011\n",
      "Iteration 2022 training loss: 0.36672893907615006, test loss: 0.7404253996878059\n",
      "Iteration 2023 training loss: 0.36666141472523395, test loss: 0.740444241500115\n",
      "Iteration 2024 training loss: 0.3665939168880672, test loss: 0.7404630858545823\n",
      "Iteration 2025 training loss: 0.3665264455468017, test loss: 0.7404819327424906\n",
      "Iteration 2026 training loss: 0.3664590006836079, test loss: 0.7405007821551491\n",
      "Iteration 2027 training loss: 0.36639158228067525, test loss: 0.7405196340838958\n",
      "Iteration 2028 training loss: 0.36632419032021146, test loss: 0.7405384885200959\n",
      "Iteration 2029 training loss: 0.366256824784443, test loss: 0.7405573454551418\n",
      "Iteration 2030 training loss: 0.3661894856556148, test loss: 0.7405762048804543\n",
      "Iteration 2031 training loss: 0.36612217291599053, test loss: 0.7405950667874805\n",
      "Iteration 2032 training loss: 0.366054886547852, test loss: 0.7406139311676948\n",
      "Iteration 2033 training loss: 0.36598762653349987, test loss: 0.7406327980125992\n",
      "Iteration 2034 training loss: 0.36592039285525285, test loss: 0.7406516673137221\n",
      "Iteration 2035 training loss: 0.3658531854954486, test loss: 0.7406705390626193\n",
      "Iteration 2036 training loss: 0.36578600443644255, test loss: 0.7406894132508723\n",
      "Iteration 2037 training loss: 0.36571884966060897, test loss: 0.7407082898700911\n",
      "Iteration 2038 training loss: 0.3656517211503401, test loss: 0.7407271689119104\n",
      "Iteration 2039 training loss: 0.36558461888804683, test loss: 0.740746050367992\n",
      "Iteration 2040 training loss: 0.365517542856158, test loss: 0.7407649342300244\n",
      "Iteration 2041 training loss: 0.3654504930371209, test loss: 0.7407838204897219\n",
      "Iteration 2042 training loss: 0.36538346941340105, test loss: 0.7408027091388251\n",
      "Iteration 2043 training loss: 0.36531647196748174, test loss: 0.7408216001691\n",
      "Iteration 2044 training loss: 0.3652495006818652, test loss: 0.7408404935723397\n",
      "Iteration 2045 training loss: 0.36518255553907103, test loss: 0.7408593893403626\n",
      "Iteration 2046 training loss: 0.3651156365216373, test loss: 0.740878287465012\n",
      "Iteration 2047 training loss: 0.3650487436121202, test loss: 0.7408971879381584\n",
      "Iteration 2048 training loss: 0.36498187679309374, test loss: 0.7409160907516958\n",
      "Iteration 2049 training loss: 0.3649150360471501, test loss: 0.7409349958975459\n",
      "Iteration 2050 training loss: 0.36484822135689937, test loss: 0.7409539033676537\n",
      "Iteration 2051 training loss: 0.36478143270496977, test loss: 0.7409728131539908\n",
      "Iteration 2052 training loss: 0.36471467007400715, test loss: 0.7409917252485532\n",
      "Iteration 2053 training loss: 0.3646479334466757, test loss: 0.7410106396433623\n",
      "Iteration 2054 training loss: 0.3645812228056569, test loss: 0.741029556330464\n",
      "Iteration 2055 training loss: 0.36451453813365076, test loss: 0.7410484753019295\n",
      "Iteration 2056 training loss: 0.36444787941337464, test loss: 0.7410673965498545\n",
      "Iteration 2057 training loss: 0.3643812466275638, test loss: 0.7410863200663592\n",
      "Iteration 2058 training loss: 0.3643146397589714, test loss: 0.7411052458435888\n",
      "Iteration 2059 training loss: 0.36424805879036815, test loss: 0.7411241738737124\n",
      "Iteration 2060 training loss: 0.3641815037045426, test loss: 0.7411431041489237\n",
      "Iteration 2061 training loss: 0.364114974484301, test loss: 0.7411620366614409\n",
      "Iteration 2062 training loss: 0.36404847111246724, test loss: 0.7411809714035063\n",
      "Iteration 2063 training loss: 0.36398199357188277, test loss: 0.7411999083673854\n",
      "Iteration 2064 training loss: 0.3639155418454067, test loss: 0.7412188475453689\n",
      "Iteration 2065 training loss: 0.3638491159159158, test loss: 0.7412377889297711\n",
      "Iteration 2066 training loss: 0.36378271576630433, test loss: 0.7412567325129296\n",
      "Iteration 2067 training loss: 0.36371634137948405, test loss: 0.741275678287206\n",
      "Iteration 2068 training loss: 0.3636499927383843, test loss: 0.7412946262449854\n",
      "Iteration 2069 training loss: 0.36358366982595164, test loss: 0.7413135763786769\n",
      "Iteration 2070 training loss: 0.36351737262515055, test loss: 0.7413325286807124\n",
      "Iteration 2071 training loss: 0.36345110111896245, test loss: 0.7413514831435478\n",
      "Iteration 2072 training loss: 0.36338485529038655, test loss: 0.7413704397596612\n",
      "Iteration 2073 training loss: 0.36331863512243917, test loss: 0.7413893985215554\n",
      "Iteration 2074 training loss: 0.36325244059815404, test loss: 0.741408359421755\n",
      "Iteration 2075 training loss: 0.3631862717005822, test loss: 0.741427322452808\n",
      "Iteration 2076 training loss: 0.36312012841279206, test loss: 0.7414462876072857\n",
      "Iteration 2077 training loss: 0.36305401071786914, test loss: 0.7414652548777816\n",
      "Iteration 2078 training loss: 0.36298791859891644, test loss: 0.7414842242569122\n",
      "Iteration 2079 training loss: 0.36292185203905397, test loss: 0.7415031957373167\n",
      "Iteration 2080 training loss: 0.36285581102141895, test loss: 0.7415221693116573\n",
      "Iteration 2081 training loss: 0.36278979552916585, test loss: 0.7415411449726175\n",
      "Iteration 2082 training loss: 0.3627238055454661, test loss: 0.7415601227129043\n",
      "Iteration 2083 training loss: 0.36265784105350857, test loss: 0.7415791025252468\n",
      "Iteration 2084 training loss: 0.36259190203649877, test loss: 0.7415980844023957\n",
      "Iteration 2085 training loss: 0.3625259884776597, test loss: 0.7416170683371245\n",
      "Iteration 2086 training loss: 0.3624601003602311, test loss: 0.7416360543222287\n",
      "Iteration 2087 training loss: 0.3623942376674699, test loss: 0.7416550423505255\n",
      "Iteration 2088 training loss: 0.36232840038264985, test loss: 0.7416740324148539\n",
      "Iteration 2089 training loss: 0.36226258848906173, test loss: 0.7416930245080753\n",
      "Iteration 2090 training loss: 0.36219680197001336, test loss: 0.7417120186230725\n",
      "Iteration 2091 training loss: 0.3621310408088293, test loss: 0.7417310147527497\n",
      "Iteration 2092 training loss: 0.36206530498885103, test loss: 0.7417500128900328\n",
      "Iteration 2093 training loss: 0.3619995944934369, test loss: 0.7417690130278696\n",
      "Iteration 2094 training loss: 0.3619339093059621, test loss: 0.7417880151592288\n",
      "Iteration 2095 training loss: 0.36186824940981877, test loss: 0.7418070192771008\n",
      "Iteration 2096 training loss: 0.3618026147884156, test loss: 0.7418260253744967\n",
      "Iteration 2097 training loss: 0.361737005425178, test loss: 0.7418450334444496\n",
      "Iteration 2098 training loss: 0.36167142130354846, test loss: 0.741864043480013\n",
      "Iteration 2099 training loss: 0.36160586240698583, test loss: 0.7418830554742618\n",
      "Iteration 2100 training loss: 0.3615403287189658, test loss: 0.7419020694202912\n",
      "Iteration 2101 training loss: 0.36147482022298066, test loss: 0.7419210853112185\n",
      "Iteration 2102 training loss: 0.36140933690253935, test loss: 0.7419401031401804\n",
      "Iteration 2103 training loss: 0.3613438787411674, test loss: 0.7419591229003353\n",
      "Iteration 2104 training loss: 0.3612784457224071, test loss: 0.741978144584862\n",
      "Iteration 2105 training loss: 0.36121303782981695, test loss: 0.7419971681869592\n",
      "Iteration 2106 training loss: 0.3611476550469723, test loss: 0.742016193699847\n",
      "Iteration 2107 training loss: 0.36108229735746483, test loss: 0.7420352211167652\n",
      "Iteration 2108 training loss: 0.3610169647449028, test loss: 0.7420542504309746\n",
      "Iteration 2109 training loss: 0.3609516571929109, test loss: 0.7420732816357557\n",
      "Iteration 2110 training loss: 0.36088637468513024, test loss: 0.7420923147244093\n",
      "Iteration 2111 training loss: 0.3608211172052183, test loss: 0.7421113496902566\n",
      "Iteration 2112 training loss: 0.3607558847368491, test loss: 0.7421303865266381\n",
      "Iteration 2113 training loss: 0.36069067726371307, test loss: 0.7421494252269151\n",
      "Iteration 2114 training loss: 0.3606254947695165, test loss: 0.7421684657844683\n",
      "Iteration 2115 training loss: 0.36056033723798253, test loss: 0.7421875081926989\n",
      "Iteration 2116 training loss: 0.3604952046528504, test loss: 0.7422065524450263\n",
      "Iteration 2117 training loss: 0.36043009699787565, test loss: 0.7422255985348913\n",
      "Iteration 2118 training loss: 0.36036501425683, test loss: 0.7422446464557533\n",
      "Iteration 2119 training loss: 0.36029995641350143, test loss: 0.7422636962010914\n",
      "Iteration 2120 training loss: 0.36023492345169417, test loss: 0.7422827477644045\n",
      "Iteration 2121 training loss: 0.36016991535522846, test loss: 0.7423018011392104\n",
      "Iteration 2122 training loss: 0.36010493210794087, test loss: 0.7423208563190462\n",
      "Iteration 2123 training loss: 0.36003997369368407, test loss: 0.7423399132974686\n",
      "Iteration 2124 training loss: 0.35997504009632675, test loss: 0.7423589720680538\n",
      "Iteration 2125 training loss: 0.3599101312997538, test loss: 0.7423780326243961\n",
      "Iteration 2126 training loss: 0.35984524728786593, test loss: 0.7423970949601095\n",
      "Iteration 2127 training loss: 0.35978038804458023, test loss: 0.742416159068827\n",
      "Iteration 2128 training loss: 0.3597155535538295, test loss: 0.7424352249442\n",
      "Iteration 2129 training loss: 0.3596507437995628, test loss: 0.7424542925798993\n",
      "Iteration 2130 training loss: 0.3595859587657448, test loss: 0.7424733619696141\n",
      "Iteration 2131 training loss: 0.3595211984363563, test loss: 0.7424924331070527\n",
      "Iteration 2132 training loss: 0.3594564627953943, test loss: 0.7425115059859412\n",
      "Iteration 2133 training loss: 0.35939175182687116, test loss: 0.7425305806000253\n",
      "Iteration 2134 training loss: 0.3593270655148155, test loss: 0.7425496569430686\n",
      "Iteration 2135 training loss: 0.35926240384327157, test loss: 0.7425687350088528\n",
      "Iteration 2136 training loss: 0.3591977667962997, test loss: 0.7425878147911789\n",
      "Iteration 2137 training loss: 0.3591331543579757, test loss: 0.7426068962838652\n",
      "Iteration 2138 training loss: 0.35906856651239144, test loss: 0.7426259794807488\n",
      "Iteration 2139 training loss: 0.35900400324365445, test loss: 0.7426450643756856\n",
      "Iteration 2140 training loss: 0.3589394645358878, test loss: 0.7426641509625478\n",
      "Iteration 2141 training loss: 0.35887495037323064, test loss: 0.7426832392352272\n",
      "Iteration 2142 training loss: 0.3588104607398376, test loss: 0.742702329187633\n",
      "Iteration 2143 training loss: 0.35874599561987885, test loss: 0.7427214208136922\n",
      "Iteration 2144 training loss: 0.35868155499754045, test loss: 0.7427405141073499\n",
      "Iteration 2145 training loss: 0.35861713885702406, test loss: 0.7427596090625688\n",
      "Iteration 2146 training loss: 0.35855274718254676, test loss: 0.7427787056733297\n",
      "Iteration 2147 training loss: 0.35848837995834143, test loss: 0.7427978039336304\n",
      "Iteration 2148 training loss: 0.35842403716865623, test loss: 0.7428169038374867\n",
      "Iteration 2149 training loss: 0.3583597187977552, test loss: 0.7428360053789321\n",
      "Iteration 2150 training loss: 0.3582954248299175, test loss: 0.742855108552017\n",
      "Iteration 2151 training loss: 0.35823115524943805, test loss: 0.7428742133508096\n",
      "Iteration 2152 training loss: 0.35816691004062723, test loss: 0.7428933197693953\n",
      "Iteration 2153 training loss: 0.35810268918781085, test loss: 0.7429124278018772\n",
      "Iteration 2154 training loss: 0.35803849267533, test loss: 0.7429315374423747\n",
      "Iteration 2155 training loss: 0.3579743204875412, test loss: 0.7429506486850255\n",
      "Iteration 2156 training loss: 0.3579101726088166, test loss: 0.7429697615239832\n",
      "Iteration 2157 training loss: 0.35784604902354333, test loss: 0.7429888759534196\n",
      "Iteration 2158 training loss: 0.35778194971612426, test loss: 0.7430079919675227\n",
      "Iteration 2159 training loss: 0.35771787467097715, test loss: 0.7430271095604974\n",
      "Iteration 2160 training loss: 0.35765382387253536, test loss: 0.7430462287265661\n",
      "Iteration 2161 training loss: 0.3575897973052474, test loss: 0.7430653494599673\n",
      "Iteration 2162 training loss: 0.35752579495357717, test loss: 0.7430844717549566\n",
      "Iteration 2163 training loss: 0.3574618168020035, test loss: 0.7431035956058066\n",
      "Iteration 2164 training loss: 0.35739786283502073, test loss: 0.7431227210068054\n",
      "Iteration 2165 training loss: 0.3573339330371381, test loss: 0.7431418479522589\n",
      "Iteration 2166 training loss: 0.35727002739288033, test loss: 0.7431609764364889\n",
      "Iteration 2167 training loss: 0.35720614588678706, test loss: 0.7431801064538338\n",
      "Iteration 2168 training loss: 0.3571422885034131, test loss: 0.7431992379986483\n",
      "Iteration 2169 training loss: 0.3570784552273283, test loss: 0.7432183710653034\n",
      "Iteration 2170 training loss: 0.3570146460431178, test loss: 0.7432375056481866\n",
      "Iteration 2171 training loss: 0.3569508609353814, test loss: 0.7432566417417011\n",
      "Iteration 2172 training loss: 0.3568870998887345, test loss: 0.7432757793402672\n",
      "Iteration 2173 training loss: 0.35682336288780686, test loss: 0.7432949184383204\n",
      "Iteration 2174 training loss: 0.35675964991724385, test loss: 0.7433140590303126\n",
      "Iteration 2175 training loss: 0.3566959609617054, test loss: 0.7433332011107118\n",
      "Iteration 2176 training loss: 0.3566322960058665, test loss: 0.7433523446740017\n",
      "Iteration 2177 training loss: 0.35656865503441704, test loss: 0.743371489714682\n",
      "Iteration 2178 training loss: 0.35650503803206196, test loss: 0.7433906362272683\n",
      "Iteration 2179 training loss: 0.356441444983521, test loss: 0.743409784206292\n",
      "Iteration 2180 training loss: 0.35637787587352854, test loss: 0.7434289336463001\n",
      "Iteration 2181 training loss: 0.35631433068683427, test loss: 0.7434480845418552\n",
      "Iteration 2182 training loss: 0.35625080940820236, test loss: 0.7434672368875356\n",
      "Iteration 2183 training loss: 0.3561873120224118, test loss: 0.7434863906779352\n",
      "Iteration 2184 training loss: 0.35612383851425644, test loss: 0.7435055459076635\n",
      "Iteration 2185 training loss: 0.356060388868545, test loss: 0.743524702571345\n",
      "Iteration 2186 training loss: 0.3559969630701008, test loss: 0.7435438606636201\n",
      "Iteration 2187 training loss: 0.35593356110376184, test loss: 0.7435630201791447\n",
      "Iteration 2188 training loss: 0.35587018295438094, test loss: 0.7435821811125891\n",
      "Iteration 2189 training loss: 0.35580682860682555, test loss: 0.7436013434586397\n",
      "Iteration 2190 training loss: 0.3557434980459778, test loss: 0.7436205072119976\n",
      "Iteration 2191 training loss: 0.3556801912567343, test loss: 0.7436396723673793\n",
      "Iteration 2192 training loss: 0.3556169082240066, test loss: 0.7436588389195165\n",
      "Iteration 2193 training loss: 0.3555536489327206, test loss: 0.7436780068631553\n",
      "Iteration 2194 training loss: 0.3554904133678168, test loss: 0.7436971761930575\n",
      "Iteration 2195 training loss: 0.3554272015142503, test loss: 0.7437163469039993\n",
      "Iteration 2196 training loss: 0.35536401335699075, test loss: 0.7437355189907723\n",
      "Iteration 2197 training loss: 0.35530084888102237, test loss: 0.7437546924481824\n",
      "Iteration 2198 training loss: 0.3552377080713437, test loss: 0.7437738672710507\n",
      "Iteration 2199 training loss: 0.355174590912968, test loss: 0.7437930434542127\n",
      "Iteration 2200 training loss: 0.3551114973909228, test loss: 0.7438122209925186\n",
      "Iteration 2201 training loss: 0.35504842749025, test loss: 0.7438313998808334\n",
      "Iteration 2202 training loss: 0.3549853811960062, test loss: 0.7438505801140369\n",
      "Iteration 2203 training loss: 0.35492235849326226, test loss: 0.7438697616870229\n",
      "Iteration 2204 training loss: 0.3548593593671033, test loss: 0.7438889445946997\n",
      "Iteration 2205 training loss: 0.3547963838026288, test loss: 0.7439081288319905\n",
      "Iteration 2206 training loss: 0.3547334317849528, test loss: 0.7439273143938326\n",
      "Iteration 2207 training loss: 0.3546705032992035, test loss: 0.7439465012751778\n",
      "Iteration 2208 training loss: 0.3546075983305235, test loss: 0.7439656894709915\n",
      "Iteration 2209 training loss: 0.35454471686406946, test loss: 0.7439848789762544\n",
      "Iteration 2210 training loss: 0.3544818588850126, test loss: 0.7440040697859605\n",
      "Iteration 2211 training loss: 0.354419024378538, test loss: 0.7440232618951186\n",
      "Iteration 2212 training loss: 0.3543562133298454, test loss: 0.7440424552987508\n",
      "Iteration 2213 training loss: 0.3542934257241484, test loss: 0.7440616499918942\n",
      "Iteration 2214 training loss: 0.3542306615466749, test loss: 0.744080845969599\n",
      "Iteration 2215 training loss: 0.354167920782667, test loss: 0.7441000432269299\n",
      "Iteration 2216 training loss: 0.35410520341738094, test loss: 0.7441192417589656\n",
      "Iteration 2217 training loss: 0.35404250943608695, test loss: 0.7441384415607981\n",
      "Iteration 2218 training loss: 0.35397983882406947, test loss: 0.7441576426275333\n",
      "Iteration 2219 training loss: 0.35391719156662704, test loss: 0.7441768449542916\n",
      "Iteration 2220 training loss: 0.3538545676490723, test loss: 0.7441960485362061\n",
      "Iteration 2221 training loss: 0.3537919670567318, test loss: 0.7442152533684244\n",
      "Iteration 2222 training loss: 0.35372938977494617, test loss: 0.7442344594461073\n",
      "Iteration 2223 training loss: 0.35366683578907004, test loss: 0.7442536667644292\n",
      "Iteration 2224 training loss: 0.3536043050844722, test loss: 0.7442728753185778\n",
      "Iteration 2225 training loss: 0.3535417976465351, test loss: 0.7442920851037552\n",
      "Iteration 2226 training loss: 0.3534793134606553, test loss: 0.7443112961151757\n",
      "Iteration 2227 training loss: 0.3534168525122434, test loss: 0.7443305083480678\n",
      "Iteration 2228 training loss: 0.35335441478672375, test loss: 0.7443497217976733\n",
      "Iteration 2229 training loss: 0.3532920002695346, test loss: 0.7443689364592468\n",
      "Iteration 2230 training loss: 0.3532296089461281, test loss: 0.744388152328057\n",
      "Iteration 2231 training loss: 0.3531672408019704, test loss: 0.7444073693993849\n",
      "Iteration 2232 training loss: 0.3531048958225411, test loss: 0.7444265876685254\n",
      "Iteration 2233 training loss: 0.3530425739933343, test loss: 0.744445807130786\n",
      "Iteration 2234 training loss: 0.35298027529985704, test loss: 0.7444650277814878\n",
      "Iteration 2235 training loss: 0.3529179997276308, test loss: 0.7444842496159644\n",
      "Iteration 2236 training loss: 0.3528557472621905, test loss: 0.7445034726295627\n",
      "Iteration 2237 training loss: 0.3527935178890849, test loss: 0.7445226968176428\n",
      "Iteration 2238 training loss: 0.35273131159387666, test loss: 0.744541922175577\n",
      "Iteration 2239 training loss: 0.3526691283621417, test loss: 0.7445611486987512\n",
      "Iteration 2240 training loss: 0.35260696817947, test loss: 0.744580376382564\n",
      "Iteration 2241 training loss: 0.3525448310314651, test loss: 0.7445996052224264\n",
      "Iteration 2242 training loss: 0.35248271690374416, test loss: 0.7446188352137622\n",
      "Iteration 2243 training loss: 0.3524206257819381, test loss: 0.7446380663520081\n",
      "Iteration 2244 training loss: 0.3523585576516913, test loss: 0.744657298632614\n",
      "Iteration 2245 training loss: 0.3522965124986617, test loss: 0.744676532051041\n",
      "Iteration 2246 training loss: 0.3522344903085209, test loss: 0.7446957666027642\n",
      "Iteration 2247 training loss: 0.35217249106695414, test loss: 0.7447150022832704\n",
      "Iteration 2248 training loss: 0.35211051475966015, test loss: 0.7447342390880595\n",
      "Iteration 2249 training loss: 0.35204856137235113, test loss: 0.7447534770126429\n",
      "Iteration 2250 training loss: 0.35198663089075277, test loss: 0.7447727160525454\n",
      "Iteration 2251 training loss: 0.35192472330060426, test loss: 0.744791956203304\n",
      "Iteration 2252 training loss: 0.3518628385876583, test loss: 0.7448111974604672\n",
      "Iteration 2253 training loss: 0.351800976737681, test loss: 0.7448304398195968\n",
      "Iteration 2254 training loss: 0.35173913773645193, test loss: 0.7448496832762663\n",
      "Iteration 2255 training loss: 0.35167732156976406, test loss: 0.744868927826062\n",
      "Iteration 2256 training loss: 0.35161552822342373, test loss: 0.7448881734645814\n",
      "Iteration 2257 training loss: 0.35155375768325065, test loss: 0.7449074201874347\n",
      "Iteration 2258 training loss: 0.35149200993507806, test loss: 0.7449266679902443\n",
      "Iteration 2259 training loss: 0.3514302849647522, test loss: 0.7449459168686448\n",
      "Iteration 2260 training loss: 0.3513685827581331, test loss: 0.7449651668182818\n",
      "Iteration 2261 training loss: 0.35130690330109354, test loss: 0.7449844178348144\n",
      "Iteration 2262 training loss: 0.35124524657952, test loss: 0.7450036699139121\n",
      "Iteration 2263 training loss: 0.35118361257931213, test loss: 0.7450229230512573\n",
      "Iteration 2264 training loss: 0.35112200128638277, test loss: 0.745042177242544\n",
      "Iteration 2265 training loss: 0.35106041268665805, test loss: 0.745061432483478\n",
      "Iteration 2266 training loss: 0.35099884676607734, test loss: 0.7450806887697768\n",
      "Iteration 2267 training loss: 0.35093730351059305, test loss: 0.7450999460971697\n",
      "Iteration 2268 training loss: 0.35087578290617094, test loss: 0.7451192044613978\n",
      "Iteration 2269 training loss: 0.35081428493878997, test loss: 0.7451384638582135\n",
      "Iteration 2270 training loss: 0.35075280959444205, test loss: 0.7451577242833812\n",
      "Iteration 2271 training loss: 0.3506913568591324, test loss: 0.7451769857326771\n",
      "Iteration 2272 training loss: 0.3506299267188792, test loss: 0.7451962482018883\n",
      "Iteration 2273 training loss: 0.35056851915971393, test loss: 0.7452155116868139\n",
      "Iteration 2274 training loss: 0.35050713416768087, test loss: 0.7452347761832641\n",
      "Iteration 2275 training loss: 0.3504457717288376, test loss: 0.7452540416870611\n",
      "Iteration 2276 training loss: 0.3503844318292547, test loss: 0.745273308194038\n",
      "Iteration 2277 training loss: 0.3503231144550156, test loss: 0.745292575700039\n",
      "Iteration 2278 training loss: 0.350261819592217, test loss: 0.7453118442009208\n",
      "Iteration 2279 training loss: 0.3502005472269683, test loss: 0.7453311136925503\n",
      "Iteration 2280 training loss: 0.35013929734539234, test loss: 0.745350384170806\n",
      "Iteration 2281 training loss: 0.3500780699336243, test loss: 0.7453696556315774\n",
      "Iteration 2282 training loss: 0.35001686497781254, test loss: 0.7453889280707661\n",
      "Iteration 2283 training loss: 0.3499556824641186, test loss: 0.7454082014842834\n",
      "Iteration 2284 training loss: 0.3498945223787168, test loss: 0.7454274758680528\n",
      "Iteration 2285 training loss: 0.34983338470779424, test loss: 0.7454467512180086\n",
      "Iteration 2286 training loss: 0.34977226943755074, test loss: 0.745466027530096\n",
      "Iteration 2287 training loss: 0.34971117655419925, test loss: 0.7454853048002714\n",
      "Iteration 2288 training loss: 0.3496501060439656, test loss: 0.7455045830245017\n",
      "Iteration 2289 training loss: 0.3495890578930882, test loss: 0.7455238621987655\n",
      "Iteration 2290 training loss: 0.3495280320878184, test loss: 0.745543142319052\n",
      "Iteration 2291 training loss: 0.34946702861442025, test loss: 0.7455624233813607\n",
      "Iteration 2292 training loss: 0.3494060474591708, test loss: 0.7455817053817027\n",
      "Iteration 2293 training loss: 0.34934508860835944, test loss: 0.7456009883160994\n",
      "Iteration 2294 training loss: 0.3492841520482888, test loss: 0.7456202721805835\n",
      "Iteration 2295 training loss: 0.3492232377652738, test loss: 0.7456395569711977\n",
      "Iteration 2296 training loss: 0.3491623457456422, test loss: 0.745658842683996\n",
      "Iteration 2297 training loss: 0.34910147597573465, test loss: 0.7456781293150426\n",
      "Iteration 2298 training loss: 0.34904062844190414, test loss: 0.7456974168604129\n",
      "Iteration 2299 training loss: 0.3489798031305165, test loss: 0.7457167053161922\n",
      "Iteration 2300 training loss: 0.34891900002795023, test loss: 0.7457359946784768\n",
      "Iteration 2301 training loss: 0.3488582191205962, test loss: 0.7457552849433736\n",
      "Iteration 2302 training loss: 0.3487974603948584, test loss: 0.7457745761069998\n",
      "Iteration 2303 training loss: 0.3487367238371528, test loss: 0.745793868165483\n",
      "Iteration 2304 training loss: 0.3486760094339083, test loss: 0.7458131611149613\n",
      "Iteration 2305 training loss: 0.3486153171715663, test loss: 0.745832454951583\n",
      "Iteration 2306 training loss: 0.3485546470365806, test loss: 0.7458517496715072\n",
      "Iteration 2307 training loss: 0.3484939990154177, test loss: 0.7458710452709032\n",
      "Iteration 2308 training loss: 0.3484333730945565, test loss: 0.7458903417459499\n",
      "Iteration 2309 training loss: 0.3483727692604886, test loss: 0.7459096390928378\n",
      "Iteration 2310 training loss: 0.34831218749971765, test loss: 0.7459289373077664\n",
      "Iteration 2311 training loss: 0.34825162779876007, test loss: 0.7459482363869461\n",
      "Iteration 2312 training loss: 0.3481910901441446, test loss: 0.7459675363265966\n",
      "Iteration 2313 training loss: 0.34813057452241264, test loss: 0.7459868371229491\n",
      "Iteration 2314 training loss: 0.3480700809201176, test loss: 0.7460061387722438\n",
      "Iteration 2315 training loss: 0.3480096093238257, test loss: 0.7460254412707313\n",
      "Iteration 2316 training loss: 0.3479491597201151, test loss: 0.7460447446146724\n",
      "Iteration 2317 training loss: 0.3478887320955766, test loss: 0.7460640488003378\n",
      "Iteration 2318 training loss: 0.3478283264368134, test loss: 0.7460833538240081\n",
      "Iteration 2319 training loss: 0.3477679427304408, test loss: 0.7461026596819733\n",
      "Iteration 2320 training loss: 0.34770758096308657, test loss: 0.7461219663705349\n",
      "Iteration 2321 training loss: 0.3476472411213908, test loss: 0.7461412738860025\n",
      "Iteration 2322 training loss: 0.3475869231920056, test loss: 0.7461605822246966\n",
      "Iteration 2323 training loss: 0.3475266271615957, test loss: 0.746179891382947\n",
      "Iteration 2324 training loss: 0.34746635301683787, test loss: 0.746199201357094\n",
      "Iteration 2325 training loss: 0.34740610074442113, test loss: 0.7462185121434866\n",
      "Iteration 2326 training loss: 0.3473458703310468, test loss: 0.7462378237384848\n",
      "Iteration 2327 training loss: 0.3472856617634283, test loss: 0.7462571361384569\n",
      "Iteration 2328 training loss: 0.34722547502829115, test loss: 0.7462764493397819\n",
      "Iteration 2329 training loss: 0.34716531011237334, test loss: 0.7462957633388481\n",
      "Iteration 2330 training loss: 0.3471051670024247, test loss: 0.7463150781320532\n",
      "Iteration 2331 training loss: 0.34704504568520755, test loss: 0.7463343937158049\n",
      "Iteration 2332 training loss: 0.3469849461474958, test loss: 0.7463537100865204\n",
      "Iteration 2333 training loss: 0.34692486837607595, test loss: 0.7463730272406256\n",
      "Iteration 2334 training loss: 0.3468648123577465, test loss: 0.746392345174557\n",
      "Iteration 2335 training loss: 0.3468047780793179, test loss: 0.74641166388476\n",
      "Iteration 2336 training loss: 0.3467447655276126, test loss: 0.7464309833676896\n",
      "Iteration 2337 training loss: 0.34668477468946524, test loss: 0.74645030361981\n",
      "Iteration 2338 training loss: 0.34662480555172237, test loss: 0.7464696246375947\n",
      "Iteration 2339 training loss: 0.34656485810124277, test loss: 0.7464889464175272\n",
      "Iteration 2340 training loss: 0.34650493232489704, test loss: 0.7465082689560992\n",
      "Iteration 2341 training loss: 0.3464450282095678, test loss: 0.7465275922498128\n",
      "Iteration 2342 training loss: 0.3463851457421496, test loss: 0.7465469162951788\n",
      "Iteration 2343 training loss: 0.34632528490954906, test loss: 0.746566241088717\n",
      "Iteration 2344 training loss: 0.3462654456986845, test loss: 0.746585566626957\n",
      "Iteration 2345 training loss: 0.3462056280964865, test loss: 0.7466048929064371\n",
      "Iteration 2346 training loss: 0.3461458320898972, test loss: 0.746624219923705\n",
      "Iteration 2347 training loss: 0.3460860576658708, test loss: 0.746643547675317\n",
      "Iteration 2348 training loss: 0.3460263048113735, test loss: 0.7466628761578394\n",
      "Iteration 2349 training loss: 0.34596657351338317, test loss: 0.7466822053678469\n",
      "Iteration 2350 training loss: 0.3459068637588894, test loss: 0.7467015353019233\n",
      "Iteration 2351 training loss: 0.3458471755348941, test loss: 0.7467208659566614\n",
      "Iteration 2352 training loss: 0.3457875088284105, test loss: 0.7467401973286631\n",
      "Iteration 2353 training loss: 0.3457278636264638, test loss: 0.7467595294145392\n",
      "Iteration 2354 training loss: 0.34566823991609097, test loss: 0.7467788622109094\n",
      "Iteration 2355 training loss: 0.3456086376843409, test loss: 0.7467981957144023\n",
      "Iteration 2356 training loss: 0.3455490569182742, test loss: 0.7468175299216551\n",
      "Iteration 2357 training loss: 0.3454894976049629, test loss: 0.7468368648293144\n",
      "Iteration 2358 training loss: 0.34542995973149104, test loss: 0.7468562004340354\n",
      "Iteration 2359 training loss: 0.3453704432849544, test loss: 0.7468755367324817\n",
      "Iteration 2360 training loss: 0.3453109482524604, test loss: 0.7468948737213261\n",
      "Iteration 2361 training loss: 0.34525147462112793, test loss: 0.7469142113972498\n",
      "Iteration 2362 training loss: 0.34519202237808794, test loss: 0.7469335497569428\n",
      "Iteration 2363 training loss: 0.3451325915104827, test loss: 0.746952888797104\n",
      "Iteration 2364 training loss: 0.3450731820054662, test loss: 0.7469722285144407\n",
      "Iteration 2365 training loss: 0.34501379385020425, test loss: 0.7469915689056688\n",
      "Iteration 2366 training loss: 0.34495442703187396, test loss: 0.7470109099675133\n",
      "Iteration 2367 training loss: 0.3448950815376642, test loss: 0.7470302516967069\n",
      "Iteration 2368 training loss: 0.34483575735477534, test loss: 0.7470495940899913\n",
      "Iteration 2369 training loss: 0.34477645447041944, test loss: 0.7470689371441172\n",
      "Iteration 2370 training loss: 0.3447171728718201, test loss: 0.7470882808558427\n",
      "Iteration 2371 training loss: 0.34465791254621214, test loss: 0.7471076252219353\n",
      "Iteration 2372 training loss: 0.3445986734808424, test loss: 0.7471269702391706\n",
      "Iteration 2373 training loss: 0.3445394556629688, test loss: 0.7471463159043329\n",
      "Iteration 2374 training loss: 0.344480259079861, test loss: 0.7471656622142141\n",
      "Iteration 2375 training loss: 0.34442108371880004, test loss: 0.7471850091656151\n",
      "Iteration 2376 training loss: 0.3443619295670784, test loss: 0.7472043567553452\n",
      "Iteration 2377 training loss: 0.34430279661200003, test loss: 0.7472237049802217\n",
      "Iteration 2378 training loss: 0.34424368484088036, test loss: 0.7472430538370703\n",
      "Iteration 2379 training loss: 0.34418459424104614, test loss: 0.7472624033227252\n",
      "Iteration 2380 training loss: 0.3441255247998356, test loss: 0.7472817534340281\n",
      "Iteration 2381 training loss: 0.3440664765045983, test loss: 0.74730110416783\n",
      "Iteration 2382 training loss: 0.3440074493426952, test loss: 0.7473204555209889\n",
      "Iteration 2383 training loss: 0.3439484433014989, test loss: 0.7473398074903721\n",
      "Iteration 2384 training loss: 0.34388945836839263, test loss: 0.7473591600728542\n",
      "Iteration 2385 training loss: 0.3438304945307717, test loss: 0.7473785132653183\n",
      "Iteration 2386 training loss: 0.3437715517760423, test loss: 0.7473978670646558\n",
      "Iteration 2387 training loss: 0.343712630091622, test loss: 0.7474172214677655\n",
      "Iteration 2388 training loss: 0.34365372946494, test loss: 0.7474365764715545\n",
      "Iteration 2389 training loss: 0.34359484988343625, test loss: 0.7474559320729384\n",
      "Iteration 2390 training loss: 0.34353599133456225, test loss: 0.74747528826884\n",
      "Iteration 2391 training loss: 0.34347715380578064, test loss: 0.747494645056191\n",
      "Iteration 2392 training loss: 0.34341833728456544, test loss: 0.74751400243193\n",
      "Iteration 2393 training loss: 0.3433595417584017, test loss: 0.7475333603930044\n",
      "Iteration 2394 training loss: 0.34330076721478586, test loss: 0.747552718936369\n",
      "Iteration 2395 training loss: 0.3432420136412255, test loss: 0.7475720780589864\n",
      "Iteration 2396 training loss: 0.3431832810252392, test loss: 0.7475914377578278\n",
      "Iteration 2397 training loss: 0.34312456935435687, test loss: 0.7476107980298713\n",
      "Iteration 2398 training loss: 0.34306587861611965, test loss: 0.747630158872103\n",
      "Iteration 2399 training loss: 0.3430072087980796, test loss: 0.7476495202815174\n",
      "Iteration 2400 training loss: 0.3429485598877999, test loss: 0.7476688822551162\n",
      "Iteration 2401 training loss: 0.3428899318728551, test loss: 0.7476882447899087\n",
      "Iteration 2402 training loss: 0.34283132474083067, test loss: 0.7477076078829125\n",
      "Iteration 2403 training loss: 0.34277273847932316, test loss: 0.747726971531152\n",
      "Iteration 2404 training loss: 0.34271417307594015, test loss: 0.7477463357316605\n",
      "Iteration 2405 training loss: 0.3426556285183003, test loss: 0.7477657004814776\n",
      "Iteration 2406 training loss: 0.3425971047940332, test loss: 0.7477850657776518\n",
      "Iteration 2407 training loss: 0.34253860189077995, test loss: 0.7478044316172381\n",
      "Iteration 2408 training loss: 0.34248011979619203, test loss: 0.7478237979972997\n",
      "Iteration 2409 training loss: 0.3424216584979322, test loss: 0.7478431649149071\n",
      "Iteration 2410 training loss: 0.34236321798367425, test loss: 0.7478625323671384\n",
      "Iteration 2411 training loss: 0.3423047982411028, test loss: 0.7478819003510792\n",
      "Iteration 2412 training loss: 0.3422463992579136, test loss: 0.7479012688638225\n",
      "Iteration 2413 training loss: 0.3421880210218132, test loss: 0.7479206379024691\n",
      "Iteration 2414 training loss: 0.34212966352051905, test loss: 0.7479400074641266\n",
      "Iteration 2415 training loss: 0.34207132674175966, test loss: 0.7479593775459109\n",
      "Iteration 2416 training loss: 0.3420130106732744, test loss: 0.7479787481449445\n",
      "Iteration 2417 training loss: 0.3419547153028134, test loss: 0.7479981192583574\n",
      "Iteration 2418 training loss: 0.34189644061813784, test loss: 0.7480174908832874\n",
      "Iteration 2419 training loss: 0.34183818660701965, test loss: 0.7480368630168793\n",
      "Iteration 2420 training loss: 0.34177995325724164, test loss: 0.7480562356562851\n",
      "Iteration 2421 training loss: 0.3417217405565974, test loss: 0.7480756087986643\n",
      "Iteration 2422 training loss: 0.3416635484928915, test loss: 0.7480949824411836\n",
      "Iteration 2423 training loss: 0.34160537705393906, test loss: 0.748114356581017\n",
      "Iteration 2424 training loss: 0.3415472262275663, test loss: 0.7481337312153453\n",
      "Iteration 2425 training loss: 0.34148909600161, test loss: 0.7481531063413573\n",
      "Iteration 2426 training loss: 0.3414309863639177, test loss: 0.7481724819562485\n",
      "Iteration 2427 training loss: 0.3413728973023479, test loss: 0.7481918580572213\n",
      "Iteration 2428 training loss: 0.34131482880476965, test loss: 0.7482112346414858\n",
      "Iteration 2429 training loss: 0.34125678085906275, test loss: 0.7482306117062586\n",
      "Iteration 2430 training loss: 0.34119875345311773, test loss: 0.7482499892487641\n",
      "Iteration 2431 training loss: 0.341140746574836, test loss: 0.748269367266233\n",
      "Iteration 2432 training loss: 0.34108276021212935, test loss: 0.7482887457559041\n",
      "Iteration 2433 training loss: 0.3410247943529204, test loss: 0.7483081247150218\n",
      "Iteration 2434 training loss: 0.34096684898514246, test loss: 0.7483275041408386\n",
      "Iteration 2435 training loss: 0.34090892409673945, test loss: 0.7483468840306139\n",
      "Iteration 2436 training loss: 0.34085101967566594, test loss: 0.7483662643816138\n",
      "Iteration 2437 training loss: 0.340793135709887, test loss: 0.7483856451911111\n",
      "Iteration 2438 training loss: 0.34073527218737865, test loss: 0.7484050264563862\n",
      "Iteration 2439 training loss: 0.3406774290961271, test loss: 0.7484244081747258\n",
      "Iteration 2440 training loss: 0.34061960642412936, test loss: 0.7484437903434237\n",
      "Iteration 2441 training loss: 0.340561804159393, test loss: 0.7484631729597807\n",
      "Iteration 2442 training loss: 0.3405040222899362, test loss: 0.7484825560211041\n",
      "Iteration 2443 training loss: 0.34044626080378737, test loss: 0.7485019395247089\n",
      "Iteration 2444 training loss: 0.34038851968898587, test loss: 0.7485213234679159\n",
      "Iteration 2445 training loss: 0.34033079893358137, test loss: 0.7485407078480526\n",
      "Iteration 2446 training loss: 0.3402730985256341, test loss: 0.7485600926624544\n",
      "Iteration 2447 training loss: 0.34021541845321474, test loss: 0.7485794779084626\n",
      "Iteration 2448 training loss: 0.34015775870440434, test loss: 0.7485988635834252\n",
      "Iteration 2449 training loss: 0.34010011926729466, test loss: 0.7486182496846974\n",
      "Iteration 2450 training loss: 0.34004250012998777, test loss: 0.7486376362096404\n",
      "Iteration 2451 training loss: 0.3399849012805963, test loss: 0.7486570231556231\n",
      "Iteration 2452 training loss: 0.3399273227072431, test loss: 0.7486764105200199\n",
      "Iteration 2453 training loss: 0.3398697643980616, test loss: 0.7486957983002124\n",
      "Iteration 2454 training loss: 0.3398122263411957, test loss: 0.748715186493589\n",
      "Iteration 2455 training loss: 0.3397547085247993, test loss: 0.7487345750975442\n",
      "Iteration 2456 training loss: 0.33969721093703714, test loss: 0.7487539641094797\n",
      "Iteration 2457 training loss: 0.3396397335660842, test loss: 0.7487733535268031\n",
      "Iteration 2458 training loss: 0.3395822764001257, test loss: 0.7487927433469289\n",
      "Iteration 2459 training loss: 0.33952483942735734, test loss: 0.748812133567278\n",
      "Iteration 2460 training loss: 0.33946742263598495, test loss: 0.748831524185278\n",
      "Iteration 2461 training loss: 0.3394100260142248, test loss: 0.7488509151983627\n",
      "Iteration 2462 training loss: 0.33935264955030353, test loss: 0.7488703066039725\n",
      "Iteration 2463 training loss: 0.33929529323245794, test loss: 0.7488896983995541\n",
      "Iteration 2464 training loss: 0.3392379570489352, test loss: 0.7489090905825608\n",
      "Iteration 2465 training loss: 0.3391806409879927, test loss: 0.7489284831504525\n",
      "Iteration 2466 training loss: 0.33912334503789815, test loss: 0.7489478761006948\n",
      "Iteration 2467 training loss: 0.33906606918692944, test loss: 0.7489672694307604\n",
      "Iteration 2468 training loss: 0.3390088134233746, test loss: 0.7489866631381281\n",
      "Iteration 2469 training loss: 0.33895157773553203, test loss: 0.7490060572202826\n",
      "Iteration 2470 training loss: 0.3388943621117103, test loss: 0.7490254516747157\n",
      "Iteration 2471 training loss: 0.3388371665402281, test loss: 0.7490448464989249\n",
      "Iteration 2472 training loss: 0.33877999100941447, test loss: 0.7490642416904142\n",
      "Iteration 2473 training loss: 0.33872283550760823, test loss: 0.7490836372466938\n",
      "Iteration 2474 training loss: 0.33866570002315893, test loss: 0.74910303316528\n",
      "Iteration 2475 training loss: 0.33860858454442583, test loss: 0.7491224294436959\n",
      "Iteration 2476 training loss: 0.33855148905977833, test loss: 0.7491418260794701\n",
      "Iteration 2477 training loss: 0.3384944135575962, test loss: 0.7491612230701375\n",
      "Iteration 2478 training loss: 0.338437358026269, test loss: 0.7491806204132399\n",
      "Iteration 2479 training loss: 0.33838032245419675, test loss: 0.749200018106324\n",
      "Iteration 2480 training loss: 0.33832330682978934, test loss: 0.7492194161469435\n",
      "Iteration 2481 training loss: 0.33826631114146666, test loss: 0.7492388145326584\n",
      "Iteration 2482 training loss: 0.3382093353776587, test loss: 0.7492582132610344\n",
      "Iteration 2483 training loss: 0.33815237952680577, test loss: 0.749277612329643\n",
      "Iteration 2484 training loss: 0.3380954435773576, test loss: 0.7492970117360622\n",
      "Iteration 2485 training loss: 0.33803852751777475, test loss: 0.7493164114778761\n",
      "Iteration 2486 training loss: 0.3379816313365271, test loss: 0.7493358115526743\n",
      "Iteration 2487 training loss: 0.33792475502209485, test loss: 0.749355211958053\n",
      "Iteration 2488 training loss: 0.33786789856296806, test loss: 0.7493746126916142\n",
      "Iteration 2489 training loss: 0.33781106194764704, test loss: 0.7493940137509658\n",
      "Iteration 2490 training loss: 0.33775424516464153, test loss: 0.7494134151337217\n",
      "Iteration 2491 training loss: 0.3376974482024718, test loss: 0.7494328168375018\n",
      "Iteration 2492 training loss: 0.3376406710496676, test loss: 0.7494522188599314\n",
      "Iteration 2493 training loss: 0.3375839136947689, test loss: 0.7494716211986431\n",
      "Iteration 2494 training loss: 0.3375271761263255, test loss: 0.7494910238512733\n",
      "Iteration 2495 training loss: 0.337470458332897, test loss: 0.7495104268154665\n",
      "Iteration 2496 training loss: 0.3374137603030531, test loss: 0.7495298300888714\n",
      "Iteration 2497 training loss: 0.33735708202537307, test loss: 0.7495492336691434\n",
      "Iteration 2498 training loss: 0.33730042348844635, test loss: 0.7495686375539433\n",
      "Iteration 2499 training loss: 0.33724378468087207, test loss: 0.749588041740938\n",
      "Iteration 2500 training loss: 0.3371871655912593, test loss: 0.7496074462278001\n",
      "Iteration 2501 training loss: 0.3371305662082266, test loss: 0.7496268510122076\n",
      "Iteration 2502 training loss: 0.33707398652040305, test loss: 0.7496462560918451\n",
      "Iteration 2503 training loss: 0.33701742651642685, test loss: 0.7496656614644023\n",
      "Iteration 2504 training loss: 0.33696088618494635, test loss: 0.7496850671275748\n",
      "Iteration 2505 training loss: 0.3369043655146195, test loss: 0.7497044730790632\n",
      "Iteration 2506 training loss: 0.33684786449411414, test loss: 0.7497238793165758\n",
      "Iteration 2507 training loss: 0.33679138311210793, test loss: 0.7497432858378242\n",
      "Iteration 2508 training loss: 0.33673492135728805, test loss: 0.749762692640527\n",
      "Iteration 2509 training loss: 0.33667847921835176, test loss: 0.7497820997224084\n",
      "Iteration 2510 training loss: 0.3366220566840056, test loss: 0.7498015070811977\n",
      "Iteration 2511 training loss: 0.3365656537429664, test loss: 0.7498209147146302\n",
      "Iteration 2512 training loss: 0.33650927038395995, test loss: 0.749840322620447\n",
      "Iteration 2513 training loss: 0.33645290659572236, test loss: 0.7498597307963941\n",
      "Iteration 2514 training loss: 0.3363965623669993, test loss: 0.7498791392402235\n",
      "Iteration 2515 training loss: 0.3363402376865458, test loss: 0.7498985479496929\n",
      "Iteration 2516 training loss: 0.33628393254312683, test loss: 0.7499179569225655\n",
      "Iteration 2517 training loss: 0.33622764692551704, test loss: 0.7499373661566094\n",
      "Iteration 2518 training loss: 0.3361713808225005, test loss: 0.7499567756495991\n",
      "Iteration 2519 training loss: 0.336115134222871, test loss: 0.7499761853993139\n",
      "Iteration 2520 training loss: 0.33605890711543185, test loss: 0.749995595403539\n",
      "Iteration 2521 training loss: 0.3360026994889962, test loss: 0.7500150056600645\n",
      "Iteration 2522 training loss: 0.33594651133238657, test loss: 0.7500344161666872\n",
      "Iteration 2523 training loss: 0.3358903426344352, test loss: 0.7500538269212076\n",
      "Iteration 2524 training loss: 0.3358341933839838, test loss: 0.7500732379214329\n",
      "Iteration 2525 training loss: 0.3357780635698835, test loss: 0.7500926491651749\n",
      "Iteration 2526 training loss: 0.33572195318099535, test loss: 0.7501120606502518\n",
      "Iteration 2527 training loss: 0.3356658622061895, test loss: 0.750131472374486\n",
      "Iteration 2528 training loss: 0.335609790634346, test loss: 0.750150884335706\n",
      "Iteration 2529 training loss: 0.33555373845435416, test loss: 0.7501702965317453\n",
      "Iteration 2530 training loss: 0.33549770565511283, test loss: 0.7501897089604429\n",
      "Iteration 2531 training loss: 0.33544169222553044, test loss: 0.7502091216196433\n",
      "Iteration 2532 training loss: 0.33538569815452485, test loss: 0.7502285345071954\n",
      "Iteration 2533 training loss: 0.33532972343102324, test loss: 0.7502479476209545\n",
      "Iteration 2534 training loss: 0.3352737680439624, test loss: 0.7502673609587807\n",
      "Iteration 2535 training loss: 0.3352178319822887, test loss: 0.7502867745185388\n",
      "Iteration 2536 training loss: 0.3351619152349577, test loss: 0.7503061882980999\n",
      "Iteration 2537 training loss: 0.3351060177909343, test loss: 0.7503256022953398\n",
      "Iteration 2538 training loss: 0.33505013963919306, test loss: 0.7503450165081391\n",
      "Iteration 2539 training loss: 0.3349942807687178, test loss: 0.7503644309343844\n",
      "Iteration 2540 training loss: 0.33493844116850185, test loss: 0.7503838455719668\n",
      "Iteration 2541 training loss: 0.3348826208275477, test loss: 0.7504032604187826\n",
      "Iteration 2542 training loss: 0.33482681973486744, test loss: 0.7504226754727337\n",
      "Iteration 2543 training loss: 0.33477103787948237, test loss: 0.7504420907317267\n",
      "Iteration 2544 training loss: 0.3347152752504231, test loss: 0.7504615061936738\n",
      "Iteration 2545 training loss: 0.3346595318367297, test loss: 0.7504809218564918\n",
      "Iteration 2546 training loss: 0.33460380762745134, test loss: 0.7505003377181029\n",
      "Iteration 2547 training loss: 0.33454810261164697, test loss: 0.7505197537764342\n",
      "Iteration 2548 training loss: 0.33449241677838426, test loss: 0.750539170029418\n",
      "Iteration 2549 training loss: 0.33443675011674046, test loss: 0.7505585864749914\n",
      "Iteration 2550 training loss: 0.3343811026158022, test loss: 0.7505780031110968\n",
      "Iteration 2551 training loss: 0.334325474264665, test loss: 0.7505974199356815\n",
      "Iteration 2552 training loss: 0.3342698650524341, test loss: 0.7506168369466982\n",
      "Iteration 2553 training loss: 0.33421427496822365, test loss: 0.7506362541421037\n",
      "Iteration 2554 training loss: 0.3341587040011572, test loss: 0.7506556715198605\n",
      "Iteration 2555 training loss: 0.33410315214036734, test loss: 0.7506750890779358\n",
      "Iteration 2556 training loss: 0.33404761937499605, test loss: 0.7506945068143022\n",
      "Iteration 2557 training loss: 0.3339921056941945, test loss: 0.7507139247269361\n",
      "Iteration 2558 training loss: 0.3339366110871231, test loss: 0.7507333428138202\n",
      "Iteration 2559 training loss: 0.33388113554295107, test loss: 0.7507527610729415\n",
      "Iteration 2560 training loss: 0.3338256790508572, test loss: 0.7507721795022917\n",
      "Iteration 2561 training loss: 0.3337702416000294, test loss: 0.7507915980998675\n",
      "Iteration 2562 training loss: 0.3337148231796645, test loss: 0.7508110168636705\n",
      "Iteration 2563 training loss: 0.3336594237789686, test loss: 0.750830435791707\n",
      "Iteration 2564 training loss: 0.3336040433871571, test loss: 0.7508498548819889\n",
      "Iteration 2565 training loss: 0.33354868199345417, test loss: 0.7508692741325322\n",
      "Iteration 2566 training loss: 0.33349333958709326, test loss: 0.7508886935413575\n",
      "Iteration 2567 training loss: 0.33343801615731694, test loss: 0.7509081131064907\n",
      "Iteration 2568 training loss: 0.3333827116933768, test loss: 0.7509275328259627\n",
      "Iteration 2569 training loss: 0.33332742618453376, test loss: 0.7509469526978082\n",
      "Iteration 2570 training loss: 0.33327215962005724, test loss: 0.7509663727200677\n",
      "Iteration 2571 training loss: 0.3332169119892262, test loss: 0.7509857928907865\n",
      "Iteration 2572 training loss: 0.33316168328132845, test loss: 0.7510052132080134\n",
      "Iteration 2573 training loss: 0.33310647348566097, test loss: 0.751024633669803\n",
      "Iteration 2574 training loss: 0.3330512825915295, test loss: 0.7510440542742144\n",
      "Iteration 2575 training loss: 0.3329961105882492, test loss: 0.7510634750193109\n",
      "Iteration 2576 training loss: 0.3329409574651436, test loss: 0.7510828959031617\n",
      "Iteration 2577 training loss: 0.3328858232115459, test loss: 0.7511023169238391\n",
      "Iteration 2578 training loss: 0.33283070781679785, test loss: 0.7511217380794211\n",
      "Iteration 2579 training loss: 0.3327756112702503, test loss: 0.7511411593679903\n",
      "Iteration 2580 training loss: 0.332720533561263, test loss: 0.7511605807876334\n",
      "Iteration 2581 training loss: 0.33266547467920493, test loss: 0.7511800023364421\n",
      "Iteration 2582 training loss: 0.3326104346134534, test loss: 0.7511994240125125\n",
      "Iteration 2583 training loss: 0.3325554133533953, test loss: 0.7512188458139458\n",
      "Iteration 2584 training loss: 0.332500410888426, test loss: 0.7512382677388473\n",
      "Iteration 2585 training loss: 0.33244542720795006, test loss: 0.7512576897853265\n",
      "Iteration 2586 training loss: 0.33239046230138064, test loss: 0.7512771119514987\n",
      "Iteration 2587 training loss: 0.3323355161581401, test loss: 0.7512965342354823\n",
      "Iteration 2588 training loss: 0.3322805887676594, test loss: 0.7513159566354013\n",
      "Iteration 2589 training loss: 0.33222568011937864, test loss: 0.7513353791493838\n",
      "Iteration 2590 training loss: 0.33217079020274637, test loss: 0.751354801775562\n",
      "Iteration 2591 training loss: 0.3321159190072205, test loss: 0.7513742245120738\n",
      "Iteration 2592 training loss: 0.33206106652226736, test loss: 0.7513936473570604\n",
      "Iteration 2593 training loss: 0.3320062327373623, test loss: 0.7514130703086679\n",
      "Iteration 2594 training loss: 0.33195141764198943, test loss: 0.7514324933650467\n",
      "Iteration 2595 training loss: 0.33189662122564156, test loss: 0.7514519165243523\n",
      "Iteration 2596 training loss: 0.3318418434778206, test loss: 0.7514713397847436\n",
      "Iteration 2597 training loss: 0.3317870843880369, test loss: 0.7514907631443848\n",
      "Iteration 2598 training loss: 0.3317323439458098, test loss: 0.751510186601444\n",
      "Iteration 2599 training loss: 0.3316776221406672, test loss: 0.7515296101540938\n",
      "Iteration 2600 training loss: 0.33162291896214613, test loss: 0.7515490338005114\n",
      "Iteration 2601 training loss: 0.3315682343997919, test loss: 0.7515684575388781\n",
      "Iteration 2602 training loss: 0.3315135684431588, test loss: 0.75158788136738\n",
      "Iteration 2603 training loss: 0.33145892108180985, test loss: 0.7516073052842068\n",
      "Iteration 2604 training loss: 0.33140429230531676, test loss: 0.7516267292875534\n",
      "Iteration 2605 training loss: 0.3313496821032599, test loss: 0.7516461533756181\n",
      "Iteration 2606 training loss: 0.33129509046522826, test loss: 0.7516655775466045\n",
      "Iteration 2607 training loss: 0.3312405173808198, test loss: 0.7516850017987199\n",
      "Iteration 2608 training loss: 0.33118596283964086, test loss: 0.7517044261301757\n",
      "Iteration 2609 training loss: 0.3311314268313064, test loss: 0.7517238505391886\n",
      "Iteration 2610 training loss: 0.3310769093454405, test loss: 0.751743275023978\n",
      "Iteration 2611 training loss: 0.3310224103716753, test loss: 0.7517626995827692\n",
      "Iteration 2612 training loss: 0.33096792989965185, test loss: 0.7517821242137904\n",
      "Iteration 2613 training loss: 0.33091346791902, test loss: 0.751801548915275\n",
      "Iteration 2614 training loss: 0.3308590244194378, test loss: 0.75182097368546\n",
      "Iteration 2615 training loss: 0.3308045993905722, test loss: 0.7518403985225869\n",
      "Iteration 2616 training loss: 0.3307501928220987, test loss: 0.7518598234249014\n",
      "Iteration 2617 training loss: 0.3306958047037013, test loss: 0.7518792483906533\n",
      "Iteration 2618 training loss: 0.33064143502507265, test loss: 0.7518986734180966\n",
      "Iteration 2619 training loss: 0.33058708377591395, test loss: 0.7519180985054894\n",
      "Iteration 2620 training loss: 0.3305327509459349, test loss: 0.7519375236510941\n",
      "Iteration 2621 training loss: 0.3304784365248537, test loss: 0.7519569488531774\n",
      "Iteration 2622 training loss: 0.3304241405023973, test loss: 0.7519763741100095\n",
      "Iteration 2623 training loss: 0.33036986286830106, test loss: 0.7519957994198655\n",
      "Iteration 2624 training loss: 0.3303156036123088, test loss: 0.752015224781024\n",
      "Iteration 2625 training loss: 0.33026136272417284, test loss: 0.7520346501917677\n",
      "Iteration 2626 training loss: 0.330207140193654, test loss: 0.7520540756503844\n",
      "Iteration 2627 training loss: 0.33015293601052176, test loss: 0.7520735011551647\n",
      "Iteration 2628 training loss: 0.33009875016455387, test loss: 0.7520929267044036\n",
      "Iteration 2629 training loss: 0.3300445826455366, test loss: 0.7521123522964007\n",
      "Iteration 2630 training loss: 0.32999043344326473, test loss: 0.752131777929459\n",
      "Iteration 2631 training loss: 0.3299363025475414, test loss: 0.752151203601886\n",
      "Iteration 2632 training loss: 0.3298821899481783, test loss: 0.752170629311993\n",
      "Iteration 2633 training loss: 0.32982809563499543, test loss: 0.7521900550580956\n",
      "Iteration 2634 training loss: 0.3297740195978213, test loss: 0.7522094808385121\n",
      "Iteration 2635 training loss: 0.3297199618264929, test loss: 0.7522289066515674\n",
      "Iteration 2636 training loss: 0.32966592231085534, test loss: 0.7522483324955876\n",
      "Iteration 2637 training loss: 0.3296119010407624, test loss: 0.7522677583689045\n",
      "Iteration 2638 training loss: 0.329557898006076, test loss: 0.7522871842698533\n",
      "Iteration 2639 training loss: 0.3295039131966667, test loss: 0.752306610196773\n",
      "Iteration 2640 training loss: 0.32944994660241333, test loss: 0.7523260361480072\n",
      "Iteration 2641 training loss: 0.3293959982132029, test loss: 0.7523454621219022\n",
      "Iteration 2642 training loss: 0.329342068018931, test loss: 0.7523648881168097\n",
      "Iteration 2643 training loss: 0.3292881560095014, test loss: 0.7523843141310843\n",
      "Iteration 2644 training loss: 0.3292342621748262, test loss: 0.7524037401630846\n",
      "Iteration 2645 training loss: 0.3291803865048259, test loss: 0.7524231662111737\n",
      "Iteration 2646 training loss: 0.3291265289894292, test loss: 0.7524425922737176\n",
      "Iteration 2647 training loss: 0.32907268961857333, test loss: 0.7524620183490871\n",
      "Iteration 2648 training loss: 0.32901886838220334, test loss: 0.7524814444356565\n",
      "Iteration 2649 training loss: 0.328965065270273, test loss: 0.7525008705318036\n",
      "Iteration 2650 training loss: 0.3289112802727443, test loss: 0.7525202966359105\n",
      "Iteration 2651 training loss: 0.3288575133795873, test loss: 0.7525397227463629\n",
      "Iteration 2652 training loss: 0.32880376458078037, test loss: 0.7525591488615504\n",
      "Iteration 2653 training loss: 0.32875003386631, test loss: 0.7525785749798664\n",
      "Iteration 2654 training loss: 0.32869632122617126, test loss: 0.7525980010997081\n",
      "Iteration 2655 training loss: 0.3286426266503672, test loss: 0.7526174272194763\n",
      "Iteration 2656 training loss: 0.328588950128909, test loss: 0.7526368533375762\n",
      "Iteration 2657 training loss: 0.3285352916518163, test loss: 0.7526562794524156\n",
      "Iteration 2658 training loss: 0.3284816512091166, test loss: 0.7526757055624073\n",
      "Iteration 2659 training loss: 0.32842802879084587, test loss: 0.7526951316659668\n",
      "Iteration 2660 training loss: 0.3283744243870483, test loss: 0.7527145577615144\n",
      "Iteration 2661 training loss: 0.32832083798777584, test loss: 0.7527339838474731\n",
      "Iteration 2662 training loss: 0.32826726958308894, test loss: 0.75275340992227\n",
      "Iteration 2663 training loss: 0.3282137191630562, test loss: 0.7527728359843362\n",
      "Iteration 2664 training loss: 0.32816018671775427, test loss: 0.7527922620321064\n",
      "Iteration 2665 training loss: 0.3281066722372678, test loss: 0.7528116880640185\n",
      "Iteration 2666 training loss: 0.3280531757116898, test loss: 0.7528311140785148\n",
      "Iteration 2667 training loss: 0.3279996971311211, test loss: 0.7528505400740403\n",
      "Iteration 2668 training loss: 0.3279462364856711, test loss: 0.7528699660490444\n",
      "Iteration 2669 training loss: 0.32789279376545677, test loss: 0.7528893920019806\n",
      "Iteration 2670 training loss: 0.32783936896060345, test loss: 0.7529088179313047\n",
      "Iteration 2671 training loss: 0.3277859620612446, test loss: 0.7529282438354772\n",
      "Iteration 2672 training loss: 0.3277325730575215, test loss: 0.7529476697129616\n",
      "Iteration 2673 training loss: 0.3276792019395837, test loss: 0.7529670955622254\n",
      "Iteration 2674 training loss: 0.3276258486975887, test loss: 0.7529865213817396\n",
      "Iteration 2675 training loss: 0.3275725133217022, test loss: 0.7530059471699786\n",
      "Iteration 2676 training loss: 0.3275191958020975, test loss: 0.7530253729254208\n",
      "Iteration 2677 training loss: 0.32746589612895644, test loss: 0.7530447986465476\n",
      "Iteration 2678 training loss: 0.3274126142924685, test loss: 0.7530642243318445\n",
      "Iteration 2679 training loss: 0.32735935028283153, test loss: 0.7530836499797999\n",
      "Iteration 2680 training loss: 0.32730610409025096, test loss: 0.7531030755889067\n",
      "Iteration 2681 training loss: 0.3272528757049403, test loss: 0.7531225011576603\n",
      "Iteration 2682 training loss: 0.32719966511712134, test loss: 0.7531419266845605\n",
      "Iteration 2683 training loss: 0.3271464723170236, test loss: 0.7531613521681096\n",
      "Iteration 2684 training loss: 0.3270932972948845, test loss: 0.7531807776068147\n",
      "Iteration 2685 training loss: 0.3270401400409494, test loss: 0.7532002029991854\n",
      "Iteration 2686 training loss: 0.32698700054547186, test loss: 0.7532196283437349\n",
      "Iteration 2687 training loss: 0.32693387879871316, test loss: 0.7532390536389805\n",
      "Iteration 2688 training loss: 0.32688077479094246, test loss: 0.7532584788834421\n",
      "Iteration 2689 training loss: 0.3268276885124369, test loss: 0.7532779040756437\n",
      "Iteration 2690 training loss: 0.32677461995348167, test loss: 0.7532973292141126\n",
      "Iteration 2691 training loss: 0.32672156910436967, test loss: 0.7533167542973794\n",
      "Iteration 2692 training loss: 0.32666853595540163, test loss: 0.7533361793239783\n",
      "Iteration 2693 training loss: 0.32661552049688647, test loss: 0.7533556042924465\n",
      "Iteration 2694 training loss: 0.32656252271914044, test loss: 0.7533750292013254\n",
      "Iteration 2695 training loss: 0.3265095426124884, test loss: 0.7533944540491592\n",
      "Iteration 2696 training loss: 0.3264565801672623, test loss: 0.7534138788344954\n",
      "Iteration 2697 training loss: 0.32640363537380246, test loss: 0.7534333035558853\n",
      "Iteration 2698 training loss: 0.3263507082224568, test loss: 0.7534527282118835\n",
      "Iteration 2699 training loss: 0.3262977987035811, test loss: 0.7534721528010477\n",
      "Iteration 2700 training loss: 0.326244906807539, test loss: 0.753491577321939\n",
      "Iteration 2701 training loss: 0.3261920325247019, test loss: 0.7535110017731226\n",
      "Iteration 2702 training loss: 0.32613917584544916, test loss: 0.7535304261531659\n",
      "Iteration 2703 training loss: 0.32608633676016757, test loss: 0.7535498504606403\n",
      "Iteration 2704 training loss: 0.326033515259252, test loss: 0.7535692746941203\n",
      "Iteration 2705 training loss: 0.32598071133310497, test loss: 0.7535886988521836\n",
      "Iteration 2706 training loss: 0.325927924972137, test loss: 0.753608122933412\n",
      "Iteration 2707 training loss: 0.325875156166766, test loss: 0.7536275469363894\n",
      "Iteration 2708 training loss: 0.3258224049074178, test loss: 0.7536469708597041\n",
      "Iteration 2709 training loss: 0.3257696711845261, test loss: 0.7536663947019469\n",
      "Iteration 2710 training loss: 0.32571695498853204, test loss: 0.7536858184617118\n",
      "Iteration 2711 training loss: 0.3256642563098849, test loss: 0.753705242137597\n",
      "Iteration 2712 training loss: 0.32561157513904104, test loss: 0.7537246657282032\n",
      "Iteration 2713 training loss: 0.3255589114664652, test loss: 0.7537440892321342\n",
      "Iteration 2714 training loss: 0.3255062652826295, test loss: 0.753763512647998\n",
      "Iteration 2715 training loss: 0.32545363657801357, test loss: 0.7537829359744044\n",
      "Iteration 2716 training loss: 0.32540102534310505, test loss: 0.7538023592099676\n",
      "Iteration 2717 training loss: 0.32534843156839915, test loss: 0.7538217823533047\n",
      "Iteration 2718 training loss: 0.3252958552443986, test loss: 0.7538412054030356\n",
      "Iteration 2719 training loss: 0.325243296361614, test loss: 0.7538606283577841\n",
      "Iteration 2720 training loss: 0.3251907549105634, test loss: 0.7538800512161765\n",
      "Iteration 2721 training loss: 0.3251382308817726, test loss: 0.7538994739768429\n",
      "Iteration 2722 training loss: 0.32508572426577503, test loss: 0.7539188966384157\n",
      "Iteration 2723 training loss: 0.32503323505311166, test loss: 0.7539383191995315\n",
      "Iteration 2724 training loss: 0.3249807632343311, test loss: 0.7539577416588297\n",
      "Iteration 2725 training loss: 0.3249283087999897, test loss: 0.753977164014952\n",
      "Iteration 2726 training loss: 0.3248758717406513, test loss: 0.7539965862665446\n",
      "Iteration 2727 training loss: 0.32482345204688706, test loss: 0.7540160084122559\n",
      "Iteration 2728 training loss: 0.32477104970927617, test loss: 0.7540354304507381\n",
      "Iteration 2729 training loss: 0.32471866471840527, test loss: 0.7540548523806456\n",
      "Iteration 2730 training loss: 0.3246662970648684, test loss: 0.7540742742006368\n",
      "Iteration 2731 training loss: 0.32461394673926713, test loss: 0.7540936959093726\n",
      "Iteration 2732 training loss: 0.3245616137322108, test loss: 0.7541131175055171\n",
      "Iteration 2733 training loss: 0.32450929803431616, test loss: 0.7541325389877381\n",
      "Iteration 2734 training loss: 0.3244569996362075, test loss: 0.7541519603547057\n",
      "Iteration 2735 training loss: 0.3244047185285166, test loss: 0.7541713816050931\n",
      "Iteration 2736 training loss: 0.3243524547018828, test loss: 0.7541908027375772\n",
      "Iteration 2737 training loss: 0.3243002081469529, test loss: 0.7542102237508376\n",
      "Iteration 2738 training loss: 0.3242479788543812, test loss: 0.7542296446435564\n",
      "Iteration 2739 training loss: 0.32419576681482953, test loss: 0.7542490654144196\n",
      "Iteration 2740 training loss: 0.3241435720189672, test loss: 0.7542684860621157\n",
      "Iteration 2741 training loss: 0.3240913944574709, test loss: 0.7542879065853364\n",
      "Iteration 2742 training loss: 0.3240392341210249, test loss: 0.7543073269827765\n",
      "Iteration 2743 training loss: 0.3239870910003209, test loss: 0.7543267472531338\n",
      "Iteration 2744 training loss: 0.3239349650860579, test loss: 0.7543461673951086\n",
      "Iteration 2745 training loss: 0.3238828563689426, test loss: 0.7543655874074048\n",
      "Iteration 2746 training loss: 0.32383076483968876, test loss: 0.7543850072887288\n",
      "Iteration 2747 training loss: 0.32377869048901803, test loss: 0.7544044270377908\n",
      "Iteration 2748 training loss: 0.32372663330765905, test loss: 0.754423846653303\n",
      "Iteration 2749 training loss: 0.32367459328634823, test loss: 0.7544432661339808\n",
      "Iteration 2750 training loss: 0.323622570415829, test loss: 0.754462685478543\n",
      "Iteration 2751 training loss: 0.3235705646868524, test loss: 0.7544821046857111\n",
      "Iteration 2752 training loss: 0.3235185760901769, test loss: 0.7545015237542091\n",
      "Iteration 2753 training loss: 0.32346660461656823, test loss: 0.7545209426827646\n",
      "Iteration 2754 training loss: 0.3234146502567995, test loss: 0.7545403614701076\n",
      "Iteration 2755 training loss: 0.32336271300165126, test loss: 0.7545597801149715\n",
      "Iteration 2756 training loss: 0.32331079284191117, test loss: 0.7545791986160921\n",
      "Iteration 2757 training loss: 0.32325888976837464, test loss: 0.7545986169722084\n",
      "Iteration 2758 training loss: 0.323207003771844, test loss: 0.7546180351820623\n",
      "Iteration 2759 training loss: 0.3231551348431292, test loss: 0.7546374532443987\n",
      "Iteration 2760 training loss: 0.3231032829730473, test loss: 0.7546568711579645\n",
      "Iteration 2761 training loss: 0.32305144815242276, test loss: 0.7546762889215108\n",
      "Iteration 2762 training loss: 0.3229996303720875, test loss: 0.7546957065337907\n",
      "Iteration 2763 training loss: 0.32294782962288027, test loss: 0.7547151239935604\n",
      "Iteration 2764 training loss: 0.3228960458956477, test loss: 0.7547345412995785\n",
      "Iteration 2765 training loss: 0.3228442791812432, test loss: 0.7547539584506076\n",
      "Iteration 2766 training loss: 0.32279252947052783, test loss: 0.7547733754454118\n",
      "Iteration 2767 training loss: 0.32274079675436956, test loss: 0.7547927922827586\n",
      "Iteration 2768 training loss: 0.32268908102364385, test loss: 0.7548122089614191\n",
      "Iteration 2769 training loss: 0.32263738226923333, test loss: 0.7548316254801652\n",
      "Iteration 2770 training loss: 0.32258570048202806, test loss: 0.7548510418377736\n",
      "Iteration 2771 training loss: 0.32253403565292493, test loss: 0.7548704580330228\n",
      "Iteration 2772 training loss: 0.3224823877728282, test loss: 0.7548898740646943\n",
      "Iteration 2773 training loss: 0.3224307568326498, test loss: 0.7549092899315726\n",
      "Iteration 2774 training loss: 0.3223791428233081, test loss: 0.7549287056324445\n",
      "Iteration 2775 training loss: 0.3223275457357292, test loss: 0.7549481211661\n",
      "Iteration 2776 training loss: 0.3222759655608463, test loss: 0.7549675365313314\n",
      "Iteration 2777 training loss: 0.3222244022895996, test loss: 0.7549869517269343\n",
      "Iteration 2778 training loss: 0.3221728559129367, test loss: 0.7550063667517065\n",
      "Iteration 2779 training loss: 0.32212132642181224, test loss: 0.7550257816044494\n",
      "Iteration 2780 training loss: 0.322069813807188, test loss: 0.7550451962839662\n",
      "Iteration 2781 training loss: 0.32201831806003295, test loss: 0.7550646107890628\n",
      "Iteration 2782 training loss: 0.32196683917132324, test loss: 0.7550840251185489\n",
      "Iteration 2783 training loss: 0.3219153771320422, test loss: 0.7551034392712357\n",
      "Iteration 2784 training loss: 0.32186393193318014, test loss: 0.7551228532459375\n",
      "Iteration 2785 training loss: 0.32181250356573465, test loss: 0.7551422670414719\n",
      "Iteration 2786 training loss: 0.3217610920207102, test loss: 0.7551616806566585\n",
      "Iteration 2787 training loss: 0.3217096972891187, test loss: 0.7551810940903195\n",
      "Iteration 2788 training loss: 0.32165831936197886, test loss: 0.7552005073412803\n",
      "Iteration 2789 training loss: 0.32160695823031654, test loss: 0.7552199204083686\n",
      "Iteration 2790 training loss: 0.32155561388516485, test loss: 0.7552393332904149\n",
      "Iteration 2791 training loss: 0.3215042863175639, test loss: 0.7552587459862526\n",
      "Iteration 2792 training loss: 0.32145297551856067, test loss: 0.755278158494717\n",
      "Iteration 2793 training loss: 0.32140168147920944, test loss: 0.755297570814647\n",
      "Iteration 2794 training loss: 0.32135040419057154, test loss: 0.7553169829448834\n",
      "Iteration 2795 training loss: 0.32129914364371515, test loss: 0.7553363948842697\n",
      "Iteration 2796 training loss: 0.32124789982971563, test loss: 0.7553558066316526\n",
      "Iteration 2797 training loss: 0.32119667273965535, test loss: 0.7553752181858808\n",
      "Iteration 2798 training loss: 0.32114546236462377, test loss: 0.7553946295458058\n",
      "Iteration 2799 training loss: 0.3210942686957171, test loss: 0.755414040710282\n",
      "Iteration 2800 training loss: 0.321043091724039, test loss: 0.7554334516781658\n",
      "Iteration 2801 training loss: 0.32099193144069965, test loss: 0.7554528624483166\n",
      "Iteration 2802 training loss: 0.3209407878368165, test loss: 0.7554722730195964\n",
      "Iteration 2803 training loss: 0.32088966090351406, test loss: 0.7554916833908696\n",
      "Iteration 2804 training loss: 0.3208385506319235, test loss: 0.7555110935610031\n",
      "Iteration 2805 training loss: 0.32078745701318323, test loss: 0.755530503528867\n",
      "Iteration 2806 training loss: 0.3207363800384386, test loss: 0.755549913293333\n",
      "Iteration 2807 training loss: 0.32068531969884173, test loss: 0.7555693228532758\n",
      "Iteration 2808 training loss: 0.32063427598555183, test loss: 0.7555887322075729\n",
      "Iteration 2809 training loss: 0.3205832488897351, test loss: 0.7556081413551039\n",
      "Iteration 2810 training loss: 0.3205322384025645, test loss: 0.7556275502947513\n",
      "Iteration 2811 training loss: 0.3204812445152201, test loss: 0.7556469590253999\n",
      "Iteration 2812 training loss: 0.32043026721888873, test loss: 0.7556663675459367\n",
      "Iteration 2813 training loss: 0.3203793065047642, test loss: 0.7556857758552519\n",
      "Iteration 2814 training loss: 0.3203283623640474, test loss: 0.7557051839522381\n",
      "Iteration 2815 training loss: 0.32027743478794557, test loss: 0.7557245918357897\n",
      "Iteration 2816 training loss: 0.3202265237676735, test loss: 0.7557439995048043\n",
      "Iteration 2817 training loss: 0.3201756292944525, test loss: 0.7557634069581817\n",
      "Iteration 2818 training loss: 0.32012475135951074, test loss: 0.7557828141948243\n",
      "Iteration 2819 training loss: 0.32007388995408337, test loss: 0.7558022212136368\n",
      "Iteration 2820 training loss: 0.3200230450694123, test loss: 0.7558216280135264\n",
      "Iteration 2821 training loss: 0.3199722166967465, test loss: 0.7558410345934033\n",
      "Iteration 2822 training loss: 0.3199214048273415, test loss: 0.755860440952179\n",
      "Iteration 2823 training loss: 0.3198706094524599, test loss: 0.7558798470887687\n",
      "Iteration 2824 training loss: 0.3198198305633708, test loss: 0.7558992530020889\n",
      "Iteration 2825 training loss: 0.31976906815135064, test loss: 0.7559186586910595\n",
      "Iteration 2826 training loss: 0.31971832220768215, test loss: 0.7559380641546024\n",
      "Iteration 2827 training loss: 0.31966759272365525, test loss: 0.7559574693916419\n",
      "Iteration 2828 training loss: 0.3196168796905664, test loss: 0.755976874401105\n",
      "Iteration 2829 training loss: 0.319566183099719, test loss: 0.7559962791819207\n",
      "Iteration 2830 training loss: 0.3195155029424232, test loss: 0.7560156837330203\n",
      "Iteration 2831 training loss: 0.3194648392099959, test loss: 0.7560350880533383\n",
      "Iteration 2832 training loss: 0.3194141918937608, test loss: 0.756054492141811\n",
      "Iteration 2833 training loss: 0.3193635609850484, test loss: 0.7560738959973765\n",
      "Iteration 2834 training loss: 0.31931294647519587, test loss: 0.7560932996189771\n",
      "Iteration 2835 training loss: 0.31926234835554707, test loss: 0.7561127030055556\n",
      "Iteration 2836 training loss: 0.31921176661745276, test loss: 0.7561321061560576\n",
      "Iteration 2837 training loss: 0.31916120125227054, test loss: 0.7561515090694323\n",
      "Iteration 2838 training loss: 0.3191106522513643, test loss: 0.7561709117446295\n",
      "Iteration 2839 training loss: 0.3190601196061052, test loss: 0.7561903141806023\n",
      "Iteration 2840 training loss: 0.3190096033078706, test loss: 0.7562097163763066\n",
      "Iteration 2841 training loss: 0.31895910334804495, test loss: 0.7562291183306994\n",
      "Iteration 2842 training loss: 0.3189086197180193, test loss: 0.7562485200427409\n",
      "Iteration 2843 training loss: 0.3188581524091911, test loss: 0.7562679215113938\n",
      "Iteration 2844 training loss: 0.31880770141296494, test loss: 0.756287322735622\n",
      "Iteration 2845 training loss: 0.3187572667207517, test loss: 0.7563067237143929\n",
      "Iteration 2846 training loss: 0.3187068483239693, test loss: 0.7563261244466757\n",
      "Iteration 2847 training loss: 0.3186564462140421, test loss: 0.7563455249314419\n",
      "Iteration 2848 training loss: 0.3186060603824009, test loss: 0.7563649251676655\n",
      "Iteration 2849 training loss: 0.3185556908204837, test loss: 0.7563843251543224\n",
      "Iteration 2850 training loss: 0.31850533751973464, test loss: 0.7564037248903912\n",
      "Iteration 2851 training loss: 0.3184550004716048, test loss: 0.7564231243748529\n",
      "Iteration 2852 training loss: 0.31840467966755176, test loss: 0.75644252360669\n",
      "Iteration 2853 training loss: 0.3183543750990397, test loss: 0.7564619225848883\n",
      "Iteration 2854 training loss: 0.3183040867575394, test loss: 0.7564813213084347\n",
      "Iteration 2855 training loss: 0.3182538146345284, test loss: 0.7565007197763196\n",
      "Iteration 2856 training loss: 0.31820355872149075, test loss: 0.7565201179875347\n",
      "Iteration 2857 training loss: 0.31815331900991706, test loss: 0.7565395159410743\n",
      "Iteration 2858 training loss: 0.3181030954913045, test loss: 0.7565589136359351\n",
      "Iteration 2859 training loss: 0.31805288815715693, test loss: 0.7565783110711158\n",
      "Iteration 2860 training loss: 0.3180026969989847, test loss: 0.7565977082456172\n",
      "Iteration 2861 training loss: 0.31795252200830465, test loss: 0.7566171051584427\n",
      "Iteration 2862 training loss: 0.31790236317664045, test loss: 0.7566365018085979\n",
      "Iteration 2863 training loss: 0.31785222049552203, test loss: 0.7566558981950902\n",
      "Iteration 2864 training loss: 0.31780209395648606, test loss: 0.7566752943169297\n",
      "Iteration 2865 training loss: 0.3177519835510756, test loss: 0.7566946901731283\n",
      "Iteration 2866 training loss: 0.31770188927084025, test loss: 0.7567140857627004\n",
      "Iteration 2867 training loss: 0.3176518111073364, test loss: 0.7567334810846621\n",
      "Iteration 2868 training loss: 0.3176017490521267, test loss: 0.7567528761380324\n",
      "Iteration 2869 training loss: 0.3175517030967801, test loss: 0.7567722709218322\n",
      "Iteration 2870 training loss: 0.3175016732328726, test loss: 0.756791665435084\n",
      "Iteration 2871 training loss: 0.3174516594519863, test loss: 0.7568110596768136\n",
      "Iteration 2872 training loss: 0.31740166174570983, test loss: 0.756830453646048\n",
      "Iteration 2873 training loss: 0.31735168010563847, test loss: 0.7568498473418164\n",
      "Iteration 2874 training loss: 0.317301714523374, test loss: 0.7568692407631511\n",
      "Iteration 2875 training loss: 0.31725176499052427, test loss: 0.7568886339090855\n",
      "Iteration 2876 training loss: 0.317201831498704, test loss: 0.7569080267786554\n",
      "Iteration 2877 training loss: 0.3171519140395343, test loss: 0.7569274193708992\n",
      "Iteration 2878 training loss: 0.3171020126046426, test loss: 0.7569468116848569\n",
      "Iteration 2879 training loss: 0.3170521271856629, test loss: 0.7569662037195709\n",
      "Iteration 2880 training loss: 0.3170022577742354, test loss: 0.7569855954740856\n",
      "Iteration 2881 training loss: 0.316952404362007, test loss: 0.7570049869474479\n",
      "Iteration 2882 training loss: 0.3169025669406311, test loss: 0.7570243781387059\n",
      "Iteration 2883 training loss: 0.31685274550176706, test loss: 0.7570437690469108\n",
      "Iteration 2884 training loss: 0.31680294003708104, test loss: 0.7570631596711153\n",
      "Iteration 2885 training loss: 0.31675315053824543, test loss: 0.7570825500103744\n",
      "Iteration 2886 training loss: 0.3167033769969392, test loss: 0.7571019400637453\n",
      "Iteration 2887 training loss: 0.31665361940484743, test loss: 0.7571213298302871\n",
      "Iteration 2888 training loss: 0.3166038777536618, test loss: 0.7571407193090609\n",
      "Iteration 2889 training loss: 0.31655415203508025, test loss: 0.7571601084991304\n",
      "Iteration 2890 training loss: 0.3165044422408072, test loss: 0.7571794973995604\n",
      "Iteration 2891 training loss: 0.3164547483625534, test loss: 0.7571988860094189\n",
      "Iteration 2892 training loss: 0.3164050703920358, test loss: 0.7572182743277748\n",
      "Iteration 2893 training loss: 0.3163554083209779, test loss: 0.7572376623537002\n",
      "Iteration 2894 training loss: 0.31630576214110945, test loss: 0.7572570500862685\n",
      "Iteration 2895 training loss: 0.3162561318441664, test loss: 0.7572764375245553\n",
      "Iteration 2896 training loss: 0.31620651742189143, test loss: 0.7572958246676382\n",
      "Iteration 2897 training loss: 0.3161569188660331, test loss: 0.7573152115145975\n",
      "Iteration 2898 training loss: 0.31610733616834646, test loss: 0.7573345980645142\n",
      "Iteration 2899 training loss: 0.31605776932059293, test loss: 0.7573539843164722\n",
      "Iteration 2900 training loss: 0.3160082183145402, test loss: 0.7573733702695579\n",
      "Iteration 2901 training loss: 0.31595868314196224, test loss: 0.7573927559228585\n",
      "Iteration 2902 training loss: 0.3159091637946393, test loss: 0.7574121412754637\n",
      "Iteration 2903 training loss: 0.3158596602643578, test loss: 0.7574315263264659\n",
      "Iteration 2904 training loss: 0.31581017254291066, test loss: 0.7574509110749587\n",
      "Iteration 2905 training loss: 0.31576070062209693, test loss: 0.7574702955200378\n",
      "Iteration 2906 training loss: 0.3157112444937219, test loss: 0.7574896796608007\n",
      "Iteration 2907 training loss: 0.31566180414959727, test loss: 0.7575090634963477\n",
      "Iteration 2908 training loss: 0.31561237958154076, test loss: 0.7575284470257803\n",
      "Iteration 2909 training loss: 0.31556297078137663, test loss: 0.7575478302482024\n",
      "Iteration 2910 training loss: 0.315513577740935, test loss: 0.7575672131627194\n",
      "Iteration 2911 training loss: 0.31546420045205265, test loss: 0.7575865957684391\n",
      "Iteration 2912 training loss: 0.3154148389065722, test loss: 0.7576059780644714\n",
      "Iteration 2913 training loss: 0.3153654930963426, test loss: 0.7576253600499275\n",
      "Iteration 2914 training loss: 0.3153161630132192, test loss: 0.7576447417239212\n",
      "Iteration 2915 training loss: 0.31526684864906335, test loss: 0.7576641230855677\n",
      "Iteration 2916 training loss: 0.3152175499957426, test loss: 0.7576835041339846\n",
      "Iteration 2917 training loss: 0.3151682670451309, test loss: 0.7577028848682913\n",
      "Iteration 2918 training loss: 0.315118999789108, test loss: 0.7577222652876088\n",
      "Iteration 2919 training loss: 0.31506974821956035, test loss: 0.7577416453910605\n",
      "Iteration 2920 training loss: 0.31502051232838, test loss: 0.7577610251777718\n",
      "Iteration 2921 training loss: 0.3149712921074657, test loss: 0.7577804046468691\n",
      "Iteration 2922 training loss: 0.314922087548722, test loss: 0.7577997837974818\n",
      "Iteration 2923 training loss: 0.31487289864405954, test loss: 0.7578191626287406\n",
      "Iteration 2924 training loss: 0.31482372538539555, test loss: 0.7578385411397783\n",
      "Iteration 2925 training loss: 0.3147745677646529, test loss: 0.7578579193297297\n",
      "Iteration 2926 training loss: 0.31472542577376106, test loss: 0.7578772971977313\n",
      "Iteration 2927 training loss: 0.31467629940465525, test loss: 0.7578966747429212\n",
      "Iteration 2928 training loss: 0.3146271886492769, test loss: 0.7579160519644402\n",
      "Iteration 2929 training loss: 0.3145780934995737, test loss: 0.7579354288614303\n",
      "Iteration 2930 training loss: 0.31452901394749927, test loss: 0.7579548054330352\n",
      "Iteration 2931 training loss: 0.3144799499850135, test loss: 0.7579741816784014\n",
      "Iteration 2932 training loss: 0.3144309016040822, test loss: 0.7579935575966764\n",
      "Iteration 2933 training loss: 0.3143818687966775, test loss: 0.7580129331870101\n",
      "Iteration 2934 training loss: 0.31433285155477725, test loss: 0.7580323084485537\n",
      "Iteration 2935 training loss: 0.31428384987036584, test loss: 0.7580516833804609\n",
      "Iteration 2936 training loss: 0.31423486373543325, test loss: 0.7580710579818866\n",
      "Iteration 2937 training loss: 0.314185893141976, test loss: 0.758090432251988\n",
      "Iteration 2938 training loss: 0.3141369380819963, test loss: 0.7581098061899242\n",
      "Iteration 2939 training loss: 0.3140879985475025, test loss: 0.7581291797948556\n",
      "Iteration 2940 training loss: 0.3140390745305091, test loss: 0.7581485530659448\n",
      "Iteration 2941 training loss: 0.3139901660230366, test loss: 0.7581679260023563\n",
      "Iteration 2942 training loss: 0.3139412730171115, test loss: 0.7581872986032564\n",
      "Iteration 2943 training loss: 0.3138923955047663, test loss: 0.7582066708678129\n",
      "Iteration 2944 training loss: 0.3138435334780395, test loss: 0.7582260427951958\n",
      "Iteration 2945 training loss: 0.31379468692897583, test loss: 0.7582454143845766\n",
      "Iteration 2946 training loss: 0.3137458558496257, test loss: 0.7582647856351287\n",
      "Iteration 2947 training loss: 0.3136970402320459, test loss: 0.7582841565460273\n",
      "Iteration 2948 training loss: 0.3136482400682989, test loss: 0.7583035271164497\n",
      "Iteration 2949 training loss: 0.31359945535045314, test loss: 0.7583228973455742\n",
      "Iteration 2950 training loss: 0.31355068607058334, test loss: 0.7583422672325822\n",
      "Iteration 2951 training loss: 0.31350193222076994, test loss: 0.7583616367766552\n",
      "Iteration 2952 training loss: 0.3134531937930995, test loss: 0.758381005976978\n",
      "Iteration 2953 training loss: 0.31340447077966443, test loss: 0.7584003748327361\n",
      "Iteration 2954 training loss: 0.3133557631725632, test loss: 0.7584197433431173\n",
      "Iteration 2955 training loss: 0.3133070709639001, test loss: 0.7584391115073111\n",
      "Iteration 2956 training loss: 0.31325839414578555, test loss: 0.7584584793245088\n",
      "Iteration 2957 training loss: 0.31320973271033575, test loss: 0.758477846793903\n",
      "Iteration 2958 training loss: 0.313161086649673, test loss: 0.7584972139146888\n",
      "Iteration 2959 training loss: 0.3131124559559253, test loss: 0.7585165806860624\n",
      "Iteration 2960 training loss: 0.3130638406212267, test loss: 0.7585359471072222\n",
      "Iteration 2961 training loss: 0.3130152406377173, test loss: 0.7585553131773679\n",
      "Iteration 2962 training loss: 0.3129666559975428, test loss: 0.7585746788957012\n",
      "Iteration 2963 training loss: 0.31291808669285515, test loss: 0.7585940442614255\n",
      "Iteration 2964 training loss: 0.31286953271581186, test loss: 0.7586134092737459\n",
      "Iteration 2965 training loss: 0.3128209940585765, test loss: 0.7586327739318695\n",
      "Iteration 2966 training loss: 0.3127724707133187, test loss: 0.7586521382350043\n",
      "Iteration 2967 training loss: 0.31272396267221364, test loss: 0.7586715021823611\n",
      "Iteration 2968 training loss: 0.3126754699274427, test loss: 0.7586908657731515\n",
      "Iteration 2969 training loss: 0.31262699247119274, test loss: 0.7587102290065892\n",
      "Iteration 2970 training loss: 0.31257853029565674, test loss: 0.7587295918818898\n",
      "Iteration 2971 training loss: 0.3125300833930336, test loss: 0.7587489543982703\n",
      "Iteration 2972 training loss: 0.31248165175552794, test loss: 0.7587683165549489\n",
      "Iteration 2973 training loss: 0.3124332353753502, test loss: 0.7587876783511469\n",
      "Iteration 2974 training loss: 0.31238483424471664, test loss: 0.7588070397860857\n",
      "Iteration 2975 training loss: 0.31233644835584967, test loss: 0.7588264008589892\n",
      "Iteration 2976 training loss: 0.312288077700977, test loss: 0.7588457615690832\n",
      "Iteration 2977 training loss: 0.3122397222723325, test loss: 0.7588651219155946\n",
      "Iteration 2978 training loss: 0.3121913820621558, test loss: 0.7588844818977523\n",
      "Iteration 2979 training loss: 0.3121430570626924, test loss: 0.7589038415147864\n",
      "Iteration 2980 training loss: 0.31209474726619346, test loss: 0.7589232007659297\n",
      "Iteration 2981 training loss: 0.312046452664916, test loss: 0.758942559650415\n",
      "Iteration 2982 training loss: 0.3119981732511229, test loss: 0.7589619181674786\n",
      "Iteration 2983 training loss: 0.3119499090170826, test loss: 0.7589812763163575\n",
      "Iteration 2984 training loss: 0.31190165995506963, test loss: 0.7590006340962897\n",
      "Iteration 2985 training loss: 0.31185342605736416, test loss: 0.759019991506516\n",
      "Iteration 2986 training loss: 0.311805207316252, test loss: 0.7590393485462783\n",
      "Iteration 2987 training loss: 0.31175700372402493, test loss: 0.7590587052148204\n",
      "Iteration 2988 training loss: 0.3117088152729803, test loss: 0.7590780615113873\n",
      "Iteration 2989 training loss: 0.3116606419554213, test loss: 0.7590974174352259\n",
      "Iteration 2990 training loss: 0.3116124837636568, test loss: 0.7591167729855847\n",
      "Iteration 2991 training loss: 0.31156434069000166, test loss: 0.7591361281617135\n",
      "Iteration 2992 training loss: 0.3115162127267761, test loss: 0.7591554829628644\n",
      "Iteration 2993 training loss: 0.31146809986630636, test loss: 0.7591748373882903\n",
      "Iteration 2994 training loss: 0.3114200021009241, test loss: 0.7591941914372463\n",
      "Iteration 2995 training loss: 0.31137191942296705, test loss: 0.7592135451089889\n",
      "Iteration 2996 training loss: 0.3113238518247785, test loss: 0.7592328984027761\n",
      "Iteration 2997 training loss: 0.3112757992987072, test loss: 0.7592522513178676\n",
      "Iteration 2998 training loss: 0.311227761837108, test loss: 0.7592716038535247\n",
      "Iteration 2999 training loss: 0.3111797394323413, test loss: 0.7592909560090101\n",
      "Iteration 3000 training loss: 0.3111317320767729, test loss: 0.7593103077835882\n",
      "Iteration 3001 training loss: 0.3110837397627748, test loss: 0.7593296591765254\n",
      "Iteration 3002 training loss: 0.3110357624827242, test loss: 0.7593490101870887\n",
      "Iteration 3003 training loss: 0.3109878002290042, test loss: 0.7593683608145475\n",
      "Iteration 3004 training loss: 0.31093985299400356, test loss: 0.7593877110581726\n",
      "Iteration 3005 training loss: 0.31089192077011674, test loss: 0.7594070609172359\n",
      "Iteration 3006 training loss: 0.3108440035497436, test loss: 0.7594264103910114\n",
      "Iteration 3007 training loss: 0.31079610132528984, test loss: 0.7594457594787744\n",
      "Iteration 3008 training loss: 0.310748214089167, test loss: 0.759465108179802\n",
      "Iteration 3009 training loss: 0.31070034183379175, test loss: 0.7594844564933725\n",
      "Iteration 3010 training loss: 0.31065248455158684, test loss: 0.7595038044187656\n",
      "Iteration 3011 training loss: 0.3106046422349804, test loss: 0.7595231519552633\n",
      "Iteration 3012 training loss: 0.3105568148764063, test loss: 0.7595424991021483\n",
      "Iteration 3013 training loss: 0.3105090024683039, test loss: 0.7595618458587053\n",
      "Iteration 3014 training loss: 0.3104612050031183, test loss: 0.7595811922242204\n",
      "Iteration 3015 training loss: 0.31041342247330006, test loss: 0.7596005381979813\n",
      "Iteration 3016 training loss: 0.31036565487130546, test loss: 0.7596198837792772\n",
      "Iteration 3017 training loss: 0.3103179021895963, test loss: 0.7596392289673984\n",
      "Iteration 3018 training loss: 0.3102701644206401, test loss: 0.7596585737616374\n",
      "Iteration 3019 training loss: 0.31022244155690964, test loss: 0.7596779181612876\n",
      "Iteration 3020 training loss: 0.3101747335908836, test loss: 0.7596972621656446\n",
      "Iteration 3021 training loss: 0.310127040515046, test loss: 0.7597166057740047\n",
      "Iteration 3022 training loss: 0.31007936232188654, test loss: 0.7597359489856661\n",
      "Iteration 3023 training loss: 0.31003169900390054, test loss: 0.7597552917999286\n",
      "Iteration 3024 training loss: 0.30998405055358863, test loss: 0.7597746342160931\n",
      "Iteration 3025 training loss: 0.30993641696345725, test loss: 0.7597939762334627\n",
      "Iteration 3026 training loss: 0.3098887982260183, test loss: 0.759813317851341\n",
      "Iteration 3027 training loss: 0.3098411943337889, test loss: 0.7598326590690339\n",
      "Iteration 3028 training loss: 0.3097936052792923, test loss: 0.7598519998858485\n",
      "Iteration 3029 training loss: 0.3097460310550568, test loss: 0.7598713403010932\n",
      "Iteration 3030 training loss: 0.30969847165361636, test loss: 0.7598906803140778\n",
      "Iteration 3031 training loss: 0.3096509270675103, test loss: 0.7599100199241142\n",
      "Iteration 3032 training loss: 0.30960339728928393, test loss: 0.7599293591305151\n",
      "Iteration 3033 training loss: 0.3095558823114874, test loss: 0.7599486979325948\n",
      "Iteration 3034 training loss: 0.3095083821266768, test loss: 0.7599680363296692\n",
      "Iteration 3035 training loss: 0.3094608967274136, test loss: 0.7599873743210557\n",
      "Iteration 3036 training loss: 0.30941342610626477, test loss: 0.7600067119060729\n",
      "Iteration 3037 training loss: 0.30936597025580254, test loss: 0.760026049084041\n",
      "Iteration 3038 training loss: 0.30931852916860497, test loss: 0.7600453858542817\n",
      "Iteration 3039 training loss: 0.30927110283725523, test loss: 0.7600647222161178\n",
      "Iteration 3040 training loss: 0.30922369125434235, test loss: 0.7600840581688741\n",
      "Iteration 3041 training loss: 0.3091762944124605, test loss: 0.7601033937118761\n",
      "Iteration 3042 training loss: 0.3091289123042093, test loss: 0.7601227288444515\n",
      "Iteration 3043 training loss: 0.30908154492219403, test loss: 0.760142063565929\n",
      "Iteration 3044 training loss: 0.30903419225902523, test loss: 0.7601613978756384\n",
      "Iteration 3045 training loss: 0.30898685430731887, test loss: 0.760180731772912\n",
      "Iteration 3046 training loss: 0.3089395310596965, test loss: 0.7602000652570818\n",
      "Iteration 3047 training loss: 0.308892222508785, test loss: 0.760219398327483\n",
      "Iteration 3048 training loss: 0.30884492864721663, test loss: 0.7602387309834513\n",
      "Iteration 3049 training loss: 0.3087976494676291, test loss: 0.7602580632243233\n",
      "Iteration 3050 training loss: 0.30875038496266555, test loss: 0.7602773950494383\n",
      "Iteration 3051 training loss: 0.3087031351249744, test loss: 0.7602967264581361\n",
      "Iteration 3052 training loss: 0.3086558999472097, test loss: 0.7603160574497579\n",
      "Iteration 3053 training loss: 0.30860867942203063, test loss: 0.7603353880236465\n",
      "Iteration 3054 training loss: 0.30856147354210195, test loss: 0.7603547181791463\n",
      "Iteration 3055 training loss: 0.30851428230009365, test loss: 0.7603740479156027\n",
      "Iteration 3056 training loss: 0.30846710568868124, test loss: 0.7603933772323624\n",
      "Iteration 3057 training loss: 0.30841994370054543, test loss: 0.7604127061287738\n",
      "Iteration 3058 training loss: 0.30837279632837244, test loss: 0.7604320346041867\n",
      "Iteration 3059 training loss: 0.30832566356485386, test loss: 0.7604513626579522\n",
      "Iteration 3060 training loss: 0.3082785454026865, test loss: 0.7604706902894223\n",
      "Iteration 3061 training loss: 0.3082314418345726, test loss: 0.760490017497951\n",
      "Iteration 3062 training loss: 0.3081843528532197, test loss: 0.7605093442828933\n",
      "Iteration 3063 training loss: 0.30813727845134076, test loss: 0.760528670643606\n",
      "Iteration 3064 training loss: 0.30809021862165387, test loss: 0.7605479965794463\n",
      "Iteration 3065 training loss: 0.3080431733568828, test loss: 0.7605673220897738\n",
      "Iteration 3066 training loss: 0.30799614264975633, test loss: 0.7605866471739487\n",
      "Iteration 3067 training loss: 0.3079491264930086, test loss: 0.7606059718313333\n",
      "Iteration 3068 training loss: 0.3079021248793792, test loss: 0.7606252960612904\n",
      "Iteration 3069 training loss: 0.30785513780161283, test loss: 0.7606446198631845\n",
      "Iteration 3070 training loss: 0.30780816525245974, test loss: 0.7606639432363816\n",
      "Iteration 3071 training loss: 0.30776120722467515, test loss: 0.7606832661802487\n",
      "Iteration 3072 training loss: 0.30771426371101995, test loss: 0.7607025886941546\n",
      "Iteration 3073 training loss: 0.30766733470426005, test loss: 0.7607219107774689\n",
      "Iteration 3074 training loss: 0.3076204201971666, test loss: 0.7607412324295626\n",
      "Iteration 3075 training loss: 0.3075735201825162, test loss: 0.7607605536498083\n",
      "Iteration 3076 training loss: 0.30752663465309066, test loss: 0.76077987443758\n",
      "Iteration 3077 training loss: 0.30747976360167706, test loss: 0.7607991947922522\n",
      "Iteration 3078 training loss: 0.3074329070210676, test loss: 0.7608185147132018\n",
      "Iteration 3079 training loss: 0.30738606490405995, test loss: 0.7608378341998063\n",
      "Iteration 3080 training loss: 0.3073392372434569, test loss: 0.7608571532514448\n",
      "Iteration 3081 training loss: 0.30729242403206647, test loss: 0.7608764718674971\n",
      "Iteration 3082 training loss: 0.307245625262702, test loss: 0.760895790047345\n",
      "Iteration 3083 training loss: 0.30719884092818195, test loss: 0.7609151077903716\n",
      "Iteration 3084 training loss: 0.30715207102133013, test loss: 0.760934425095961\n",
      "Iteration 3085 training loss: 0.3071053155349754, test loss: 0.7609537419634983\n",
      "Iteration 3086 training loss: 0.3070585744619521, test loss: 0.7609730583923706\n",
      "Iteration 3087 training loss: 0.3070118477950995, test loss: 0.7609923743819657\n",
      "Iteration 3088 training loss: 0.3069651355272623, test loss: 0.7610116899316728\n",
      "Iteration 3089 training loss: 0.3069184376512902, test loss: 0.7610310050408825\n",
      "Iteration 3090 training loss: 0.30687175416003826, test loss: 0.7610503197089865\n",
      "Iteration 3091 training loss: 0.3068250850463665, test loss: 0.761069633935378\n",
      "Iteration 3092 training loss: 0.30677843030314056, test loss: 0.7610889477194512\n",
      "Iteration 3093 training loss: 0.3067317899232309, test loss: 0.7611082610606019\n",
      "Iteration 3094 training loss: 0.30668516389951317, test loss: 0.7611275739582266\n",
      "Iteration 3095 training loss: 0.3066385522248683, test loss: 0.7611468864117237\n",
      "Iteration 3096 training loss: 0.3065919548921823, test loss: 0.7611661984204925\n",
      "Iteration 3097 training loss: 0.30654537189434644, test loss: 0.7611855099839334\n",
      "Iteration 3098 training loss: 0.3064988032242571, test loss: 0.7612048211014482\n",
      "Iteration 3099 training loss: 0.3064522488748158, test loss: 0.7612241317724403\n",
      "Iteration 3100 training loss: 0.30640570883892915, test loss: 0.7612434419963141\n",
      "Iteration 3101 training loss: 0.306359183109509, test loss: 0.7612627517724745\n",
      "Iteration 3102 training loss: 0.3063126716794723, test loss: 0.7612820611003286\n",
      "Iteration 3103 training loss: 0.30626617454174104, test loss: 0.7613013699792844\n",
      "Iteration 3104 training loss: 0.30621969168924246, test loss: 0.7613206784087517\n",
      "Iteration 3105 training loss: 0.30617322311490897, test loss: 0.7613399863881402\n",
      "Iteration 3106 training loss: 0.30612676881167783, test loss: 0.7613592939168619\n",
      "Iteration 3107 training loss: 0.30608032877249164, test loss: 0.7613786009943294\n",
      "Iteration 3108 training loss: 0.30603390299029803, test loss: 0.7613979076199574\n",
      "Iteration 3109 training loss: 0.30598749145804965, test loss: 0.7614172137931606\n",
      "Iteration 3110 training loss: 0.3059410941687045, test loss: 0.7614365195133559\n",
      "Iteration 3111 training loss: 0.30589471111522537, test loss: 0.7614558247799609\n",
      "Iteration 3112 training loss: 0.30584834229058033, test loss: 0.7614751295923944\n",
      "Iteration 3113 training loss: 0.3058019876877423, test loss: 0.7614944339500769\n",
      "Iteration 3114 training loss: 0.3057556472996896, test loss: 0.7615137378524295\n",
      "Iteration 3115 training loss: 0.30570932111940546, test loss: 0.7615330412988749\n",
      "Iteration 3116 training loss: 0.30566300913987804, test loss: 0.7615523442888364\n",
      "Iteration 3117 training loss: 0.3056167113541008, test loss: 0.7615716468217395\n",
      "Iteration 3118 training loss: 0.30557042775507204, test loss: 0.76159094889701\n",
      "Iteration 3119 training loss: 0.30552415833579516, test loss: 0.7616102505140749\n",
      "Iteration 3120 training loss: 0.30547790308927875, test loss: 0.7616295516723631\n",
      "Iteration 3121 training loss: 0.3054316620085364, test loss: 0.7616488523713041\n",
      "Iteration 3122 training loss: 0.30538543508658655, test loss: 0.7616681526103286\n",
      "Iteration 3123 training loss: 0.3053392223164527, test loss: 0.7616874523888685\n",
      "Iteration 3124 training loss: 0.3052930236911637, test loss: 0.7617067517063573\n",
      "Iteration 3125 training loss: 0.305246839203753, test loss: 0.7617260505622292\n",
      "Iteration 3126 training loss: 0.30520066884725927, test loss: 0.7617453489559194\n",
      "Iteration 3127 training loss: 0.3051545126147263, test loss: 0.7617646468868647\n",
      "Iteration 3128 training loss: 0.3051083704992026, test loss: 0.7617839443545029\n",
      "Iteration 3129 training loss: 0.30506224249374186, test loss: 0.761803241358273\n",
      "Iteration 3130 training loss: 0.3050161285914028, test loss: 0.7618225378976151\n",
      "Iteration 3131 training loss: 0.30497002878524904, test loss: 0.7618418339719705\n",
      "Iteration 3132 training loss: 0.304923943068349, test loss: 0.7618611295807813\n",
      "Iteration 3133 training loss: 0.30487787143377665, test loss: 0.7618804247234916\n",
      "Iteration 3134 training loss: 0.3048318138746104, test loss: 0.7618997193995453\n",
      "Iteration 3135 training loss: 0.3047857703839336, test loss: 0.7619190136083887\n",
      "Iteration 3136 training loss: 0.30473974095483514, test loss: 0.761938307349469\n",
      "Iteration 3137 training loss: 0.3046937255804083, test loss: 0.7619576006222337\n",
      "Iteration 3138 training loss: 0.3046477242537515, test loss: 0.7619768934261324\n",
      "Iteration 3139 training loss: 0.30460173696796816, test loss: 0.7619961857606152\n",
      "Iteration 3140 training loss: 0.30455576371616666, test loss: 0.762015477625134\n",
      "Iteration 3141 training loss: 0.30450980449146015, test loss: 0.7620347690191407\n",
      "Iteration 3142 training loss: 0.3044638592869669, test loss: 0.7620540599420897\n",
      "Iteration 3143 training loss: 0.3044179280958101, test loss: 0.7620733503934353\n",
      "Iteration 3144 training loss: 0.30437201091111776, test loss: 0.762092640372634\n",
      "Iteration 3145 training loss: 0.30432610772602287, test loss: 0.7621119298791421\n",
      "Iteration 3146 training loss: 0.3042802185336633, test loss: 0.7621312189124183\n",
      "Iteration 3147 training loss: 0.30423434332718197, test loss: 0.7621505074719218\n",
      "Iteration 3148 training loss: 0.30418848209972654, test loss: 0.762169795557113\n",
      "Iteration 3149 training loss: 0.30414263484444964, test loss: 0.7621890831674529\n",
      "Iteration 3150 training loss: 0.30409680155450886, test loss: 0.7622083703024048\n",
      "Iteration 3151 training loss: 0.3040509822230664, test loss: 0.7622276569614316\n",
      "Iteration 3152 training loss: 0.30400517684328987, test loss: 0.7622469431439989\n",
      "Iteration 3153 training loss: 0.30395938540835127, test loss: 0.7622662288495714\n",
      "Iteration 3154 training loss: 0.30391360791142774, test loss: 0.7622855140776174\n",
      "Iteration 3155 training loss: 0.30386784434570113, test loss: 0.7623047988276038\n",
      "Iteration 3156 training loss: 0.3038220947043584, test loss: 0.7623240830990001\n",
      "Iteration 3157 training loss: 0.303776358980591, test loss: 0.7623433668912767\n",
      "Iteration 3158 training loss: 0.30373063716759574, test loss: 0.7623626502039046\n",
      "Iteration 3159 training loss: 0.3036849292585738, test loss: 0.7623819330363559\n",
      "Iteration 3160 training loss: 0.3036392352467316, test loss: 0.7624012153881043\n",
      "Iteration 3161 training loss: 0.30359355512528013, test loss: 0.7624204972586246\n",
      "Iteration 3162 training loss: 0.3035478888874353, test loss: 0.7624397786473915\n",
      "Iteration 3163 training loss: 0.30350223652641795, test loss: 0.7624590595538826\n",
      "Iteration 3164 training loss: 0.3034565980354535, test loss: 0.7624783399775747\n",
      "Iteration 3165 training loss: 0.3034109734077726, test loss: 0.7624976199179465\n",
      "Iteration 3166 training loss: 0.3033653626366104, test loss: 0.7625168993744785\n",
      "Iteration 3167 training loss: 0.30331976571520697, test loss: 0.7625361783466512\n",
      "Iteration 3168 training loss: 0.3032741826368072, test loss: 0.7625554568339461\n",
      "Iteration 3169 training loss: 0.30322861339466073, test loss: 0.7625747348358467\n",
      "Iteration 3170 training loss: 0.30318305798202205, test loss: 0.7625940123518365\n",
      "Iteration 3171 training loss: 0.3031375163921506, test loss: 0.7626132893814008\n",
      "Iteration 3172 training loss: 0.30309198861831027, test loss: 0.7626325659240257\n",
      "Iteration 3173 training loss: 0.30304647465377005, test loss: 0.7626518419791982\n",
      "Iteration 3174 training loss: 0.3030009744918035, test loss: 0.7626711175464064\n",
      "Iteration 3175 training loss: 0.3029554881256892, test loss: 0.7626903926251393\n",
      "Iteration 3176 training loss: 0.30291001554871017, test loss: 0.7627096672148875\n",
      "Iteration 3177 training loss: 0.3028645567541546, test loss: 0.762728941315142\n",
      "Iteration 3178 training loss: 0.3028191117353151, test loss: 0.762748214925395\n",
      "Iteration 3179 training loss: 0.30277368048548925, test loss: 0.76276748804514\n",
      "Iteration 3180 training loss: 0.30272826299797934, test loss: 0.7627867606738712\n",
      "Iteration 3181 training loss: 0.30268285926609234, test loss: 0.7628060328110837\n",
      "Iteration 3182 training loss: 0.30263746928314, test loss: 0.7628253044562742\n",
      "Iteration 3183 training loss: 0.30259209304243895, test loss: 0.7628445756089397\n",
      "Iteration 3184 training loss: 0.3025467305373104, test loss: 0.762863846268579\n",
      "Iteration 3185 training loss: 0.30250138176108027, test loss: 0.762883116434691\n",
      "Iteration 3186 training loss: 0.30245604670707926, test loss: 0.7629023861067764\n",
      "Iteration 3187 training loss: 0.302410725368643, test loss: 0.7629216552843364\n",
      "Iteration 3188 training loss: 0.30236541773911146, test loss: 0.7629409239668736\n",
      "Iteration 3189 training loss: 0.30232012381182966, test loss: 0.7629601921538912\n",
      "Iteration 3190 training loss: 0.30227484358014717, test loss: 0.7629794598448937\n",
      "Iteration 3191 training loss: 0.30222957703741826, test loss: 0.7629987270393864\n",
      "Iteration 3192 training loss: 0.3021843241770019, test loss: 0.7630179937368755\n",
      "Iteration 3193 training loss: 0.30213908499226183, test loss: 0.7630372599368688\n",
      "Iteration 3194 training loss: 0.30209385947656653, test loss: 0.7630565256388742\n",
      "Iteration 3195 training loss: 0.3020486476232889, test loss: 0.7630757908424015\n",
      "Iteration 3196 training loss: 0.3020034494258068, test loss: 0.7630950555469607\n",
      "Iteration 3197 training loss: 0.3019582648775028, test loss: 0.763114319752063\n",
      "Iteration 3198 training loss: 0.30191309397176386, test loss: 0.763133583457221\n",
      "Iteration 3199 training loss: 0.3018679367019818, test loss: 0.7631528466619479\n",
      "Iteration 3200 training loss: 0.3018227930615532, test loss: 0.7631721093657579\n",
      "Iteration 3201 training loss: 0.3017776630438791, test loss: 0.7631913715681659\n",
      "Iteration 3202 training loss: 0.3017325466423652, test loss: 0.7632106332686882\n",
      "Iteration 3203 training loss: 0.3016874438504221, test loss: 0.7632298944668423\n",
      "Iteration 3204 training loss: 0.30164235466146483, test loss: 0.7632491551621461\n",
      "Iteration 3205 training loss: 0.3015972790689131, test loss: 0.7632684153541186\n",
      "Iteration 3206 training loss: 0.30155221706619123, test loss: 0.7632876750422798\n",
      "Iteration 3207 training loss: 0.30150716864672833, test loss: 0.7633069342261507\n",
      "Iteration 3208 training loss: 0.3014621338039579, test loss: 0.7633261929052533\n",
      "Iteration 3209 training loss: 0.3014171125313184, test loss: 0.7633454510791103\n",
      "Iteration 3210 training loss: 0.3013721048222525, test loss: 0.7633647087472458\n",
      "Iteration 3211 training loss: 0.30132711067020795, test loss: 0.7633839659091843\n",
      "Iteration 3212 training loss: 0.3012821300686366, test loss: 0.7634032225644521\n",
      "Iteration 3213 training loss: 0.3012371630109953, test loss: 0.7634224787125753\n",
      "Iteration 3214 training loss: 0.3011922094907453, test loss: 0.7634417343530819\n",
      "Iteration 3215 training loss: 0.3011472695013528, test loss: 0.7634609894855001\n",
      "Iteration 3216 training loss: 0.30110234303628797, test loss: 0.76348024410936\n",
      "Iteration 3217 training loss: 0.3010574300890261, test loss: 0.763499498224191\n",
      "Iteration 3218 training loss: 0.3010125306530469, test loss: 0.7635187518295254\n",
      "Iteration 3219 training loss: 0.30096764472183457, test loss: 0.7635380049248955\n",
      "Iteration 3220 training loss: 0.3009227722888781, test loss: 0.7635572575098338\n",
      "Iteration 3221 training loss: 0.30087791334767083, test loss: 0.7635765095838749\n",
      "Iteration 3222 training loss: 0.30083306789171077, test loss: 0.7635957611465543\n",
      "Iteration 3223 training loss: 0.3007882359145005, test loss: 0.7636150121974071\n",
      "Iteration 3224 training loss: 0.3007434174095472, test loss: 0.763634262735971\n",
      "Iteration 3225 training loss: 0.3006986123703625, test loss: 0.7636535127617831\n",
      "Iteration 3226 training loss: 0.30065382079046266, test loss: 0.7636727622743832\n",
      "Iteration 3227 training loss: 0.30060904266336835, test loss: 0.7636920112733098\n",
      "Iteration 3228 training loss: 0.3005642779826052, test loss: 0.7637112597581043\n",
      "Iteration 3229 training loss: 0.3005195267417028, test loss: 0.763730507728308\n",
      "Iteration 3230 training loss: 0.30047478893419566, test loss: 0.7637497551834631\n",
      "Iteration 3231 training loss: 0.3004300645536227, test loss: 0.7637690021231128\n",
      "Iteration 3232 training loss: 0.3003853535935273, test loss: 0.7637882485468018\n",
      "Iteration 3233 training loss: 0.3003406560474575, test loss: 0.7638074944540749\n",
      "Iteration 3234 training loss: 0.30029597190896595, test loss: 0.7638267398444778\n",
      "Iteration 3235 training loss: 0.3002513011716095, test loss: 0.7638459847175579\n",
      "Iteration 3236 training loss: 0.3002066438289495, test loss: 0.7638652290728626\n",
      "Iteration 3237 training loss: 0.3001619998745524, test loss: 0.763884472909941\n",
      "Iteration 3238 training loss: 0.30011736930198835, test loss: 0.7639037162283422\n",
      "Iteration 3239 training loss: 0.3000727521048326, test loss: 0.7639229590276172\n",
      "Iteration 3240 training loss: 0.30002814827666446, test loss: 0.7639422013073165\n",
      "Iteration 3241 training loss: 0.2999835578110681, test loss: 0.7639614430669933\n",
      "Iteration 3242 training loss: 0.29993898070163194, test loss: 0.7639806843062003\n",
      "Iteration 3243 training loss: 0.29989441694194896, test loss: 0.7639999250244912\n",
      "Iteration 3244 training loss: 0.29984986652561646, test loss: 0.7640191652214212\n",
      "Iteration 3245 training loss: 0.2998053294462366, test loss: 0.7640384048965461\n",
      "Iteration 3246 training loss: 0.29976080569741553, test loss: 0.7640576440494222\n",
      "Iteration 3247 training loss: 0.29971629527276417, test loss: 0.764076882679607\n",
      "Iteration 3248 training loss: 0.29967179816589784, test loss: 0.7640961207866592\n",
      "Iteration 3249 training loss: 0.29962731437043616, test loss: 0.7641153583701378\n",
      "Iteration 3250 training loss: 0.29958284388000356, test loss: 0.7641345954296026\n",
      "Iteration 3251 training loss: 0.2995383866882284, test loss: 0.764153831964615\n",
      "Iteration 3252 training loss: 0.29949394278874386, test loss: 0.7641730679747366\n",
      "Iteration 3253 training loss: 0.2994495121751876, test loss: 0.7641923034595303\n",
      "Iteration 3254 training loss: 0.29940509484120137, test loss: 0.7642115384185592\n",
      "Iteration 3255 training loss: 0.2993606907804317, test loss: 0.7642307728513879\n",
      "Iteration 3256 training loss: 0.2993162999865291, test loss: 0.7642500067575816\n",
      "Iteration 3257 training loss: 0.29927192245314915, test loss: 0.7642692401367063\n",
      "Iteration 3258 training loss: 0.2992275581739513, test loss: 0.7642884729883292\n",
      "Iteration 3259 training loss: 0.2991832071425996, test loss: 0.7643077053120176\n",
      "Iteration 3260 training loss: 0.2991388693527625, test loss: 0.7643269371073406\n",
      "Iteration 3261 training loss: 0.299094544798113, test loss: 0.7643461683738673\n",
      "Iteration 3262 training loss: 0.2990502334723281, test loss: 0.7643653991111683\n",
      "Iteration 3263 training loss: 0.29900593536908965, test loss: 0.7643846293188142\n",
      "Iteration 3264 training loss: 0.2989616504820836, test loss: 0.7644038589963776\n",
      "Iteration 3265 training loss: 0.2989173788050004, test loss: 0.7644230881434311\n",
      "Iteration 3266 training loss: 0.2988731203315349, test loss: 0.7644423167595481\n",
      "Iteration 3267 training loss: 0.29882887505538613, test loss: 0.7644615448443033\n",
      "Iteration 3268 training loss: 0.2987846429702578, test loss: 0.7644807723972719\n",
      "Iteration 3269 training loss: 0.2987404240698579, test loss: 0.7644999994180302\n",
      "Iteration 3270 training loss: 0.2986962183478985, test loss: 0.7645192259061547\n",
      "Iteration 3271 training loss: 0.2986520257980964, test loss: 0.7645384518612236\n",
      "Iteration 3272 training loss: 0.29860784641417254, test loss: 0.7645576772828155\n",
      "Iteration 3273 training loss: 0.29856368018985247, test loss: 0.7645769021705096\n",
      "Iteration 3274 training loss: 0.2985195271188656, test loss: 0.764596126523886\n",
      "Iteration 3275 training loss: 0.29847538719494626, test loss: 0.7646153503425259\n",
      "Iteration 3276 training loss: 0.2984312604118327, test loss: 0.7646345736260115\n",
      "Iteration 3277 training loss: 0.29838714676326783, test loss: 0.7646537963739247\n",
      "Iteration 3278 training loss: 0.2983430462429985, test loss: 0.7646730185858499\n",
      "Iteration 3279 training loss: 0.29829895884477625, test loss: 0.7646922402613705\n",
      "Iteration 3280 training loss: 0.2982548845623567, test loss: 0.7647114614000722\n",
      "Iteration 3281 training loss: 0.29821082338950006, test loss: 0.7647306820015405\n",
      "Iteration 3282 training loss: 0.2981667753199706, test loss: 0.764749902065362\n",
      "Iteration 3283 training loss: 0.29812274034753705, test loss: 0.7647691215911249\n",
      "Iteration 3284 training loss: 0.2980787184659723, test loss: 0.7647883405784167\n",
      "Iteration 3285 training loss: 0.29803470966905354, test loss: 0.7648075590268267\n",
      "Iteration 3286 training loss: 0.29799071395056265, test loss: 0.764826776935945\n",
      "Iteration 3287 training loss: 0.29794673130428534, test loss: 0.7648459943053619\n",
      "Iteration 3288 training loss: 0.2979027617240118, test loss: 0.7648652111346694\n",
      "Iteration 3289 training loss: 0.29785880520353647, test loss: 0.7648844274234593\n",
      "Iteration 3290 training loss: 0.2978148617366581, test loss: 0.7649036431713245\n",
      "Iteration 3291 training loss: 0.29777093131717985, test loss: 0.764922858377859\n",
      "Iteration 3292 training loss: 0.29772701393890894, test loss: 0.7649420730426577\n",
      "Iteration 3293 training loss: 0.29768310959565686, test loss: 0.7649612871653156\n",
      "Iteration 3294 training loss: 0.29763921828123957, test loss: 0.764980500745429\n",
      "Iteration 3295 training loss: 0.2975953399894772, test loss: 0.7649997137825949\n",
      "Iteration 3296 training loss: 0.29755147471419385, test loss: 0.7650189262764107\n",
      "Iteration 3297 training loss: 0.2975076224492186, test loss: 0.7650381382264753\n",
      "Iteration 3298 training loss: 0.2974637831883839, test loss: 0.7650573496323873\n",
      "Iteration 3299 training loss: 0.29741995692552725, test loss: 0.7650765604937475\n",
      "Iteration 3300 training loss: 0.2973761436544898, test loss: 0.7650957708101563\n",
      "Iteration 3301 training loss: 0.2973323433691171, test loss: 0.7651149805812151\n",
      "Iteration 3302 training loss: 0.2972885560632591, test loss: 0.7651341898065265\n",
      "Iteration 3303 training loss: 0.29724478173077, test loss: 0.7651533984856934\n",
      "Iteration 3304 training loss: 0.29720102036550794, test loss: 0.7651726066183198\n",
      "Iteration 3305 training loss: 0.29715727196133557, test loss: 0.7651918142040103\n",
      "Iteration 3306 training loss: 0.29711353651211947, test loss: 0.76521102124237\n",
      "Iteration 3307 training loss: 0.29706981401173077, test loss: 0.7652302277330051\n",
      "Iteration 3308 training loss: 0.29702610445404454, test loss: 0.7652494336755226\n",
      "Iteration 3309 training loss: 0.2969824078329403, test loss: 0.7652686390695302\n",
      "Iteration 3310 training loss: 0.29693872414230155, test loss: 0.7652878439146361\n",
      "Iteration 3311 training loss: 0.2968950533760161, test loss: 0.7653070482104494\n",
      "Iteration 3312 training loss: 0.2968513955279759, test loss: 0.76532625195658\n",
      "Iteration 3313 training loss: 0.2968077505920773, test loss: 0.7653454551526383\n",
      "Iteration 3314 training loss: 0.2967641185622205, test loss: 0.7653646577982359\n",
      "Iteration 3315 training loss: 0.2967204994323103, test loss: 0.7653838598929847\n",
      "Iteration 3316 training loss: 0.29667689319625506, test loss: 0.7654030614364982\n",
      "Iteration 3317 training loss: 0.296633299847968, test loss: 0.765422262428389\n",
      "Iteration 3318 training loss: 0.2965897193813662, test loss: 0.7654414628682716\n",
      "Iteration 3319 training loss: 0.2965461517903707, test loss: 0.7654606627557615\n",
      "Iteration 3320 training loss: 0.29650259706890714, test loss: 0.7654798620904741\n",
      "Iteration 3321 training loss: 0.2964590552109051, test loss: 0.765499060872026\n",
      "Iteration 3322 training loss: 0.29641552621029826, test loss: 0.7655182591000345\n",
      "Iteration 3323 training loss: 0.2963720100610245, test loss: 0.7655374567741174\n",
      "Iteration 3324 training loss: 0.29632850675702593, test loss: 0.7655566538938936\n",
      "Iteration 3325 training loss: 0.2962850162922488, test loss: 0.7655758504589819\n",
      "Iteration 3326 training loss: 0.2962415386606433, test loss: 0.7655950464690032\n",
      "Iteration 3327 training loss: 0.2961980738561639, test loss: 0.765614241923578\n",
      "Iteration 3328 training loss: 0.2961546218727696, test loss: 0.7656334368223278\n",
      "Iteration 3329 training loss: 0.2961111827044226, test loss: 0.7656526311648749\n",
      "Iteration 3330 training loss: 0.2960677563450901, test loss: 0.7656718249508423\n",
      "Iteration 3331 training loss: 0.29602434278874296, test loss: 0.765691018179854\n",
      "Iteration 3332 training loss: 0.2959809420293565, test loss: 0.7657102108515336\n",
      "Iteration 3333 training loss: 0.29593755406090966, test loss: 0.7657294029655072\n",
      "Iteration 3334 training loss: 0.29589417887738606, test loss: 0.7657485945214\n",
      "Iteration 3335 training loss: 0.2958508164727729, test loss: 0.7657677855188387\n",
      "Iteration 3336 training loss: 0.2958074668410619, test loss: 0.7657869759574509\n",
      "Iteration 3337 training loss: 0.2957641299762487, test loss: 0.7658061658368639\n",
      "Iteration 3338 training loss: 0.29572080587233296, test loss: 0.7658253551567069\n",
      "Iteration 3339 training loss: 0.29567749452331865, test loss: 0.7658445439166089\n",
      "Iteration 3340 training loss: 0.29563419592321366, test loss: 0.7658637321162002\n",
      "Iteration 3341 training loss: 0.2955909100660299, test loss: 0.7658829197551112\n",
      "Iteration 3342 training loss: 0.29554763694578357, test loss: 0.7659021068329739\n",
      "Iteration 3343 training loss: 0.2955043765564949, test loss: 0.7659212933494199\n",
      "Iteration 3344 training loss: 0.29546112889218806, test loss: 0.7659404793040822\n",
      "Iteration 3345 training loss: 0.29541789394689133, test loss: 0.7659596646965943\n",
      "Iteration 3346 training loss: 0.29537467171463727, test loss: 0.7659788495265905\n",
      "Iteration 3347 training loss: 0.2953314621894621, test loss: 0.7659980337937057\n",
      "Iteration 3348 training loss: 0.2952882653654065, test loss: 0.7660172174975753\n",
      "Iteration 3349 training loss: 0.29524508123651494, test loss: 0.7660364006378356\n",
      "Iteration 3350 training loss: 0.2952019097968361, test loss: 0.7660555832141238\n",
      "Iteration 3351 training loss: 0.29515875104042266, test loss: 0.7660747652260773\n",
      "Iteration 3352 training loss: 0.2951156049613312, test loss: 0.7660939466733343\n",
      "Iteration 3353 training loss: 0.29507247155362265, test loss: 0.7661131275555341\n",
      "Iteration 3354 training loss: 0.29502935081136167, test loss: 0.766132307872316\n",
      "Iteration 3355 training loss: 0.29498624272861707, test loss: 0.7661514876233204\n",
      "Iteration 3356 training loss: 0.29494314729946175, test loss: 0.7661706668081885\n",
      "Iteration 3357 training loss: 0.29490006451797257, test loss: 0.766189845426562\n",
      "Iteration 3358 training loss: 0.2948569943782303, test loss: 0.7662090234780828\n",
      "Iteration 3359 training loss: 0.2948139368743201, test loss: 0.7662282009623944\n",
      "Iteration 3360 training loss: 0.29477089200033063, test loss: 0.7662473778791401\n",
      "Iteration 3361 training loss: 0.29472785975035504, test loss: 0.7662665542279645\n",
      "Iteration 3362 training loss: 0.29468484011849005, test loss: 0.7662857300085127\n",
      "Iteration 3363 training loss: 0.29464183309883685, test loss: 0.7663049052204299\n",
      "Iteration 3364 training loss: 0.2945988386855002, test loss: 0.7663240798633627\n",
      "Iteration 3365 training loss: 0.2945558568725891, test loss: 0.7663432539369581\n",
      "Iteration 3366 training loss: 0.2945128876542165, test loss: 0.7663624274408639\n",
      "Iteration 3367 training loss: 0.29446993102449925, test loss: 0.7663816003747278\n",
      "Iteration 3368 training loss: 0.29442698697755826, test loss: 0.7664007727381996\n",
      "Iteration 3369 training loss: 0.29438405550751845, test loss: 0.7664199445309282\n",
      "Iteration 3370 training loss: 0.2943411366085087, test loss: 0.7664391157525641\n",
      "Iteration 3371 training loss: 0.2942982302746619, test loss: 0.7664582864027583\n",
      "Iteration 3372 training loss: 0.2942553365001147, test loss: 0.766477456481162\n",
      "Iteration 3373 training loss: 0.29421245527900797, test loss: 0.7664966259874276\n",
      "Iteration 3374 training loss: 0.2941695866054864, test loss: 0.7665157949212081\n",
      "Iteration 3375 training loss: 0.2941267304736987, test loss: 0.7665349632821568\n",
      "Iteration 3376 training loss: 0.2940838868777975, test loss: 0.7665541310699278\n",
      "Iteration 3377 training loss: 0.29404105581193946, test loss: 0.7665732982841759\n",
      "Iteration 3378 training loss: 0.29399823727028507, test loss: 0.7665924649245563\n",
      "Iteration 3379 training loss: 0.2939554312469988, test loss: 0.7666116309907256\n",
      "Iteration 3380 training loss: 0.2939126377362491, test loss: 0.7666307964823401\n",
      "Iteration 3381 training loss: 0.29386985673220833, test loss: 0.7666499613990568\n",
      "Iteration 3382 training loss: 0.2938270882290529, test loss: 0.7666691257405339\n",
      "Iteration 3383 training loss: 0.2937843322209629, test loss: 0.7666882895064303\n",
      "Iteration 3384 training loss: 0.2937415887021225, test loss: 0.7667074526964045\n",
      "Iteration 3385 training loss: 0.29369885766671994, test loss: 0.766726615310117\n",
      "Iteration 3386 training loss: 0.2936561391089471, test loss: 0.766745777347228\n",
      "Iteration 3387 training loss: 0.2936134330229998, test loss: 0.7667649388073985\n",
      "Iteration 3388 training loss: 0.2935707394030781, test loss: 0.7667840996902904\n",
      "Iteration 3389 training loss: 0.2935280582433856, test loss: 0.7668032599955653\n",
      "Iteration 3390 training loss: 0.29348538953812986, test loss: 0.7668224197228872\n",
      "Iteration 3391 training loss: 0.29344273328152265, test loss: 0.7668415788719191\n",
      "Iteration 3392 training loss: 0.2934000894677792, test loss: 0.7668607374423251\n",
      "Iteration 3393 training loss: 0.2933574580911188, test loss: 0.7668798954337701\n",
      "Iteration 3394 training loss: 0.293314839145765, test loss: 0.7668990528459196\n",
      "Iteration 3395 training loss: 0.29327223262594465, test loss: 0.7669182096784397\n",
      "Iteration 3396 training loss: 0.2932296385258888, test loss: 0.7669373659309963\n",
      "Iteration 3397 training loss: 0.2931870568398323, test loss: 0.7669565216032577\n",
      "Iteration 3398 training loss: 0.293144487562014, test loss: 0.7669756766948913\n",
      "Iteration 3399 training loss: 0.29310193068667645, test loss: 0.7669948312055654\n",
      "Iteration 3400 training loss: 0.2930593862080662, test loss: 0.7670139851349491\n",
      "Iteration 3401 training loss: 0.2930168541204335, test loss: 0.7670331384827124\n",
      "Iteration 3402 training loss: 0.2929743344180327, test loss: 0.7670522912485251\n",
      "Iteration 3403 training loss: 0.29293182709512183, test loss: 0.7670714434320585\n",
      "Iteration 3404 training loss: 0.2928893321459627, test loss: 0.7670905950329839\n",
      "Iteration 3405 training loss: 0.2928468495648213, test loss: 0.767109746050973\n",
      "Iteration 3406 training loss: 0.2928043793459673, test loss: 0.7671288964856995\n",
      "Iteration 3407 training loss: 0.2927619214836739, test loss: 0.7671480463368355\n",
      "Iteration 3408 training loss: 0.29271947597221865, test loss: 0.7671671956040556\n",
      "Iteration 3409 training loss: 0.29267704280588275, test loss: 0.7671863442870338\n",
      "Iteration 3410 training loss: 0.292634621978951, test loss: 0.7672054923854456\n",
      "Iteration 3411 training loss: 0.29259221348571235, test loss: 0.7672246398989663\n",
      "Iteration 3412 training loss: 0.29254981732045937, test loss: 0.7672437868272721\n",
      "Iteration 3413 training loss: 0.29250743347748864, test loss: 0.7672629331700402\n",
      "Iteration 3414 training loss: 0.2924650619511004, test loss: 0.7672820789269477\n",
      "Iteration 3415 training loss: 0.29242270273559884, test loss: 0.7673012240976727\n",
      "Iteration 3416 training loss: 0.29238035582529176, test loss: 0.7673203686818932\n",
      "Iteration 3417 training loss: 0.29233802121449093, test loss: 0.7673395126792896\n",
      "Iteration 3418 training loss: 0.2922956988975119, test loss: 0.7673586560895406\n",
      "Iteration 3419 training loss: 0.2922533888686741, test loss: 0.7673777989123266\n",
      "Iteration 3420 training loss: 0.2922110911223006, test loss: 0.767396941147329\n",
      "Iteration 3421 training loss: 0.2921688056527183, test loss: 0.7674160827942288\n",
      "Iteration 3422 training loss: 0.29212653245425807, test loss: 0.7674352238527082\n",
      "Iteration 3423 training loss: 0.29208427152125427, test loss: 0.7674543643224501\n",
      "Iteration 3424 training loss: 0.29204202284804537, test loss: 0.7674735042031371\n",
      "Iteration 3425 training loss: 0.2919997864289733, test loss: 0.7674926434944535\n",
      "Iteration 3426 training loss: 0.29195756225838404, test loss: 0.7675117821960833\n",
      "Iteration 3427 training loss: 0.29191535033062715, test loss: 0.7675309203077114\n",
      "Iteration 3428 training loss: 0.2918731506400562, test loss: 0.7675500578290236\n",
      "Iteration 3429 training loss: 0.29183096318102814, test loss: 0.7675691947597056\n",
      "Iteration 3430 training loss: 0.2917887879479042, test loss: 0.7675883310994441\n",
      "Iteration 3431 training loss: 0.2917466249350488, test loss: 0.7676074668479265\n",
      "Iteration 3432 training loss: 0.2917044741368307, test loss: 0.76762660200484\n",
      "Iteration 3433 training loss: 0.2916623355476219, test loss: 0.7676457365698734\n",
      "Iteration 3434 training loss: 0.2916202091617984, test loss: 0.7676648705427154\n",
      "Iteration 3435 training loss: 0.2915780949737401, test loss: 0.7676840039230554\n",
      "Iteration 3436 training loss: 0.29153599297783034, test loss: 0.767703136710583\n",
      "Iteration 3437 training loss: 0.2914939031684563, test loss: 0.7677222689049892\n",
      "Iteration 3438 training loss: 0.29145182554000904, test loss: 0.7677414005059647\n",
      "Iteration 3439 training loss: 0.2914097600868832, test loss: 0.7677605315132014\n",
      "Iteration 3440 training loss: 0.291367706803477, test loss: 0.7677796619263915\n",
      "Iteration 3441 training loss: 0.29132566568419294, test loss: 0.7677987917452276\n",
      "Iteration 3442 training loss: 0.2912836367234366, test loss: 0.7678179209694026\n",
      "Iteration 3443 training loss: 0.2912416199156177, test loss: 0.7678370495986113\n",
      "Iteration 3444 training loss: 0.2911996152551496, test loss: 0.7678561776325471\n",
      "Iteration 3445 training loss: 0.29115762273644913, test loss: 0.7678753050709054\n",
      "Iteration 3446 training loss: 0.2911156423539373, test loss: 0.7678944319133816\n",
      "Iteration 3447 training loss: 0.29107367410203816, test loss: 0.7679135581596714\n",
      "Iteration 3448 training loss: 0.29103171797518024, test loss: 0.7679326838094719\n",
      "Iteration 3449 training loss: 0.29098977396779513, test loss: 0.7679518088624797\n",
      "Iteration 3450 training loss: 0.29094784207431845, test loss: 0.7679709333183926\n",
      "Iteration 3451 training loss: 0.29090592228918954, test loss: 0.7679900571769086\n",
      "Iteration 3452 training loss: 0.29086401460685113, test loss: 0.7680091804377266\n",
      "Iteration 3453 training loss: 0.29082211902175, test loss: 0.768028303100546\n",
      "Iteration 3454 training loss: 0.29078023552833643, test loss: 0.7680474251650663\n",
      "Iteration 3455 training loss: 0.2907383641210644, test loss: 0.7680665466309875\n",
      "Iteration 3456 training loss: 0.2906965047943914, test loss: 0.768085667498011\n",
      "Iteration 3457 training loss: 0.2906546575427789, test loss: 0.768104787765838\n",
      "Iteration 3458 training loss: 0.290612822360692, test loss: 0.76812390743417\n",
      "Iteration 3459 training loss: 0.29057099924259927, test loss: 0.7681430265027099\n",
      "Iteration 3460 training loss: 0.2905291881829729, test loss: 0.7681621449711603\n",
      "Iteration 3461 training loss: 0.29048738917628913, test loss: 0.768181262839225\n",
      "Iteration 3462 training loss: 0.2904456022170276, test loss: 0.7682003801066077\n",
      "Iteration 3463 training loss: 0.29040382729967135, test loss: 0.768219496773013\n",
      "Iteration 3464 training loss: 0.29036206441870765, test loss: 0.7682386128381461\n",
      "Iteration 3465 training loss: 0.2903203135686269, test loss: 0.7682577283017122\n",
      "Iteration 3466 training loss: 0.29027857474392355, test loss: 0.7682768431634175\n",
      "Iteration 3467 training loss: 0.2902368479390953, test loss: 0.768295957422969\n",
      "Iteration 3468 training loss: 0.2901951331486439, test loss: 0.7683150710800731\n",
      "Iteration 3469 training loss: 0.29015343036707425, test loss: 0.7683341841344381\n",
      "Iteration 3470 training loss: 0.29011173958889536, test loss: 0.7683532965857716\n",
      "Iteration 3471 training loss: 0.29007006080861947, test loss: 0.7683724084337825\n",
      "Iteration 3472 training loss: 0.29002839402076286, test loss: 0.7683915196781801\n",
      "Iteration 3473 training loss: 0.28998673921984514, test loss: 0.7684106303186736\n",
      "Iteration 3474 training loss: 0.28994509640038946, test loss: 0.7684297403549738\n",
      "Iteration 3475 training loss: 0.2899034655569228, test loss: 0.768448849786791\n",
      "Iteration 3476 training loss: 0.28986184668397585, test loss: 0.7684679586138361\n",
      "Iteration 3477 training loss: 0.2898202397760825, test loss: 0.7684870668358217\n",
      "Iteration 3478 training loss: 0.2897786448277806, test loss: 0.7685061744524591\n",
      "Iteration 3479 training loss: 0.28973706183361153, test loss: 0.7685252814634617\n",
      "Iteration 3480 training loss: 0.2896954907881202, test loss: 0.7685443878685421\n",
      "Iteration 3481 training loss: 0.28965393168585507, test loss: 0.7685634936674144\n",
      "Iteration 3482 training loss: 0.28961238452136834, test loss: 0.7685825988597926\n",
      "Iteration 3483 training loss: 0.28957084928921567, test loss: 0.7686017034453919\n",
      "Iteration 3484 training loss: 0.2895293259839564, test loss: 0.7686208074239267\n",
      "Iteration 3485 training loss: 0.28948781460015355, test loss: 0.7686399107951134\n",
      "Iteration 3486 training loss: 0.28944631513237334, test loss: 0.768659013558668\n",
      "Iteration 3487 training loss: 0.289404827575186, test loss: 0.768678115714307\n",
      "Iteration 3488 training loss: 0.289363351923165, test loss: 0.7686972172617477\n",
      "Iteration 3489 training loss: 0.28932188817088755, test loss: 0.768716318200708\n",
      "Iteration 3490 training loss: 0.28928043631293465, test loss: 0.7687354185309058\n",
      "Iteration 3491 training loss: 0.28923899634389033, test loss: 0.7687545182520596\n",
      "Iteration 3492 training loss: 0.2891975682583426, test loss: 0.7687736173638893\n",
      "Iteration 3493 training loss: 0.2891561520508828, test loss: 0.7687927158661135\n",
      "Iteration 3494 training loss: 0.289114747716106, test loss: 0.7688118137584532\n",
      "Iteration 3495 training loss: 0.2890733552486107, test loss: 0.7688309110406285\n",
      "Iteration 3496 training loss: 0.28903197464299923, test loss: 0.7688500077123607\n",
      "Iteration 3497 training loss: 0.28899060589387676, test loss: 0.768869103773371\n",
      "Iteration 3498 training loss: 0.28894924899585295, test loss: 0.7688881992233821\n",
      "Iteration 3499 training loss: 0.28890790394354027, test loss: 0.7689072940621161\n",
      "Iteration 3500 training loss: 0.28886657073155486, test loss: 0.7689263882892962\n",
      "Iteration 3501 training loss: 0.28882524935451676, test loss: 0.7689454819046457\n",
      "Iteration 3502 training loss: 0.2887839398070491, test loss: 0.7689645749078888\n",
      "Iteration 3503 training loss: 0.2887426420837788, test loss: 0.76898366729875\n",
      "Iteration 3504 training loss: 0.2887013561793363, test loss: 0.7690027590769536\n",
      "Iteration 3505 training loss: 0.2886600820883553, test loss: 0.7690218502422255\n",
      "Iteration 3506 training loss: 0.2886188198054733, test loss: 0.7690409407942919\n",
      "Iteration 3507 training loss: 0.28857756932533124, test loss: 0.7690600307328785\n",
      "Iteration 3508 training loss: 0.28853633064257356, test loss: 0.7690791200577122\n",
      "Iteration 3509 training loss: 0.2884951037518481, test loss: 0.7690982087685205\n",
      "Iteration 3510 training loss: 0.2884538886478064, test loss: 0.7691172968650313\n",
      "Iteration 3511 training loss: 0.28841268532510334, test loss: 0.7691363843469723\n",
      "Iteration 3512 training loss: 0.2883714937783973, test loss: 0.7691554712140727\n",
      "Iteration 3513 training loss: 0.28833031400235026, test loss: 0.7691745574660611\n",
      "Iteration 3514 training loss: 0.2882891459916277, test loss: 0.7691936431026678\n",
      "Iteration 3515 training loss: 0.2882479897408985, test loss: 0.7692127281236223\n",
      "Iteration 3516 training loss: 0.28820684524483503, test loss: 0.7692318125286554\n",
      "Iteration 3517 training loss: 0.28816571249811307, test loss: 0.7692508963174979\n",
      "Iteration 3518 training loss: 0.28812459149541214, test loss: 0.7692699794898813\n",
      "Iteration 3519 training loss: 0.28808348223141506, test loss: 0.7692890620455378\n",
      "Iteration 3520 training loss: 0.2880423847008081, test loss: 0.7693081439841996\n",
      "Iteration 3521 training loss: 0.28800129889828097, test loss: 0.7693272253055993\n",
      "Iteration 3522 training loss: 0.28796022481852696, test loss: 0.7693463060094704\n",
      "Iteration 3523 training loss: 0.2879191624562429, test loss: 0.7693653860955465\n",
      "Iteration 3524 training loss: 0.28787811180612877, test loss: 0.769384465563562\n",
      "Iteration 3525 training loss: 0.2878370728628884, test loss: 0.7694035444132514\n",
      "Iteration 3526 training loss: 0.2877960456212286, test loss: 0.7694226226443496\n",
      "Iteration 3527 training loss: 0.2877550300758601, test loss: 0.7694417002565928\n",
      "Iteration 3528 training loss: 0.28771402622149694, test loss: 0.7694607772497163\n",
      "Iteration 3529 training loss: 0.2876730340528564, test loss: 0.7694798536234566\n",
      "Iteration 3530 training loss: 0.2876320535646594, test loss: 0.7694989293775508\n",
      "Iteration 3531 training loss: 0.2875910847516302, test loss: 0.7695180045117366\n",
      "Iteration 3532 training loss: 0.2875501276084967, test loss: 0.769537079025751\n",
      "Iteration 3533 training loss: 0.2875091821299899, test loss: 0.7695561529193327\n",
      "Iteration 3534 training loss: 0.2874682483108446, test loss: 0.7695752261922203\n",
      "Iteration 3535 training loss: 0.28742732614579863, test loss: 0.7695942988441528\n",
      "Iteration 3536 training loss: 0.2873864156295936, test loss: 0.7696133708748698\n",
      "Iteration 3537 training loss: 0.28734551675697445, test loss: 0.7696324422841112\n",
      "Iteration 3538 training loss: 0.28730462952268915, test loss: 0.7696515130716177\n",
      "Iteration 3539 training loss: 0.28726375392148984, test loss: 0.7696705832371299\n",
      "Iteration 3540 training loss: 0.2872228899481314, test loss: 0.7696896527803894\n",
      "Iteration 3541 training loss: 0.2871820375973726, test loss: 0.7697087217011375\n",
      "Iteration 3542 training loss: 0.287141196863975, test loss: 0.7697277899991164\n",
      "Iteration 3543 training loss: 0.28710036774270425, test loss: 0.7697468576740689\n",
      "Iteration 3544 training loss: 0.28705955022832913, test loss: 0.7697659247257383\n",
      "Iteration 3545 training loss: 0.2870187443156215, test loss: 0.7697849911538677\n",
      "Iteration 3546 training loss: 0.2869779499993573, test loss: 0.7698040569582011\n",
      "Iteration 3547 training loss: 0.28693716727431506, test loss: 0.7698231221384826\n",
      "Iteration 3548 training loss: 0.28689639613527734, test loss: 0.7698421866944573\n",
      "Iteration 3549 training loss: 0.28685563657702984, test loss: 0.7698612506258701\n",
      "Iteration 3550 training loss: 0.2868148885943616, test loss: 0.7698803139324668\n",
      "Iteration 3551 training loss: 0.2867741521820651, test loss: 0.7698993766139935\n",
      "Iteration 3552 training loss: 0.2867334273349362, test loss: 0.7699184386701964\n",
      "Iteration 3553 training loss: 0.28669271404777397, test loss: 0.7699375001008227\n",
      "Iteration 3554 training loss: 0.28665201231538123, test loss: 0.7699565609056191\n",
      "Iteration 3555 training loss: 0.2866113221325639, test loss: 0.7699756210843345\n",
      "Iteration 3556 training loss: 0.28657064349413114, test loss: 0.7699946806367156\n",
      "Iteration 3557 training loss: 0.28652997639489575, test loss: 0.7700137395625118\n",
      "Iteration 3558 training loss: 0.28648932082967377, test loss: 0.7700327978614724\n",
      "Iteration 3559 training loss: 0.28644867679328456, test loss: 0.770051855533346\n",
      "Iteration 3560 training loss: 0.28640804428055094, test loss: 0.7700709125778828\n",
      "Iteration 3561 training loss: 0.28636742328629894, test loss: 0.7700899689948332\n",
      "Iteration 3562 training loss: 0.28632681380535796, test loss: 0.7701090247839474\n",
      "Iteration 3563 training loss: 0.28628621583256103, test loss: 0.7701280799449768\n",
      "Iteration 3564 training loss: 0.28624562936274395, test loss: 0.7701471344776731\n",
      "Iteration 3565 training loss: 0.2862050543907465, test loss: 0.7701661883817875\n",
      "Iteration 3566 training loss: 0.2861644909114113, test loss: 0.7701852416570728\n",
      "Iteration 3567 training loss: 0.2861239389195845, test loss: 0.7702042943032819\n",
      "Iteration 3568 training loss: 0.28608339841011554, test loss: 0.7702233463201673\n",
      "Iteration 3569 training loss: 0.28604286937785733, test loss: 0.7702423977074829\n",
      "Iteration 3570 training loss: 0.2860023518176659, test loss: 0.7702614484649828\n",
      "Iteration 3571 training loss: 0.28596184572440075, test loss: 0.7702804985924209\n",
      "Iteration 3572 training loss: 0.2859213510929246, test loss: 0.7702995480895526\n",
      "Iteration 3573 training loss: 0.2858808679181035, test loss: 0.7703185969561323\n",
      "Iteration 3574 training loss: 0.2858403961948069, test loss: 0.770337645191916\n",
      "Iteration 3575 training loss: 0.2857999359179074, test loss: 0.7703566927966595\n",
      "Iteration 3576 training loss: 0.2857594870822811, test loss: 0.7703757397701192\n",
      "Iteration 3577 training loss: 0.2857190496828072, test loss: 0.7703947861120521\n",
      "Iteration 3578 training loss: 0.28567862371436836, test loss: 0.7704138318222149\n",
      "Iteration 3579 training loss: 0.28563820917185045, test loss: 0.7704328769003655\n",
      "Iteration 3580 training loss: 0.28559780605014273, test loss: 0.7704519213462616\n",
      "Iteration 3581 training loss: 0.2855574143441375, test loss: 0.770470965159662\n",
      "Iteration 3582 training loss: 0.28551703404873074, test loss: 0.770490008340325\n",
      "Iteration 3583 training loss: 0.2854766651588215, test loss: 0.7705090508880098\n",
      "Iteration 3584 training loss: 0.285436307669312, test loss: 0.770528092802476\n",
      "Iteration 3585 training loss: 0.2853959615751079, test loss: 0.770547134083484\n",
      "Iteration 3586 training loss: 0.28535562687111815, test loss: 0.7705661747307935\n",
      "Iteration 3587 training loss: 0.2853153035522549, test loss: 0.7705852147441653\n",
      "Iteration 3588 training loss: 0.2852749916134336, test loss: 0.7706042541233604\n",
      "Iteration 3589 training loss: 0.2852346910495729, test loss: 0.770623292868141\n",
      "Iteration 3590 training loss: 0.28519440185559486, test loss: 0.7706423309782684\n",
      "Iteration 3591 training loss: 0.28515412402642476, test loss: 0.770661368453505\n",
      "Iteration 3592 training loss: 0.285113857556991, test loss: 0.7706804052936134\n",
      "Iteration 3593 training loss: 0.28507360244222535, test loss: 0.7706994414983567\n",
      "Iteration 3594 training loss: 0.285033358677063, test loss: 0.7707184770674981\n",
      "Iteration 3595 training loss: 0.28499312625644196, test loss: 0.770737512000802\n",
      "Iteration 3596 training loss: 0.28495290517530386, test loss: 0.7707565462980318\n",
      "Iteration 3597 training loss: 0.28491269542859354, test loss: 0.7707755799589531\n",
      "Iteration 3598 training loss: 0.2848724970112589, test loss: 0.7707946129833299\n",
      "Iteration 3599 training loss: 0.2848323099182512, test loss: 0.7708136453709281\n",
      "Iteration 3600 training loss: 0.28479213414452503, test loss: 0.7708326771215133\n",
      "Iteration 3601 training loss: 0.284751969685038, test loss: 0.7708517082348515\n",
      "Iteration 3602 training loss: 0.2847118165347511, test loss: 0.7708707387107093\n",
      "Iteration 3603 training loss: 0.2846716746886284, test loss: 0.7708897685488535\n",
      "Iteration 3604 training loss: 0.28463154414163755, test loss: 0.7709087977490514\n",
      "Iteration 3605 training loss: 0.284591424888749, test loss: 0.7709278263110704\n",
      "Iteration 3606 training loss: 0.28455131692493657, test loss: 0.7709468542346788\n",
      "Iteration 3607 training loss: 0.28451122024517744, test loss: 0.7709658815196448\n",
      "Iteration 3608 training loss: 0.2844711348444519, test loss: 0.7709849081657373\n",
      "Iteration 3609 training loss: 0.2844310607177433, test loss: 0.7710039341727248\n",
      "Iteration 3610 training loss: 0.2843909978600385, test loss: 0.7710229595403776\n",
      "Iteration 3611 training loss: 0.28435094626632734, test loss: 0.771041984268465\n",
      "Iteration 3612 training loss: 0.284310905931603, test loss: 0.7710610083567575\n",
      "Iteration 3613 training loss: 0.2842708768508617, test loss: 0.7710800318050257\n",
      "Iteration 3614 training loss: 0.28423085901910317, test loss: 0.7710990546130404\n",
      "Iteration 3615 training loss: 0.2841908524313298, test loss: 0.7711180767805726\n",
      "Iteration 3616 training loss: 0.2841508570825476, test loss: 0.7711370983073949\n",
      "Iteration 3617 training loss: 0.28411087296776594, test loss: 0.7711561191932785\n",
      "Iteration 3618 training loss: 0.2840709000819967, test loss: 0.7711751394379961\n",
      "Iteration 3619 training loss: 0.2840309384202556, test loss: 0.7711941590413206\n",
      "Iteration 3620 training loss: 0.2839909879775612, test loss: 0.771213178003025\n",
      "Iteration 3621 training loss: 0.28395104874893545, test loss: 0.771232196322883\n",
      "Iteration 3622 training loss: 0.2839111207294032, test loss: 0.7712512140006679\n",
      "Iteration 3623 training loss: 0.28387120391399273, test loss: 0.7712702310361548\n",
      "Iteration 3624 training loss: 0.28383129829773535, test loss: 0.7712892474291174\n",
      "Iteration 3625 training loss: 0.2837914038756655, test loss: 0.7713082631793314\n",
      "Iteration 3626 training loss: 0.2837515206428209, test loss: 0.7713272782865718\n",
      "Iteration 3627 training loss: 0.2837116485942426, test loss: 0.7713462927506141\n",
      "Iteration 3628 training loss: 0.28367178772497437, test loss: 0.7713653065712344\n",
      "Iteration 3629 training loss: 0.2836319380300634, test loss: 0.7713843197482094\n",
      "Iteration 3630 training loss: 0.2835920995045601, test loss: 0.7714033322813153\n",
      "Iteration 3631 training loss: 0.2835522721435179, test loss: 0.7714223441703295\n",
      "Iteration 3632 training loss: 0.2835124559419934, test loss: 0.7714413554150295\n",
      "Iteration 3633 training loss: 0.28347265089504636, test loss: 0.7714603660151926\n",
      "Iteration 3634 training loss: 0.28343285699773985, test loss: 0.7714793759705978\n",
      "Iteration 3635 training loss: 0.2833930742451397, test loss: 0.7714983852810227\n",
      "Iteration 3636 training loss: 0.28335330263231506, test loss: 0.7715173939462469\n",
      "Iteration 3637 training loss: 0.28331354215433846, test loss: 0.7715364019660487\n",
      "Iteration 3638 training loss: 0.28327379280628534, test loss: 0.7715554093402084\n",
      "Iteration 3639 training loss: 0.2832340545832342, test loss: 0.7715744160685056\n",
      "Iteration 3640 training loss: 0.2831943274802667, test loss: 0.7715934221507206\n",
      "Iteration 3641 training loss: 0.2831546114924678, test loss: 0.7716124275866338\n",
      "Iteration 3642 training loss: 0.28311490661492533, test loss: 0.7716314323760264\n",
      "Iteration 3643 training loss: 0.2830752128427305, test loss: 0.7716504365186795\n",
      "Iteration 3644 training loss: 0.2830355301709775, test loss: 0.7716694400143749\n",
      "Iteration 3645 training loss: 0.28299585859476356, test loss: 0.7716884428628941\n",
      "Iteration 3646 training loss: 0.28295619810918915, test loss: 0.7717074450640198\n",
      "Iteration 3647 training loss: 0.28291654870935773, test loss: 0.7717264466175343\n",
      "Iteration 3648 training loss: 0.2828769103903761, test loss: 0.7717454475232208\n",
      "Iteration 3649 training loss: 0.2828372831473539, test loss: 0.7717644477808626\n",
      "Iteration 3650 training loss: 0.28279766697540387, test loss: 0.7717834473902432\n",
      "Iteration 3651 training loss: 0.2827580618696421, test loss: 0.7718024463511469\n",
      "Iteration 3652 training loss: 0.28271846782518756, test loss: 0.7718214446633578\n",
      "Iteration 3653 training loss: 0.28267888483716236, test loss: 0.7718404423266608\n",
      "Iteration 3654 training loss: 0.28263931290069183, test loss: 0.7718594393408403\n",
      "Iteration 3655 training loss: 0.28259975201090415, test loss: 0.7718784357056823\n",
      "Iteration 3656 training loss: 0.2825602021629308, test loss: 0.7718974314209723\n",
      "Iteration 3657 training loss: 0.2825206633519061, test loss: 0.7719164264864959\n",
      "Iteration 3658 training loss: 0.28248113557296783, test loss: 0.77193542090204\n",
      "Iteration 3659 training loss: 0.2824416188212564, test loss: 0.7719544146673909\n",
      "Iteration 3660 training loss: 0.2824021130919156, test loss: 0.7719734077823357\n",
      "Iteration 3661 training loss: 0.2823626183800922, test loss: 0.7719924002466616\n",
      "Iteration 3662 training loss: 0.28232313468093606, test loss: 0.7720113920601565\n",
      "Iteration 3663 training loss: 0.2822836619896001, test loss: 0.7720303832226083\n",
      "Iteration 3664 training loss: 0.2822442003012402, test loss: 0.7720493737338051\n",
      "Iteration 3665 training loss: 0.2822047496110155, test loss: 0.772068363593536\n",
      "Iteration 3666 training loss: 0.2821653099140881, test loss: 0.7720873528015895\n",
      "Iteration 3667 training loss: 0.28212588120562304, test loss: 0.7721063413577549\n",
      "Iteration 3668 training loss: 0.28208646348078853, test loss: 0.7721253292618223\n",
      "Iteration 3669 training loss: 0.28204705673475594, test loss: 0.7721443165135811\n",
      "Iteration 3670 training loss: 0.28200766096269947, test loss: 0.7721633031128219\n",
      "Iteration 3671 training loss: 0.2819682761597964, test loss: 0.7721822890593353\n",
      "Iteration 3672 training loss: 0.2819289023212273, test loss: 0.7722012743529119\n",
      "Iteration 3673 training loss: 0.28188953944217554, test loss: 0.772220258993343\n",
      "Iteration 3674 training loss: 0.2818501875178274, test loss: 0.7722392429804206\n",
      "Iteration 3675 training loss: 0.2818108465433727, test loss: 0.772258226313936\n",
      "Iteration 3676 training loss: 0.2817715165140036, test loss: 0.7722772089936819\n",
      "Iteration 3677 training loss: 0.281732197424916, test loss: 0.7722961910194504\n",
      "Iteration 3678 training loss: 0.2816928892713082, test loss: 0.7723151723910343\n",
      "Iteration 3679 training loss: 0.2816535920483821, test loss: 0.7723341531082273\n",
      "Iteration 3680 training loss: 0.2816143057513421, test loss: 0.7723531331708222\n",
      "Iteration 3681 training loss: 0.281575030375396, test loss: 0.7723721125786129\n",
      "Iteration 3682 training loss: 0.28153576591575447, test loss: 0.7723910913313942\n",
      "Iteration 3683 training loss: 0.281496512367631, test loss: 0.7724100694289595\n",
      "Iteration 3684 training loss: 0.28145726972624246, test loss: 0.7724290468711041\n",
      "Iteration 3685 training loss: 0.28141803798680864, test loss: 0.772448023657623\n",
      "Iteration 3686 training loss: 0.28137881714455193, test loss: 0.7724669997883113\n",
      "Iteration 3687 training loss: 0.28133960719469836, test loss: 0.7724859752629651\n",
      "Iteration 3688 training loss: 0.28130040813247664, test loss: 0.7725049500813796\n",
      "Iteration 3689 training loss: 0.28126121995311826, test loss: 0.7725239242433519\n",
      "Iteration 3690 training loss: 0.2812220426518581, test loss: 0.772542897748678\n",
      "Iteration 3691 training loss: 0.2811828762239338, test loss: 0.7725618705971551\n",
      "Iteration 3692 training loss: 0.2811437206645861, test loss: 0.7725808427885805\n",
      "Iteration 3693 training loss: 0.2811045759690587, test loss: 0.7725998143227513\n",
      "Iteration 3694 training loss: 0.28106544213259815, test loss: 0.7726187851994657\n",
      "Iteration 3695 training loss: 0.2810263191504543, test loss: 0.7726377554185215\n",
      "Iteration 3696 training loss: 0.28098720701787966, test loss: 0.7726567249797175\n",
      "Iteration 3697 training loss: 0.2809481057301299, test loss: 0.772675693882852\n",
      "Iteration 3698 training loss: 0.2809090152824636, test loss: 0.7726946621277246\n",
      "Iteration 3699 training loss: 0.28086993567014223, test loss: 0.772713629714134\n",
      "Iteration 3700 training loss: 0.2808308668884305, test loss: 0.7727325966418801\n",
      "Iteration 3701 training loss: 0.28079180893259575, test loss: 0.7727515629107631\n",
      "Iteration 3702 training loss: 0.2807527617979086, test loss: 0.7727705285205829\n",
      "Iteration 3703 training loss: 0.28071372547964235, test loss: 0.7727894934711401\n",
      "Iteration 3704 training loss: 0.2806746999730734, test loss: 0.7728084577622355\n",
      "Iteration 3705 training loss: 0.2806356852734814, test loss: 0.7728274213936704\n",
      "Iteration 3706 training loss: 0.2805966813761482, test loss: 0.7728463843652461\n",
      "Iteration 3707 training loss: 0.28055768827635935, test loss: 0.7728653466767647\n",
      "Iteration 3708 training loss: 0.2805187059694031, test loss: 0.7728843083280277\n",
      "Iteration 3709 training loss: 0.28047973445057034, test loss: 0.7729032693188377\n",
      "Iteration 3710 training loss: 0.2804407737151554, test loss: 0.7729222296489974\n",
      "Iteration 3711 training loss: 0.28040182375845535, test loss: 0.7729411893183095\n",
      "Iteration 3712 training loss: 0.2803628845757701, test loss: 0.7729601483265771\n",
      "Iteration 3713 training loss: 0.2803239561624026, test loss: 0.7729791066736043\n",
      "Iteration 3714 training loss: 0.2802850385136586, test loss: 0.7729980643591944\n",
      "Iteration 3715 training loss: 0.2802461316248472, test loss: 0.7730170213831513\n",
      "Iteration 3716 training loss: 0.28020723549127985, test loss: 0.7730359777452802\n",
      "Iteration 3717 training loss: 0.2801683501082713, test loss: 0.7730549334453851\n",
      "Iteration 3718 training loss: 0.28012947547113914, test loss: 0.773073888483271\n",
      "Iteration 3719 training loss: 0.2800906115752039, test loss: 0.7730928428587435\n",
      "Iteration 3720 training loss: 0.2800517584157889, test loss: 0.7731117965716077\n",
      "Iteration 3721 training loss: 0.2800129159882207, test loss: 0.7731307496216697\n",
      "Iteration 3722 training loss: 0.2799740842878284, test loss: 0.7731497020087357\n",
      "Iteration 3723 training loss: 0.27993526330994417, test loss: 0.7731686537326117\n",
      "Iteration 3724 training loss: 0.27989645304990324, test loss: 0.7731876047931051\n",
      "Iteration 3725 training loss: 0.2798576535030435, test loss: 0.7732065551900221\n",
      "Iteration 3726 training loss: 0.2798188646647058, test loss: 0.7732255049231705\n",
      "Iteration 3727 training loss: 0.2797800865302341, test loss: 0.7732444539923579\n",
      "Iteration 3728 training loss: 0.27974131909497507, test loss: 0.7732634023973914\n",
      "Iteration 3729 training loss: 0.27970256235427815, test loss: 0.7732823501380798\n",
      "Iteration 3730 training loss: 0.27966381630349607, test loss: 0.7733012972142314\n",
      "Iteration 3731 training loss: 0.27962508093798416, test loss: 0.7733202436256548\n",
      "Iteration 3732 training loss: 0.27958635625310063, test loss: 0.7733391893721586\n",
      "Iteration 3733 training loss: 0.27954764224420675, test loss: 0.7733581344535527\n",
      "Iteration 3734 training loss: 0.27950893890666656, test loss: 0.7733770788696462\n",
      "Iteration 3735 training loss: 0.27947024623584704, test loss: 0.773396022620249\n",
      "Iteration 3736 training loss: 0.27943156422711796, test loss: 0.7734149657051711\n",
      "Iteration 3737 training loss: 0.27939289287585206, test loss: 0.7734339081242232\n",
      "Iteration 3738 training loss: 0.2793542321774249, test loss: 0.7734528498772154\n",
      "Iteration 3739 training loss: 0.279315582127215, test loss: 0.773471790963959\n",
      "Iteration 3740 training loss: 0.27927694272060377, test loss: 0.7734907313842648\n",
      "Iteration 3741 training loss: 0.2792383139529752, test loss: 0.7735096711379448\n",
      "Iteration 3742 training loss: 0.27919969581971654, test loss: 0.7735286102248107\n",
      "Iteration 3743 training loss: 0.2791610883162177, test loss: 0.7735475486446739\n",
      "Iteration 3744 training loss: 0.2791224914378714, test loss: 0.773566486397347\n",
      "Iteration 3745 training loss: 0.2790839051800733, test loss: 0.7735854234826428\n",
      "Iteration 3746 training loss: 0.27904532953822203, test loss: 0.7736043599003738\n",
      "Iteration 3747 training loss: 0.279006764507719, test loss: 0.7736232956503533\n",
      "Iteration 3748 training loss: 0.2789682100839682, test loss: 0.7736422307323947\n",
      "Iteration 3749 training loss: 0.27892966626237703, test loss: 0.7736611651463114\n",
      "Iteration 3750 training loss: 0.2788911330383551, test loss: 0.7736800988919177\n",
      "Iteration 3751 training loss: 0.27885261040731535, test loss: 0.7736990319690272\n",
      "Iteration 3752 training loss: 0.27881409836467336, test loss: 0.7737179643774551\n",
      "Iteration 3753 training loss: 0.2787755969058476, test loss: 0.7737368961170155\n",
      "Iteration 3754 training loss: 0.27873710602625934, test loss: 0.7737558271875238\n",
      "Iteration 3755 training loss: 0.2786986257213328, test loss: 0.7737747575887951\n",
      "Iteration 3756 training loss: 0.2786601559864948, test loss: 0.7737936873206448\n",
      "Iteration 3757 training loss: 0.27862169681717525, test loss: 0.7738126163828887\n",
      "Iteration 3758 training loss: 0.27858324820880676, test loss: 0.7738315447753431\n",
      "Iteration 3759 training loss: 0.2785448101568247, test loss: 0.773850472497824\n",
      "Iteration 3760 training loss: 0.27850638265666744, test loss: 0.7738693995501482\n",
      "Iteration 3761 training loss: 0.278467965703776, test loss: 0.7738883259321325\n",
      "Iteration 3762 training loss: 0.2784295592935944, test loss: 0.7739072516435942\n",
      "Iteration 3763 training loss: 0.2783911634215693, test loss: 0.7739261766843502\n",
      "Iteration 3764 training loss: 0.2783527780831502, test loss: 0.7739451010542187\n",
      "Iteration 3765 training loss: 0.2783144032737896, test loss: 0.773964024753017\n",
      "Iteration 3766 training loss: 0.2782760389889426, test loss: 0.7739829477805636\n",
      "Iteration 3767 training loss: 0.2782376852240671, test loss: 0.7740018701366771\n",
      "Iteration 3768 training loss: 0.278199341974624, test loss: 0.7740207918211756\n",
      "Iteration 3769 training loss: 0.27816100923607695, test loss: 0.7740397128338786\n",
      "Iteration 3770 training loss: 0.2781226870038922, test loss: 0.7740586331746051\n",
      "Iteration 3771 training loss: 0.27808437527353896, test loss: 0.7740775528431746\n",
      "Iteration 3772 training loss: 0.2780460740404892, test loss: 0.7740964718394064\n",
      "Iteration 3773 training loss: 0.27800778330021786, test loss: 0.7741153901631209\n",
      "Iteration 3774 training loss: 0.27796950304820234, test loss: 0.7741343078141387\n",
      "Iteration 3775 training loss: 0.27793123327992303, test loss: 0.7741532247922791\n",
      "Iteration 3776 training loss: 0.27789297399086316, test loss: 0.7741721410973639\n",
      "Iteration 3777 training loss: 0.2778547251765085, test loss: 0.7741910567292135\n",
      "Iteration 3778 training loss: 0.277816486832348, test loss: 0.7742099716876495\n",
      "Iteration 3779 training loss: 0.27777825895387287, test loss: 0.7742288859724933\n",
      "Iteration 3780 training loss: 0.27774004153657755, test loss: 0.7742477995835665\n",
      "Iteration 3781 training loss: 0.2777018345759592, test loss: 0.7742667125206909\n",
      "Iteration 3782 training loss: 0.27766363806751737, test loss: 0.7742856247836891\n",
      "Iteration 3783 training loss: 0.2776254520067548, test loss: 0.774304536372384\n",
      "Iteration 3784 training loss: 0.2775872763891769, test loss: 0.7743234472865972\n",
      "Iteration 3785 training loss: 0.2775491112102917, test loss: 0.7743423575261527\n",
      "Iteration 3786 training loss: 0.2775109564656101, test loss: 0.7743612670908734\n",
      "Iteration 3787 training loss: 0.2774728121506456, test loss: 0.774380175980583\n",
      "Iteration 3788 training loss: 0.277434678260915, test loss: 0.7743990841951044\n",
      "Iteration 3789 training loss: 0.27739655479193714, test loss: 0.7744179917342628\n",
      "Iteration 3790 training loss: 0.277358441739234, test loss: 0.774436898597882\n",
      "Iteration 3791 training loss: 0.27732033909833037, test loss: 0.7744558047857859\n",
      "Iteration 3792 training loss: 0.27728224686475345, test loss: 0.7744747102978\n",
      "Iteration 3793 training loss: 0.27724416503403365, test loss: 0.7744936151337488\n",
      "Iteration 3794 training loss: 0.27720609360170384, test loss: 0.774512519293458\n",
      "Iteration 3795 training loss: 0.27716803256329947, test loss: 0.7745314227767525\n",
      "Iteration 3796 training loss: 0.27712998191435917, test loss: 0.7745503255834584\n",
      "Iteration 3797 training loss: 0.277091941650424, test loss: 0.7745692277134014\n",
      "Iteration 3798 training loss: 0.27705391176703775, test loss: 0.7745881291664081\n",
      "Iteration 3799 training loss: 0.27701589225974715, test loss: 0.7746070299423043\n",
      "Iteration 3800 training loss: 0.27697788312410154, test loss: 0.7746259300409175\n",
      "Iteration 3801 training loss: 0.2769398843556528, test loss: 0.7746448294620736\n",
      "Iteration 3802 training loss: 0.276901895949956, test loss: 0.7746637282056008\n",
      "Iteration 3803 training loss: 0.27686391790256853, test loss: 0.7746826262713258\n",
      "Iteration 3804 training loss: 0.2768259502090506, test loss: 0.7747015236590766\n",
      "Iteration 3805 training loss: 0.2767879928649652, test loss: 0.774720420368681\n",
      "Iteration 3806 training loss: 0.276750045865878, test loss: 0.7747393163999671\n",
      "Iteration 3807 training loss: 0.2767121092073574, test loss: 0.7747582117527632\n",
      "Iteration 3808 training loss: 0.2766741828849746, test loss: 0.7747771064268978\n",
      "Iteration 3809 training loss: 0.27663626689430326, test loss: 0.7747960004222001\n",
      "Iteration 3810 training loss: 0.27659836123092, test loss: 0.7748148937384989\n",
      "Iteration 3811 training loss: 0.27656046589040423, test loss: 0.7748337863756234\n",
      "Iteration 3812 training loss: 0.27652258086833753, test loss: 0.7748526783334035\n",
      "Iteration 3813 training loss: 0.27648470616030485, test loss: 0.7748715696116686\n",
      "Iteration 3814 training loss: 0.27644684176189344, test loss: 0.774890460210249\n",
      "Iteration 3815 training loss: 0.2764089876686933, test loss: 0.7749093501289748\n",
      "Iteration 3816 training loss: 0.2763711438762972, test loss: 0.7749282393676763\n",
      "Iteration 3817 training loss: 0.27633331038030057, test loss: 0.7749471279261847\n",
      "Iteration 3818 training loss: 0.27629548717630154, test loss: 0.7749660158043304\n",
      "Iteration 3819 training loss: 0.27625767425990094, test loss: 0.7749849030019451\n",
      "Iteration 3820 training loss: 0.27621987162670225, test loss: 0.7750037895188597\n",
      "Iteration 3821 training loss: 0.27618207927231175, test loss: 0.7750226753549061\n",
      "Iteration 3822 training loss: 0.27614429719233813, test loss: 0.7750415605099163\n",
      "Iteration 3823 training loss: 0.2761065253823931, test loss: 0.7750604449837221\n",
      "Iteration 3824 training loss: 0.2760687638380908, test loss: 0.7750793287761559\n",
      "Iteration 3825 training loss: 0.27603101255504814, test loss: 0.7750982118870506\n",
      "Iteration 3826 training loss: 0.2759932715288847, test loss: 0.7751170943162384\n",
      "Iteration 3827 training loss: 0.2759555407552228, test loss: 0.7751359760635528\n",
      "Iteration 3828 training loss: 0.2759178202296872, test loss: 0.7751548571288268\n",
      "Iteration 3829 training loss: 0.2758801099479056, test loss: 0.7751737375118939\n",
      "Iteration 3830 training loss: 0.27584240990550823, test loss: 0.7751926172125877\n",
      "Iteration 3831 training loss: 0.27580472009812806, test loss: 0.7752114962307423\n",
      "Iteration 3832 training loss: 0.2757670405214006, test loss: 0.7752303745661921\n",
      "Iteration 3833 training loss: 0.2757293711709641, test loss: 0.7752492522187706\n",
      "Iteration 3834 training loss: 0.27569171204245924, test loss: 0.7752681291883131\n",
      "Iteration 3835 training loss: 0.27565406313152996, test loss: 0.7752870054746545\n",
      "Iteration 3836 training loss: 0.2756164244338222, test loss: 0.7753058810776293\n",
      "Iteration 3837 training loss: 0.27557879594498474, test loss: 0.7753247559970734\n",
      "Iteration 3838 training loss: 0.27554117766066927, test loss: 0.7753436302328216\n",
      "Iteration 3839 training loss: 0.27550356957652977, test loss: 0.77536250378471\n",
      "Iteration 3840 training loss: 0.275465971688223, test loss: 0.7753813766525747\n",
      "Iteration 3841 training loss: 0.2754283839914087, test loss: 0.7754002488362514\n",
      "Iteration 3842 training loss: 0.27539080648174846, test loss: 0.7754191203355766\n",
      "Iteration 3843 training loss: 0.2753532391549071, test loss: 0.7754379911503871\n",
      "Iteration 3844 training loss: 0.2753156820065522, test loss: 0.7754568612805198\n",
      "Iteration 3845 training loss: 0.27527813503235354, test loss: 0.7754757307258114\n",
      "Iteration 3846 training loss: 0.2752405982279837, test loss: 0.7754945994860989\n",
      "Iteration 3847 training loss: 0.27520307158911783, test loss: 0.7755134675612207\n",
      "Iteration 3848 training loss: 0.2751655551114339, test loss: 0.7755323349510137\n",
      "Iteration 3849 training loss: 0.2751280487906124, test loss: 0.7755512016553161\n",
      "Iteration 3850 training loss: 0.2750905526223363, test loss: 0.775570067673966\n",
      "Iteration 3851 training loss: 0.2750530666022914, test loss: 0.7755889330068014\n",
      "Iteration 3852 training loss: 0.27501559072616594, test loss: 0.7756077976536613\n",
      "Iteration 3853 training loss: 0.2749781249896509, test loss: 0.7756266616143842\n",
      "Iteration 3854 training loss: 0.27494066938843986, test loss: 0.7756455248888092\n",
      "Iteration 3855 training loss: 0.27490322391822886, test loss: 0.7756643874767755\n",
      "Iteration 3856 training loss: 0.27486578857471683, test loss: 0.7756832493781224\n",
      "Iteration 3857 training loss: 0.2748283633536051, test loss: 0.7757021105926899\n",
      "Iteration 3858 training loss: 0.2747909482505976, test loss: 0.7757209711203172\n",
      "Iteration 3859 training loss: 0.2747535432614009, test loss: 0.7757398309608446\n",
      "Iteration 3860 training loss: 0.27471614838172426, test loss: 0.7757586901141126\n",
      "Iteration 3861 training loss: 0.27467876360727944, test loss: 0.7757775485799615\n",
      "Iteration 3862 training loss: 0.2746413889337808, test loss: 0.7757964063582323\n",
      "Iteration 3863 training loss: 0.27460402435694536, test loss: 0.7758152634487653\n",
      "Iteration 3864 training loss: 0.27456666987249256, test loss: 0.7758341198514018\n",
      "Iteration 3865 training loss: 0.27452932547614467, test loss: 0.7758529755659834\n",
      "Iteration 3866 training loss: 0.2744919911636263, test loss: 0.7758718305923514\n",
      "Iteration 3867 training loss: 0.2744546669306649, test loss: 0.7758906849303475\n",
      "Iteration 3868 training loss: 0.27441735277299034, test loss: 0.7759095385798137\n",
      "Iteration 3869 training loss: 0.2743800486863351, test loss: 0.7759283915405922\n",
      "Iteration 3870 training loss: 0.27434275466643426, test loss: 0.7759472438125253\n",
      "Iteration 3871 training loss: 0.27430547070902545, test loss: 0.7759660953954556\n",
      "Iteration 3872 training loss: 0.2742681968098489, test loss: 0.775984946289226\n",
      "Iteration 3873 training loss: 0.2742309329646473, test loss: 0.7760037964936789\n",
      "Iteration 3874 training loss: 0.2741936791691661, test loss: 0.7760226460086581\n",
      "Iteration 3875 training loss: 0.27415643541915324, test loss: 0.776041494834007\n",
      "Iteration 3876 training loss: 0.27411920171035925, test loss: 0.7760603429695688\n",
      "Iteration 3877 training loss: 0.27408197803853707, test loss: 0.7760791904151877\n",
      "Iteration 3878 training loss: 0.2740447643994425, test loss: 0.7760980371707071\n",
      "Iteration 3879 training loss: 0.2740075607888334, test loss: 0.7761168832359719\n",
      "Iteration 3880 training loss: 0.273970367202471, test loss: 0.7761357286108259\n",
      "Iteration 3881 training loss: 0.27393318363611807, test loss: 0.7761545732951143\n",
      "Iteration 3882 training loss: 0.27389601008554076, test loss: 0.7761734172886816\n",
      "Iteration 3883 training loss: 0.27385884654650744, test loss: 0.7761922605913729\n",
      "Iteration 3884 training loss: 0.27382169301478904, test loss: 0.776211103203033\n",
      "Iteration 3885 training loss: 0.273784549486159, test loss: 0.776229945123508\n",
      "Iteration 3886 training loss: 0.27374741595639346, test loss: 0.7762487863526429\n",
      "Iteration 3887 training loss: 0.2737102924212708, test loss: 0.7762676268902843\n",
      "Iteration 3888 training loss: 0.2736731788765725, test loss: 0.7762864667362771\n",
      "Iteration 3889 training loss: 0.27363607531808193, test loss: 0.7763053058904688\n",
      "Iteration 3890 training loss: 0.27359898174158537, test loss: 0.7763241443527048\n",
      "Iteration 3891 training loss: 0.27356189814287163, test loss: 0.776342982122832\n",
      "Iteration 3892 training loss: 0.27352482451773186, test loss: 0.7763618192006975\n",
      "Iteration 3893 training loss: 0.2734877608619599, test loss: 0.7763806555861482\n",
      "Iteration 3894 training loss: 0.27345070717135217, test loss: 0.7763994912790311\n",
      "Iteration 3895 training loss: 0.2734136634417074, test loss: 0.7764183262791937\n",
      "Iteration 3896 training loss: 0.2733766296688271, test loss: 0.7764371605864838\n",
      "Iteration 3897 training loss: 0.27333960584851513, test loss: 0.7764559942007488\n",
      "Iteration 3898 training loss: 0.2733025919765778, test loss: 0.7764748271218368\n",
      "Iteration 3899 training loss: 0.2732655880488243, test loss: 0.7764936593495967\n",
      "Iteration 3900 training loss: 0.2732285940610658, test loss: 0.776512490883876\n",
      "Iteration 3901 training loss: 0.27319161000911646, test loss: 0.7765313217245234\n",
      "Iteration 3902 training loss: 0.27315463588879274, test loss: 0.776550151871388\n",
      "Iteration 3903 training loss: 0.27311767169591367, test loss: 0.7765689813243185\n",
      "Iteration 3904 training loss: 0.27308071742630063, test loss: 0.7765878100831645\n",
      "Iteration 3905 training loss: 0.2730437730757777, test loss: 0.7766066381477746\n",
      "Iteration 3906 training loss: 0.2730068386401715, test loss: 0.776625465517999\n",
      "Iteration 3907 training loss: 0.27296991411531096, test loss: 0.7766442921936872\n",
      "Iteration 3908 training loss: 0.2729329994970276, test loss: 0.7766631181746892\n",
      "Iteration 3909 training loss: 0.2728960947811554, test loss: 0.776681943460855\n",
      "Iteration 3910 training loss: 0.27285919996353086, test loss: 0.7767007680520349\n",
      "Iteration 3911 training loss: 0.27282231503999316, test loss: 0.7767195919480796\n",
      "Iteration 3912 training loss: 0.2727854400063837, test loss: 0.7767384151488395\n",
      "Iteration 3913 training loss: 0.27274857485854637, test loss: 0.7767572376541659\n",
      "Iteration 3914 training loss: 0.2727117195923278, test loss: 0.7767760594639094\n",
      "Iteration 3915 training loss: 0.2726748742035769, test loss: 0.7767948805779215\n",
      "Iteration 3916 training loss: 0.2726380386881452, test loss: 0.7768137009960537\n",
      "Iteration 3917 training loss: 0.2726012130418864, test loss: 0.7768325207181573\n",
      "Iteration 3918 training loss: 0.27256439726065707, test loss: 0.7768513397440847\n",
      "Iteration 3919 training loss: 0.2725275913403162, test loss: 0.7768701580736874\n",
      "Iteration 3920 training loss: 0.27249079527672504, test loss: 0.7768889757068178\n",
      "Iteration 3921 training loss: 0.27245400906574735, test loss: 0.7769077926433283\n",
      "Iteration 3922 training loss: 0.2724172327032497, test loss: 0.7769266088830715\n",
      "Iteration 3923 training loss: 0.2723804661851006, test loss: 0.7769454244258999\n",
      "Iteration 3924 training loss: 0.2723437095071714, test loss: 0.7769642392716666\n",
      "Iteration 3925 training loss: 0.2723069626653359, test loss: 0.7769830534202249\n",
      "Iteration 3926 training loss: 0.27227022565547027, test loss: 0.7770018668714278\n",
      "Iteration 3927 training loss: 0.272233498473453, test loss: 0.7770206796251289\n",
      "Iteration 3928 training loss: 0.2721967811151654, test loss: 0.7770394916811822\n",
      "Iteration 3929 training loss: 0.2721600735764908, test loss: 0.7770583030394411\n",
      "Iteration 3930 training loss: 0.2721233758533154, test loss: 0.77707711369976\n",
      "Iteration 3931 training loss: 0.2720866879415277, test loss: 0.7770959236619929\n",
      "Iteration 3932 training loss: 0.2720500098370185, test loss: 0.7771147329259941\n",
      "Iteration 3933 training loss: 0.27201334153568124, test loss: 0.7771335414916182\n",
      "Iteration 3934 training loss: 0.2719766830334118, test loss: 0.7771523493587207\n",
      "Iteration 3935 training loss: 0.2719400343261084, test loss: 0.7771711565271553\n",
      "Iteration 3936 training loss: 0.2719033954096717, test loss: 0.777189962996778\n",
      "Iteration 3937 training loss: 0.27186676628000483, test loss: 0.777208768767444\n",
      "Iteration 3938 training loss: 0.27183014693301355, test loss: 0.7772275738390089\n",
      "Iteration 3939 training loss: 0.2717935373646059, test loss: 0.7772463782113277\n",
      "Iteration 3940 training loss: 0.2717569375706922, test loss: 0.7772651818842572\n",
      "Iteration 3941 training loss: 0.2717203475471854, test loss: 0.7772839848576528\n",
      "Iteration 3942 training loss: 0.2716837672900008, test loss: 0.7773027871313707\n",
      "Iteration 3943 training loss: 0.2716471967950565, test loss: 0.7773215887052679\n",
      "Iteration 3944 training loss: 0.27161063605827224, test loss: 0.7773403895792\n",
      "Iteration 3945 training loss: 0.27157408507557096, test loss: 0.7773591897530246\n",
      "Iteration 3946 training loss: 0.27153754384287754, test loss: 0.7773779892265981\n",
      "Iteration 3947 training loss: 0.27150101235611956, test loss: 0.7773967879997776\n",
      "Iteration 3948 training loss: 0.27146449061122696, test loss: 0.7774155860724211\n",
      "Iteration 3949 training loss: 0.27142797860413187, test loss: 0.7774343834443851\n",
      "Iteration 3950 training loss: 0.2713914763307692, test loss: 0.7774531801155277\n",
      "Iteration 3951 training loss: 0.271354983787076, test loss: 0.7774719760857067\n",
      "Iteration 3952 training loss: 0.27131850096899196, test loss: 0.7774907713547798\n",
      "Iteration 3953 training loss: 0.27128202787245886, test loss: 0.7775095659226053\n",
      "Iteration 3954 training loss: 0.2712455644934213, test loss: 0.777528359789042\n",
      "Iteration 3955 training loss: 0.2712091108278259, test loss: 0.7775471529539475\n",
      "Iteration 3956 training loss: 0.27117266687162195, test loss: 0.7775659454171812\n",
      "Iteration 3957 training loss: 0.271136232620761, test loss: 0.7775847371786014\n",
      "Iteration 3958 training loss: 0.271099808071197, test loss: 0.7776035282380674\n",
      "Iteration 3959 training loss: 0.27106339321888645, test loss: 0.7776223185954383\n",
      "Iteration 3960 training loss: 0.2710269880597882, test loss: 0.7776411082505739\n",
      "Iteration 3961 training loss: 0.2709905925898634, test loss: 0.7776598972033332\n",
      "Iteration 3962 training loss: 0.2709542068050755, test loss: 0.777678685453576\n",
      "Iteration 3963 training loss: 0.27091783070139064, test loss: 0.7776974730011622\n",
      "Iteration 3964 training loss: 0.2708814642747771, test loss: 0.7777162598459518\n",
      "Iteration 3965 training loss: 0.27084510752120566, test loss: 0.7777350459878053\n",
      "Iteration 3966 training loss: 0.27080876043664964, test loss: 0.7777538314265829\n",
      "Iteration 3967 training loss: 0.2707724230170843, test loss: 0.7777726161621451\n",
      "Iteration 3968 training loss: 0.27073609525848763, test loss: 0.7777914001943524\n",
      "Iteration 3969 training loss: 0.27069977715684007, test loss: 0.777810183523066\n",
      "Iteration 3970 training loss: 0.2706634687081242, test loss: 0.777828966148147\n",
      "Iteration 3971 training loss: 0.27062716990832497, test loss: 0.7778477480694566\n",
      "Iteration 3972 training loss: 0.27059088075342996, test loss: 0.7778665292868561\n",
      "Iteration 3973 training loss: 0.270554601239429, test loss: 0.7778853098002068\n",
      "Iteration 3974 training loss: 0.27051833136231407, test loss: 0.7779040896093711\n",
      "Iteration 3975 training loss: 0.2704820711180797, test loss: 0.7779228687142103\n",
      "Iteration 3976 training loss: 0.27044582050272303, test loss: 0.7779416471145867\n",
      "Iteration 3977 training loss: 0.2704095795122432, test loss: 0.7779604248103625\n",
      "Iteration 3978 training loss: 0.2703733481426418, test loss: 0.7779792018014001\n",
      "Iteration 3979 training loss: 0.2703371263899229, test loss: 0.7779979780875622\n",
      "Iteration 3980 training loss: 0.2703009142500929, test loss: 0.7780167536687113\n",
      "Iteration 3981 training loss: 0.2702647117191604, test loss: 0.77803552854471\n",
      "Iteration 3982 training loss: 0.27022851879313653, test loss: 0.7780543027154223\n",
      "Iteration 3983 training loss: 0.2701923354680347, test loss: 0.7780730761807106\n",
      "Iteration 3984 training loss: 0.27015616173987067, test loss: 0.7780918489404386\n",
      "Iteration 3985 training loss: 0.2701199976046626, test loss: 0.7781106209944699\n",
      "Iteration 3986 training loss: 0.27008384305843103, test loss: 0.778129392342668\n",
      "Iteration 3987 training loss: 0.2700476980971986, test loss: 0.7781481629848966\n",
      "Iteration 3988 training loss: 0.27001156271699067, test loss: 0.7781669329210203\n",
      "Iteration 3989 training loss: 0.26997543691383463, test loss: 0.7781857021509028\n",
      "Iteration 3990 training loss: 0.2699393206837605, test loss: 0.7782044706744089\n",
      "Iteration 3991 training loss: 0.26990321402280026, test loss: 0.7782232384914027\n",
      "Iteration 3992 training loss: 0.2698671169269886, test loss: 0.7782420056017489\n",
      "Iteration 3993 training loss: 0.26983102939236225, test loss: 0.778260772005313\n",
      "Iteration 3994 training loss: 0.2697949514149605, test loss: 0.7782795377019595\n",
      "Iteration 3995 training loss: 0.2697588829908249, test loss: 0.7782983026915531\n",
      "Iteration 3996 training loss: 0.26972282411599924, test loss: 0.7783170669739599\n",
      "Iteration 3997 training loss: 0.26968677478652975, test loss: 0.778335830549045\n",
      "Iteration 3998 training loss: 0.269650734998465, test loss: 0.7783545934166742\n",
      "Iteration 3999 training loss: 0.2696147047478557, test loss: 0.7783733555767132\n",
      "Iteration 4000 training loss: 0.2695786840307551, test loss: 0.7783921170290279\n",
      "Iteration 4001 training loss: 0.2695426728432187, test loss: 0.7784108777734845\n",
      "Iteration 4002 training loss: 0.2695066711813042, test loss: 0.7784296378099492\n",
      "Iteration 4003 training loss: 0.2694706790410719, test loss: 0.7784483971382888\n",
      "Iteration 4004 training loss: 0.26943469641858403, test loss: 0.7784671557583694\n",
      "Iteration 4005 training loss: 0.2693987233099055, test loss: 0.778485913670058\n",
      "Iteration 4006 training loss: 0.2693627597111033, test loss: 0.7785046708732215\n",
      "Iteration 4007 training loss: 0.26932680561824685, test loss: 0.778523427367727\n",
      "Iteration 4008 training loss: 0.2692908610274077, test loss: 0.7785421831534415\n",
      "Iteration 4009 training loss: 0.26925492593466, test loss: 0.7785609382302325\n",
      "Iteration 4010 training loss: 0.26921900033608, test loss: 0.778579692597968\n",
      "Iteration 4011 training loss: 0.2691830842277462, test loss: 0.7785984462565148\n",
      "Iteration 4012 training loss: 0.2691471776057395, test loss: 0.778617199205741\n",
      "Iteration 4013 training loss: 0.26911128046614324, test loss: 0.7786359514455151\n",
      "Iteration 4014 training loss: 0.2690753928050428, test loss: 0.7786547029757049\n",
      "Iteration 4015 training loss: 0.269039514618526, test loss: 0.7786734537961787\n",
      "Iteration 4016 training loss: 0.26900364590268283, test loss: 0.7786922039068049\n",
      "Iteration 4017 training loss: 0.26896778665360577, test loss: 0.7787109533074523\n",
      "Iteration 4018 training loss: 0.2689319368673894, test loss: 0.7787297019979894\n",
      "Iteration 4019 training loss: 0.26889609654013075, test loss: 0.7787484499782853\n",
      "Iteration 4020 training loss: 0.268860265667929, test loss: 0.7787671972482091\n",
      "Iteration 4021 training loss: 0.2688244442468856, test loss: 0.77878594380763\n",
      "Iteration 4022 training loss: 0.2687886322731045, test loss: 0.7788046896564172\n",
      "Iteration 4023 training loss: 0.26875282974269177, test loss: 0.7788234347944406\n",
      "Iteration 4024 training loss: 0.26871703665175567, test loss: 0.7788421792215695\n",
      "Iteration 4025 training loss: 0.26868125299640677, test loss: 0.7788609229376738\n",
      "Iteration 4026 training loss: 0.2686454787727582, test loss: 0.7788796659426236\n",
      "Iteration 4027 training loss: 0.26860971397692496, test loss: 0.778898408236289\n",
      "Iteration 4028 training loss: 0.26857395860502464, test loss: 0.7789171498185401\n",
      "Iteration 4029 training loss: 0.26853821265317684, test loss: 0.7789358906892475\n",
      "Iteration 4030 training loss: 0.2685024761175038, test loss: 0.7789546308482816\n",
      "Iteration 4031 training loss: 0.2684667489941295, test loss: 0.7789733702955136\n",
      "Iteration 4032 training loss: 0.26843103127918055, test loss: 0.7789921090308138\n",
      "Iteration 4033 training loss: 0.2683953229687858, test loss: 0.7790108470540537\n",
      "Iteration 4034 training loss: 0.26835962405907626, test loss: 0.7790295843651043\n",
      "Iteration 4035 training loss: 0.26832393454618536, test loss: 0.7790483209638365\n",
      "Iteration 4036 training loss: 0.2682882544262485, test loss: 0.7790670568501223\n",
      "Iteration 4037 training loss: 0.26825258369540356, test loss: 0.7790857920238332\n",
      "Iteration 4038 training loss: 0.26821692234979067, test loss: 0.7791045264848409\n",
      "Iteration 4039 training loss: 0.26818127038555206, test loss: 0.7791232602330173\n",
      "Iteration 4040 training loss: 0.2681456277988325, test loss: 0.7791419932682346\n",
      "Iteration 4041 training loss: 0.2681099945857786, test loss: 0.7791607255903649\n",
      "Iteration 4042 training loss: 0.26807437074253954, test loss: 0.7791794571992803\n",
      "Iteration 4043 training loss: 0.26803875626526663, test loss: 0.7791981880948536\n",
      "Iteration 4044 training loss: 0.2680031511501135, test loss: 0.7792169182769576\n",
      "Iteration 4045 training loss: 0.2679675553932359, test loss: 0.7792356477454646\n",
      "Iteration 4046 training loss: 0.2679319689907919, test loss: 0.7792543765002481\n",
      "Iteration 4047 training loss: 0.26789639193894177, test loss: 0.7792731045411806\n",
      "Iteration 4048 training loss: 0.26786082423384794, test loss: 0.7792918318681357\n",
      "Iteration 4049 training loss: 0.26782526587167543, test loss: 0.7793105584809866\n",
      "Iteration 4050 training loss: 0.26778971684859093, test loss: 0.779329284379607\n",
      "Iteration 4051 training loss: 0.26775417716076383, test loss: 0.7793480095638702\n",
      "Iteration 4052 training loss: 0.26771864680436547, test loss: 0.7793667340336503\n",
      "Iteration 4053 training loss: 0.2676831257755697, test loss: 0.7793854577888212\n",
      "Iteration 4054 training loss: 0.2676476140705523, test loss: 0.7794041808292568\n",
      "Iteration 4055 training loss: 0.2676121116854915, test loss: 0.7794229031548318\n",
      "Iteration 4056 training loss: 0.26757661861656745, test loss: 0.77944162476542\n",
      "Iteration 4057 training loss: 0.2675411348599629, test loss: 0.779460345660896\n",
      "Iteration 4058 training loss: 0.26750566041186263, test loss: 0.7794790658411348\n",
      "Iteration 4059 training loss: 0.26747019526845356, test loss: 0.7794977853060108\n",
      "Iteration 4060 training loss: 0.267434739425925, test loss: 0.7795165040553991\n",
      "Iteration 4061 training loss: 0.2673992928804683, test loss: 0.7795352220891749\n",
      "Iteration 4062 training loss: 0.2673638556282773, test loss: 0.7795539394072133\n",
      "Iteration 4063 training loss: 0.26732842766554754, test loss: 0.7795726560093895\n",
      "Iteration 4064 training loss: 0.26729300898847735, test loss: 0.7795913718955791\n",
      "Iteration 4065 training loss: 0.2672575995932669, test loss: 0.779610087065658\n",
      "Iteration 4066 training loss: 0.2672221994761188, test loss: 0.779628801519502\n",
      "Iteration 4067 training loss: 0.2671868086332375, test loss: 0.7796475152569864\n",
      "Iteration 4068 training loss: 0.26715142706083006, test loss: 0.7796662282779875\n",
      "Iteration 4069 training loss: 0.2671160547551055, test loss: 0.779684940582382\n",
      "Iteration 4070 training loss: 0.26708069171227516, test loss: 0.7797036521700454\n",
      "Iteration 4071 training loss: 0.2670453379285525, test loss: 0.779722363040855\n",
      "Iteration 4072 training loss: 0.2670099934001532, test loss: 0.7797410731946869\n",
      "Iteration 4073 training loss: 0.26697465812329496, test loss: 0.7797597826314178\n",
      "Iteration 4074 training loss: 0.2669393320941981, test loss: 0.7797784913509249\n",
      "Iteration 4075 training loss: 0.2669040153090848, test loss: 0.7797971993530851\n",
      "Iteration 4076 training loss: 0.26686870776417937, test loss: 0.7798159066377751\n",
      "Iteration 4077 training loss: 0.2668334094557086, test loss: 0.7798346132048732\n",
      "Iteration 4078 training loss: 0.26679812037990114, test loss: 0.779853319054256\n",
      "Iteration 4079 training loss: 0.2667628405329881, test loss: 0.7798720241858013\n",
      "Iteration 4080 training loss: 0.2667275699112025, test loss: 0.7798907285993867\n",
      "Iteration 4081 training loss: 0.2666923085107799, test loss: 0.7799094322948903\n",
      "Iteration 4082 training loss: 0.26665705632795783, test loss: 0.7799281352721897\n",
      "Iteration 4083 training loss: 0.2666218133589758, test loss: 0.7799468375311635\n",
      "Iteration 4084 training loss: 0.2665865796000758, test loss: 0.7799655390716895\n",
      "Iteration 4085 training loss: 0.2665513550475019, test loss: 0.7799842398936461\n",
      "Iteration 4086 training loss: 0.26651613969750015, test loss: 0.7800029399969123\n",
      "Iteration 4087 training loss: 0.26648093354631924, test loss: 0.780021639381366\n",
      "Iteration 4088 training loss: 0.26644573659020954, test loss: 0.7800403380468867\n",
      "Iteration 4089 training loss: 0.2664105488254238, test loss: 0.7800590359933529\n",
      "Iteration 4090 training loss: 0.26637537024821695, test loss: 0.7800777332206437\n",
      "Iteration 4091 training loss: 0.266340200854846, test loss: 0.7800964297286382\n",
      "Iteration 4092 training loss: 0.2663050406415702, test loss: 0.7801151255172162\n",
      "Iteration 4093 training loss: 0.26626988960465087, test loss: 0.7801338205862566\n",
      "Iteration 4094 training loss: 0.2662347477403516, test loss: 0.7801525149356388\n",
      "Iteration 4095 training loss: 0.2661996150449381, test loss: 0.7801712085652432\n",
      "Iteration 4096 training loss: 0.26616449151467814, test loss: 0.7801899014749493\n",
      "Iteration 4097 training loss: 0.26612937714584173, test loss: 0.780208593664637\n",
      "Iteration 4098 training loss: 0.26609427193470103, test loss: 0.7802272851341864\n",
      "Iteration 4099 training loss: 0.2660591758775303, test loss: 0.780245975883478\n",
      "Iteration 4100 training loss: 0.2660240889706061, test loss: 0.7802646659123916\n",
      "Iteration 4101 training loss: 0.2659890112102069, test loss: 0.7802833552208087\n",
      "Iteration 4102 training loss: 0.26595394259261346, test loss: 0.780302043808609\n",
      "Iteration 4103 training loss: 0.26591888311410866, test loss: 0.7803207316756735\n",
      "Iteration 4104 training loss: 0.26588383277097755, test loss: 0.7803394188218834\n",
      "Iteration 4105 training loss: 0.26584879155950714, test loss: 0.7803581052471191\n",
      "Iteration 4106 training loss: 0.2658137594759869, test loss: 0.7803767909512626\n",
      "Iteration 4107 training loss: 0.26577873651670814, test loss: 0.7803954759341944\n",
      "Iteration 4108 training loss: 0.26574372267796453, test loss: 0.7804141601957963\n",
      "Iteration 4109 training loss: 0.2657087179560517, test loss: 0.7804328437359498\n",
      "Iteration 4110 training loss: 0.2656737223472675, test loss: 0.7804515265545366\n",
      "Iteration 4111 training loss: 0.2656387358479118, test loss: 0.7804702086514382\n",
      "Iteration 4112 training loss: 0.2656037584542868, test loss: 0.7804888900265368\n",
      "Iteration 4113 training loss: 0.2655687901626967, test loss: 0.7805075706797145\n",
      "Iteration 4114 training loss: 0.26553383096944766, test loss: 0.7805262506108536\n",
      "Iteration 4115 training loss: 0.2654988808708484, test loss: 0.7805449298198356\n",
      "Iteration 4116 training loss: 0.2654639398632093, test loss: 0.7805636083065437\n",
      "Iteration 4117 training loss: 0.26542900794284313, test loss: 0.7805822860708603\n",
      "Iteration 4118 training loss: 0.26539408510606466, test loss: 0.780600963112668\n",
      "Iteration 4119 training loss: 0.2653591713491909, test loss: 0.7806196394318499\n",
      "Iteration 4120 training loss: 0.2653242666685409, test loss: 0.7806383150282885\n",
      "Iteration 4121 training loss: 0.26528937106043576, test loss: 0.780656989901867\n",
      "Iteration 4122 training loss: 0.26525448452119876, test loss: 0.7806756640524686\n",
      "Iteration 4123 training loss: 0.2652196070471552, test loss: 0.7806943374799769\n",
      "Iteration 4124 training loss: 0.26518473863463277, test loss: 0.7807130101842749\n",
      "Iteration 4125 training loss: 0.2651498792799608, test loss: 0.7807316821652466\n",
      "Iteration 4126 training loss: 0.26511502897947115, test loss: 0.7807503534227752\n",
      "Iteration 4127 training loss: 0.2650801877294976, test loss: 0.7807690239567451\n",
      "Iteration 4128 training loss: 0.26504535552637615, test loss: 0.7807876937670396\n",
      "Iteration 4129 training loss: 0.2650105323664447, test loss: 0.7808063628535432\n",
      "Iteration 4130 training loss: 0.2649757182460433, test loss: 0.7808250312161399\n",
      "Iteration 4131 training loss: 0.26494091316151425, test loss: 0.7808436988547144\n",
      "Iteration 4132 training loss: 0.26490611710920164, test loss: 0.7808623657691507\n",
      "Iteration 4133 training loss: 0.26487133008545216, test loss: 0.7808810319593334\n",
      "Iteration 4134 training loss: 0.2648365520866142, test loss: 0.7808996974251471\n",
      "Iteration 4135 training loss: 0.26480178310903824, test loss: 0.780918362166477\n",
      "Iteration 4136 training loss: 0.26476702314907696, test loss: 0.7809370261832077\n",
      "Iteration 4137 training loss: 0.2647322722030851, test loss: 0.7809556894752246\n",
      "Iteration 4138 training loss: 0.26469753026741955, test loss: 0.7809743520424123\n",
      "Iteration 4139 training loss: 0.2646627973384391, test loss: 0.7809930138846566\n",
      "Iteration 4140 training loss: 0.2646280734125048, test loss: 0.7810116750018429\n",
      "Iteration 4141 training loss: 0.2645933584859799, test loss: 0.7810303353938564\n",
      "Iteration 4142 training loss: 0.2645586525552293, test loss: 0.781048995060583\n",
      "Iteration 4143 training loss: 0.26452395561662045, test loss: 0.7810676540019084\n",
      "Iteration 4144 training loss: 0.2644892676665225, test loss: 0.7810863122177186\n",
      "Iteration 4145 training loss: 0.26445458870130684, test loss: 0.7811049697078996\n",
      "Iteration 4146 training loss: 0.264419918717347, test loss: 0.7811236264723372\n",
      "Iteration 4147 training loss: 0.2643852577110185, test loss: 0.7811422825109184\n",
      "Iteration 4148 training loss: 0.2643506056786989, test loss: 0.7811609378235289\n",
      "Iteration 4149 training loss: 0.26431596261676776, test loss: 0.7811795924100556\n",
      "Iteration 4150 training loss: 0.26428132852160713, test loss: 0.7811982462703849\n",
      "Iteration 4151 training loss: 0.26424670338960055, test loss: 0.781216899404404\n",
      "Iteration 4152 training loss: 0.2642120872171339, test loss: 0.7812355518119988\n",
      "Iteration 4153 training loss: 0.26417748000059516, test loss: 0.7812542034930573\n",
      "Iteration 4154 training loss: 0.2641428817363743, test loss: 0.7812728544474661\n",
      "Iteration 4155 training loss: 0.2641082924208633, test loss: 0.7812915046751123\n",
      "Iteration 4156 training loss: 0.2640737120504563, test loss: 0.7813101541758837\n",
      "Iteration 4157 training loss: 0.26403914062154954, test loss: 0.7813288029496673\n",
      "Iteration 4158 training loss: 0.2640045781305411, test loss: 0.7813474509963512\n",
      "Iteration 4159 training loss: 0.26397002457383123, test loss: 0.7813660983158225\n",
      "Iteration 4160 training loss: 0.26393547994782224, test loss: 0.7813847449079694\n",
      "Iteration 4161 training loss: 0.2639009442489186, test loss: 0.7814033907726797\n",
      "Iteration 4162 training loss: 0.2638664174735266, test loss: 0.7814220359098414\n",
      "Iteration 4163 training loss: 0.26383189961805475, test loss: 0.7814406803193429\n",
      "Iteration 4164 training loss: 0.26379739067891356, test loss: 0.7814593240010723\n",
      "Iteration 4165 training loss: 0.2637628906525155, test loss: 0.781477966954918\n",
      "Iteration 4166 training loss: 0.26372839953527516, test loss: 0.7814966091807687\n",
      "Iteration 4167 training loss: 0.26369391732360914, test loss: 0.7815152506785126\n",
      "Iteration 4168 training loss: 0.2636594440139362, test loss: 0.7815338914480386\n",
      "Iteration 4169 training loss: 0.26362497960267695, test loss: 0.7815525314892359\n",
      "Iteration 4170 training loss: 0.2635905240862541, test loss: 0.7815711708019931\n",
      "Iteration 4171 training loss: 0.2635560774610925, test loss: 0.7815898093861997\n",
      "Iteration 4172 training loss: 0.26352163972361886, test loss: 0.7816084472417443\n",
      "Iteration 4173 training loss: 0.2634872108702619, test loss: 0.7816270843685168\n",
      "Iteration 4174 training loss: 0.2634527908974527, test loss: 0.7816457207664064\n",
      "Iteration 4175 training loss: 0.263418379801624, test loss: 0.7816643564353027\n",
      "Iteration 4176 training loss: 0.2633839775792107, test loss: 0.7816829913750952\n",
      "Iteration 4177 training loss: 0.26334958422664984, test loss: 0.7817016255856739\n",
      "Iteration 4178 training loss: 0.26331519974038026, test loss: 0.7817202590669287\n",
      "Iteration 4179 training loss: 0.26328082411684295, test loss: 0.7817388918187493\n",
      "Iteration 4180 training loss: 0.26324645735248087, test loss: 0.7817575238410263\n",
      "Iteration 4181 training loss: 0.26321209944373913, test loss: 0.7817761551336496\n",
      "Iteration 4182 training loss: 0.2631777503870647, test loss: 0.7817947856965097\n",
      "Iteration 4183 training loss: 0.2631434101789066, test loss: 0.781813415529497\n",
      "Iteration 4184 training loss: 0.26310907881571594, test loss: 0.7818320446325021\n",
      "Iteration 4185 training loss: 0.2630747562939457, test loss: 0.7818506730054158\n",
      "Iteration 4186 training loss: 0.26304044261005094, test loss: 0.7818693006481284\n",
      "Iteration 4187 training loss: 0.26300613776048887, test loss: 0.7818879275605314\n",
      "Iteration 4188 training loss: 0.2629718417417185, test loss: 0.7819065537425156\n",
      "Iteration 4189 training loss: 0.26293755455020096, test loss: 0.7819251791939722\n",
      "Iteration 4190 training loss: 0.26290327618239934, test loss: 0.7819438039147926\n",
      "Iteration 4191 training loss: 0.2628690066347787, test loss: 0.7819624279048678\n",
      "Iteration 4192 training loss: 0.2628347459038062, test loss: 0.7819810511640893\n",
      "Iteration 4193 training loss: 0.2628004939859509, test loss: 0.7819996736923489\n",
      "Iteration 4194 training loss: 0.26276625087768396, test loss: 0.7820182954895385\n",
      "Iteration 4195 training loss: 0.26273201657547846, test loss: 0.7820369165555494\n",
      "Iteration 4196 training loss: 0.26269779107580943, test loss: 0.7820555368902742\n",
      "Iteration 4197 training loss: 0.262663574375154, test loss: 0.7820741564936041\n",
      "Iteration 4198 training loss: 0.2626293664699913, test loss: 0.7820927753654318\n",
      "Iteration 4199 training loss: 0.26259516735680233, test loss: 0.7821113935056494\n",
      "Iteration 4200 training loss: 0.26256097703207026, test loss: 0.7821300109141491\n",
      "Iteration 4201 training loss: 0.2625267954922799, test loss: 0.7821486275908237\n",
      "Iteration 4202 training loss: 0.2624926227339185, test loss: 0.7821672435355655\n",
      "Iteration 4203 training loss: 0.262458458753475, test loss: 0.7821858587482674\n",
      "Iteration 4204 training loss: 0.2624243035474405, test loss: 0.7822044732288219\n",
      "Iteration 4205 training loss: 0.2623901571123078, test loss: 0.7822230869771224\n",
      "Iteration 4206 training loss: 0.262356019444572, test loss: 0.7822416999930614\n",
      "Iteration 4207 training loss: 0.2623218905407299, test loss: 0.7822603122765324\n",
      "Iteration 4208 training loss: 0.2622877703972806, test loss: 0.7822789238274285\n",
      "Iteration 4209 training loss: 0.2622536590107249, test loss: 0.7822975346456427\n",
      "Iteration 4210 training loss: 0.2622195563775656, test loss: 0.7823161447310691\n",
      "Iteration 4211 training loss: 0.26218546249430763, test loss: 0.7823347540836009\n",
      "Iteration 4212 training loss: 0.2621513773574578, test loss: 0.7823533627031316\n",
      "Iteration 4213 training loss: 0.26211730096352487, test loss: 0.7823719705895554\n",
      "Iteration 4214 training loss: 0.26208323330901956, test loss: 0.782390577742766\n",
      "Iteration 4215 training loss: 0.2620491743904546, test loss: 0.7824091841626574\n",
      "Iteration 4216 training loss: 0.2620151242043447, test loss: 0.7824277898491234\n",
      "Iteration 4217 training loss: 0.26198108274720644, test loss: 0.7824463948020585\n",
      "Iteration 4218 training loss: 0.2619470500155585, test loss: 0.782464999021357\n",
      "Iteration 4219 training loss: 0.26191302600592137, test loss: 0.7824836025069134\n",
      "Iteration 4220 training loss: 0.2618790107148177, test loss: 0.7825022052586221\n",
      "Iteration 4221 training loss: 0.2618450041387717, test loss: 0.7825208072763776\n",
      "Iteration 4222 training loss: 0.2618110062743101, test loss: 0.7825394085600749\n",
      "Iteration 4223 training loss: 0.26177701711796114, test loss: 0.7825580091096086\n",
      "Iteration 4224 training loss: 0.26174303666625526, test loss: 0.7825766089248737\n",
      "Iteration 4225 training loss: 0.26170906491572465, test loss: 0.7825952080057657\n",
      "Iteration 4226 training loss: 0.26167510186290355, test loss: 0.782613806352179\n",
      "Iteration 4227 training loss: 0.2616411475043284, test loss: 0.7826324039640092\n",
      "Iteration 4228 training loss: 0.26160720183653696, test loss: 0.7826510008411519\n",
      "Iteration 4229 training loss: 0.26157326485606963, test loss: 0.7826695969835024\n",
      "Iteration 4230 training loss: 0.2615393365594683, test loss: 0.7826881923909563\n",
      "Iteration 4231 training loss: 0.26150541694327695, test loss: 0.7827067870634092\n",
      "Iteration 4232 training loss: 0.26147150600404157, test loss: 0.7827253810007568\n",
      "Iteration 4233 training loss: 0.26143760373831, test loss: 0.7827439742028955\n",
      "Iteration 4234 training loss: 0.2614037101426319, test loss: 0.7827625666697207\n",
      "Iteration 4235 training loss: 0.2613698252135592, test loss: 0.7827811584011288\n",
      "Iteration 4236 training loss: 0.2613359489476455, test loss: 0.7827997493970159\n",
      "Iteration 4237 training loss: 0.2613020813414464, test loss: 0.7828183396572783\n",
      "Iteration 4238 training loss: 0.2612682223915193, test loss: 0.7828369291818126\n",
      "Iteration 4239 training loss: 0.26123437209442385, test loss: 0.782855517970515\n",
      "Iteration 4240 training loss: 0.2612005304467214, test loss: 0.7828741060232822\n",
      "Iteration 4241 training loss: 0.2611666974449752, test loss: 0.7828926933400114\n",
      "Iteration 4242 training loss: 0.26113287308575067, test loss: 0.7829112799205991\n",
      "Iteration 4243 training loss: 0.26109905736561484, test loss: 0.7829298657649417\n",
      "Iteration 4244 training loss: 0.26106525028113686, test loss: 0.782948450872937\n",
      "Iteration 4245 training loss: 0.2610314518288877, test loss: 0.7829670352444817\n",
      "Iteration 4246 training loss: 0.2609976620054404, test loss: 0.7829856188794734\n",
      "Iteration 4247 training loss: 0.2609638808073697, test loss: 0.7830042017778089\n",
      "Iteration 4248 training loss: 0.2609301082312525, test loss: 0.7830227839393863\n",
      "Iteration 4249 training loss: 0.26089634427366754, test loss: 0.7830413653641026\n",
      "Iteration 4250 training loss: 0.2608625889311953, test loss: 0.7830599460518555\n",
      "Iteration 4251 training loss: 0.26082884220041824, test loss: 0.783078526002543\n",
      "Iteration 4252 training loss: 0.26079510407792106, test loss: 0.7830971052160629\n",
      "Iteration 4253 training loss: 0.26076137456028997, test loss: 0.783115683692313\n",
      "Iteration 4254 training loss: 0.2607276536441132, test loss: 0.7831342614311915\n",
      "Iteration 4255 training loss: 0.260693941325981, test loss: 0.7831528384325962\n",
      "Iteration 4256 training loss: 0.26066023760248536, test loss: 0.7831714146964258\n",
      "Iteration 4257 training loss: 0.2606265424702204, test loss: 0.7831899902225784\n",
      "Iteration 4258 training loss: 0.2605928559257818, test loss: 0.7832085650109525\n",
      "Iteration 4259 training loss: 0.26055917796576755, test loss: 0.7832271390614467\n",
      "Iteration 4260 training loss: 0.26052550858677725, test loss: 0.7832457123739596\n",
      "Iteration 4261 training loss: 0.2604918477854124, test loss: 0.7832642849483901\n",
      "Iteration 4262 training loss: 0.26045819555827665, test loss: 0.7832828567846365\n",
      "Iteration 4263 training loss: 0.2604245519019752, test loss: 0.7833014278825985\n",
      "Iteration 4264 training loss: 0.2603909168131156, test loss: 0.7833199982421746\n",
      "Iteration 4265 training loss: 0.26035729028830684, test loss: 0.7833385678632644\n",
      "Iteration 4266 training loss: 0.26032367232416, test loss: 0.7833571367457666\n",
      "Iteration 4267 training loss: 0.260290062917288, test loss: 0.7833757048895809\n",
      "Iteration 4268 training loss: 0.2602564620643058, test loss: 0.7833942722946067\n",
      "Iteration 4269 training loss: 0.2602228697618301, test loss: 0.7834128389607435\n",
      "Iteration 4270 training loss: 0.2601892860064794, test loss: 0.7834314048878911\n",
      "Iteration 4271 training loss: 0.26015571079487443, test loss: 0.783449970075949\n",
      "Iteration 4272 training loss: 0.2601221441236373, test loss: 0.7834685345248172\n",
      "Iteration 4273 training loss: 0.26008858598939255, test loss: 0.7834870982343954\n",
      "Iteration 4274 training loss: 0.26005503638876615, test loss: 0.7835056612045841\n",
      "Iteration 4275 training loss: 0.26002149531838625, test loss: 0.7835242234352829\n",
      "Iteration 4276 training loss: 0.25998796277488273, test loss: 0.7835427849263924\n",
      "Iteration 4277 training loss: 0.2599544387548873, test loss: 0.7835613456778127\n",
      "Iteration 4278 training loss: 0.2599209232550338, test loss: 0.7835799056894445\n",
      "Iteration 4279 training loss: 0.25988741627195755, test loss: 0.7835984649611878\n",
      "Iteration 4280 training loss: 0.25985391780229616, test loss: 0.7836170234929437\n",
      "Iteration 4281 training loss: 0.25982042784268883, test loss: 0.7836355812846131\n",
      "Iteration 4282 training loss: 0.25978694638977673, test loss: 0.7836541383360961\n",
      "Iteration 4283 training loss: 0.25975347344020283, test loss: 0.7836726946472942\n",
      "Iteration 4284 training loss: 0.25972000899061215, test loss: 0.7836912502181083\n",
      "Iteration 4285 training loss: 0.2596865530376513, test loss: 0.7837098050484391\n",
      "Iteration 4286 training loss: 0.25965310557796895, test loss: 0.7837283591381884\n",
      "Iteration 4287 training loss: 0.25961966660821556, test loss: 0.7837469124872569\n",
      "Iteration 4288 training loss: 0.2595862361250436, test loss: 0.7837654650955466\n",
      "Iteration 4289 training loss: 0.25955281412510717, test loss: 0.7837840169629584\n",
      "Iteration 4290 training loss: 0.25951940060506234, test loss: 0.7838025680893942\n",
      "Iteration 4291 training loss: 0.25948599556156704, test loss: 0.7838211184747558\n",
      "Iteration 4292 training loss: 0.259452598991281, test loss: 0.7838396681189445\n",
      "Iteration 4293 training loss: 0.25941921089086595, test loss: 0.7838582170218628\n",
      "Iteration 4294 training loss: 0.25938583125698533, test loss: 0.7838767651834121\n",
      "Iteration 4295 training loss: 0.25935246008630447, test loss: 0.7838953126034948\n",
      "Iteration 4296 training loss: 0.2593190973754906, test loss: 0.7839138592820131\n",
      "Iteration 4297 training loss: 0.2592857431212127, test loss: 0.7839324052188686\n",
      "Iteration 4298 training loss: 0.2592523973201417, test loss: 0.7839509504139646\n",
      "Iteration 4299 training loss: 0.25921905996895034, test loss: 0.7839694948672029\n",
      "Iteration 4300 training loss: 0.25918573106431314, test loss: 0.7839880385784862\n",
      "Iteration 4301 training loss: 0.25915241060290656, test loss: 0.7840065815477172\n",
      "Iteration 4302 training loss: 0.2591190985814089, test loss: 0.7840251237747982\n",
      "Iteration 4303 training loss: 0.2590857949965002, test loss: 0.7840436652596326\n",
      "Iteration 4304 training loss: 0.2590524998448624, test loss: 0.7840622060021231\n",
      "Iteration 4305 training loss: 0.2590192131231794, test loss: 0.7840807460021724\n",
      "Iteration 4306 training loss: 0.2589859348281367, test loss: 0.7840992852596841\n",
      "Iteration 4307 training loss: 0.2589526649564217, test loss: 0.7841178237745611\n",
      "Iteration 4308 training loss: 0.25891940350472387, test loss: 0.7841363615467066\n",
      "Iteration 4309 training loss: 0.25888615046973423, test loss: 0.7841548985760243\n",
      "Iteration 4310 training loss: 0.2588529058481458, test loss: 0.784173434862417\n",
      "Iteration 4311 training loss: 0.25881966963665326, test loss: 0.784191970405789\n",
      "Iteration 4312 training loss: 0.25878644183195326, test loss: 0.7842105052060439\n",
      "Iteration 4313 training loss: 0.2587532224307443, test loss: 0.7842290392630848\n",
      "Iteration 4314 training loss: 0.2587200114297266, test loss: 0.7842475725768161\n",
      "Iteration 4315 training loss: 0.25868680882560224, test loss: 0.7842661051471413\n",
      "Iteration 4316 training loss: 0.2586536146150752, test loss: 0.7842846369739652\n",
      "Iteration 4317 training loss: 0.2586204287948511, test loss: 0.7843031680571908\n",
      "Iteration 4318 training loss: 0.2585872513616376, test loss: 0.7843216983967233\n",
      "Iteration 4319 training loss: 0.25855408231214405, test loss: 0.7843402279924665\n",
      "Iteration 4320 training loss: 0.25852092164308166, test loss: 0.784358756844325\n",
      "Iteration 4321 training loss: 0.25848776935116335, test loss: 0.7843772849522029\n",
      "Iteration 4322 training loss: 0.25845462543310405, test loss: 0.7843958123160052\n",
      "Iteration 4323 training loss: 0.25842148988562047, test loss: 0.7844143389356363\n",
      "Iteration 4324 training loss: 0.25838836270543086, test loss: 0.784432864811001\n",
      "Iteration 4325 training loss: 0.25835524388925557, test loss: 0.7844513899420044\n",
      "Iteration 4326 training loss: 0.25832213343381677, test loss: 0.7844699143285512\n",
      "Iteration 4327 training loss: 0.25828903133583825, test loss: 0.784488437970546\n",
      "Iteration 4328 training loss: 0.2582559375920457, test loss: 0.7845069608678947\n",
      "Iteration 4329 training loss: 0.2582228521991666, test loss: 0.7845254830205023\n",
      "Iteration 4330 training loss: 0.2581897751539303, test loss: 0.7845440044282738\n",
      "Iteration 4331 training loss: 0.25815670645306793, test loss: 0.7845625250911147\n",
      "Iteration 4332 training loss: 0.25812364609331234, test loss: 0.7845810450089307\n",
      "Iteration 4333 training loss: 0.2580905940713983, test loss: 0.7845995641816268\n",
      "Iteration 4334 training loss: 0.25805755038406225, test loss: 0.7846180826091094\n",
      "Iteration 4335 training loss: 0.25802451502804247, test loss: 0.7846366002912838\n",
      "Iteration 4336 training loss: 0.25799148800007915, test loss: 0.7846551172280559\n",
      "Iteration 4337 training loss: 0.25795846929691413, test loss: 0.7846736334193317\n",
      "Iteration 4338 training loss: 0.2579254589152911, test loss: 0.7846921488650171\n",
      "Iteration 4339 training loss: 0.2578924568519556, test loss: 0.7847106635650184\n",
      "Iteration 4340 training loss: 0.2578594631036549, test loss: 0.7847291775192413\n",
      "Iteration 4341 training loss: 0.25782647766713795, test loss: 0.7847476907275929\n",
      "Iteration 4342 training loss: 0.2577935005391557, test loss: 0.7847662031899789\n",
      "Iteration 4343 training loss: 0.2577605317164608, test loss: 0.7847847149063059\n",
      "Iteration 4344 training loss: 0.25772757119580764, test loss: 0.7848032258764808\n",
      "Iteration 4345 training loss: 0.25769461897395246, test loss: 0.78482173610041\n",
      "Iteration 4346 training loss: 0.2576616750476532, test loss: 0.7848402455779996\n",
      "Iteration 4347 training loss: 0.2576287394136697, test loss: 0.7848587543091575\n",
      "Iteration 4348 training loss: 0.2575958120687635, test loss: 0.78487726229379\n",
      "Iteration 4349 training loss: 0.2575628930096979, test loss: 0.7848957695318043\n",
      "Iteration 4350 training loss: 0.2575299822332381, test loss: 0.7849142760231071\n",
      "Iteration 4351 training loss: 0.2574970797361508, test loss: 0.7849327817676061\n",
      "Iteration 4352 training loss: 0.25746418551520495, test loss: 0.7849512867652081\n",
      "Iteration 4353 training loss: 0.25743129956717087, test loss: 0.7849697910158207\n",
      "Iteration 4354 training loss: 0.2573984218888207, test loss: 0.7849882945193514\n",
      "Iteration 4355 training loss: 0.2573655524769285, test loss: 0.7850067972757074\n",
      "Iteration 4356 training loss: 0.25733269132826997, test loss: 0.7850252992847965\n",
      "Iteration 4357 training loss: 0.2572998384396227, test loss: 0.7850438005465262\n",
      "Iteration 4358 training loss: 0.257266993807766, test loss: 0.7850623010608047\n",
      "Iteration 4359 training loss: 0.25723415742948086, test loss: 0.7850808008275395\n",
      "Iteration 4360 training loss: 0.25720132930155015, test loss: 0.7850992998466387\n",
      "Iteration 4361 training loss: 0.2571685094207584, test loss: 0.7851177981180102\n",
      "Iteration 4362 training loss: 0.25713569778389217, test loss: 0.7851362956415624\n",
      "Iteration 4363 training loss: 0.2571028943877393, test loss: 0.7851547924172032\n",
      "Iteration 4364 training loss: 0.2570700992290898, test loss: 0.785173288444841\n",
      "Iteration 4365 training loss: 0.2570373123047353, test loss: 0.7851917837243843\n",
      "Iteration 4366 training loss: 0.25700453361146924, test loss: 0.7852102782557414\n",
      "Iteration 4367 training loss: 0.25697176314608666, test loss: 0.785228772038821\n",
      "Iteration 4368 training loss: 0.2569390009053845, test loss: 0.7852472650735316\n",
      "Iteration 4369 training loss: 0.2569062468861615, test loss: 0.7852657573597821\n",
      "Iteration 4370 training loss: 0.2568735010852179, test loss: 0.7852842488974812\n",
      "Iteration 4371 training loss: 0.256840763499356, test loss: 0.7853027396865377\n",
      "Iteration 4372 training loss: 0.2568080341253796, test loss: 0.7853212297268607\n",
      "Iteration 4373 training loss: 0.25677531296009454, test loss: 0.7853397190183595\n",
      "Iteration 4374 training loss: 0.25674260000030796, test loss: 0.7853582075609428\n",
      "Iteration 4375 training loss: 0.2567098952428293, test loss: 0.7853766953545201\n",
      "Iteration 4376 training loss: 0.25667719868446925, test loss: 0.7853951823990004\n",
      "Iteration 4377 training loss: 0.2566445103220404, test loss: 0.7854136686942941\n",
      "Iteration 4378 training loss: 0.2566118301523574, test loss: 0.7854321542403094\n",
      "Iteration 4379 training loss: 0.25657915817223614, test loss: 0.7854506390369564\n",
      "Iteration 4380 training loss: 0.25654649437849464, test loss: 0.7854691230841453\n",
      "Iteration 4381 training loss: 0.2565138387679523, test loss: 0.785487606381785\n",
      "Iteration 4382 training loss: 0.25648119133743064, test loss: 0.785506088929786\n",
      "Iteration 4383 training loss: 0.25644855208375256, test loss: 0.7855245707280579\n",
      "Iteration 4384 training loss: 0.25641592100374305, test loss: 0.7855430517765104\n",
      "Iteration 4385 training loss: 0.2563832980942286, test loss: 0.785561532075054\n",
      "Iteration 4386 training loss: 0.25635068335203737, test loss: 0.7855800116235989\n",
      "Iteration 4387 training loss: 0.2563180767739995, test loss: 0.7855984904220551\n",
      "Iteration 4388 training loss: 0.2562854783569466, test loss: 0.7856169684703331\n",
      "Iteration 4389 training loss: 0.2562528880977123, test loss: 0.7856354457683433\n",
      "Iteration 4390 training loss: 0.25622030599313156, test loss: 0.7856539223159964\n",
      "Iteration 4391 training loss: 0.25618773204004147, test loss: 0.7856723981132023\n",
      "Iteration 4392 training loss: 0.2561551662352807, test loss: 0.7856908731598726\n",
      "Iteration 4393 training loss: 0.25612260857568936, test loss: 0.7857093474559173\n",
      "Iteration 4394 training loss: 0.25609005905810983, test loss: 0.7857278210012475\n",
      "Iteration 4395 training loss: 0.25605751767938567, test loss: 0.7857462937957743\n",
      "Iteration 4396 training loss: 0.2560249844363626, test loss: 0.7857647658394086\n",
      "Iteration 4397 training loss: 0.2559924593258877, test loss: 0.7857832371320613\n",
      "Iteration 4398 training loss: 0.2559599423448101, test loss: 0.7858017076736439\n",
      "Iteration 4399 training loss: 0.2559274334899803, test loss: 0.7858201774640671\n",
      "Iteration 4400 training loss: 0.2558949327582508, test loss: 0.7858386465032429\n",
      "Iteration 4401 training loss: 0.25586244014647563, test loss: 0.7858571147910822\n",
      "Iteration 4402 training loss: 0.2558299556515107, test loss: 0.7858755823274968\n",
      "Iteration 4403 training loss: 0.2557974792702134, test loss: 0.785894049112398\n",
      "Iteration 4404 training loss: 0.255765010999443, test loss: 0.7859125151456979\n",
      "Iteration 4405 training loss: 0.25573255083606056, test loss: 0.7859309804273078\n",
      "Iteration 4406 training loss: 0.2557000987769285, test loss: 0.7859494449571397\n",
      "Iteration 4407 training loss: 0.25566765481891146, test loss: 0.7859679087351056\n",
      "Iteration 4408 training loss: 0.2556352189588753, test loss: 0.7859863717611174\n",
      "Iteration 4409 training loss: 0.25560279119368784, test loss: 0.7860048340350871\n",
      "Iteration 4410 training loss: 0.25557037152021855, test loss: 0.7860232955569272\n",
      "Iteration 4411 training loss: 0.25553795993533845, test loss: 0.7860417563265492\n",
      "Iteration 4412 training loss: 0.25550555643592077, test loss: 0.786060216343866\n",
      "Iteration 4413 training loss: 0.25547316101883966, test loss: 0.7860786756087901\n",
      "Iteration 4414 training loss: 0.2554407736809716, test loss: 0.7860971341212335\n",
      "Iteration 4415 training loss: 0.2554083944191945, test loss: 0.7861155918811091\n",
      "Iteration 4416 training loss: 0.255376023230388, test loss: 0.7861340488883295\n",
      "Iteration 4417 training loss: 0.2553436601114335, test loss: 0.7861525051428072\n",
      "Iteration 4418 training loss: 0.2553113050592139, test loss: 0.7861709606444554\n",
      "Iteration 4419 training loss: 0.2552789580706142, test loss: 0.7861894153931863\n",
      "Iteration 4420 training loss: 0.25524661914252045, test loss: 0.7862078693889137\n",
      "Iteration 4421 training loss: 0.25521428827182097, test loss: 0.78622632263155\n",
      "Iteration 4422 training loss: 0.2551819654554055, test loss: 0.7862447751210087\n",
      "Iteration 4423 training loss: 0.25514965069016554, test loss: 0.7862632268572028\n",
      "Iteration 4424 training loss: 0.2551173439729943, test loss: 0.7862816778400458\n",
      "Iteration 4425 training loss: 0.25508504530078646, test loss: 0.7863001280694507\n",
      "Iteration 4426 training loss: 0.25505275467043875, test loss: 0.7863185775453314\n",
      "Iteration 4427 training loss: 0.25502047207884926, test loss: 0.7863370262676012\n",
      "Iteration 4428 training loss: 0.25498819752291796, test loss: 0.7863554742361732\n",
      "Iteration 4429 training loss: 0.2549559309995463, test loss: 0.7863739214509621\n",
      "Iteration 4430 training loss: 0.2549236725056375, test loss: 0.7863923679118808\n",
      "Iteration 4431 training loss: 0.25489142203809667, test loss: 0.7864108136188437\n",
      "Iteration 4432 training loss: 0.25485917959383037, test loss: 0.7864292585717642\n",
      "Iteration 4433 training loss: 0.2548269451697467, test loss: 0.7864477027705568\n",
      "Iteration 4434 training loss: 0.2547947187627558, test loss: 0.7864661462151352\n",
      "Iteration 4435 training loss: 0.2547625003697691, test loss: 0.786484588905414\n",
      "Iteration 4436 training loss: 0.25473028998770014, test loss: 0.7865030308413069\n",
      "Iteration 4437 training loss: 0.2546980876134635, test loss: 0.7865214720227286\n",
      "Iteration 4438 training loss: 0.2546658932439762, test loss: 0.7865399124495932\n",
      "Iteration 4439 training loss: 0.2546337068761562, test loss: 0.7865583521218156\n",
      "Iteration 4440 training loss: 0.25460152850692375, test loss: 0.7865767910393099\n",
      "Iteration 4441 training loss: 0.25456935813320025, test loss: 0.786595229201991\n",
      "Iteration 4442 training loss: 0.25453719575190903, test loss: 0.7866136666097736\n",
      "Iteration 4443 training loss: 0.2545050413599751, test loss: 0.7866321032625724\n",
      "Iteration 4444 training loss: 0.25447289495432496, test loss: 0.7866505391603024\n",
      "Iteration 4445 training loss: 0.254440756531887, test loss: 0.7866689743028782\n",
      "Iteration 4446 training loss: 0.25440862608959103, test loss: 0.7866874086902151\n",
      "Iteration 4447 training loss: 0.2543765036243687, test loss: 0.7867058423222283\n",
      "Iteration 4448 training loss: 0.2543443891331532, test loss: 0.7867242751988327\n",
      "Iteration 4449 training loss: 0.2543122826128794, test loss: 0.7867427073199439\n",
      "Iteration 4450 training loss: 0.2542801840604839, test loss: 0.7867611386854768\n",
      "Iteration 4451 training loss: 0.25424809347290495, test loss: 0.7867795692953473\n",
      "Iteration 4452 training loss: 0.25421601084708223, test loss: 0.7867979991494701\n",
      "Iteration 4453 training loss: 0.2541839361799574, test loss: 0.7868164282477622\n",
      "Iteration 4454 training loss: 0.25415186946847346, test loss: 0.7868348565901375\n",
      "Iteration 4455 training loss: 0.2541198107095753, test loss: 0.7868532841765129\n",
      "Iteration 4456 training loss: 0.2540877599002093, test loss: 0.7868717110068039\n",
      "Iteration 4457 training loss: 0.25405571703732366, test loss: 0.7868901370809263\n",
      "Iteration 4458 training loss: 0.254023682117868, test loss: 0.786908562398796\n",
      "Iteration 4459 training loss: 0.2539916551387936, test loss: 0.7869269869603295\n",
      "Iteration 4460 training loss: 0.25395963609705374, test loss: 0.7869454107654422\n",
      "Iteration 4461 training loss: 0.2539276249896029, test loss: 0.7869638338140508\n",
      "Iteration 4462 training loss: 0.2538956218133973, test loss: 0.7869822561060713\n",
      "Iteration 4463 training loss: 0.2538636265653951, test loss: 0.78700067764142\n",
      "Iteration 4464 training loss: 0.25383163924255553, test loss: 0.7870190984200136\n",
      "Iteration 4465 training loss: 0.25379965984184016, test loss: 0.7870375184417682\n",
      "Iteration 4466 training loss: 0.2537676883602117, test loss: 0.787055937706601\n",
      "Iteration 4467 training loss: 0.25373572479463447, test loss: 0.7870743562144278\n",
      "Iteration 4468 training loss: 0.25370376914207465, test loss: 0.7870927739651663\n",
      "Iteration 4469 training loss: 0.2536718213995, test loss: 0.7871111909587322\n",
      "Iteration 4470 training loss: 0.25363988156388007, test loss: 0.7871296071950432\n",
      "Iteration 4471 training loss: 0.2536079496321856, test loss: 0.7871480226740158\n",
      "Iteration 4472 training loss: 0.2535760256013892, test loss: 0.7871664373955676\n",
      "Iteration 4473 training loss: 0.25354410946846523, test loss: 0.787184851359615\n",
      "Iteration 4474 training loss: 0.25351220123038953, test loss: 0.7872032645660756\n",
      "Iteration 4475 training loss: 0.2534803008841397, test loss: 0.7872216770148666\n",
      "Iteration 4476 training loss: 0.2534484084266947, test loss: 0.787240088705905\n",
      "Iteration 4477 training loss: 0.2534165238550354, test loss: 0.7872584996391088\n",
      "Iteration 4478 training loss: 0.253384647166144, test loss: 0.7872769098143949\n",
      "Iteration 4479 training loss: 0.2533527783570046, test loss: 0.787295319231681\n",
      "Iteration 4480 training loss: 0.25332091742460283, test loss: 0.7873137278908853\n",
      "Iteration 4481 training loss: 0.2532890643659258, test loss: 0.7873321357919246\n",
      "Iteration 4482 training loss: 0.25325721917796234, test loss: 0.7873505429347173\n",
      "Iteration 4483 training loss: 0.253225381857703, test loss: 0.787368949319181\n",
      "Iteration 4484 training loss: 0.25319355240213987, test loss: 0.7873873549452338\n",
      "Iteration 4485 training loss: 0.2531617308082664, test loss: 0.7874057598127936\n",
      "Iteration 4486 training loss: 0.2531299170730781, test loss: 0.7874241639217784\n",
      "Iteration 4487 training loss: 0.2530981111935718, test loss: 0.7874425672721065\n",
      "Iteration 4488 training loss: 0.25306631316674594, test loss: 0.7874609698636961\n",
      "Iteration 4489 training loss: 0.2530345229896007, test loss: 0.7874793716964653\n",
      "Iteration 4490 training loss: 0.25300274065913775, test loss: 0.7874977727703328\n",
      "Iteration 4491 training loss: 0.2529709661723605, test loss: 0.7875161730852168\n",
      "Iteration 4492 training loss: 0.25293919952627375, test loss: 0.7875345726410361\n",
      "Iteration 4493 training loss: 0.2529074407178842, test loss: 0.7875529714377091\n",
      "Iteration 4494 training loss: 0.2528756897441998, test loss: 0.7875713694751545\n",
      "Iteration 4495 training loss: 0.25284394660223036, test loss: 0.7875897667532912\n",
      "Iteration 4496 training loss: 0.2528122112889873, test loss: 0.7876081632720376\n",
      "Iteration 4497 training loss: 0.25278048380148344, test loss: 0.7876265590313131\n",
      "Iteration 4498 training loss: 0.2527487641367333, test loss: 0.7876449540310363\n",
      "Iteration 4499 training loss: 0.2527170522917531, test loss: 0.7876633482711266\n",
      "Iteration 4500 training loss: 0.25268534826356054, test loss: 0.7876817417515026\n",
      "Iteration 4501 training loss: 0.25265365204917495, test loss: 0.7877001344720842\n",
      "Iteration 4502 training loss: 0.2526219636456171, test loss: 0.7877185264327901\n",
      "Iteration 4503 training loss: 0.2525902830499097, test loss: 0.7877369176335399\n",
      "Iteration 4504 training loss: 0.25255861025907667, test loss: 0.7877553080742528\n",
      "Iteration 4505 training loss: 0.25252694527014385, test loss: 0.7877736977548484\n",
      "Iteration 4506 training loss: 0.2524952880801384, test loss: 0.7877920866752466\n",
      "Iteration 4507 training loss: 0.25246363868608906, test loss: 0.7878104748353664\n",
      "Iteration 4508 training loss: 0.2524319970850266, test loss: 0.787828862235128\n",
      "Iteration 4509 training loss: 0.25240036327398285, test loss: 0.787847248874451\n",
      "Iteration 4510 training loss: 0.25236873724999137, test loss: 0.7878656347532553\n",
      "Iteration 4511 training loss: 0.2523371190100875, test loss: 0.7878840198714607\n",
      "Iteration 4512 training loss: 0.25230550855130784, test loss: 0.7879024042289873\n",
      "Iteration 4513 training loss: 0.252273905870691, test loss: 0.7879207878257553\n",
      "Iteration 4514 training loss: 0.2522423109652767, test loss: 0.7879391706616845\n",
      "Iteration 4515 training loss: 0.2522107238321066, test loss: 0.7879575527366953\n",
      "Iteration 4516 training loss: 0.25217914446822365, test loss: 0.7879759340507083\n",
      "Iteration 4517 training loss: 0.2521475728706727, test loss: 0.7879943146036434\n",
      "Iteration 4518 training loss: 0.2521160090364998, test loss: 0.7880126943954213\n",
      "Iteration 4519 training loss: 0.25208445296275284, test loss: 0.7880310734259623\n",
      "Iteration 4520 training loss: 0.2520529046464813, test loss: 0.7880494516951873\n",
      "Iteration 4521 training loss: 0.252021364084736, test loss: 0.7880678292030167\n",
      "Iteration 4522 training loss: 0.2519898312745697, test loss: 0.7880862059493712\n",
      "Iteration 4523 training loss: 0.25195830621303633, test loss: 0.7881045819341718\n",
      "Iteration 4524 training loss: 0.25192678889719144, test loss: 0.7881229571573392\n",
      "Iteration 4525 training loss: 0.2518952793240926, test loss: 0.7881413316187942\n",
      "Iteration 4526 training loss: 0.2518637774907983, test loss: 0.7881597053184581\n",
      "Iteration 4527 training loss: 0.2518322833943692, test loss: 0.7881780782562521\n",
      "Iteration 4528 training loss: 0.2518007970318669, test loss: 0.7881964504320972\n",
      "Iteration 4529 training loss: 0.25176931840035527, test loss: 0.7882148218459144\n",
      "Iteration 4530 training loss: 0.25173784749689904, test loss: 0.788233192497625\n",
      "Iteration 4531 training loss: 0.25170638431856507, test loss: 0.7882515623871508\n",
      "Iteration 4532 training loss: 0.2516749288624213, test loss: 0.7882699315144129\n",
      "Iteration 4533 training loss: 0.25164348112553764, test loss: 0.7882882998793329\n",
      "Iteration 4534 training loss: 0.2516120411049855, test loss: 0.7883066674818323\n",
      "Iteration 4535 training loss: 0.2515806087978374, test loss: 0.7883250343218332\n",
      "Iteration 4536 training loss: 0.251549184201168, test loss: 0.7883434003992567\n",
      "Iteration 4537 training loss: 0.25151776731205316, test loss: 0.7883617657140247\n",
      "Iteration 4538 training loss: 0.2514863581275705, test loss: 0.7883801302660595\n",
      "Iteration 4539 training loss: 0.2514549566447989, test loss: 0.7883984940552827\n",
      "Iteration 4540 training loss: 0.2514235628608191, test loss: 0.7884168570816165\n",
      "Iteration 4541 training loss: 0.25139217677271336, test loss: 0.7884352193449828\n",
      "Iteration 4542 training loss: 0.2513607983775652, test loss: 0.7884535808453039\n",
      "Iteration 4543 training loss: 0.2513294276724599, test loss: 0.7884719415825021\n",
      "Iteration 4544 training loss: 0.2512980646544844, test loss: 0.7884903015564994\n",
      "Iteration 4545 training loss: 0.251266709320727, test loss: 0.7885086607672183\n",
      "Iteration 4546 training loss: 0.2512353616682775, test loss: 0.7885270192145811\n",
      "Iteration 4547 training loss: 0.2512040216942274, test loss: 0.7885453768985108\n",
      "Iteration 4548 training loss: 0.2511726893956698, test loss: 0.7885637338189295\n",
      "Iteration 4549 training loss: 0.25114136476969906, test loss: 0.7885820899757601\n",
      "Iteration 4550 training loss: 0.2511100478134113, test loss: 0.788600445368925\n",
      "Iteration 4551 training loss: 0.2510787385239041, test loss: 0.7886187999983475\n",
      "Iteration 4552 training loss: 0.2510474368982766, test loss: 0.7886371538639501\n",
      "Iteration 4553 training loss: 0.25101614293362956, test loss: 0.7886555069656556\n",
      "Iteration 4554 training loss: 0.2509848566270651, test loss: 0.7886738593033873\n",
      "Iteration 4555 training loss: 0.25095357797568696, test loss: 0.7886922108770682\n",
      "Iteration 4556 training loss: 0.25092230697660045, test loss: 0.7887105616866215\n",
      "Iteration 4557 training loss: 0.2508910436269124, test loss: 0.7887289117319702\n",
      "Iteration 4558 training loss: 0.25085978792373115, test loss: 0.7887472610130377\n",
      "Iteration 4559 training loss: 0.2508285398641665, test loss: 0.7887656095297475\n",
      "Iteration 4560 training loss: 0.2507972994453299, test loss: 0.7887839572820227\n",
      "Iteration 4561 training loss: 0.2507660666643343, test loss: 0.7888023042697869\n",
      "Iteration 4562 training loss: 0.2507348415182941, test loss: 0.7888206504929639\n",
      "Iteration 4563 training loss: 0.2507036240043254, test loss: 0.7888389959514772\n",
      "Iteration 4564 training loss: 0.2506724141195455, test loss: 0.7888573406452501\n",
      "Iteration 4565 training loss: 0.25064121186107363, test loss: 0.788875684574207\n",
      "Iteration 4566 training loss: 0.25061001722603016, test loss: 0.7888940277382713\n",
      "Iteration 4567 training loss: 0.2505788302115373, test loss: 0.7889123701373673\n",
      "Iteration 4568 training loss: 0.2505476508147186, test loss: 0.7889307117714185\n",
      "Iteration 4569 training loss: 0.25051647903269925, test loss: 0.7889490526403493\n",
      "Iteration 4570 training loss: 0.2504853148626058, test loss: 0.7889673927440836\n",
      "Iteration 4571 training loss: 0.2504541583015662, test loss: 0.7889857320825455\n",
      "Iteration 4572 training loss: 0.2504230093467106, test loss: 0.7890040706556598\n",
      "Iteration 4573 training loss: 0.25039186799516966, test loss: 0.78902240846335\n",
      "Iteration 4574 training loss: 0.2503607342440764, test loss: 0.7890407455055412\n",
      "Iteration 4575 training loss: 0.2503296080905649, test loss: 0.7890590817821574\n",
      "Iteration 4576 training loss: 0.250298489531771, test loss: 0.7890774172931234\n",
      "Iteration 4577 training loss: 0.2502673785648318, test loss: 0.7890957520383637\n",
      "Iteration 4578 training loss: 0.25023627518688607, test loss: 0.7891140860178031\n",
      "Iteration 4579 training loss: 0.25020517939507403, test loss: 0.7891324192313661\n",
      "Iteration 4580 training loss: 0.2501740911865375, test loss: 0.7891507516789776\n",
      "Iteration 4581 training loss: 0.25014301055841975, test loss: 0.7891690833605625\n",
      "Iteration 4582 training loss: 0.2501119375078654, test loss: 0.7891874142760456\n",
      "Iteration 4583 training loss: 0.2500808720320209, test loss: 0.7892057444253519\n",
      "Iteration 4584 training loss: 0.2500498141280339, test loss: 0.7892240738084068\n",
      "Iteration 4585 training loss: 0.2500187637930538, test loss: 0.7892424024251353\n",
      "Iteration 4586 training loss: 0.24998772102423128, test loss: 0.7892607302754623\n",
      "Iteration 4587 training loss: 0.2499566858187187, test loss: 0.7892790573593134\n",
      "Iteration 4588 training loss: 0.2499256581736697, test loss: 0.7892973836766141\n",
      "Iteration 4589 training loss: 0.24989463808623957, test loss: 0.7893157092272892\n",
      "Iteration 4590 training loss: 0.24986362555358535, test loss: 0.7893340340112646\n",
      "Iteration 4591 training loss: 0.24983262057286498, test loss: 0.789352358028466\n",
      "Iteration 4592 training loss: 0.24980162314123835, test loss: 0.7893706812788189\n",
      "Iteration 4593 training loss: 0.24977063325586676, test loss: 0.7893890037622487\n",
      "Iteration 4594 training loss: 0.2497396509139129, test loss: 0.7894073254786814\n",
      "Iteration 4595 training loss: 0.24970867611254102, test loss: 0.7894256464280429\n",
      "Iteration 4596 training loss: 0.24967770884891688, test loss: 0.7894439666102588\n",
      "Iteration 4597 training loss: 0.2496467491202077, test loss: 0.7894622860252556\n",
      "Iteration 4598 training loss: 0.24961579692358216, test loss: 0.7894806046729587\n",
      "Iteration 4599 training loss: 0.24958485225621052, test loss: 0.7894989225532945\n",
      "Iteration 4600 training loss: 0.2495539151152644, test loss: 0.7895172396661889\n",
      "Iteration 4601 training loss: 0.249522985497917, test loss: 0.7895355560115687\n",
      "Iteration 4602 training loss: 0.24949206340134295, test loss: 0.7895538715893595\n",
      "Iteration 4603 training loss: 0.24946114882271847, test loss: 0.7895721863994882\n",
      "Iteration 4604 training loss: 0.24943024175922107, test loss: 0.789590500441881\n",
      "Iteration 4605 training loss: 0.24939934220802995, test loss: 0.7896088137164643\n",
      "Iteration 4606 training loss: 0.24936845016632564, test loss: 0.789627126223165\n",
      "Iteration 4607 training loss: 0.2493375656312902, test loss: 0.7896454379619094\n",
      "Iteration 4608 training loss: 0.2493066886001072, test loss: 0.7896637489326243\n",
      "Iteration 4609 training loss: 0.24927581906996166, test loss: 0.7896820591352364\n",
      "Iteration 4610 training loss: 0.24924495703804012, test loss: 0.7897003685696727\n",
      "Iteration 4611 training loss: 0.24921410250153034, test loss: 0.7897186772358599\n",
      "Iteration 4612 training loss: 0.24918325545762207, test loss: 0.7897369851337248\n",
      "Iteration 4613 training loss: 0.24915241590350606, test loss: 0.7897552922631953\n",
      "Iteration 4614 training loss: 0.24912158383637464, test loss: 0.7897735986241975\n",
      "Iteration 4615 training loss: 0.2490907592534219, test loss: 0.789791904216659\n",
      "Iteration 4616 training loss: 0.24905994215184293, test loss: 0.789810209040507\n",
      "Iteration 4617 training loss: 0.24902913252883457, test loss: 0.7898285130956688\n",
      "Iteration 4618 training loss: 0.24899833038159527, test loss: 0.7898468163820715\n",
      "Iteration 4619 training loss: 0.24896753570732458, test loss: 0.7898651188996428\n",
      "Iteration 4620 training loss: 0.24893674850322375, test loss: 0.7898834206483105\n",
      "Iteration 4621 training loss: 0.2489059687664955, test loss: 0.7899017216280015\n",
      "Iteration 4622 training loss: 0.24887519649434386, test loss: 0.789920021838644\n",
      "Iteration 4623 training loss: 0.2488444316839746, test loss: 0.7899383212801654\n",
      "Iteration 4624 training loss: 0.2488136743325947, test loss: 0.7899566199524934\n",
      "Iteration 4625 training loss: 0.24878292443741248, test loss: 0.7899749178555561\n",
      "Iteration 4626 training loss: 0.24875218199563817, test loss: 0.789993214989281\n",
      "Iteration 4627 training loss: 0.24872144700448306, test loss: 0.7900115113535965\n",
      "Iteration 4628 training loss: 0.2486907194611602, test loss: 0.7900298069484301\n",
      "Iteration 4629 training loss: 0.24865999936288377, test loss: 0.7900481017737104\n",
      "Iteration 4630 training loss: 0.24862928670686965, test loss: 0.7900663958293652\n",
      "Iteration 4631 training loss: 0.2485985814903352, test loss: 0.7900846891153233\n",
      "Iteration 4632 training loss: 0.24856788371049895, test loss: 0.790102981631512\n",
      "Iteration 4633 training loss: 0.24853719336458116, test loss: 0.7901212733778605\n",
      "Iteration 4634 training loss: 0.2485065104498035, test loss: 0.7901395643542969\n",
      "Iteration 4635 training loss: 0.24847583496338893, test loss: 0.7901578545607498\n",
      "Iteration 4636 training loss: 0.24844516690256213, test loss: 0.7901761439971479\n",
      "Iteration 4637 training loss: 0.24841450626454892, test loss: 0.7901944326634189\n",
      "Iteration 4638 training loss: 0.24838385304657684, test loss: 0.7902127205594927\n",
      "Iteration 4639 training loss: 0.2483532072458746, test loss: 0.7902310076852972\n",
      "Iteration 4640 training loss: 0.24832256885967263, test loss: 0.7902492940407618\n",
      "Iteration 4641 training loss: 0.2482919378852027, test loss: 0.7902675796258148\n",
      "Iteration 4642 training loss: 0.24826131431969792, test loss: 0.7902858644403857\n",
      "Iteration 4643 training loss: 0.24823069816039298, test loss: 0.7903041484844032\n",
      "Iteration 4644 training loss: 0.24820008940452395, test loss: 0.7903224317577964\n",
      "Iteration 4645 training loss: 0.2481694880493284, test loss: 0.7903407142604943\n",
      "Iteration 4646 training loss: 0.24813889409204531, test loss: 0.7903589959924265\n",
      "Iteration 4647 training loss: 0.24810830752991497, test loss: 0.7903772769535219\n",
      "Iteration 4648 training loss: 0.2480777283601794, test loss: 0.79039555714371\n",
      "Iteration 4649 training loss: 0.24804715658008167, test loss: 0.7904138365629203\n",
      "Iteration 4650 training loss: 0.2480165921868666, test loss: 0.7904321152110818\n",
      "Iteration 4651 training loss: 0.24798603517778037, test loss: 0.7904503930881246\n",
      "Iteration 4652 training loss: 0.24795548555007058, test loss: 0.7904686701939779\n",
      "Iteration 4653 training loss: 0.24792494330098613, test loss: 0.7904869465285715\n",
      "Iteration 4654 training loss: 0.24789440842777755, test loss: 0.790505222091835\n",
      "Iteration 4655 training loss: 0.24786388092769676, test loss: 0.7905234968836987\n",
      "Iteration 4656 training loss: 0.2478333607979969, test loss: 0.7905417709040916\n",
      "Iteration 4657 training loss: 0.2478028480359329, test loss: 0.7905600441529445\n",
      "Iteration 4658 training loss: 0.2477723426387608, test loss: 0.7905783166301867\n",
      "Iteration 4659 training loss: 0.24774184460373835, test loss: 0.7905965883357483\n",
      "Iteration 4660 training loss: 0.24771135392812435, test loss: 0.79061485926956\n",
      "Iteration 4661 training loss: 0.24768087060917943, test loss: 0.7906331294315515\n",
      "Iteration 4662 training loss: 0.24765039464416538, test loss: 0.7906513988216528\n",
      "Iteration 4663 training loss: 0.2476199260303455, test loss: 0.7906696674397946\n",
      "Iteration 4664 training loss: 0.2475894647649845, test loss: 0.7906879352859075\n",
      "Iteration 4665 training loss: 0.24755901084534848, test loss: 0.7907062023599214\n",
      "Iteration 4666 training loss: 0.24752856426870506, test loss: 0.7907244686617669\n",
      "Iteration 4667 training loss: 0.24749812503232313, test loss: 0.7907427341913746\n",
      "Iteration 4668 training loss: 0.24746769313347328, test loss: 0.7907609989486755\n",
      "Iteration 4669 training loss: 0.24743726856942705, test loss: 0.7907792629335998\n",
      "Iteration 4670 training loss: 0.2474068513374578, test loss: 0.7907975261460783\n",
      "Iteration 4671 training loss: 0.24737644143484017, test loss: 0.7908157885860421\n",
      "Iteration 4672 training loss: 0.24734603885885031, test loss: 0.7908340502534216\n",
      "Iteration 4673 training loss: 0.24731564360676545, test loss: 0.7908523111481481\n",
      "Iteration 4674 training loss: 0.24728525567586473, test loss: 0.7908705712701529\n",
      "Iteration 4675 training loss: 0.24725487506342822, test loss: 0.7908888306193665\n",
      "Iteration 4676 training loss: 0.24722450176673771, test loss: 0.7909070891957204\n",
      "Iteration 4677 training loss: 0.24719413578307636, test loss: 0.7909253469991454\n",
      "Iteration 4678 training loss: 0.24716377710972867, test loss: 0.7909436040295732\n",
      "Iteration 4679 training loss: 0.24713342574398053, test loss: 0.7909618602869348\n",
      "Iteration 4680 training loss: 0.2471030816831193, test loss: 0.7909801157711618\n",
      "Iteration 4681 training loss: 0.24707274492443382, test loss: 0.7909983704821855\n",
      "Iteration 4682 training loss: 0.247042415465214, test loss: 0.7910166244199376\n",
      "Iteration 4683 training loss: 0.2470120933027515, test loss: 0.7910348775843492\n",
      "Iteration 4684 training loss: 0.24698177843433936, test loss: 0.7910531299753527\n",
      "Iteration 4685 training loss: 0.24695147085727182, test loss: 0.7910713815928793\n",
      "Iteration 4686 training loss: 0.24692117056884466, test loss: 0.7910896324368609\n",
      "Iteration 4687 training loss: 0.24689087756635505, test loss: 0.7911078825072292\n",
      "Iteration 4688 training loss: 0.24686059184710152, test loss: 0.7911261318039162\n",
      "Iteration 4689 training loss: 0.24683031340838402, test loss: 0.7911443803268539\n",
      "Iteration 4690 training loss: 0.246800042247504, test loss: 0.7911626280759743\n",
      "Iteration 4691 training loss: 0.246769778361764, test loss: 0.7911808750512092\n",
      "Iteration 4692 training loss: 0.24673952174846833, test loss: 0.7911991212524914\n",
      "Iteration 4693 training loss: 0.2467092724049224, test loss: 0.7912173666797524\n",
      "Iteration 4694 training loss: 0.2466790303284333, test loss: 0.7912356113329249\n",
      "Iteration 4695 training loss: 0.24664879551630917, test loss: 0.791253855211941\n",
      "Iteration 4696 training loss: 0.24661856796585974, test loss: 0.7912720983167333\n",
      "Iteration 4697 training loss: 0.24658834767439622, test loss: 0.791290340647234\n",
      "Iteration 4698 training loss: 0.24655813463923099, test loss: 0.7913085822033762\n",
      "Iteration 4699 training loss: 0.246527928857678, test loss: 0.7913268229850918\n",
      "Iteration 4700 training loss: 0.24649773032705247, test loss: 0.7913450629923139\n",
      "Iteration 4701 training loss: 0.24646753904467103, test loss: 0.7913633022249746\n",
      "Iteration 4702 training loss: 0.24643735500785174, test loss: 0.7913815406830077\n",
      "Iteration 4703 training loss: 0.24640717821391395, test loss: 0.7913997783663451\n",
      "Iteration 4704 training loss: 0.24637700866017853, test loss: 0.7914180152749201\n",
      "Iteration 4705 training loss: 0.24634684634396775, test loss: 0.7914362514086656\n",
      "Iteration 4706 training loss: 0.24631669126260505, test loss: 0.7914544867675145\n",
      "Iteration 4707 training loss: 0.24628654341341544, test loss: 0.7914727213514002\n",
      "Iteration 4708 training loss: 0.24625640279372518, test loss: 0.7914909551602556\n",
      "Iteration 4709 training loss: 0.24622626940086204, test loss: 0.791509188194014\n",
      "Iteration 4710 training loss: 0.24619614323215497, test loss: 0.7915274204526087\n",
      "Iteration 4711 training loss: 0.24616602428493464, test loss: 0.791545651935973\n",
      "Iteration 4712 training loss: 0.24613591255653278, test loss: 0.79156388264404\n",
      "Iteration 4713 training loss: 0.24610580804428264, test loss: 0.7915821125767438\n",
      "Iteration 4714 training loss: 0.24607571074551862, test loss: 0.7916003417340173\n",
      "Iteration 4715 training loss: 0.2460456206575769, test loss: 0.7916185701157947\n",
      "Iteration 4716 training loss: 0.2460155377777948, test loss: 0.7916367977220089\n",
      "Iteration 4717 training loss: 0.24598546210351088, test loss: 0.7916550245525942\n",
      "Iteration 4718 training loss: 0.24595539363206534, test loss: 0.7916732506074841\n",
      "Iteration 4719 training loss: 0.2459253323607995, test loss: 0.7916914758866125\n",
      "Iteration 4720 training loss: 0.24589527828705623, test loss: 0.7917097003899132\n",
      "Iteration 4721 training loss: 0.24586523140817984, test loss: 0.7917279241173203\n",
      "Iteration 4722 training loss: 0.2458351917215157, test loss: 0.7917461470687678\n",
      "Iteration 4723 training loss: 0.24580515922441068, test loss: 0.7917643692441899\n",
      "Iteration 4724 training loss: 0.24577513391421327, test loss: 0.7917825906435205\n",
      "Iteration 4725 training loss: 0.24574511578827282, test loss: 0.7918008112666938\n",
      "Iteration 4726 training loss: 0.2457151048439406, test loss: 0.7918190311136443\n",
      "Iteration 4727 training loss: 0.2456851010785689, test loss: 0.7918372501843063\n",
      "Iteration 4728 training loss: 0.24565510448951136, test loss: 0.7918554684786137\n",
      "Iteration 4729 training loss: 0.24562511507412313, test loss: 0.7918736859965013\n",
      "Iteration 4730 training loss: 0.2455951328297607, test loss: 0.7918919027379039\n",
      "Iteration 4731 training loss: 0.24556515775378168, test loss: 0.7919101187027556\n",
      "Iteration 4732 training loss: 0.24553518984354544, test loss: 0.7919283338909916\n",
      "Iteration 4733 training loss: 0.2455052290964124, test loss: 0.7919465483025458\n",
      "Iteration 4734 training loss: 0.24547527550974446, test loss: 0.7919647619373538\n",
      "Iteration 4735 training loss: 0.24544532908090474, test loss: 0.7919829747953496\n",
      "Iteration 4736 training loss: 0.24541538980725802, test loss: 0.7920011868764687\n",
      "Iteration 4737 training loss: 0.24538545768617012, test loss: 0.7920193981806458\n",
      "Iteration 4738 training loss: 0.24535553271500826, test loss: 0.7920376087078159\n",
      "Iteration 4739 training loss: 0.24532561489114116, test loss: 0.7920558184579142\n",
      "Iteration 4740 training loss: 0.2452957042119388, test loss: 0.7920740274308755\n",
      "Iteration 4741 training loss: 0.2452658006747724, test loss: 0.7920922356266356\n",
      "Iteration 4742 training loss: 0.24523590427701494, test loss: 0.792110443045129\n",
      "Iteration 4743 training loss: 0.24520601501604014, test loss: 0.7921286496862914\n",
      "Iteration 4744 training loss: 0.24517613288922352, test loss: 0.7921468555500581\n",
      "Iteration 4745 training loss: 0.2451462578939416, test loss: 0.7921650606363646\n",
      "Iteration 4746 training loss: 0.24511639002757277, test loss: 0.7921832649451463\n",
      "Iteration 4747 training loss: 0.24508652928749625, test loss: 0.7922014684763388\n",
      "Iteration 4748 training loss: 0.24505667567109274, test loss: 0.7922196712298781\n",
      "Iteration 4749 training loss: 0.24502682917574453, test loss: 0.792237873205699\n",
      "Iteration 4750 training loss: 0.24499698979883486, test loss: 0.7922560744037378\n",
      "Iteration 4751 training loss: 0.24496715753774856, test loss: 0.7922742748239299\n",
      "Iteration 4752 training loss: 0.2449373323898718, test loss: 0.7922924744662122\n",
      "Iteration 4753 training loss: 0.244907514352592, test loss: 0.7923106733305194\n",
      "Iteration 4754 training loss: 0.24487770342329784, test loss: 0.7923288714167882\n",
      "Iteration 4755 training loss: 0.2448478995993797, test loss: 0.7923470687249542\n",
      "Iteration 4756 training loss: 0.2448181028782288, test loss: 0.7923652652549538\n",
      "Iteration 4757 training loss: 0.24478831325723796, test loss: 0.7923834610067229\n",
      "Iteration 4758 training loss: 0.24475853073380144, test loss: 0.7924016559801982\n",
      "Iteration 4759 training loss: 0.24472875530531463, test loss: 0.7924198501753152\n",
      "Iteration 4760 training loss: 0.24469898696917428, test loss: 0.7924380435920111\n",
      "Iteration 4761 training loss: 0.2446692257227785, test loss: 0.7924562362302217\n",
      "Iteration 4762 training loss: 0.24463947156352692, test loss: 0.7924744280898836\n",
      "Iteration 4763 training loss: 0.24460972448882012, test loss: 0.7924926191709335\n",
      "Iteration 4764 training loss: 0.2445799844960603, test loss: 0.7925108094733079\n",
      "Iteration 4765 training loss: 0.24455025158265087, test loss: 0.7925289989969431\n",
      "Iteration 4766 training loss: 0.24452052574599661, test loss: 0.7925471877417761\n",
      "Iteration 4767 training loss: 0.24449080698350362, test loss: 0.7925653757077443\n",
      "Iteration 4768 training loss: 0.24446109529257928, test loss: 0.7925835628947832\n",
      "Iteration 4769 training loss: 0.24443139067063233, test loss: 0.7926017493028308\n",
      "Iteration 4770 training loss: 0.24440169311507284, test loss: 0.7926199349318235\n",
      "Iteration 4771 training loss: 0.24437200262331227, test loss: 0.7926381197816984\n",
      "Iteration 4772 training loss: 0.24434231919276322, test loss: 0.7926563038523924\n",
      "Iteration 4773 training loss: 0.24431264282083975, test loss: 0.7926744871438429\n",
      "Iteration 4774 training loss: 0.24428297350495712, test loss: 0.7926926696559871\n",
      "Iteration 4775 training loss: 0.24425331124253222, test loss: 0.7927108513887622\n",
      "Iteration 4776 training loss: 0.24422365603098276, test loss: 0.7927290323421053\n",
      "Iteration 4777 training loss: 0.24419400786772819, test loss: 0.7927472125159539\n",
      "Iteration 4778 training loss: 0.24416436675018904, test loss: 0.7927653919102453\n",
      "Iteration 4779 training loss: 0.24413473267578736, test loss: 0.7927835705249173\n",
      "Iteration 4780 training loss: 0.2441051056419463, test loss: 0.7928017483599075\n",
      "Iteration 4781 training loss: 0.24407548564609036, test loss: 0.7928199254151527\n",
      "Iteration 4782 training loss: 0.24404587268564548, test loss: 0.7928381016905915\n",
      "Iteration 4783 training loss: 0.2440162667580388, test loss: 0.7928562771861611\n",
      "Iteration 4784 training loss: 0.2439866678606988, test loss: 0.7928744519017995\n",
      "Iteration 4785 training loss: 0.24395707599105543, test loss: 0.7928926258374442\n",
      "Iteration 4786 training loss: 0.24392749114653958, test loss: 0.7929107989930338\n",
      "Iteration 4787 training loss: 0.24389791332458374, test loss: 0.7929289713685055\n",
      "Iteration 4788 training loss: 0.2438683425226217, test loss: 0.7929471429637978\n",
      "Iteration 4789 training loss: 0.24383877873808837, test loss: 0.7929653137788485\n",
      "Iteration 4790 training loss: 0.24380922196842011, test loss: 0.7929834838135961\n",
      "Iteration 4791 training loss: 0.24377967221105468, test loss: 0.7930016530679782\n",
      "Iteration 4792 training loss: 0.24375012946343075, test loss: 0.7930198215419337\n",
      "Iteration 4793 training loss: 0.24372059372298888, test loss: 0.7930379892354006\n",
      "Iteration 4794 training loss: 0.24369106498717033, test loss: 0.7930561561483174\n",
      "Iteration 4795 training loss: 0.24366154325341813, test loss: 0.7930743222806224\n",
      "Iteration 4796 training loss: 0.24363202851917629, test loss: 0.7930924876322539\n",
      "Iteration 4797 training loss: 0.24360252078189037, test loss: 0.793110652203151\n",
      "Iteration 4798 training loss: 0.2435730200390071, test loss: 0.7931288159932521\n",
      "Iteration 4799 training loss: 0.24354352628797435, test loss: 0.7931469790024955\n",
      "Iteration 4800 training loss: 0.2435140395262417, test loss: 0.7931651412308205\n",
      "Iteration 4801 training loss: 0.24348455975125963, test loss: 0.7931833026781655\n",
      "Iteration 4802 training loss: 0.24345508696048004, test loss: 0.7932014633444696\n",
      "Iteration 4803 training loss: 0.24342562115135621, test loss: 0.7932196232296714\n",
      "Iteration 4804 training loss: 0.24339616232134254, test loss: 0.79323778233371\n",
      "Iteration 4805 training loss: 0.24336671046789493, test loss: 0.7932559406565247\n",
      "Iteration 4806 training loss: 0.24333726558847057, test loss: 0.7932740981980544\n",
      "Iteration 4807 training loss: 0.24330782768052764, test loss: 0.793292254958238\n",
      "Iteration 4808 training loss: 0.243278396741526, test loss: 0.793310410937015\n",
      "Iteration 4809 training loss: 0.24324897276892637, test loss: 0.7933285661343247\n",
      "Iteration 4810 training loss: 0.2432195557601913, test loss: 0.7933467205501061\n",
      "Iteration 4811 training loss: 0.24319014571278408, test loss: 0.7933648741842991\n",
      "Iteration 4812 training loss: 0.24316074262416962, test loss: 0.7933830270368427\n",
      "Iteration 4813 training loss: 0.24313134649181398, test loss: 0.7934011791076765\n",
      "Iteration 4814 training loss: 0.24310195731318462, test loss: 0.7934193303967403\n",
      "Iteration 4815 training loss: 0.24307257508575023, test loss: 0.7934374809039734\n",
      "Iteration 4816 training loss: 0.24304319980698066, test loss: 0.7934556306293155\n",
      "Iteration 4817 training loss: 0.24301383147434716, test loss: 0.7934737795727064\n",
      "Iteration 4818 training loss: 0.2429844700853224, test loss: 0.793491927734086\n",
      "Iteration 4819 training loss: 0.24295511563737998, test loss: 0.7935100751133942\n",
      "Iteration 4820 training loss: 0.24292576812799516, test loss: 0.7935282217105707\n",
      "Iteration 4821 training loss: 0.24289642755464413, test loss: 0.7935463675255553\n",
      "Iteration 4822 training loss: 0.24286709391480463, test loss: 0.7935645125582887\n",
      "Iteration 4823 training loss: 0.24283776720595554, test loss: 0.7935826568087104\n",
      "Iteration 4824 training loss: 0.24280844742557697, test loss: 0.7936008002767606\n",
      "Iteration 4825 training loss: 0.24277913457115047, test loss: 0.7936189429623799\n",
      "Iteration 4826 training loss: 0.24274982864015876, test loss: 0.7936370848655082\n",
      "Iteration 4827 training loss: 0.24272052963008584, test loss: 0.7936552259860858\n",
      "Iteration 4828 training loss: 0.24269123753841698, test loss: 0.7936733663240533\n",
      "Iteration 4829 training loss: 0.24266195236263877, test loss: 0.7936915058793511\n",
      "Iteration 4830 training loss: 0.2426326741002389, test loss: 0.7937096446519194\n",
      "Iteration 4831 training loss: 0.2426034027487066, test loss: 0.7937277826416992\n",
      "Iteration 4832 training loss: 0.24257413830553212, test loss: 0.7937459198486309\n",
      "Iteration 4833 training loss: 0.24254488076820724, test loss: 0.793764056272655\n",
      "Iteration 4834 training loss: 0.24251563013422475, test loss: 0.7937821919137124\n",
      "Iteration 4835 training loss: 0.24248638640107878, test loss: 0.7938003267717438\n",
      "Iteration 4836 training loss: 0.24245714956626482, test loss: 0.7938184608466903\n",
      "Iteration 4837 training loss: 0.24242791962727953, test loss: 0.7938365941384926\n",
      "Iteration 4838 training loss: 0.24239869658162086, test loss: 0.7938547266470918\n",
      "Iteration 4839 training loss: 0.24236948042678802, test loss: 0.7938728583724289\n",
      "Iteration 4840 training loss: 0.24234027116028153, test loss: 0.7938909893144445\n",
      "Iteration 4841 training loss: 0.2423110687796031, test loss: 0.7939091194730805\n",
      "Iteration 4842 training loss: 0.2422818732822558, test loss: 0.7939272488482776\n",
      "Iteration 4843 training loss: 0.24225268466574365, test loss: 0.793945377439977\n",
      "Iteration 4844 training loss: 0.2422235029275725, test loss: 0.7939635052481204\n",
      "Iteration 4845 training loss: 0.24219432806524885, test loss: 0.7939816322726488\n",
      "Iteration 4846 training loss: 0.24216516007628106, test loss: 0.7939997585135042\n",
      "Iteration 4847 training loss: 0.242135998958178, test loss: 0.7940178839706271\n",
      "Iteration 4848 training loss: 0.24210684470845054, test loss: 0.79403600864396\n",
      "Iteration 4849 training loss: 0.2420776973246103, test loss: 0.7940541325334439\n",
      "Iteration 4850 training loss: 0.24204855680417045, test loss: 0.7940722556390208\n",
      "Iteration 4851 training loss: 0.24201942314464522, test loss: 0.7940903779606324\n",
      "Iteration 4852 training loss: 0.24199029634355027, test loss: 0.7941084994982202\n",
      "Iteration 4853 training loss: 0.24196117639840223, test loss: 0.7941266202517261\n",
      "Iteration 4854 training loss: 0.24193206330671935, test loss: 0.7941447402210922\n",
      "Iteration 4855 training loss: 0.24190295706602077, test loss: 0.7941628594062604\n",
      "Iteration 4856 training loss: 0.24187385767382716, test loss: 0.7941809778071725\n",
      "Iteration 4857 training loss: 0.2418447651276603, test loss: 0.7941990954237705\n",
      "Iteration 4858 training loss: 0.24181567942504326, test loss: 0.7942172122559971\n",
      "Iteration 4859 training loss: 0.24178660056350035, test loss: 0.7942353283037941\n",
      "Iteration 4860 training loss: 0.241757528540557, test loss: 0.7942534435671035\n",
      "Iteration 4861 training loss: 0.2417284633537401, test loss: 0.7942715580458678\n",
      "Iteration 4862 training loss: 0.24169940500057768, test loss: 0.7942896717400296\n",
      "Iteration 4863 training loss: 0.241670353478599, test loss: 0.7943077846495308\n",
      "Iteration 4864 training loss: 0.2416413087853346, test loss: 0.7943258967743144\n",
      "Iteration 4865 training loss: 0.2416122709183162, test loss: 0.7943440081143226\n",
      "Iteration 4866 training loss: 0.24158323987507682, test loss: 0.794362118669498\n",
      "Iteration 4867 training loss: 0.24155421565315066, test loss: 0.7943802284397833\n",
      "Iteration 4868 training loss: 0.2415251982500733, test loss: 0.7943983374251212\n",
      "Iteration 4869 training loss: 0.2414961876633813, test loss: 0.7944164456254543\n",
      "Iteration 4870 training loss: 0.24146718389061272, test loss: 0.7944345530407256\n",
      "Iteration 4871 training loss: 0.2414381869293068, test loss: 0.794452659670878\n",
      "Iteration 4872 training loss: 0.24140919677700387, test loss: 0.7944707655158543\n",
      "Iteration 4873 training loss: 0.24138021343124555, test loss: 0.7944888705755976\n",
      "Iteration 4874 training loss: 0.2413512368895748, test loss: 0.7945069748500504\n",
      "Iteration 4875 training loss: 0.24132226714953584, test loss: 0.7945250783391565\n",
      "Iteration 4876 training loss: 0.2412933042086738, test loss: 0.7945431810428589\n",
      "Iteration 4877 training loss: 0.24126434806453545, test loss: 0.7945612829611008\n",
      "Iteration 4878 training loss: 0.24123539871466865, test loss: 0.7945793840938252\n",
      "Iteration 4879 training loss: 0.24120645615662223, test loss: 0.7945974844409758\n",
      "Iteration 4880 training loss: 0.2411775203879467, test loss: 0.7946155840024954\n",
      "Iteration 4881 training loss: 0.24114859140619332, test loss: 0.7946336827783285\n",
      "Iteration 4882 training loss: 0.2411196692089151, test loss: 0.794651780768417\n",
      "Iteration 4883 training loss: 0.24109075379366582, test loss: 0.7946698779727062\n",
      "Iteration 4884 training loss: 0.2410618451580007, test loss: 0.7946879743911384\n",
      "Iteration 4885 training loss: 0.2410329432994762, test loss: 0.7947060700236577\n",
      "Iteration 4886 training loss: 0.24100404821565, test loss: 0.7947241648702081\n",
      "Iteration 4887 training loss: 0.24097515990408078, test loss: 0.7947422589307328\n",
      "Iteration 4888 training loss: 0.24094627836232874, test loss: 0.7947603522051765\n",
      "Iteration 4889 training loss: 0.2409174035879552, test loss: 0.7947784446934824\n",
      "Iteration 4890 training loss: 0.24088853557852255, test loss: 0.7947965363955944\n",
      "Iteration 4891 training loss: 0.24085967433159475, test loss: 0.794814627311457\n",
      "Iteration 4892 training loss: 0.24083081984473662, test loss: 0.7948327174410139\n",
      "Iteration 4893 training loss: 0.24080197211551435, test loss: 0.7948508067842093\n",
      "Iteration 4894 training loss: 0.24077313114149534, test loss: 0.7948688953409873\n",
      "Iteration 4895 training loss: 0.24074429692024835, test loss: 0.7948869831112922\n",
      "Iteration 4896 training loss: 0.2407154694493429, test loss: 0.7949050700950687\n",
      "Iteration 4897 training loss: 0.24068664872635037, test loss: 0.7949231562922604\n",
      "Iteration 4898 training loss: 0.24065783474884278, test loss: 0.7949412417028122\n",
      "Iteration 4899 training loss: 0.2406290275143938, test loss: 0.7949593263266687\n",
      "Iteration 4900 training loss: 0.24060022702057804, test loss: 0.7949774101637738\n",
      "Iteration 4901 training loss: 0.2405714332649713, test loss: 0.7949954932140724\n",
      "Iteration 4902 training loss: 0.2405426462451508, test loss: 0.7950135754775092\n",
      "Iteration 4903 training loss: 0.24051386595869478, test loss: 0.795031656954029\n",
      "Iteration 4904 training loss: 0.2404850924031829, test loss: 0.7950497376435764\n",
      "Iteration 4905 training loss: 0.24045632557619578, test loss: 0.7950678175460961\n",
      "Iteration 4906 training loss: 0.2404275654753155, test loss: 0.7950858966615333\n",
      "Iteration 4907 training loss: 0.240398812098125, test loss: 0.7951039749898324\n",
      "Iteration 4908 training loss: 0.24037006544220885, test loss: 0.7951220525309385\n",
      "Iteration 4909 training loss: 0.24034132550515258, test loss: 0.7951401292847972\n",
      "Iteration 4910 training loss: 0.24031259228454283, test loss: 0.795158205251353\n",
      "Iteration 4911 training loss: 0.24028386577796776, test loss: 0.7951762804305509\n",
      "Iteration 4912 training loss: 0.24025514598301642, test loss: 0.7951943548223366\n",
      "Iteration 4913 training loss: 0.2402264328972792, test loss: 0.7952124284266551\n",
      "Iteration 4914 training loss: 0.24019772651834784, test loss: 0.7952305012434517\n",
      "Iteration 4915 training loss: 0.24016902684381486, test loss: 0.7952485732726718\n",
      "Iteration 4916 training loss: 0.24014033387127448, test loss: 0.7952666445142611\n",
      "Iteration 4917 training loss: 0.24011164759832174, test loss: 0.7952847149681647\n",
      "Iteration 4918 training loss: 0.24008296802255308, test loss: 0.7953027846343279\n",
      "Iteration 4919 training loss: 0.24005429514156612, test loss: 0.7953208535126969\n",
      "Iteration 4920 training loss: 0.2400256289529595, test loss: 0.7953389216032171\n",
      "Iteration 4921 training loss: 0.23999696945433346, test loss: 0.7953569889058342\n",
      "Iteration 4922 training loss: 0.23996831664328883, test loss: 0.7953750554204936\n",
      "Iteration 4923 training loss: 0.2399396705174283, test loss: 0.7953931211471419\n",
      "Iteration 4924 training loss: 0.23991103107435513, test loss: 0.7954111860857241\n",
      "Iteration 4925 training loss: 0.23988239831167418, test loss: 0.7954292502361869\n",
      "Iteration 4926 training loss: 0.23985377222699153, test loss: 0.7954473135984758\n",
      "Iteration 4927 training loss: 0.23982515281791422, test loss: 0.7954653761725372\n",
      "Iteration 4928 training loss: 0.23979654008205054, test loss: 0.7954834379583166\n",
      "Iteration 4929 training loss: 0.23976793401700996, test loss: 0.7955014989557605\n",
      "Iteration 4930 training loss: 0.23973933462040328, test loss: 0.7955195591648151\n",
      "Iteration 4931 training loss: 0.23971074188984243, test loss: 0.795537618585427\n",
      "Iteration 4932 training loss: 0.23968215582294036, test loss: 0.795555677217542\n",
      "Iteration 4933 training loss: 0.2396535764173114, test loss: 0.7955737350611065\n",
      "Iteration 4934 training loss: 0.23962500367057105, test loss: 0.7955917921160675\n",
      "Iteration 4935 training loss: 0.2395964375803359, test loss: 0.7956098483823707\n",
      "Iteration 4936 training loss: 0.23956787814422376, test loss: 0.7956279038599632\n",
      "Iteration 4937 training loss: 0.2395393253598537, test loss: 0.7956459585487912\n",
      "Iteration 4938 training loss: 0.23951077922484576, test loss: 0.7956640124488019\n",
      "Iteration 4939 training loss: 0.23948223973682145, test loss: 0.7956820655599417\n",
      "Iteration 4940 training loss: 0.23945370689340326, test loss: 0.7957001178821572\n",
      "Iteration 4941 training loss: 0.23942518069221502, test loss: 0.7957181694153952\n",
      "Iteration 4942 training loss: 0.23939666113088154, test loss: 0.7957362201596031\n",
      "Iteration 4943 training loss: 0.23936814820702892, test loss: 0.7957542701147273\n",
      "Iteration 4944 training loss: 0.23933964191828444, test loss: 0.7957723192807151\n",
      "Iteration 4945 training loss: 0.23931114226227662, test loss: 0.7957903676575132\n",
      "Iteration 4946 training loss: 0.23928264923663503, test loss: 0.7958084152450693\n",
      "Iteration 4947 training loss: 0.23925416283899037, test loss: 0.7958264620433296\n",
      "Iteration 4948 training loss: 0.23922568306697484, test loss: 0.7958445080522424\n",
      "Iteration 4949 training loss: 0.2391972099182214, test loss: 0.7958625532717539\n",
      "Iteration 4950 training loss: 0.23916874339036445, test loss: 0.7958805977018125\n",
      "Iteration 4951 training loss: 0.2391402834810394, test loss: 0.7958986413423648\n",
      "Iteration 4952 training loss: 0.23911183018788318, test loss: 0.7959166841933585\n",
      "Iteration 4953 training loss: 0.23908338350853325, test loss: 0.795934726254741\n",
      "Iteration 4954 training loss: 0.2390549434406289, test loss: 0.79595276752646\n",
      "Iteration 4955 training loss: 0.23902650998181021, test loss: 0.7959708080084629\n",
      "Iteration 4956 training loss: 0.23899808312971854, test loss: 0.7959888477006973\n",
      "Iteration 4957 training loss: 0.23896966288199645, test loss: 0.7960068866031114\n",
      "Iteration 4958 training loss: 0.2389412492362875, test loss: 0.7960249247156526\n",
      "Iteration 4959 training loss: 0.23891284219023667, test loss: 0.7960429620382685\n",
      "Iteration 4960 training loss: 0.23888444174148998, test loss: 0.7960609985709072\n",
      "Iteration 4961 training loss: 0.23885604788769457, test loss: 0.7960790343135171\n",
      "Iteration 4962 training loss: 0.2388276606264988, test loss: 0.7960970692660452\n",
      "Iteration 4963 training loss: 0.2387992799555521, test loss: 0.7961151034284405\n",
      "Iteration 4964 training loss: 0.2387709058725052, test loss: 0.7961331368006503\n",
      "Iteration 4965 training loss: 0.23874253837501005, test loss: 0.7961511693826235\n",
      "Iteration 4966 training loss: 0.2387141774607195, test loss: 0.7961692011743078\n",
      "Iteration 4967 training loss: 0.23868582312728778, test loss: 0.7961872321756515\n",
      "Iteration 4968 training loss: 0.23865747537237014, test loss: 0.7962052623866032\n",
      "Iteration 4969 training loss: 0.23862913419362317, test loss: 0.7962232918071109\n",
      "Iteration 4970 training loss: 0.23860079958870442, test loss: 0.7962413204371231\n",
      "Iteration 4971 training loss: 0.23857247155527278, test loss: 0.7962593482765887\n",
      "Iteration 4972 training loss: 0.23854415009098795, test loss: 0.7962773753254561\n",
      "Iteration 4973 training loss: 0.23851583519351147, test loss: 0.7962954015836735\n",
      "Iteration 4974 training loss: 0.23848752686050512, test loss: 0.7963134270511898\n",
      "Iteration 4975 training loss: 0.2384592250896327, test loss: 0.7963314517279538\n",
      "Iteration 4976 training loss: 0.2384309298785586, test loss: 0.7963494756139137\n",
      "Iteration 4977 training loss: 0.23840264122494847, test loss: 0.796367498709019\n",
      "Iteration 4978 training loss: 0.23837435912646934, test loss: 0.7963855210132185\n",
      "Iteration 4979 training loss: 0.23834608358078926, test loss: 0.7964035425264608\n",
      "Iteration 4980 training loss: 0.23831781458557733, test loss: 0.7964215632486951\n",
      "Iteration 4981 training loss: 0.23828955213850386, test loss: 0.7964395831798704\n",
      "Iteration 4982 training loss: 0.23826129623724035, test loss: 0.7964576023199358\n",
      "Iteration 4983 training loss: 0.23823304687945948, test loss: 0.7964756206688404\n",
      "Iteration 4984 training loss: 0.23820480406283492, test loss: 0.796493638226533\n",
      "Iteration 4985 training loss: 0.2381765677850417, test loss: 0.7965116549929636\n",
      "Iteration 4986 training loss: 0.23814833804375582, test loss: 0.7965296709680812\n",
      "Iteration 4987 training loss: 0.23812011483665463, test loss: 0.796547686151835\n",
      "Iteration 4988 training loss: 0.23809189816141627, test loss: 0.7965657005441745\n",
      "Iteration 4989 training loss: 0.2380636880157204, test loss: 0.7965837141450491\n",
      "Iteration 4990 training loss: 0.23803548439724762, test loss: 0.7966017269544088\n",
      "Iteration 4991 training loss: 0.23800728730367968, test loss: 0.7966197389722025\n",
      "Iteration 4992 training loss: 0.23797909673269962, test loss: 0.7966377501983802\n",
      "Iteration 4993 training loss: 0.23795091268199148, test loss: 0.7966557606328918\n",
      "Iteration 4994 training loss: 0.2379227351492403, test loss: 0.7966737702756865\n",
      "Iteration 4995 training loss: 0.23789456413213272, test loss: 0.7966917791267146\n",
      "Iteration 4996 training loss: 0.2378663996283561, test loss: 0.7967097871859254\n",
      "Iteration 4997 training loss: 0.237838241635599, test loss: 0.7967277944532692\n",
      "Iteration 4998 training loss: 0.23781009015155125, test loss: 0.7967458009286963\n",
      "Iteration 4999 training loss: 0.23778194517390375, test loss: 0.7967638066121562\n",
      "Iteration 5000 training loss: 0.23775380670034868, test loss: 0.7967818115035992\n",
      "Iteration 5001 training loss: 0.237725674728579, test loss: 0.7967998156029752\n",
      "Iteration 5002 training loss: 0.23769754925628908, test loss: 0.7968178189102346\n",
      "Iteration 5003 training loss: 0.2376694302811745, test loss: 0.7968358214253274\n",
      "Iteration 5004 training loss: 0.23764131780093156, test loss: 0.7968538231482043\n",
      "Iteration 5005 training loss: 0.23761321181325834, test loss: 0.7968718240788152\n",
      "Iteration 5006 training loss: 0.23758511231585344, test loss: 0.796889824217111\n",
      "Iteration 5007 training loss: 0.23755701930641682, test loss: 0.7969078235630414\n",
      "Iteration 5008 training loss: 0.23752893278264964, test loss: 0.7969258221165576\n",
      "Iteration 5009 training loss: 0.23750085274225427, test loss: 0.7969438198776101\n",
      "Iteration 5010 training loss: 0.23747277918293386, test loss: 0.7969618168461492\n",
      "Iteration 5011 training loss: 0.237444712102393, test loss: 0.7969798130221256\n",
      "Iteration 5012 training loss: 0.2374166514983372, test loss: 0.7969978084054904\n",
      "Iteration 5013 training loss: 0.23738859736847345, test loss: 0.797015802996194\n",
      "Iteration 5014 training loss: 0.23736054971050946, test loss: 0.7970337967941873\n",
      "Iteration 5015 training loss: 0.23733250852215418, test loss: 0.7970517897994215\n",
      "Iteration 5016 training loss: 0.23730447380111777, test loss: 0.7970697820118474\n",
      "Iteration 5017 training loss: 0.23727644554511146, test loss: 0.7970877734314157\n",
      "Iteration 5018 training loss: 0.23724842375184776, test loss: 0.7971057640580773\n",
      "Iteration 5019 training loss: 0.23722040841903996, test loss: 0.7971237538917841\n",
      "Iteration 5020 training loss: 0.23719239954440274, test loss: 0.797141742932487\n",
      "Iteration 5021 training loss: 0.23716439712565188, test loss: 0.7971597311801367\n",
      "Iteration 5022 training loss: 0.23713640116050422, test loss: 0.7971777186346851\n",
      "Iteration 5023 training loss: 0.23710841164667765, test loss: 0.797195705296083\n",
      "Iteration 5024 training loss: 0.23708042858189138, test loss: 0.7972136911642823\n",
      "Iteration 5025 training loss: 0.23705245196386548, test loss: 0.7972316762392339\n",
      "Iteration 5026 training loss: 0.23702448179032132, test loss: 0.7972496605208896\n",
      "Iteration 5027 training loss: 0.23699651805898142, test loss: 0.797267644009201\n",
      "Iteration 5028 training loss: 0.23696856076756934, test loss: 0.7972856267041196\n",
      "Iteration 5029 training loss: 0.23694060991380958, test loss: 0.797303608605597\n",
      "Iteration 5030 training loss: 0.236912665495428, test loss: 0.7973215897135847\n",
      "Iteration 5031 training loss: 0.23688472751015172, test loss: 0.797339570028035\n",
      "Iteration 5032 training loss: 0.23685679595570847, test loss: 0.7973575495488994\n",
      "Iteration 5033 training loss: 0.2368288708298274, test loss: 0.7973755282761295\n",
      "Iteration 5034 training loss: 0.23680095213023894, test loss: 0.7973935062096777\n",
      "Iteration 5035 training loss: 0.23677303985467424, test loss: 0.7974114833494956\n",
      "Iteration 5036 training loss: 0.23674513400086575, test loss: 0.7974294596955358\n",
      "Iteration 5037 training loss: 0.23671723456654722, test loss: 0.7974474352477497\n",
      "Iteration 5038 training loss: 0.23668934154945315, test loss: 0.7974654100060897\n",
      "Iteration 5039 training loss: 0.2366614549473195, test loss: 0.797483383970508\n",
      "Iteration 5040 training loss: 0.236633574757883, test loss: 0.797501357140957\n",
      "Iteration 5041 training loss: 0.23660570097888162, test loss: 0.7975193295173888\n",
      "Iteration 5042 training loss: 0.23657783360805457, test loss: 0.7975373010997556\n",
      "Iteration 5043 training loss: 0.236549972643142, test loss: 0.7975552718880101\n",
      "Iteration 5044 training loss: 0.23652211808188528, test loss: 0.7975732418821049\n",
      "Iteration 5045 training loss: 0.23649426992202674, test loss: 0.797591211081992\n",
      "Iteration 5046 training loss: 0.23646642816131, test loss: 0.7976091794876243\n",
      "Iteration 5047 training loss: 0.23643859279747947, test loss: 0.7976271470989545\n",
      "Iteration 5048 training loss: 0.23641076382828116, test loss: 0.7976451139159351\n",
      "Iteration 5049 training loss: 0.23638294125146173, test loss: 0.7976630799385188\n",
      "Iteration 5050 training loss: 0.23635512506476905, test loss: 0.7976810451666583\n",
      "Iteration 5051 training loss: 0.2363273152659523, test loss: 0.7976990096003069\n",
      "Iteration 5052 training loss: 0.23629951185276146, test loss: 0.7977169732394168\n",
      "Iteration 5053 training loss: 0.23627171482294773, test loss: 0.7977349360839416\n",
      "Iteration 5054 training loss: 0.23624392417426357, test loss: 0.797752898133834\n",
      "Iteration 5055 training loss: 0.23621613990446227, test loss: 0.7977708593890469\n",
      "Iteration 5056 training loss: 0.23618836201129836, test loss: 0.7977888198495335\n",
      "Iteration 5057 training loss: 0.2361605904925275, test loss: 0.7978067795152474\n",
      "Iteration 5058 training loss: 0.23613282534590632, test loss: 0.7978247383861411\n",
      "Iteration 5059 training loss: 0.23610506656919264, test loss: 0.7978426964621684\n",
      "Iteration 5060 training loss: 0.23607731416014527, test loss: 0.7978606537432824\n",
      "Iteration 5061 training loss: 0.23604956811652428, test loss: 0.7978786102294363\n",
      "Iteration 5062 training loss: 0.2360218284360907, test loss: 0.7978965659205839\n",
      "Iteration 5063 training loss: 0.2359940951166067, test loss: 0.7979145208166785\n",
      "Iteration 5064 training loss: 0.23596636815583558, test loss: 0.7979324749176735\n",
      "Iteration 5065 training loss: 0.2359386475515415, test loss: 0.7979504282235228\n",
      "Iteration 5066 training loss: 0.23591093330149007, test loss: 0.7979683807341796\n",
      "Iteration 5067 training loss: 0.2358832254034477, test loss: 0.797986332449598\n",
      "Iteration 5068 training loss: 0.23585552385518202, test loss: 0.7980042833697316\n",
      "Iteration 5069 training loss: 0.23582782865446175, test loss: 0.7980222334945343\n",
      "Iteration 5070 training loss: 0.2358001397990567, test loss: 0.7980401828239597\n",
      "Iteration 5071 training loss: 0.23577245728673757, test loss: 0.7980581313579618\n",
      "Iteration 5072 training loss: 0.23574478111527644, test loss: 0.7980760790964948\n",
      "Iteration 5073 training loss: 0.23571711128244624, test loss: 0.7980940260395124\n",
      "Iteration 5074 training loss: 0.2356894477860212, test loss: 0.7981119721869688\n",
      "Iteration 5075 training loss: 0.23566179062377643, test loss: 0.798129917538818\n",
      "Iteration 5076 training loss: 0.23563413979348827, test loss: 0.7981478620950144\n",
      "Iteration 5077 training loss: 0.23560649529293395, test loss: 0.7981658058555122\n",
      "Iteration 5078 training loss: 0.2355788571198921, test loss: 0.7981837488202655\n",
      "Iteration 5079 training loss: 0.235551225272142, test loss: 0.7982016909892289\n",
      "Iteration 5080 training loss: 0.2355235997474645, test loss: 0.7982196323623563\n",
      "Iteration 5081 training loss: 0.2354959805436411, test loss: 0.7982375729396025\n",
      "Iteration 5082 training loss: 0.2354683676584546, test loss: 0.7982555127209222\n",
      "Iteration 5083 training loss: 0.23544076108968884, test loss: 0.7982734517062695\n",
      "Iteration 5084 training loss: 0.2354131608351287, test loss: 0.7982913898955993\n",
      "Iteration 5085 training loss: 0.23538556689256018, test loss: 0.7983093272888662\n",
      "Iteration 5086 training loss: 0.23535797925977034, test loss: 0.7983272638860246\n",
      "Iteration 5087 training loss: 0.23533039793454733, test loss: 0.7983451996870297\n",
      "Iteration 5088 training loss: 0.23530282291468033, test loss: 0.798363134691836\n",
      "Iteration 5089 training loss: 0.23527525419795955, test loss: 0.7983810689003986\n",
      "Iteration 5090 training loss: 0.23524769178217644, test loss: 0.7983990023126725\n",
      "Iteration 5091 training loss: 0.23522013566512345, test loss: 0.7984169349286121\n",
      "Iteration 5092 training loss: 0.23519258584459402, test loss: 0.7984348667481728\n",
      "Iteration 5093 training loss: 0.23516504231838256, test loss: 0.7984527977713101\n",
      "Iteration 5094 training loss: 0.235137505084285, test loss: 0.7984707279979785\n",
      "Iteration 5095 training loss: 0.23510997414009777, test loss: 0.7984886574281334\n",
      "Iteration 5096 training loss: 0.23508244948361876, test loss: 0.7985065860617299\n",
      "Iteration 5097 training loss: 0.2350549311126469, test loss: 0.7985245138987237\n",
      "Iteration 5098 training loss: 0.23502741902498187, test loss: 0.7985424409390697\n",
      "Iteration 5099 training loss: 0.23499991321842478, test loss: 0.7985603671827233\n",
      "Iteration 5100 training loss: 0.2349724136907777, test loss: 0.7985782926296403\n",
      "Iteration 5101 training loss: 0.2349449204398436, test loss: 0.7985962172797759\n",
      "Iteration 5102 training loss: 0.23491743346342675, test loss: 0.7986141411330858\n",
      "Iteration 5103 training loss: 0.23488995275933242, test loss: 0.7986320641895256\n",
      "Iteration 5104 training loss: 0.23486247832536675, test loss: 0.7986499864490514\n",
      "Iteration 5105 training loss: 0.23483501015933714, test loss: 0.7986679079116178\n",
      "Iteration 5106 training loss: 0.23480754825905203, test loss: 0.7986858285771812\n",
      "Iteration 5107 training loss: 0.2347800926223209, test loss: 0.7987037484456978\n",
      "Iteration 5108 training loss: 0.23475264324695436, test loss: 0.7987216675171228\n",
      "Iteration 5109 training loss: 0.2347252001307638, test loss: 0.7987395857914127\n",
      "Iteration 5110 training loss: 0.234697763271562, test loss: 0.798757503268523\n",
      "Iteration 5111 training loss: 0.2346703326671627, test loss: 0.7987754199484101\n",
      "Iteration 5112 training loss: 0.2346429083153806, test loss: 0.7987933358310298\n",
      "Iteration 5113 training loss: 0.23461549021403152, test loss: 0.7988112509163382\n",
      "Iteration 5114 training loss: 0.2345880783609324, test loss: 0.7988291652042921\n",
      "Iteration 5115 training loss: 0.23456067275390116, test loss: 0.7988470786948471\n",
      "Iteration 5116 training loss: 0.23453327339075677, test loss: 0.7988649913879596\n",
      "Iteration 5117 training loss: 0.23450588026931926, test loss: 0.798882903283586\n",
      "Iteration 5118 training loss: 0.23447849338740967, test loss: 0.7989008143816827\n",
      "Iteration 5119 training loss: 0.23445111274285027, test loss: 0.7989187246822066\n",
      "Iteration 5120 training loss: 0.23442373833346422, test loss: 0.7989366341851132\n",
      "Iteration 5121 training loss: 0.23439637015707576, test loss: 0.7989545428903599\n",
      "Iteration 5122 training loss: 0.23436900821151016, test loss: 0.798972450797903\n",
      "Iteration 5123 training loss: 0.23434165249459393, test loss: 0.7989903579076992\n",
      "Iteration 5124 training loss: 0.23431430300415418, test loss: 0.7990082642197053\n",
      "Iteration 5125 training loss: 0.23428695973801966, test loss: 0.7990261697338781\n",
      "Iteration 5126 training loss: 0.23425962269401973, test loss: 0.7990440744501742\n",
      "Iteration 5127 training loss: 0.23423229186998484, test loss: 0.7990619783685505\n",
      "Iteration 5128 training loss: 0.23420496726374676, test loss: 0.7990798814889641\n",
      "Iteration 5129 training loss: 0.2341776488731381, test loss: 0.799097783811372\n",
      "Iteration 5130 training loss: 0.23415033669599253, test loss: 0.7991156853357311\n",
      "Iteration 5131 training loss: 0.23412303073014465, test loss: 0.7991335860619982\n",
      "Iteration 5132 training loss: 0.23409573097343034, test loss: 0.7991514859901311\n",
      "Iteration 5133 training loss: 0.23406843742368647, test loss: 0.7991693851200866\n",
      "Iteration 5134 training loss: 0.2340411500787509, test loss: 0.7991872834518217\n",
      "Iteration 5135 training loss: 0.23401386893646234, test loss: 0.7992051809852943\n",
      "Iteration 5136 training loss: 0.23398659399466096, test loss: 0.7992230777204611\n",
      "Iteration 5137 training loss: 0.2339593252511877, test loss: 0.7992409736572802\n",
      "Iteration 5138 training loss: 0.2339320627038844, test loss: 0.7992588687957081\n",
      "Iteration 5139 training loss: 0.23390480635059432, test loss: 0.7992767631357032\n",
      "Iteration 5140 training loss: 0.2338775561891615, test loss: 0.7992946566772227\n",
      "Iteration 5141 training loss: 0.2338503122174311, test loss: 0.7993125494202244\n",
      "Iteration 5142 training loss: 0.2338230744332492, test loss: 0.7993304413646652\n",
      "Iteration 5143 training loss: 0.23379584283446317, test loss: 0.7993483325105041\n",
      "Iteration 5144 training loss: 0.23376861741892124, test loss: 0.7993662228576977\n",
      "Iteration 5145 training loss: 0.23374139818447245, test loss: 0.7993841124062046\n",
      "Iteration 5146 training loss: 0.23371418512896747, test loss: 0.7994020011559821\n",
      "Iteration 5147 training loss: 0.23368697825025742, test loss: 0.7994198891069885\n",
      "Iteration 5148 training loss: 0.2336597775461947, test loss: 0.7994377762591817\n",
      "Iteration 5149 training loss: 0.23363258301463286, test loss: 0.7994556626125192\n",
      "Iteration 5150 training loss: 0.23360539465342625, test loss: 0.7994735481669599\n",
      "Iteration 5151 training loss: 0.23357821246043042, test loss: 0.7994914329224615\n",
      "Iteration 5152 training loss: 0.2335510364335019, test loss: 0.7995093168789826\n",
      "Iteration 5153 training loss: 0.23352386657049806, test loss: 0.7995272000364807\n",
      "Iteration 5154 training loss: 0.23349670286927765, test loss: 0.7995450823949146\n",
      "Iteration 5155 training loss: 0.23346954532770023, test loss: 0.7995629639542425\n",
      "Iteration 5156 training loss: 0.2334423939436264, test loss: 0.7995808447144228\n",
      "Iteration 5157 training loss: 0.23341524871491792, test loss: 0.7995987246754138\n",
      "Iteration 5158 training loss: 0.23338810963943732, test loss: 0.7996166038371744\n",
      "Iteration 5159 training loss: 0.2333609767150485, test loss: 0.7996344821996626\n",
      "Iteration 5160 training loss: 0.23333384993961606, test loss: 0.7996523597628377\n",
      "Iteration 5161 training loss: 0.23330672931100574, test loss: 0.7996702365266574\n",
      "Iteration 5162 training loss: 0.23327961482708448, test loss: 0.7996881124910814\n",
      "Iteration 5163 training loss: 0.23325250648572, test loss: 0.7997059876560677\n",
      "Iteration 5164 training loss: 0.23322540428478108, test loss: 0.7997238620215755\n",
      "Iteration 5165 training loss: 0.2331983082221376, test loss: 0.7997417355875637\n",
      "Iteration 5166 training loss: 0.2331712182956606, test loss: 0.799759608353991\n",
      "Iteration 5167 training loss: 0.23314413450322177, test loss: 0.7997774803208166\n",
      "Iteration 5168 training loss: 0.23311705684269413, test loss: 0.799795351487999\n",
      "Iteration 5169 training loss: 0.2330899853119517, test loss: 0.7998132218554979\n",
      "Iteration 5170 training loss: 0.2330629199088692, test loss: 0.7998310914232718\n",
      "Iteration 5171 training loss: 0.2330358606313229, test loss: 0.7998489601912805\n",
      "Iteration 5172 training loss: 0.23300880747718958, test loss: 0.799866828159483\n",
      "Iteration 5173 training loss: 0.23298176044434743, test loss: 0.7998846953278385\n",
      "Iteration 5174 training loss: 0.2329547195306753, test loss: 0.7999025616963062\n",
      "Iteration 5175 training loss: 0.23292768473405334, test loss: 0.7999204272648456\n",
      "Iteration 5176 training loss: 0.23290065605236263, test loss: 0.7999382920334163\n",
      "Iteration 5177 training loss: 0.23287363348348514, test loss: 0.7999561560019773\n",
      "Iteration 5178 training loss: 0.23284661702530413, test loss: 0.7999740191704889\n",
      "Iteration 5179 training loss: 0.23281960667570367, test loss: 0.7999918815389098\n",
      "Iteration 5180 training loss: 0.23279260243256863, test loss: 0.8000097431072005\n",
      "Iteration 5181 training loss: 0.2327656042937855, test loss: 0.8000276038753201\n",
      "Iteration 5182 training loss: 0.23273861225724116, test loss: 0.8000454638432286\n",
      "Iteration 5183 training loss: 0.2327116263208239, test loss: 0.8000633230108859\n",
      "Iteration 5184 training loss: 0.2326846464824228, test loss: 0.800081181378251\n",
      "Iteration 5185 training loss: 0.23265767273992816, test loss: 0.8000990389452852\n",
      "Iteration 5186 training loss: 0.23263070509123102, test loss: 0.8001168957119472\n",
      "Iteration 5187 training loss: 0.2326037435342237, test loss: 0.8001347516781977\n",
      "Iteration 5188 training loss: 0.23257678806679927, test loss: 0.8001526068439966\n",
      "Iteration 5189 training loss: 0.23254983868685195, test loss: 0.8001704612093038\n",
      "Iteration 5190 training loss: 0.23252289539227708, test loss: 0.8001883147740798\n",
      "Iteration 5191 training loss: 0.2324959581809708, test loss: 0.8002061675382846\n",
      "Iteration 5192 training loss: 0.23246902705083025, test loss: 0.8002240195018783\n",
      "Iteration 5193 training loss: 0.23244210199975385, test loss: 0.8002418706648216\n",
      "Iteration 5194 training loss: 0.23241518302564068, test loss: 0.8002597210270745\n",
      "Iteration 5195 training loss: 0.232388270126391, test loss: 0.8002775705885978\n",
      "Iteration 5196 training loss: 0.23236136329990606, test loss: 0.8002954193493514\n",
      "Iteration 5197 training loss: 0.23233446254408813, test loss: 0.8003132673092964\n",
      "Iteration 5198 training loss: 0.23230756785684042, test loss: 0.800331114468393\n",
      "Iteration 5199 training loss: 0.2322806792360672, test loss: 0.8003489608266019\n",
      "Iteration 5200 training loss: 0.23225379667967366, test loss: 0.800366806383884\n",
      "Iteration 5201 training loss: 0.2322269201855662, test loss: 0.8003846511401999\n",
      "Iteration 5202 training loss: 0.23220004975165176, test loss: 0.8004024950955099\n",
      "Iteration 5203 training loss: 0.23217318537583892, test loss: 0.8004203382497755\n",
      "Iteration 5204 training loss: 0.23214632705603677, test loss: 0.8004381806029572\n",
      "Iteration 5205 training loss: 0.2321194747901555, test loss: 0.8004560221550162\n",
      "Iteration 5206 training loss: 0.23209262857610638, test loss: 0.8004738629059132\n",
      "Iteration 5207 training loss: 0.23206578841180156, test loss: 0.8004917028556093\n",
      "Iteration 5208 training loss: 0.23203895429515442, test loss: 0.8005095420040657\n",
      "Iteration 5209 training loss: 0.23201212622407916, test loss: 0.8005273803512434\n",
      "Iteration 5210 training loss: 0.2319853041964909, test loss: 0.8005452178971036\n",
      "Iteration 5211 training loss: 0.23195848821030585, test loss: 0.8005630546416077\n",
      "Iteration 5212 training loss: 0.23193167826344122, test loss: 0.8005808905847173\n",
      "Iteration 5213 training loss: 0.2319048743538153, test loss: 0.8005987257263925\n",
      "Iteration 5214 training loss: 0.23187807647934708, test loss: 0.800616560066596\n",
      "Iteration 5215 training loss: 0.23185128463795696, test loss: 0.8006343936052887\n",
      "Iteration 5216 training loss: 0.23182449882756587, test loss: 0.800652226342432\n",
      "Iteration 5217 training loss: 0.23179771904609617, test loss: 0.8006700582779879\n",
      "Iteration 5218 training loss: 0.23177094529147094, test loss: 0.8006878894119173\n",
      "Iteration 5219 training loss: 0.23174417756161422, test loss: 0.8007057197441825\n",
      "Iteration 5220 training loss: 0.23171741585445124, test loss: 0.8007235492747449\n",
      "Iteration 5221 training loss: 0.231690660167908, test loss: 0.8007413780035663\n",
      "Iteration 5222 training loss: 0.23166391049991172, test loss: 0.8007592059306083\n",
      "Iteration 5223 training loss: 0.23163716684839036, test loss: 0.800777033055833\n",
      "Iteration 5224 training loss: 0.23161042921127306, test loss: 0.8007948593792023\n",
      "Iteration 5225 training loss: 0.2315836975864899, test loss: 0.8008126849006781\n",
      "Iteration 5226 training loss: 0.23155697197197175, test loss: 0.8008305096202225\n",
      "Iteration 5227 training loss: 0.2315302523656508, test loss: 0.8008483335377973\n",
      "Iteration 5228 training loss: 0.23150353876545993, test loss: 0.8008661566533648\n",
      "Iteration 5229 training loss: 0.23147683116933324, test loss: 0.8008839789668871\n",
      "Iteration 5230 training loss: 0.23145012957520564, test loss: 0.8009018004783266\n",
      "Iteration 5231 training loss: 0.23142343398101303, test loss: 0.8009196211876453\n",
      "Iteration 5232 training loss: 0.23139674438469235, test loss: 0.8009374410948057\n",
      "Iteration 5233 training loss: 0.2313700607841815, test loss: 0.8009552601997699\n",
      "Iteration 5234 training loss: 0.2313433831774195, test loss: 0.8009730785025008\n",
      "Iteration 5235 training loss: 0.23131671156234598, test loss: 0.8009908960029604\n",
      "Iteration 5236 training loss: 0.231290045936902, test loss: 0.8010087127011113\n",
      "Iteration 5237 training loss: 0.23126338629902926, test loss: 0.8010265285969159\n",
      "Iteration 5238 training loss: 0.23123673264667055, test loss: 0.8010443436903375\n",
      "Iteration 5239 training loss: 0.23121008497776968, test loss: 0.8010621579813383\n",
      "Iteration 5240 training loss: 0.23118344329027138, test loss: 0.8010799714698807\n",
      "Iteration 5241 training loss: 0.23115680758212134, test loss: 0.8010977841559284\n",
      "Iteration 5242 training loss: 0.23113017785126636, test loss: 0.8011155960394432\n",
      "Iteration 5243 training loss: 0.23110355409565403, test loss: 0.8011334071203886\n",
      "Iteration 5244 training loss: 0.23107693631323303, test loss: 0.8011512173987273\n",
      "Iteration 5245 training loss: 0.2310503245019529, test loss: 0.8011690268744224\n",
      "Iteration 5246 training loss: 0.23102371865976426, test loss: 0.8011868355474369\n",
      "Iteration 5247 training loss: 0.23099711878461882, test loss: 0.8012046434177338\n",
      "Iteration 5248 training loss: 0.23097052487446887, test loss: 0.8012224504852762\n",
      "Iteration 5249 training loss: 0.23094393692726808, test loss: 0.8012402567500272\n",
      "Iteration 5250 training loss: 0.2309173549409709, test loss: 0.8012580622119505\n",
      "Iteration 5251 training loss: 0.23089077891353266, test loss: 0.8012758668710089\n",
      "Iteration 5252 training loss: 0.23086420884290987, test loss: 0.8012936707271657\n",
      "Iteration 5253 training loss: 0.23083764472705992, test loss: 0.801311473780385\n",
      "Iteration 5254 training loss: 0.23081108656394106, test loss: 0.801329276030629\n",
      "Iteration 5255 training loss: 0.2307845343515126, test loss: 0.8013470774778626\n",
      "Iteration 5256 training loss: 0.230757988087735, test loss: 0.801364878122048\n",
      "Iteration 5257 training loss: 0.2307314477705693, test loss: 0.8013826779631499\n",
      "Iteration 5258 training loss: 0.2307049133979778, test loss: 0.8014004770011314\n",
      "Iteration 5259 training loss: 0.23067838496792362, test loss: 0.8014182752359561\n",
      "Iteration 5260 training loss: 0.23065186247837088, test loss: 0.8014360726675879\n",
      "Iteration 5261 training loss: 0.2306253459272848, test loss: 0.8014538692959905\n",
      "Iteration 5262 training loss: 0.2305988353126313, test loss: 0.8014716651211279\n",
      "Iteration 5263 training loss: 0.2305723306323775, test loss: 0.8014894601429635\n",
      "Iteration 5264 training loss: 0.23054583188449132, test loss: 0.8015072543614622\n",
      "Iteration 5265 training loss: 0.23051933906694175, test loss: 0.8015250477765874\n",
      "Iteration 5266 training loss: 0.23049285217769858, test loss: 0.8015428403883031\n",
      "Iteration 5267 training loss: 0.2304663712147328, test loss: 0.8015606321965733\n",
      "Iteration 5268 training loss: 0.2304398961760162, test loss: 0.8015784232013625\n",
      "Iteration 5269 training loss: 0.23041342705952156, test loss: 0.8015962134026346\n",
      "Iteration 5270 training loss: 0.23038696386322255, test loss: 0.8016140028003539\n",
      "Iteration 5271 training loss: 0.230360506585094, test loss: 0.8016317913944847\n",
      "Iteration 5272 training loss: 0.23033405522311132, test loss: 0.8016495791849917\n",
      "Iteration 5273 training loss: 0.23030760977525142, test loss: 0.8016673661718388\n",
      "Iteration 5274 training loss: 0.2302811702394916, test loss: 0.8016851523549906\n",
      "Iteration 5275 training loss: 0.23025473661381066, test loss: 0.8017029377344118\n",
      "Iteration 5276 training loss: 0.23022830889618773, test loss: 0.8017207223100666\n",
      "Iteration 5277 training loss: 0.23020188708460348, test loss: 0.8017385060819198\n",
      "Iteration 5278 training loss: 0.23017547117703918, test loss: 0.8017562890499363\n",
      "Iteration 5279 training loss: 0.2301490611714773, test loss: 0.8017740712140803\n",
      "Iteration 5280 training loss: 0.23012265706590093, test loss: 0.8017918525743167\n",
      "Iteration 5281 training loss: 0.23009625885829435, test loss: 0.8018096331306105\n",
      "Iteration 5282 training loss: 0.23006986654664285, test loss: 0.8018274128829266\n",
      "Iteration 5283 training loss: 0.23004348012893247, test loss: 0.8018451918312295\n",
      "Iteration 5284 training loss: 0.23001709960315023, test loss: 0.8018629699754843\n",
      "Iteration 5285 training loss: 0.22999072496728434, test loss: 0.8018807473156565\n",
      "Iteration 5286 training loss: 0.22996435621932373, test loss: 0.8018985238517106\n",
      "Iteration 5287 training loss: 0.22993799335725817, test loss: 0.8019162995836115\n",
      "Iteration 5288 training loss: 0.22991163637907872, test loss: 0.8019340745113253\n",
      "Iteration 5289 training loss: 0.2298852852827771, test loss: 0.8019518486348161\n",
      "Iteration 5290 training loss: 0.22985894006634608, test loss: 0.8019696219540499\n",
      "Iteration 5291 training loss: 0.22983260072777945, test loss: 0.8019873944689918\n",
      "Iteration 5292 training loss: 0.22980626726507178, test loss: 0.8020051661796073\n",
      "Iteration 5293 training loss: 0.2297799396762187, test loss: 0.8020229370858613\n",
      "Iteration 5294 training loss: 0.22975361795921675, test loss: 0.8020407071877196\n",
      "Iteration 5295 training loss: 0.22972730211206346, test loss: 0.8020584764851478\n",
      "Iteration 5296 training loss: 0.22970099213275724, test loss: 0.8020762449781111\n",
      "Iteration 5297 training loss: 0.22967468801929744, test loss: 0.8020940126665757\n",
      "Iteration 5298 training loss: 0.22964838976968435, test loss: 0.8021117795505069\n",
      "Iteration 5299 training loss: 0.22962209738191927, test loss: 0.8021295456298704\n",
      "Iteration 5300 training loss: 0.22959581085400438, test loss: 0.8021473109046319\n",
      "Iteration 5301 training loss: 0.2295695301839428, test loss: 0.8021650753747572\n",
      "Iteration 5302 training loss: 0.22954325536973857, test loss: 0.8021828390402126\n",
      "Iteration 5303 training loss: 0.22951698640939686, test loss: 0.802200601900963\n",
      "Iteration 5304 training loss: 0.22949072330092343, test loss: 0.8022183639569755\n",
      "Iteration 5305 training loss: 0.22946446604232526, test loss: 0.8022361252082155\n",
      "Iteration 5306 training loss: 0.22943821463161013, test loss: 0.8022538856546492\n",
      "Iteration 5307 training loss: 0.22941196906678682, test loss: 0.8022716452962427\n",
      "Iteration 5308 training loss: 0.22938572934586504, test loss: 0.8022894041329622\n",
      "Iteration 5309 training loss: 0.22935949546685538, test loss: 0.8023071621647738\n",
      "Iteration 5310 training loss: 0.22933326742776947, test loss: 0.8023249193916437\n",
      "Iteration 5311 training loss: 0.22930704522661974, test loss: 0.8023426758135386\n",
      "Iteration 5312 training loss: 0.22928082886141965, test loss: 0.8023604314304242\n",
      "Iteration 5313 training loss: 0.22925461833018354, test loss: 0.8023781862422674\n",
      "Iteration 5314 training loss: 0.22922841363092675, test loss: 0.8023959402490349\n",
      "Iteration 5315 training loss: 0.22920221476166544, test loss: 0.8024136934506925\n",
      "Iteration 5316 training loss: 0.22917602172041684, test loss: 0.8024314458472073\n",
      "Iteration 5317 training loss: 0.22914983450519896, test loss: 0.8024491974385455\n",
      "Iteration 5318 training loss: 0.22912365311403093, test loss: 0.8024669482246742\n",
      "Iteration 5319 training loss: 0.22909747754493257, test loss: 0.80248469820556\n",
      "Iteration 5320 training loss: 0.22907130779592486, test loss: 0.8025024473811689\n",
      "Iteration 5321 training loss: 0.22904514386502947, test loss: 0.802520195751469\n",
      "Iteration 5322 training loss: 0.22901898575026927, test loss: 0.8025379433164264\n",
      "Iteration 5323 training loss: 0.2289928334496679, test loss: 0.802555690076008\n",
      "Iteration 5324 training loss: 0.22896668696124983, test loss: 0.8025734360301809\n",
      "Iteration 5325 training loss: 0.22894054628304075, test loss: 0.802591181178912\n",
      "Iteration 5326 training loss: 0.22891441141306693, test loss: 0.8026089255221688\n",
      "Iteration 5327 training loss: 0.2288882823493558, test loss: 0.8026266690599179\n",
      "Iteration 5328 training loss: 0.22886215908993568, test loss: 0.8026444117921265\n",
      "Iteration 5329 training loss: 0.2288360416328357, test loss: 0.802662153718762\n",
      "Iteration 5330 training loss: 0.228809929976086, test loss: 0.8026798948397916\n",
      "Iteration 5331 training loss: 0.22878382411771767, test loss: 0.8026976351551826\n",
      "Iteration 5332 training loss: 0.22875772405576272, test loss: 0.8027153746649022\n",
      "Iteration 5333 training loss: 0.22873162978825393, test loss: 0.8027331133689181\n",
      "Iteration 5334 training loss: 0.2287055413132253, test loss: 0.8027508512671975\n",
      "Iteration 5335 training loss: 0.2286794586287113, test loss: 0.8027685883597081\n",
      "Iteration 5336 training loss: 0.22865338173274782, test loss: 0.8027863246464172\n",
      "Iteration 5337 training loss: 0.22862731062337138, test loss: 0.802804060127293\n",
      "Iteration 5338 training loss: 0.22860124529861942, test loss: 0.8028217948023025\n",
      "Iteration 5339 training loss: 0.22857518575653035, test loss: 0.8028395286714135\n",
      "Iteration 5340 training loss: 0.22854913199514354, test loss: 0.8028572617345942\n",
      "Iteration 5341 training loss: 0.22852308401249924, test loss: 0.8028749939918121\n",
      "Iteration 5342 training loss: 0.2284970418066386, test loss: 0.8028927254430347\n",
      "Iteration 5343 training loss: 0.22847100537560372, test loss: 0.8029104560882305\n",
      "Iteration 5344 training loss: 0.2284449747174375, test loss: 0.8029281859273671\n",
      "Iteration 5345 training loss: 0.22841894983018396, test loss: 0.8029459149604128\n",
      "Iteration 5346 training loss: 0.2283929307118879, test loss: 0.8029636431873355\n",
      "Iteration 5347 training loss: 0.22836691736059497, test loss: 0.802981370608103\n",
      "Iteration 5348 training loss: 0.22834090977435192, test loss: 0.802999097222684\n",
      "Iteration 5349 training loss: 0.22831490795120624, test loss: 0.8030168230310464\n",
      "Iteration 5350 training loss: 0.22828891188920644, test loss: 0.8030345480331584\n",
      "Iteration 5351 training loss: 0.22826292158640193, test loss: 0.8030522722289882\n",
      "Iteration 5352 training loss: 0.22823693704084289, test loss: 0.8030699956185045\n",
      "Iteration 5353 training loss: 0.22821095825058066, test loss: 0.8030877182016756\n",
      "Iteration 5354 training loss: 0.22818498521366729, test loss: 0.80310543997847\n",
      "Iteration 5355 training loss: 0.2281590179281558, test loss: 0.8031231609488557\n",
      "Iteration 5356 training loss: 0.2281330563921001, test loss: 0.8031408811128017\n",
      "Iteration 5357 training loss: 0.22810710060355505, test loss: 0.803158600470277\n",
      "Iteration 5358 training loss: 0.22808115056057646, test loss: 0.8031763190212493\n",
      "Iteration 5359 training loss: 0.2280552062612208, test loss: 0.8031940367656881\n",
      "Iteration 5360 training loss: 0.22802926770354584, test loss: 0.8032117537035616\n",
      "Iteration 5361 training loss: 0.22800333488560992, test loss: 0.803229469834839\n",
      "Iteration 5362 training loss: 0.2279774078054725, test loss: 0.8032471851594886\n",
      "Iteration 5363 training loss: 0.22795148646119376, test loss: 0.8032648996774798\n",
      "Iteration 5364 training loss: 0.22792557085083492, test loss: 0.8032826133887816\n",
      "Iteration 5365 training loss: 0.22789966097245803, test loss: 0.8033003262933627\n",
      "Iteration 5366 training loss: 0.22787375682412617, test loss: 0.8033180383911922\n",
      "Iteration 5367 training loss: 0.22784785840390306, test loss: 0.8033357496822392\n",
      "Iteration 5368 training loss: 0.22782196570985366, test loss: 0.8033534601664729\n",
      "Iteration 5369 training loss: 0.22779607874004357, test loss: 0.8033711698438627\n",
      "Iteration 5370 training loss: 0.22777019749253935, test loss: 0.8033888787143773\n",
      "Iteration 5371 training loss: 0.22774432196540864, test loss: 0.8034065867779864\n",
      "Iteration 5372 training loss: 0.22771845215671968, test loss: 0.8034242940346593\n",
      "Iteration 5373 training loss: 0.2276925880645417, test loss: 0.8034420004843653\n",
      "Iteration 5374 training loss: 0.22766672968694512, test loss: 0.8034597061270736\n",
      "Iteration 5375 training loss: 0.22764087702200084, test loss: 0.8034774109627543\n",
      "Iteration 5376 training loss: 0.227615030067781, test loss: 0.8034951149913763\n",
      "Iteration 5377 training loss: 0.22758918882235826, test loss: 0.8035128182129092\n",
      "Iteration 5378 training loss: 0.22756335328380659, test loss: 0.8035305206273237\n",
      "Iteration 5379 training loss: 0.22753752345020056, test loss: 0.8035482222345883\n",
      "Iteration 5380 training loss: 0.2275116993196158, test loss: 0.8035659230346728\n",
      "Iteration 5381 training loss: 0.2274858808901287, test loss: 0.8035836230275478\n",
      "Iteration 5382 training loss: 0.22746006815981665, test loss: 0.8036013222131824\n",
      "Iteration 5383 training loss: 0.22743426112675794, test loss: 0.8036190205915466\n",
      "Iteration 5384 training loss: 0.22740845978903165, test loss: 0.8036367181626106\n",
      "Iteration 5385 training loss: 0.2273826641447179, test loss: 0.803654414926344\n",
      "Iteration 5386 training loss: 0.2273568741918975, test loss: 0.8036721108827172\n",
      "Iteration 5387 training loss: 0.2273310899286524, test loss: 0.8036898060317003\n",
      "Iteration 5388 training loss: 0.22730531135306523, test loss: 0.8037075003732629\n",
      "Iteration 5389 training loss: 0.22727953846321955, test loss: 0.8037251939073756\n",
      "Iteration 5390 training loss: 0.2272537712571999, test loss: 0.8037428866340084\n",
      "Iteration 5391 training loss: 0.22722800973309168, test loss: 0.8037605785531322\n",
      "Iteration 5392 training loss: 0.22720225388898113, test loss: 0.8037782696647164\n",
      "Iteration 5393 training loss: 0.22717650372295545, test loss: 0.803795959968732\n",
      "Iteration 5394 training loss: 0.22715075923310263, test loss: 0.8038136494651493\n",
      "Iteration 5395 training loss: 0.22712502041751165, test loss: 0.8038313381539384\n",
      "Iteration 5396 training loss: 0.2270992872742722, test loss: 0.8038490260350705\n",
      "Iteration 5397 training loss: 0.22707355980147517, test loss: 0.8038667131085157\n",
      "Iteration 5398 training loss: 0.22704783799721212, test loss: 0.8038843993742445\n",
      "Iteration 5399 training loss: 0.2270221218595753, test loss: 0.8039020848322276\n",
      "Iteration 5400 training loss: 0.2269964113866584, test loss: 0.803919769482436\n",
      "Iteration 5401 training loss: 0.22697070657655538, test loss: 0.8039374533248408\n",
      "Iteration 5402 training loss: 0.22694500742736148, test loss: 0.8039551363594117\n",
      "Iteration 5403 training loss: 0.22691931393717288, test loss: 0.8039728185861206\n",
      "Iteration 5404 training loss: 0.2268936261040861, test loss: 0.8039905000049379\n",
      "Iteration 5405 training loss: 0.22686794392619922, test loss: 0.8040081806158347\n",
      "Iteration 5406 training loss: 0.2268422674016108, test loss: 0.8040258604187821\n",
      "Iteration 5407 training loss: 0.22681659652842034, test loss: 0.8040435394137505\n",
      "Iteration 5408 training loss: 0.22679093130472824, test loss: 0.8040612176007121\n",
      "Iteration 5409 training loss: 0.22676527172863592, test loss: 0.8040788949796374\n",
      "Iteration 5410 training loss: 0.22673961779824542, test loss: 0.8040965715504975\n",
      "Iteration 5411 training loss: 0.2267139695116599, test loss: 0.804114247313264\n",
      "Iteration 5412 training loss: 0.22668832686698315, test loss: 0.8041319222679079\n",
      "Iteration 5413 training loss: 0.22666268986232008, test loss: 0.8041495964144011\n",
      "Iteration 5414 training loss: 0.2266370584957764, test loss: 0.8041672697527141\n",
      "Iteration 5415 training loss: 0.22661143276545861, test loss: 0.8041849422828191\n",
      "Iteration 5416 training loss: 0.22658581266947417, test loss: 0.8042026140046874\n",
      "Iteration 5417 training loss: 0.2265601982059314, test loss: 0.8042202849182902\n",
      "Iteration 5418 training loss: 0.2265345893729394, test loss: 0.8042379550235994\n",
      "Iteration 5419 training loss: 0.22650898616860846, test loss: 0.804255624320587\n",
      "Iteration 5420 training loss: 0.2264833885910493, test loss: 0.8042732928092239\n",
      "Iteration 5421 training loss: 0.22645779663837373, test loss: 0.8042909604894823\n",
      "Iteration 5422 training loss: 0.22643221030869462, test loss: 0.804308627361334\n",
      "Iteration 5423 training loss: 0.2264066296001253, test loss: 0.8043262934247508\n",
      "Iteration 5424 training loss: 0.22638105451078036, test loss: 0.8043439586797044\n",
      "Iteration 5425 training loss: 0.22635548503877503, test loss: 0.804361623126167\n",
      "Iteration 5426 training loss: 0.22632992118222545, test loss: 0.8043792867641104\n",
      "Iteration 5427 training loss: 0.22630436293924874, test loss: 0.8043969495935067\n",
      "Iteration 5428 training loss: 0.22627881030796268, test loss: 0.8044146116143278\n",
      "Iteration 5429 training loss: 0.2262532632864862, test loss: 0.8044322728265462\n",
      "Iteration 5430 training loss: 0.22622772187293888, test loss: 0.8044499332301338\n",
      "Iteration 5431 training loss: 0.22620218606544124, test loss: 0.8044675928250629\n",
      "Iteration 5432 training loss: 0.22617665586211452, test loss: 0.8044852516113057\n",
      "Iteration 5433 training loss: 0.22615113126108125, test loss: 0.8045029095888345\n",
      "Iteration 5434 training loss: 0.22612561226046435, test loss: 0.8045205667576221\n",
      "Iteration 5435 training loss: 0.22610009885838783, test loss: 0.8045382231176402\n",
      "Iteration 5436 training loss: 0.22607459105297653, test loss: 0.8045558786688616\n",
      "Iteration 5437 training loss: 0.22604908884235617, test loss: 0.804573533411259\n",
      "Iteration 5438 training loss: 0.22602359222465335, test loss: 0.8045911873448052\n",
      "Iteration 5439 training loss: 0.22599810119799552, test loss: 0.8046088404694717\n",
      "Iteration 5440 training loss: 0.225972615760511, test loss: 0.8046264927852322\n",
      "Iteration 5441 training loss: 0.22594713591032886, test loss: 0.8046441442920589\n",
      "Iteration 5442 training loss: 0.22592166164557917, test loss: 0.8046617949899252\n",
      "Iteration 5443 training loss: 0.22589619296439287, test loss: 0.804679444878803\n",
      "Iteration 5444 training loss: 0.22587072986490161, test loss: 0.804697093958666\n",
      "Iteration 5445 training loss: 0.22584527234523818, test loss: 0.8047147422294864\n",
      "Iteration 5446 training loss: 0.22581982040353582, test loss: 0.8047323896912374\n",
      "Iteration 5447 training loss: 0.225794374037929, test loss: 0.8047500363438923\n",
      "Iteration 5448 training loss: 0.22576893324655292, test loss: 0.8047676821874238\n",
      "Iteration 5449 training loss: 0.22574349802754357, test loss: 0.8047853272218051\n",
      "Iteration 5450 training loss: 0.22571806837903793, test loss: 0.8048029714470092\n",
      "Iteration 5451 training loss: 0.22569264429917374, test loss: 0.8048206148630095\n",
      "Iteration 5452 training loss: 0.22566722578608953, test loss: 0.8048382574697793\n",
      "Iteration 5453 training loss: 0.2256418128379249, test loss: 0.8048558992672915\n",
      "Iteration 5454 training loss: 0.2256164054528201, test loss: 0.8048735402555197\n",
      "Iteration 5455 training loss: 0.22559100362891643, test loss: 0.8048911804344375\n",
      "Iteration 5456 training loss: 0.2255656073643558, test loss: 0.8049088198040181\n",
      "Iteration 5457 training loss: 0.22554021665728125, test loss: 0.804926458364235\n",
      "Iteration 5458 training loss: 0.2255148315058364, test loss: 0.8049440961150616\n",
      "Iteration 5459 training loss: 0.22548945190816594, test loss: 0.8049617330564715\n",
      "Iteration 5460 training loss: 0.2254640778624153, test loss: 0.8049793691884386\n",
      "Iteration 5461 training loss: 0.2254387093667309, test loss: 0.8049970045109364\n",
      "Iteration 5462 training loss: 0.2254133464192597, test loss: 0.8050146390239384\n",
      "Iteration 5463 training loss: 0.22538798901814988, test loss: 0.8050322727274188\n",
      "Iteration 5464 training loss: 0.2253626371615503, test loss: 0.805049905621351\n",
      "Iteration 5465 training loss: 0.22533729084761056, test loss: 0.8050675377057094\n",
      "Iteration 5466 training loss: 0.22531195007448138, test loss: 0.8050851689804673\n",
      "Iteration 5467 training loss: 0.22528661484031406, test loss: 0.805102799445599\n",
      "Iteration 5468 training loss: 0.22526128514326094, test loss: 0.8051204291010785\n",
      "Iteration 5469 training loss: 0.22523596098147514, test loss: 0.8051380579468798\n",
      "Iteration 5470 training loss: 0.22521064235311056, test loss: 0.8051556859829769\n",
      "Iteration 5471 training loss: 0.2251853292563221, test loss: 0.8051733132093442\n",
      "Iteration 5472 training loss: 0.22516002168926533, test loss: 0.8051909396259557\n",
      "Iteration 5473 training loss: 0.2251347196500968, test loss: 0.8052085652327857\n",
      "Iteration 5474 training loss: 0.22510942313697385, test loss: 0.8052261900298083\n",
      "Iteration 5475 training loss: 0.22508413214805464, test loss: 0.8052438140169986\n",
      "Iteration 5476 training loss: 0.2250588466814983, test loss: 0.80526143719433\n",
      "Iteration 5477 training loss: 0.2250335667354647, test loss: 0.8052790595617776\n",
      "Iteration 5478 training loss: 0.22500829230811453, test loss: 0.8052966811193155\n",
      "Iteration 5479 training loss: 0.22498302339760937, test loss: 0.8053143018669185\n",
      "Iteration 5480 training loss: 0.22495776000211157, test loss: 0.8053319218045611\n",
      "Iteration 5481 training loss: 0.22493250211978452, test loss: 0.805349540932218\n",
      "Iteration 5482 training loss: 0.22490724974879223, test loss: 0.8053671592498639\n",
      "Iteration 5483 training loss: 0.22488200288729965, test loss: 0.8053847767574733\n",
      "Iteration 5484 training loss: 0.22485676153347267, test loss: 0.8054023934550213\n",
      "Iteration 5485 training loss: 0.22483152568547776, test loss: 0.8054200093424823\n",
      "Iteration 5486 training loss: 0.22480629534148244, test loss: 0.8054376244198315\n",
      "Iteration 5487 training loss: 0.224781070499655, test loss: 0.8054552386870438\n",
      "Iteration 5488 training loss: 0.22475585115816468, test loss: 0.8054728521440943\n",
      "Iteration 5489 training loss: 0.22473063731518134, test loss: 0.8054904647909574\n",
      "Iteration 5490 training loss: 0.22470542896887585, test loss: 0.8055080766276088\n",
      "Iteration 5491 training loss: 0.22468022611741975, test loss: 0.8055256876540233\n",
      "Iteration 5492 training loss: 0.22465502875898583, test loss: 0.8055432978701763\n",
      "Iteration 5493 training loss: 0.2246298368917471, test loss: 0.8055609072760429\n",
      "Iteration 5494 training loss: 0.22460465051387785, test loss: 0.8055785158715983\n",
      "Iteration 5495 training loss: 0.22457946962355316, test loss: 0.8055961236568177\n",
      "Iteration 5496 training loss: 0.22455429421894868, test loss: 0.8056137306316763\n",
      "Iteration 5497 training loss: 0.22452912429824123, test loss: 0.8056313367961502\n",
      "Iteration 5498 training loss: 0.22450395985960828, test loss: 0.8056489421502143\n",
      "Iteration 5499 training loss: 0.22447880090122813, test loss: 0.8056665466938442\n",
      "Iteration 5500 training loss: 0.22445364742127993, test loss: 0.8056841504270154\n",
      "Iteration 5501 training loss: 0.2244284994179438, test loss: 0.8057017533497035\n",
      "Iteration 5502 training loss: 0.22440335688940047, test loss: 0.805719355461884\n",
      "Iteration 5503 training loss: 0.22437821983383158, test loss: 0.8057369567635332\n",
      "Iteration 5504 training loss: 0.2243530882494197, test loss: 0.8057545572546262\n",
      "Iteration 5505 training loss: 0.22432796213434816, test loss: 0.8057721569351386\n",
      "Iteration 5506 training loss: 0.22430284148680107, test loss: 0.8057897558050471\n",
      "Iteration 5507 training loss: 0.22427772630496348, test loss: 0.8058073538643269\n",
      "Iteration 5508 training loss: 0.22425261658702111, test loss: 0.8058249511129538\n",
      "Iteration 5509 training loss: 0.22422751233116073, test loss: 0.8058425475509042\n",
      "Iteration 5510 training loss: 0.2242024135355696, test loss: 0.8058601431781538\n",
      "Iteration 5511 training loss: 0.22417732019843625, test loss: 0.805877737994679\n",
      "Iteration 5512 training loss: 0.22415223231794965, test loss: 0.8058953320004556\n",
      "Iteration 5513 training loss: 0.22412714989229984, test loss: 0.8059129251954602\n",
      "Iteration 5514 training loss: 0.22410207291967757, test loss: 0.8059305175796683\n",
      "Iteration 5515 training loss: 0.22407700139827447, test loss: 0.8059481091530566\n",
      "Iteration 5516 training loss: 0.2240519353262829, test loss: 0.8059656999156015\n",
      "Iteration 5517 training loss: 0.22402687470189617, test loss: 0.8059832898672789\n",
      "Iteration 5518 training loss: 0.2240018195233083, test loss: 0.8060008790080655\n",
      "Iteration 5519 training loss: 0.22397676978871428, test loss: 0.8060184673379381\n",
      "Iteration 5520 training loss: 0.22395172549630976, test loss: 0.8060360548568728\n",
      "Iteration 5521 training loss: 0.2239266866442913, test loss: 0.8060536415648458\n",
      "Iteration 5522 training loss: 0.22390165323085626, test loss: 0.8060712274618341\n",
      "Iteration 5523 training loss: 0.22387662525420293, test loss: 0.8060888125478146\n",
      "Iteration 5524 training loss: 0.22385160271253024, test loss: 0.8061063968227633\n",
      "Iteration 5525 training loss: 0.22382658560403798, test loss: 0.8061239802866573\n",
      "Iteration 5526 training loss: 0.22380157392692687, test loss: 0.8061415629394735\n",
      "Iteration 5527 training loss: 0.2237765676793983, test loss: 0.8061591447811883\n",
      "Iteration 5528 training loss: 0.22375156685965464, test loss: 0.8061767258117789\n",
      "Iteration 5529 training loss: 0.22372657146589894, test loss: 0.8061943060312224\n",
      "Iteration 5530 training loss: 0.22370158149633526, test loss: 0.8062118854394953\n",
      "Iteration 5531 training loss: 0.22367659694916817, test loss: 0.8062294640365746\n",
      "Iteration 5532 training loss: 0.22365161782260337, test loss: 0.8062470418224377\n",
      "Iteration 5533 training loss: 0.22362664411484715, test loss: 0.8062646187970616\n",
      "Iteration 5534 training loss: 0.2236016758241067, test loss: 0.8062821949604236\n",
      "Iteration 5535 training loss: 0.22357671294859016, test loss: 0.8062997703125007\n",
      "Iteration 5536 training loss: 0.22355175548650622, test loss: 0.8063173448532699\n",
      "Iteration 5537 training loss: 0.22352680343606451, test loss: 0.8063349185827089\n",
      "Iteration 5538 training loss: 0.2235018567954756, test loss: 0.8063524915007948\n",
      "Iteration 5539 training loss: 0.2234769155629507, test loss: 0.8063700636075053\n",
      "Iteration 5540 training loss: 0.22345197973670186, test loss: 0.8063876349028175\n",
      "Iteration 5541 training loss: 0.22342704931494203, test loss: 0.8064052053867089\n",
      "Iteration 5542 training loss: 0.2234021242958849, test loss: 0.8064227750591572\n",
      "Iteration 5543 training loss: 0.22337720467774497, test loss: 0.8064403439201399\n",
      "Iteration 5544 training loss: 0.22335229045873764, test loss: 0.8064579119696343\n",
      "Iteration 5545 training loss: 0.223327381637079, test loss: 0.8064754792076186\n",
      "Iteration 5546 training loss: 0.22330247821098595, test loss: 0.8064930456340705\n",
      "Iteration 5547 training loss: 0.22327758017867635, test loss: 0.8065106112489675\n",
      "Iteration 5548 training loss: 0.22325268753836874, test loss: 0.8065281760522873\n",
      "Iteration 5549 training loss: 0.22322780028828249, test loss: 0.8065457400440081\n",
      "Iteration 5550 training loss: 0.22320291842663775, test loss: 0.8065633032241075\n",
      "Iteration 5551 training loss: 0.22317804195165555, test loss: 0.8065808655925634\n",
      "Iteration 5552 training loss: 0.22315317086155775, test loss: 0.8065984271493543\n",
      "Iteration 5553 training loss: 0.22312830515456694, test loss: 0.8066159878944579\n",
      "Iteration 5554 training loss: 0.2231034448289065, test loss: 0.8066335478278521\n",
      "Iteration 5555 training loss: 0.2230785898828007, test loss: 0.8066511069495154\n",
      "Iteration 5556 training loss: 0.2230537403144745, test loss: 0.8066686652594257\n",
      "Iteration 5557 training loss: 0.22302889612215376, test loss: 0.8066862227575614\n",
      "Iteration 5558 training loss: 0.22300405730406525, test loss: 0.8067037794439007\n",
      "Iteration 5559 training loss: 0.22297922385843624, test loss: 0.8067213353184219\n",
      "Iteration 5560 training loss: 0.22295439578349505, test loss: 0.8067388903811037\n",
      "Iteration 5561 training loss: 0.22292957307747074, test loss: 0.8067564446319241\n",
      "Iteration 5562 training loss: 0.22290475573859317, test loss: 0.8067739980708617\n",
      "Iteration 5563 training loss: 0.22287994376509299, test loss: 0.8067915506978951\n",
      "Iteration 5564 training loss: 0.22285513715520164, test loss: 0.8068091025130026\n",
      "Iteration 5565 training loss: 0.22283033590715143, test loss: 0.8068266535161632\n",
      "Iteration 5566 training loss: 0.22280554001917532, test loss: 0.8068442037073552\n",
      "Iteration 5567 training loss: 0.2227807494895073, test loss: 0.8068617530865575\n",
      "Iteration 5568 training loss: 0.22275596431638195, test loss: 0.8068793016537488\n",
      "Iteration 5569 training loss: 0.2227311844980348, test loss: 0.8068968494089079\n",
      "Iteration 5570 training loss: 0.22270641003270203, test loss: 0.8069143963520137\n",
      "Iteration 5571 training loss: 0.22268164091862078, test loss: 0.8069319424830447\n",
      "Iteration 5572 training loss: 0.22265687715402885, test loss: 0.8069494878019803\n",
      "Iteration 5573 training loss: 0.22263211873716496, test loss: 0.8069670323087996\n",
      "Iteration 5574 training loss: 0.22260736566626846, test loss: 0.806984576003481\n",
      "Iteration 5575 training loss: 0.22258261793957962, test loss: 0.8070021188860039\n",
      "Iteration 5576 training loss: 0.22255787555533962, test loss: 0.8070196609563477\n",
      "Iteration 5577 training loss: 0.22253313851179016, test loss: 0.807037202214491\n",
      "Iteration 5578 training loss: 0.22250840680717396, test loss: 0.8070547426604134\n",
      "Iteration 5579 training loss: 0.22248368043973438, test loss: 0.8070722822940941\n",
      "Iteration 5580 training loss: 0.22245895940771568, test loss: 0.8070898211155123\n",
      "Iteration 5581 training loss: 0.2224342437093629, test loss: 0.8071073591246476\n",
      "Iteration 5582 training loss: 0.22240953334292185, test loss: 0.8071248963214791\n",
      "Iteration 5583 training loss: 0.2223848283066391, test loss: 0.8071424327059863\n",
      "Iteration 5584 training loss: 0.22236012859876209, test loss: 0.8071599682781488\n",
      "Iteration 5585 training loss: 0.22233543421753896, test loss: 0.8071775030379462\n",
      "Iteration 5586 training loss: 0.22231074516121876, test loss: 0.8071950369853578\n",
      "Iteration 5587 training loss: 0.22228606142805124, test loss: 0.8072125701203636\n",
      "Iteration 5588 training loss: 0.22226138301628692, test loss: 0.8072301024429428\n",
      "Iteration 5589 training loss: 0.22223670992417718, test loss: 0.8072476339530754\n",
      "Iteration 5590 training loss: 0.22221204214997428, test loss: 0.8072651646507414\n",
      "Iteration 5591 training loss: 0.22218737969193098, test loss: 0.8072826945359203\n",
      "Iteration 5592 training loss: 0.22216272254830108, test loss: 0.8073002236085922\n",
      "Iteration 5593 training loss: 0.2221380707173391, test loss: 0.8073177518687366\n",
      "Iteration 5594 training loss: 0.2221134241973003, test loss: 0.8073352793163335\n",
      "Iteration 5595 training loss: 0.22208878298644077, test loss: 0.8073528059513638\n",
      "Iteration 5596 training loss: 0.22206414708301744, test loss: 0.8073703317738063\n",
      "Iteration 5597 training loss: 0.22203951648528789, test loss: 0.8073878567836418\n",
      "Iteration 5598 training loss: 0.2220148911915106, test loss: 0.8074053809808504\n",
      "Iteration 5599 training loss: 0.22199027119994486, test loss: 0.807422904365412\n",
      "Iteration 5600 training loss: 0.22196565650885067, test loss: 0.807440426937307\n",
      "Iteration 5601 training loss: 0.22194104711648874, test loss: 0.807457948696516\n",
      "Iteration 5602 training loss: 0.22191644302112068, test loss: 0.8074754696430186\n",
      "Iteration 5603 training loss: 0.22189184422100902, test loss: 0.8074929897767956\n",
      "Iteration 5604 training loss: 0.22186725071441665, test loss: 0.8075105090978277\n",
      "Iteration 5605 training loss: 0.2218426624996077, test loss: 0.8075280276060949\n",
      "Iteration 5606 training loss: 0.22181807957484684, test loss: 0.807545545301578\n",
      "Iteration 5607 training loss: 0.22179350193839953, test loss: 0.8075630621842572\n",
      "Iteration 5608 training loss: 0.22176892958853206, test loss: 0.8075805782541134\n",
      "Iteration 5609 training loss: 0.2217443625235115, test loss: 0.8075980935111271\n",
      "Iteration 5610 training loss: 0.22171980074160577, test loss: 0.8076156079552792\n",
      "Iteration 5611 training loss: 0.22169524424108336, test loss: 0.8076331215865504\n",
      "Iteration 5612 training loss: 0.22167069302021378, test loss: 0.8076506344049212\n",
      "Iteration 5613 training loss: 0.22164614707726715, test loss: 0.8076681464103727\n",
      "Iteration 5614 training loss: 0.22162160641051445, test loss: 0.8076856576028856\n",
      "Iteration 5615 training loss: 0.2215970710182274, test loss: 0.8077031679824411\n",
      "Iteration 5616 training loss: 0.22157254089867853, test loss: 0.8077206775490199\n",
      "Iteration 5617 training loss: 0.22154801605014118, test loss: 0.807738186302603\n",
      "Iteration 5618 training loss: 0.22152349647088931, test loss: 0.8077556942431716\n",
      "Iteration 5619 training loss: 0.2214989821591979, test loss: 0.8077732013707072\n",
      "Iteration 5620 training loss: 0.22147447311334248, test loss: 0.80779070768519\n",
      "Iteration 5621 training loss: 0.2214499693315994, test loss: 0.807808213186602\n",
      "Iteration 5622 training loss: 0.22142547081224595, test loss: 0.8078257178749241\n",
      "Iteration 5623 training loss: 0.22140097755356009, test loss: 0.8078432217501375\n",
      "Iteration 5624 training loss: 0.22137648955382042, test loss: 0.8078607248122239\n",
      "Iteration 5625 training loss: 0.22135200681130648, test loss: 0.8078782270611645\n",
      "Iteration 5626 training loss: 0.22132752932429856, test loss: 0.8078957284969406\n",
      "Iteration 5627 training loss: 0.22130305709107775, test loss: 0.8079132291195338\n",
      "Iteration 5628 training loss: 0.22127859010992576, test loss: 0.8079307289289258\n",
      "Iteration 5629 training loss: 0.2212541283791253, test loss: 0.8079482279250979\n",
      "Iteration 5630 training loss: 0.22122967189695955, test loss: 0.8079657261080317\n",
      "Iteration 5631 training loss: 0.22120522066171283, test loss: 0.8079832234777091\n",
      "Iteration 5632 training loss: 0.22118077467166994, test loss: 0.8080007200341116\n",
      "Iteration 5633 training loss: 0.22115633392511655, test loss: 0.8080182157772212\n",
      "Iteration 5634 training loss: 0.2211318984203391, test loss: 0.8080357107070193\n",
      "Iteration 5635 training loss: 0.2211074681556248, test loss: 0.8080532048234879\n",
      "Iteration 5636 training loss: 0.22108304312926166, test loss: 0.808070698126609\n",
      "Iteration 5637 training loss: 0.22105862333953838, test loss: 0.8080881906163644\n",
      "Iteration 5638 training loss: 0.22103420878474445, test loss: 0.8081056822927362\n",
      "Iteration 5639 training loss: 0.22100979946317026, test loss: 0.8081231731557063\n",
      "Iteration 5640 training loss: 0.22098539537310674, test loss: 0.8081406632052569\n",
      "Iteration 5641 training loss: 0.22096099651284576, test loss: 0.80815815244137\n",
      "Iteration 5642 training loss: 0.2209366028806799, test loss: 0.8081756408640282\n",
      "Iteration 5643 training loss: 0.2209122144749025, test loss: 0.8081931284732128\n",
      "Iteration 5644 training loss: 0.22088783129380765, test loss: 0.8082106152689069\n",
      "Iteration 5645 training loss: 0.22086345333569027, test loss: 0.8082281012510921\n",
      "Iteration 5646 training loss: 0.2208390805988459, test loss: 0.8082455864197513\n",
      "Iteration 5647 training loss: 0.22081471308157102, test loss: 0.8082630707748669\n",
      "Iteration 5648 training loss: 0.22079035078216283, test loss: 0.8082805543164211\n",
      "Iteration 5649 training loss: 0.22076599369891925, test loss: 0.8082980370443963\n",
      "Iteration 5650 training loss: 0.2207416418301389, test loss: 0.8083155189587753\n",
      "Iteration 5651 training loss: 0.22071729517412128, test loss: 0.8083330000595402\n",
      "Iteration 5652 training loss: 0.22069295372916659, test loss: 0.8083504803466742\n",
      "Iteration 5653 training loss: 0.22066861749357577, test loss: 0.8083679598201594\n",
      "Iteration 5654 training loss: 0.2206442864656507, test loss: 0.8083854384799792\n",
      "Iteration 5655 training loss: 0.22061996064369369, test loss: 0.8084029163261158\n",
      "Iteration 5656 training loss: 0.22059564002600818, test loss: 0.8084203933585523\n",
      "Iteration 5657 training loss: 0.220571324610898, test loss: 0.8084378695772714\n",
      "Iteration 5658 training loss: 0.22054701439666807, test loss: 0.8084553449822556\n",
      "Iteration 5659 training loss: 0.22052270938162388, test loss: 0.8084728195734885\n",
      "Iteration 5660 training loss: 0.22049840956407166, test loss: 0.8084902933509528\n",
      "Iteration 5661 training loss: 0.22047411494231858, test loss: 0.8085077663146316\n",
      "Iteration 5662 training loss: 0.22044982551467232, test loss: 0.808525238464508\n",
      "Iteration 5663 training loss: 0.2204255412794416, test loss: 0.8085427098005646\n",
      "Iteration 5664 training loss: 0.22040126223493559, test loss: 0.8085601803227852\n",
      "Iteration 5665 training loss: 0.22037698837946446, test loss: 0.808577650031153\n",
      "Iteration 5666 training loss: 0.22035271971133893, test loss: 0.8085951189256507\n",
      "Iteration 5667 training loss: 0.22032845622887073, test loss: 0.808612587006262\n",
      "Iteration 5668 training loss: 0.22030419793037215, test loss: 0.8086300542729704\n",
      "Iteration 5669 training loss: 0.22027994481415625, test loss: 0.8086475207257591\n",
      "Iteration 5670 training loss: 0.22025569687853688, test loss: 0.8086649863646114\n",
      "Iteration 5671 training loss: 0.22023145412182865, test loss: 0.8086824511895108\n",
      "Iteration 5672 training loss: 0.22020721654234698, test loss: 0.8086999152004408\n",
      "Iteration 5673 training loss: 0.22018298413840792, test loss: 0.8087173783973856\n",
      "Iteration 5674 training loss: 0.22015875690832826, test loss: 0.8087348407803279\n",
      "Iteration 5675 training loss: 0.22013453485042572, test loss: 0.8087523023492517\n",
      "Iteration 5676 training loss: 0.22011031796301864, test loss: 0.808769763104141\n",
      "Iteration 5677 training loss: 0.22008610624442612, test loss: 0.808787223044979\n",
      "Iteration 5678 training loss: 0.220061899692968, test loss: 0.80880468217175\n",
      "Iteration 5679 training loss: 0.22003769830696498, test loss: 0.8088221404844373\n",
      "Iteration 5680 training loss: 0.2200135020847383, test loss: 0.8088395979830255\n",
      "Iteration 5681 training loss: 0.2199893110246101, test loss: 0.8088570546674982\n",
      "Iteration 5682 training loss: 0.21996512512490332, test loss: 0.8088745105378391\n",
      "Iteration 5683 training loss: 0.2199409443839415, test loss: 0.8088919655940326\n",
      "Iteration 5684 training loss: 0.21991676880004896, test loss: 0.8089094198360627\n",
      "Iteration 5685 training loss: 0.21989259837155095, test loss: 0.8089268732639131\n",
      "Iteration 5686 training loss: 0.21986843309677317, test loss: 0.8089443258775687\n",
      "Iteration 5687 training loss: 0.21984427297404222, test loss: 0.8089617776770129\n",
      "Iteration 5688 training loss: 0.21982011800168552, test loss: 0.8089792286622305\n",
      "Iteration 5689 training loss: 0.21979596817803118, test loss: 0.8089966788332055\n",
      "Iteration 5690 training loss: 0.21977182350140786, test loss: 0.8090141281899226\n",
      "Iteration 5691 training loss: 0.21974768397014535, test loss: 0.8090315767323657\n",
      "Iteration 5692 training loss: 0.21972354958257384, test loss: 0.8090490244605196\n",
      "Iteration 5693 training loss: 0.21969942033702436, test loss: 0.8090664713743686\n",
      "Iteration 5694 training loss: 0.21967529623182888, test loss: 0.8090839174738974\n",
      "Iteration 5695 training loss: 0.21965117726531974, test loss: 0.8091013627590903\n",
      "Iteration 5696 training loss: 0.2196270634358304, test loss: 0.8091188072299319\n",
      "Iteration 5697 training loss: 0.21960295474169486, test loss: 0.809136250886407\n",
      "Iteration 5698 training loss: 0.2195788511812478, test loss: 0.8091536937285001\n",
      "Iteration 5699 training loss: 0.2195547527528249, test loss: 0.8091711357561966\n",
      "Iteration 5700 training loss: 0.21953065945476224, test loss: 0.8091885769694802\n",
      "Iteration 5701 training loss: 0.21950657128539694, test loss: 0.8092060173683365\n",
      "Iteration 5702 training loss: 0.2194824882430667, test loss: 0.8092234569527506\n",
      "Iteration 5703 training loss: 0.21945841032611, test loss: 0.8092408957227064\n",
      "Iteration 5704 training loss: 0.21943433753286612, test loss: 0.8092583336781897\n",
      "Iteration 5705 training loss: 0.21941026986167483, test loss: 0.8092757708191849\n",
      "Iteration 5706 training loss: 0.21938620731087702, test loss: 0.8092932071456781\n",
      "Iteration 5707 training loss: 0.21936214987881397, test loss: 0.8093106426576532\n",
      "Iteration 5708 training loss: 0.21933809756382802, test loss: 0.8093280773550963\n",
      "Iteration 5709 training loss: 0.21931405036426194, test loss: 0.8093455112379918\n",
      "Iteration 5710 training loss: 0.21929000827845938, test loss: 0.8093629443063254\n",
      "Iteration 5711 training loss: 0.21926597130476475, test loss: 0.8093803765600823\n",
      "Iteration 5712 training loss: 0.21924193944152323, test loss: 0.8093978079992477\n",
      "Iteration 5713 training loss: 0.2192179126870805, test loss: 0.8094152386238069\n",
      "Iteration 5714 training loss: 0.21919389103978335, test loss: 0.8094326684337456\n",
      "Iteration 5715 training loss: 0.21916987449797887, test loss: 0.8094500974290493\n",
      "Iteration 5716 training loss: 0.21914586306001527, test loss: 0.8094675256097028\n",
      "Iteration 5717 training loss: 0.21912185672424125, test loss: 0.8094849529756925\n",
      "Iteration 5718 training loss: 0.21909785548900643, test loss: 0.8095023795270035\n",
      "Iteration 5719 training loss: 0.21907385935266097, test loss: 0.8095198052636217\n",
      "Iteration 5720 training loss: 0.21904986831355594, test loss: 0.8095372301855325\n",
      "Iteration 5721 training loss: 0.2190258823700429, test loss: 0.809554654292722\n",
      "Iteration 5722 training loss: 0.21900190152047444, test loss: 0.8095720775851756\n",
      "Iteration 5723 training loss: 0.21897792576320366, test loss: 0.809589500062879\n",
      "Iteration 5724 training loss: 0.2189539550965845, test loss: 0.8096069217258188\n",
      "Iteration 5725 training loss: 0.2189299895189716, test loss: 0.80962434257398\n",
      "Iteration 5726 training loss: 0.2189060290287204, test loss: 0.8096417626073491\n",
      "Iteration 5727 training loss: 0.21888207362418682, test loss: 0.8096591818259116\n",
      "Iteration 5728 training loss: 0.21885812330372778, test loss: 0.8096766002296542\n",
      "Iteration 5729 training loss: 0.21883417806570085, test loss: 0.8096940178185625\n",
      "Iteration 5730 training loss: 0.2188102379084643, test loss: 0.809711434592623\n",
      "Iteration 5731 training loss: 0.2187863028303771, test loss: 0.8097288505518213\n",
      "Iteration 5732 training loss: 0.21876237282979905, test loss: 0.8097462656961442\n",
      "Iteration 5733 training loss: 0.2187384479050906, test loss: 0.8097636800255774\n",
      "Iteration 5734 training loss: 0.21871452805461283, test loss: 0.8097810935401076\n",
      "Iteration 5735 training loss: 0.2186906132767278, test loss: 0.809798506239721\n",
      "Iteration 5736 training loss: 0.21866670356979812, test loss: 0.8098159181244041\n",
      "Iteration 5737 training loss: 0.21864279893218708, test loss: 0.8098333291941433\n",
      "Iteration 5738 training loss: 0.2186188993622589, test loss: 0.8098507394489248\n",
      "Iteration 5739 training loss: 0.2185950048583782, test loss: 0.8098681488887357\n",
      "Iteration 5740 training loss: 0.21857111541891072, test loss: 0.809885557513562\n",
      "Iteration 5741 training loss: 0.21854723104222265, test loss: 0.8099029653233902\n",
      "Iteration 5742 training loss: 0.21852335172668086, test loss: 0.809920372318208\n",
      "Iteration 5743 training loss: 0.21849947747065326, test loss: 0.8099377784980006\n",
      "Iteration 5744 training loss: 0.21847560827250814, test loss: 0.8099551838627558\n",
      "Iteration 5745 training loss: 0.21845174413061474, test loss: 0.8099725884124601\n",
      "Iteration 5746 training loss: 0.2184278850433428, test loss: 0.8099899921471003\n",
      "Iteration 5747 training loss: 0.218404031009063, test loss: 0.8100073950666634\n",
      "Iteration 5748 training loss: 0.2183801820261467, test loss: 0.8100247971711361\n",
      "Iteration 5749 training loss: 0.21835633809296584, test loss: 0.8100421984605057\n",
      "Iteration 5750 training loss: 0.21833249920789324, test loss: 0.8100595989347584\n",
      "Iteration 5751 training loss: 0.21830866536930232, test loss: 0.8100769985938824\n",
      "Iteration 5752 training loss: 0.21828483657556735, test loss: 0.8100943974378639\n",
      "Iteration 5753 training loss: 0.21826101282506316, test loss: 0.8101117954666907\n",
      "Iteration 5754 training loss: 0.21823719411616538, test loss: 0.8101291926803492\n",
      "Iteration 5755 training loss: 0.21821338044725044, test loss: 0.810146589078827\n",
      "Iteration 5756 training loss: 0.21818957181669532, test loss: 0.8101639846621117\n",
      "Iteration 5757 training loss: 0.21816576822287786, test loss: 0.8101813794301904\n",
      "Iteration 5758 training loss: 0.21814196966417648, test loss: 0.8101987733830502\n",
      "Iteration 5759 training loss: 0.21811817613897047, test loss: 0.8102161665206787\n",
      "Iteration 5760 training loss: 0.21809438764563968, test loss: 0.8102335588430635\n",
      "Iteration 5761 training loss: 0.21807060418256483, test loss: 0.8102509503501919\n",
      "Iteration 5762 training loss: 0.21804682574812734, test loss: 0.8102683410420513\n",
      "Iteration 5763 training loss: 0.21802305234070907, test loss: 0.8102857309186295\n",
      "Iteration 5764 training loss: 0.21799928395869297, test loss: 0.8103031199799141\n",
      "Iteration 5765 training loss: 0.2179755206004625, test loss: 0.8103205082258929\n",
      "Iteration 5766 training loss: 0.21795176226440185, test loss: 0.8103378956565533\n",
      "Iteration 5767 training loss: 0.21792800894889594, test loss: 0.8103552822718829\n",
      "Iteration 5768 training loss: 0.21790426065233046, test loss: 0.8103726680718703\n",
      "Iteration 5769 training loss: 0.21788051737309172, test loss: 0.8103900530565024\n",
      "Iteration 5770 training loss: 0.21785677910956686, test loss: 0.8104074372257678\n",
      "Iteration 5771 training loss: 0.21783304586014351, test loss: 0.8104248205796541\n",
      "Iteration 5772 training loss: 0.21780931762321026, test loss: 0.810442203118149\n",
      "Iteration 5773 training loss: 0.2177855943971563, test loss: 0.8104595848412409\n",
      "Iteration 5774 training loss: 0.21776187618037146, test loss: 0.8104769657489178\n",
      "Iteration 5775 training loss: 0.2177381629712464, test loss: 0.8104943458411681\n",
      "Iteration 5776 training loss: 0.21771445476817247, test loss: 0.8105117251179793\n",
      "Iteration 5777 training loss: 0.21769075156954165, test loss: 0.8105291035793398\n",
      "Iteration 5778 training loss: 0.21766705337374664, test loss: 0.8105464812252384\n",
      "Iteration 5779 training loss: 0.21764336017918104, test loss: 0.8105638580556623\n",
      "Iteration 5780 training loss: 0.21761967198423884, test loss: 0.8105812340706008\n",
      "Iteration 5781 training loss: 0.2175959887873151, test loss: 0.8105986092700419\n",
      "Iteration 5782 training loss: 0.2175723105868051, test loss: 0.8106159836539738\n",
      "Iteration 5783 training loss: 0.21754863738110536, test loss: 0.8106333572223855\n",
      "Iteration 5784 training loss: 0.21752496916861275, test loss: 0.810650729975265\n",
      "Iteration 5785 training loss: 0.217501305947725, test loss: 0.8106681019126011\n",
      "Iteration 5786 training loss: 0.21747764771684047, test loss: 0.8106854730343817\n",
      "Iteration 5787 training loss: 0.21745399447435826, test loss: 0.8107028433405966\n",
      "Iteration 5788 training loss: 0.21743034621867816, test loss: 0.8107202128312337\n",
      "Iteration 5789 training loss: 0.21740670294820066, test loss: 0.810737581506282\n",
      "Iteration 5790 training loss: 0.21738306466132704, test loss: 0.8107549493657298\n",
      "Iteration 5791 training loss: 0.21735943135645913, test loss: 0.8107723164095665\n",
      "Iteration 5792 training loss: 0.21733580303199956, test loss: 0.8107896826377806\n",
      "Iteration 5793 training loss: 0.2173121796863517, test loss: 0.810807048050361\n",
      "Iteration 5794 training loss: 0.21728856131791952, test loss: 0.8108244126472967\n",
      "Iteration 5795 training loss: 0.21726494792510767, test loss: 0.8108417764285766\n",
      "Iteration 5796 training loss: 0.21724133950632168, test loss: 0.8108591393941899\n",
      "Iteration 5797 training loss: 0.21721773605996758, test loss: 0.8108765015441254\n",
      "Iteration 5798 training loss: 0.2171941375844523, test loss: 0.8108938628783725\n",
      "Iteration 5799 training loss: 0.21717054407818326, test loss: 0.8109112233969197\n",
      "Iteration 5800 training loss: 0.2171469555395687, test loss: 0.8109285830997572\n",
      "Iteration 5801 training loss: 0.2171233719670176, test loss: 0.8109459419868738\n",
      "Iteration 5802 training loss: 0.21709979335893947, test loss: 0.810963300058258\n",
      "Iteration 5803 training loss: 0.21707621971374466, test loss: 0.8109806573139002\n",
      "Iteration 5804 training loss: 0.21705265102984428, test loss: 0.8109980137537894\n",
      "Iteration 5805 training loss: 0.2170290873056499, test loss: 0.811015369377915\n",
      "Iteration 5806 training loss: 0.217005528539574, test loss: 0.8110327241862662\n",
      "Iteration 5807 training loss: 0.21698197473002964, test loss: 0.8110500781788328\n",
      "Iteration 5808 training loss: 0.21695842587543074, test loss: 0.811067431355604\n",
      "Iteration 5809 training loss: 0.2169348819741916, test loss: 0.8110847837165699\n",
      "Iteration 5810 training loss: 0.2169113430247276, test loss: 0.8111021352617197\n",
      "Iteration 5811 training loss: 0.2168878090254545, test loss: 0.8111194859910432\n",
      "Iteration 5812 training loss: 0.21686427997478888, test loss: 0.8111368359045301\n",
      "Iteration 5813 training loss: 0.21684075587114812, test loss: 0.8111541850021703\n",
      "Iteration 5814 training loss: 0.21681723671295008, test loss: 0.8111715332839533\n",
      "Iteration 5815 training loss: 0.21679372249861348, test loss: 0.811188880749869\n",
      "Iteration 5816 training loss: 0.21677021322655762, test loss: 0.8112062273999072\n",
      "Iteration 5817 training loss: 0.21674670889520264, test loss: 0.8112235732340584\n",
      "Iteration 5818 training loss: 0.21672320950296925, test loss: 0.8112409182523118\n",
      "Iteration 5819 training loss: 0.21669971504827884, test loss: 0.8112582624546578\n",
      "Iteration 5820 training loss: 0.2166762255295535, test loss: 0.8112756058410865\n",
      "Iteration 5821 training loss: 0.21665274094521617, test loss: 0.8112929484115879\n",
      "Iteration 5822 training loss: 0.21662926129369026, test loss: 0.8113102901661522\n",
      "Iteration 5823 training loss: 0.2166057865734, test loss: 0.8113276311047692\n",
      "Iteration 5824 training loss: 0.21658231678277023, test loss: 0.8113449712274299\n",
      "Iteration 5825 training loss: 0.21655885192022653, test loss: 0.8113623105341237\n",
      "Iteration 5826 training loss: 0.21653539198419527, test loss: 0.8113796490248415\n",
      "Iteration 5827 training loss: 0.2165119369731033, test loss: 0.8113969866995733\n",
      "Iteration 5828 training loss: 0.21648848688537825, test loss: 0.8114143235583098\n",
      "Iteration 5829 training loss: 0.21646504171944853, test loss: 0.8114316596010412\n",
      "Iteration 5830 training loss: 0.2164416014737431, test loss: 0.8114489948277579\n",
      "Iteration 5831 training loss: 0.21641816614669168, test loss: 0.811466329238451\n",
      "Iteration 5832 training loss: 0.21639473573672466, test loss: 0.8114836628331102\n",
      "Iteration 5833 training loss: 0.21637131024227316, test loss: 0.8115009956117271\n",
      "Iteration 5834 training loss: 0.21634788966176882, test loss: 0.8115183275742913\n",
      "Iteration 5835 training loss: 0.2163244739936442, test loss: 0.8115356587207945\n",
      "Iteration 5836 training loss: 0.2163010632363324, test loss: 0.8115529890512266\n",
      "Iteration 5837 training loss: 0.2162776573882673, test loss: 0.8115703185655789\n",
      "Iteration 5838 training loss: 0.21625425644788326, test loss: 0.8115876472638419\n",
      "Iteration 5839 training loss: 0.21623086041361558, test loss: 0.8116049751460066\n",
      "Iteration 5840 training loss: 0.21620746928390014, test loss: 0.811622302212064\n",
      "Iteration 5841 training loss: 0.2161840830571734, test loss: 0.8116396284620049\n",
      "Iteration 5842 training loss: 0.21616070173187274, test loss: 0.8116569538958206\n",
      "Iteration 5843 training loss: 0.21613732530643595, test loss: 0.8116742785135017\n",
      "Iteration 5844 training loss: 0.21611395377930173, test loss: 0.8116916023150395\n",
      "Iteration 5845 training loss: 0.21609058714890925, test loss: 0.811708925300425\n",
      "Iteration 5846 training loss: 0.21606722541369858, test loss: 0.8117262474696497\n",
      "Iteration 5847 training loss: 0.21604386857211028, test loss: 0.8117435688227045\n",
      "Iteration 5848 training loss: 0.21602051662258576, test loss: 0.8117608893595807\n",
      "Iteration 5849 training loss: 0.215997169563567, test loss: 0.8117782090802693\n",
      "Iteration 5850 training loss: 0.2159738273934967, test loss: 0.8117955279847621\n",
      "Iteration 5851 training loss: 0.21595049011081816, test loss: 0.8118128460730503\n",
      "Iteration 5852 training loss: 0.21592715771397553, test loss: 0.8118301633451254\n",
      "Iteration 5853 training loss: 0.21590383020141343, test loss: 0.811847479800979\n",
      "Iteration 5854 training loss: 0.2158805075715774, test loss: 0.811864795440602\n",
      "Iteration 5855 training loss: 0.2158571898229133, test loss: 0.8118821102639865\n",
      "Iteration 5856 training loss: 0.21583387695386805, test loss: 0.811899424271124\n",
      "Iteration 5857 training loss: 0.21581056896288908, test loss: 0.811916737462006\n",
      "Iteration 5858 training loss: 0.21578726584842448, test loss: 0.8119340498366244\n",
      "Iteration 5859 training loss: 0.21576396760892308, test loss: 0.8119513613949705\n",
      "Iteration 5860 training loss: 0.21574067424283425, test loss: 0.8119686721370363\n",
      "Iteration 5861 training loss: 0.21571738574860821, test loss: 0.8119859820628137\n",
      "Iteration 5862 training loss: 0.21569410212469572, test loss: 0.8120032911722944\n",
      "Iteration 5863 training loss: 0.21567082336954835, test loss: 0.8120205994654703\n",
      "Iteration 5864 training loss: 0.21564754948161818, test loss: 0.8120379069423335\n",
      "Iteration 5865 training loss: 0.2156242804593581, test loss: 0.8120552136028757\n",
      "Iteration 5866 training loss: 0.21560101630122158, test loss: 0.8120725194470891\n",
      "Iteration 5867 training loss: 0.21557775700566284, test loss: 0.8120898244749656\n",
      "Iteration 5868 training loss: 0.21555450257113676, test loss: 0.8121071286864973\n",
      "Iteration 5869 training loss: 0.21553125299609882, test loss: 0.8121244320816764\n",
      "Iteration 5870 training loss: 0.21550800827900526, test loss: 0.8121417346604956\n",
      "Iteration 5871 training loss: 0.21548476841831296, test loss: 0.8121590364229462\n",
      "Iteration 5872 training loss: 0.21546153341247945, test loss: 0.812176337369021\n",
      "Iteration 5873 training loss: 0.2154383032599629, test loss: 0.8121936374987121\n",
      "Iteration 5874 training loss: 0.21541507795922238, test loss: 0.812210936812012\n",
      "Iteration 5875 training loss: 0.2153918575087172, test loss: 0.812228235308913\n",
      "Iteration 5876 training loss: 0.21536864190690777, test loss: 0.8122455329894075\n",
      "Iteration 5877 training loss: 0.2153454311522549, test loss: 0.812262829853488\n",
      "Iteration 5878 training loss: 0.2153222252432202, test loss: 0.8122801259011472\n",
      "Iteration 5879 training loss: 0.21529902417826588, test loss: 0.8122974211323775\n",
      "Iteration 5880 training loss: 0.21527582795585493, test loss: 0.8123147155471715\n",
      "Iteration 5881 training loss: 0.21525263657445076, test loss: 0.8123320091455217\n",
      "Iteration 5882 training loss: 0.21522945003251776, test loss: 0.8123493019274209\n",
      "Iteration 5883 training loss: 0.21520626832852077, test loss: 0.8123665938928624\n",
      "Iteration 5884 training loss: 0.21518309146092537, test loss: 0.812383885041838\n",
      "Iteration 5885 training loss: 0.2151599194281979, test loss: 0.8124011753743409\n",
      "Iteration 5886 training loss: 0.21513675222880513, test loss: 0.8124184648903641\n",
      "Iteration 5887 training loss: 0.21511358986121465, test loss: 0.8124357535899003\n",
      "Iteration 5888 training loss: 0.21509043232389474, test loss: 0.812453041472943\n",
      "Iteration 5889 training loss: 0.21506727961531433, test loss: 0.812470328539484\n",
      "Iteration 5890 training loss: 0.21504413173394296, test loss: 0.8124876147895175\n",
      "Iteration 5891 training loss: 0.21502098867825087, test loss: 0.812504900223036\n",
      "Iteration 5892 training loss: 0.21499785044670897, test loss: 0.8125221848400326\n",
      "Iteration 5893 training loss: 0.21497471703778875, test loss: 0.8125394686405009\n",
      "Iteration 5894 training loss: 0.21495158844996254, test loss: 0.8125567516244332\n",
      "Iteration 5895 training loss: 0.21492846468170312, test loss: 0.8125740337918237\n",
      "Iteration 5896 training loss: 0.21490534573148415, test loss: 0.8125913151426648\n",
      "Iteration 5897 training loss: 0.21488223159777978, test loss: 0.8126085956769504\n",
      "Iteration 5898 training loss: 0.2148591222790649, test loss: 0.8126258753946739\n",
      "Iteration 5899 training loss: 0.21483601777381497, test loss: 0.8126431542958283\n",
      "Iteration 5900 training loss: 0.21481291808050637, test loss: 0.8126604323804074\n",
      "Iteration 5901 training loss: 0.21478982319761578, test loss: 0.8126777096484044\n",
      "Iteration 5902 training loss: 0.21476673312362077, test loss: 0.8126949860998128\n",
      "Iteration 5903 training loss: 0.21474364785699956, test loss: 0.8127122617346263\n",
      "Iteration 5904 training loss: 0.21472056739623088, test loss: 0.8127295365528386\n",
      "Iteration 5905 training loss: 0.21469749173979438, test loss: 0.8127468105544433\n",
      "Iteration 5906 training loss: 0.21467442088617006, test loss: 0.8127640837394341\n",
      "Iteration 5907 training loss: 0.21465135483383885, test loss: 0.8127813561078048\n",
      "Iteration 5908 training loss: 0.2146282935812822, test loss: 0.8127986276595487\n",
      "Iteration 5909 training loss: 0.21460523712698212, test loss: 0.8128158983946601\n",
      "Iteration 5910 training loss: 0.21458218546942154, test loss: 0.8128331683131325\n",
      "Iteration 5911 training loss: 0.21455913860708384, test loss: 0.8128504374149603\n",
      "Iteration 5912 training loss: 0.21453609653845318, test loss: 0.8128677057001369\n",
      "Iteration 5913 training loss: 0.2145130592620142, test loss: 0.8128849731686566\n",
      "Iteration 5914 training loss: 0.21449002677625242, test loss: 0.8129022398205134\n",
      "Iteration 5915 training loss: 0.21446699907965383, test loss: 0.812919505655701\n",
      "Iteration 5916 training loss: 0.21444397617070524, test loss: 0.812936770674214\n",
      "Iteration 5917 training loss: 0.2144209580478939, test loss: 0.8129540348760466\n",
      "Iteration 5918 training loss: 0.21439794470970794, test loss: 0.8129712982611922\n",
      "Iteration 5919 training loss: 0.214374936154636, test loss: 0.812988560829646\n",
      "Iteration 5920 training loss: 0.2143519323811674, test loss: 0.8130058225814015\n",
      "Iteration 5921 training loss: 0.21432893338779227, test loss: 0.8130230835164536\n",
      "Iteration 5922 training loss: 0.21430593917300106, test loss: 0.8130403436347964\n",
      "Iteration 5923 training loss: 0.2142829497352852, test loss: 0.8130576029364239\n",
      "Iteration 5924 training loss: 0.21425996507313658, test loss: 0.8130748614213313\n",
      "Iteration 5925 training loss: 0.21423698518504783, test loss: 0.8130921190895125\n",
      "Iteration 5926 training loss: 0.21421401006951216, test loss: 0.8131093759409622\n",
      "Iteration 5927 training loss: 0.21419103972502349, test loss: 0.8131266319756749\n",
      "Iteration 5928 training loss: 0.21416807415007638, test loss: 0.8131438871936455\n",
      "Iteration 5929 training loss: 0.2141451133431661, test loss: 0.8131611415948685\n",
      "Iteration 5930 training loss: 0.2141221573027884, test loss: 0.8131783951793382\n",
      "Iteration 5931 training loss: 0.21409920602743984, test loss: 0.8131956479470499\n",
      "Iteration 5932 training loss: 0.21407625951561754, test loss: 0.8132128998979974\n",
      "Iteration 5933 training loss: 0.21405331776581937, test loss: 0.8132301510321769\n",
      "Iteration 5934 training loss: 0.21403038077654366, test loss: 0.8132474013495822\n",
      "Iteration 5935 training loss: 0.21400744854628964, test loss: 0.8132646508502085\n",
      "Iteration 5936 training loss: 0.21398452107355695, test loss: 0.8132818995340506\n",
      "Iteration 5937 training loss: 0.21396159835684606, test loss: 0.813299147401104\n",
      "Iteration 5938 training loss: 0.213938680394658, test loss: 0.8133163944513631\n",
      "Iteration 5939 training loss: 0.2139157671854944, test loss: 0.813333640684823\n",
      "Iteration 5940 training loss: 0.21389285872785768, test loss: 0.8133508861014791\n",
      "Iteration 5941 training loss: 0.21386995502025072, test loss: 0.8133681307013266\n",
      "Iteration 5942 training loss: 0.21384705606117715, test loss: 0.8133853744843602\n",
      "Iteration 5943 training loss: 0.21382416184914144, test loss: 0.8134026174505755\n",
      "Iteration 5944 training loss: 0.21380127238264823, test loss: 0.8134198595999677\n",
      "Iteration 5945 training loss: 0.21377838766020327, test loss: 0.8134371009325317\n",
      "Iteration 5946 training loss: 0.21375550768031268, test loss: 0.8134543414482636\n",
      "Iteration 5947 training loss: 0.21373263244148338, test loss: 0.8134715811471581\n",
      "Iteration 5948 training loss: 0.21370976194222274, test loss: 0.8134888200292112\n",
      "Iteration 5949 training loss: 0.21368689618103895, test loss: 0.8135060580944177\n",
      "Iteration 5950 training loss: 0.21366403515644086, test loss: 0.8135232953427738\n",
      "Iteration 5951 training loss: 0.21364117886693776, test loss: 0.8135405317742741\n",
      "Iteration 5952 training loss: 0.2136183273110399, test loss: 0.8135577673889155\n",
      "Iteration 5953 training loss: 0.2135954804872578, test loss: 0.8135750021866923\n",
      "Iteration 5954 training loss: 0.2135726383941029, test loss: 0.8135922361676011\n",
      "Iteration 5955 training loss: 0.2135498010300871, test loss: 0.813609469331637\n",
      "Iteration 5956 training loss: 0.21352696839372315, test loss: 0.8136267016787964\n",
      "Iteration 5957 training loss: 0.2135041404835242, test loss: 0.8136439332090746\n",
      "Iteration 5958 training loss: 0.21348131729800426, test loss: 0.8136611639224672\n",
      "Iteration 5959 training loss: 0.2134584988356778, test loss: 0.8136783938189707\n",
      "Iteration 5960 training loss: 0.21343568509506003, test loss: 0.8136956228985806\n",
      "Iteration 5961 training loss: 0.21341287607466677, test loss: 0.8137128511612929\n",
      "Iteration 5962 training loss: 0.21339007177301444, test loss: 0.8137300786071038\n",
      "Iteration 5963 training loss: 0.2133672721886203, test loss: 0.8137473052360091\n",
      "Iteration 5964 training loss: 0.21334447732000192, test loss: 0.813764531048005\n",
      "Iteration 5965 training loss: 0.21332168716567773, test loss: 0.8137817560430874\n",
      "Iteration 5966 training loss: 0.21329890172416677, test loss: 0.8137989802212531\n",
      "Iteration 5967 training loss: 0.21327612099398863, test loss: 0.8138162035824972\n",
      "Iteration 5968 training loss: 0.21325334497366366, test loss: 0.8138334261268172\n",
      "Iteration 5969 training loss: 0.21323057366171277, test loss: 0.8138506478542085\n",
      "Iteration 5970 training loss: 0.21320780705665748, test loss: 0.8138678687646677\n",
      "Iteration 5971 training loss: 0.21318504515702003, test loss: 0.8138850888581909\n",
      "Iteration 5972 training loss: 0.21316228796132317, test loss: 0.8139023081347746\n",
      "Iteration 5973 training loss: 0.21313953546809047, test loss: 0.8139195265944155\n",
      "Iteration 5974 training loss: 0.21311678767584596, test loss: 0.8139367442371102\n",
      "Iteration 5975 training loss: 0.21309404458311434, test loss: 0.8139539610628546\n",
      "Iteration 5976 training loss: 0.2130713061884211, test loss: 0.8139711770716457\n",
      "Iteration 5977 training loss: 0.2130485724902921, test loss: 0.8139883922634801\n",
      "Iteration 5978 training loss: 0.21302584348725404, test loss: 0.8140056066383542\n",
      "Iteration 5979 training loss: 0.21300311917783413, test loss: 0.8140228201962653\n",
      "Iteration 5980 training loss: 0.21298039956056036, test loss: 0.8140400329372091\n",
      "Iteration 5981 training loss: 0.21295768463396117, test loss: 0.814057244861183\n",
      "Iteration 5982 training loss: 0.21293497439656575, test loss: 0.8140744559681838\n",
      "Iteration 5983 training loss: 0.21291226884690387, test loss: 0.8140916662582084\n",
      "Iteration 5984 training loss: 0.21288956798350603, test loss: 0.8141088757312532\n",
      "Iteration 5985 training loss: 0.2128668718049032, test loss: 0.8141260843873157\n",
      "Iteration 5986 training loss: 0.21284418030962704, test loss: 0.8141432922263926\n",
      "Iteration 5987 training loss: 0.21282149349620996, test loss: 0.8141604992484809\n",
      "Iteration 5988 training loss: 0.21279881136318474, test loss: 0.8141777054535777\n",
      "Iteration 5989 training loss: 0.21277613390908512, test loss: 0.81419491084168\n",
      "Iteration 5990 training loss: 0.21275346113244514, test loss: 0.8142121154127847\n",
      "Iteration 5991 training loss: 0.21273079303179981, test loss: 0.8142293191668898\n",
      "Iteration 5992 training loss: 0.21270812960568444, test loss: 0.8142465221039917\n",
      "Iteration 5993 training loss: 0.2126854708526351, test loss: 0.814263724224088\n",
      "Iteration 5994 training loss: 0.2126628167711886, test loss: 0.8142809255271757\n",
      "Iteration 5995 training loss: 0.2126401673598822, test loss: 0.8142981260132524\n",
      "Iteration 5996 training loss: 0.2126175226172539, test loss: 0.8143153256823152\n",
      "Iteration 5997 training loss: 0.21259488254184222, test loss: 0.814332524534362\n",
      "Iteration 5998 training loss: 0.21257224713218653, test loss: 0.8143497225693898\n",
      "Iteration 5999 training loss: 0.21254961638682646, test loss: 0.8143669197873962\n",
      "Iteration 6000 training loss: 0.21252699030430255, test loss: 0.8143841161883786\n",
      "Iteration 6001 training loss: 0.21250436888315602, test loss: 0.8144013117723349\n",
      "Iteration 6002 training loss: 0.21248175212192844, test loss: 0.8144185065392624\n",
      "Iteration 6003 training loss: 0.2124591400191622, test loss: 0.814435700489159\n",
      "Iteration 6004 training loss: 0.21243653257340023, test loss: 0.8144528936220218\n",
      "Iteration 6005 training loss: 0.21241392978318607, test loss: 0.8144700859378494\n",
      "Iteration 6006 training loss: 0.21239133164706409, test loss: 0.8144872774366387\n",
      "Iteration 6007 training loss: 0.21236873816357893, test loss: 0.8145044681183882\n",
      "Iteration 6008 training loss: 0.21234614933127618, test loss: 0.8145216579830956\n",
      "Iteration 6009 training loss: 0.21232356514870188, test loss: 0.8145388470307587\n",
      "Iteration 6010 training loss: 0.2123009856144027, test loss: 0.8145560352613752\n",
      "Iteration 6011 training loss: 0.21227841072692594, test loss: 0.8145732226749435\n",
      "Iteration 6012 training loss: 0.21225584048481955, test loss: 0.814590409271461\n",
      "Iteration 6013 training loss: 0.21223327488663216, test loss: 0.8146075950509261\n",
      "Iteration 6014 training loss: 0.21221071393091287, test loss: 0.8146247800133372\n",
      "Iteration 6015 training loss: 0.21218815761621151, test loss: 0.8146419641586921\n",
      "Iteration 6016 training loss: 0.21216560594107853, test loss: 0.8146591474869888\n",
      "Iteration 6017 training loss: 0.2121430589040649, test loss: 0.8146763299982256\n",
      "Iteration 6018 training loss: 0.21212051650372232, test loss: 0.8146935116924008\n",
      "Iteration 6019 training loss: 0.21209797873860306, test loss: 0.8147106925695132\n",
      "Iteration 6020 training loss: 0.21207544560726, test loss: 0.81472787262956\n",
      "Iteration 6021 training loss: 0.2120529171082467, test loss: 0.8147450518725406\n",
      "Iteration 6022 training loss: 0.21203039324011722, test loss: 0.8147622302984528\n",
      "Iteration 6023 training loss: 0.21200787400142634, test loss: 0.8147794079072956\n",
      "Iteration 6024 training loss: 0.21198535939072943, test loss: 0.8147965846990668\n",
      "Iteration 6025 training loss: 0.21196284940658253, test loss: 0.8148137606737651\n",
      "Iteration 6026 training loss: 0.21194034404754214, test loss: 0.8148309358313898\n",
      "Iteration 6027 training loss: 0.21191784331216545, test loss: 0.8148481101719381\n",
      "Iteration 6028 training loss: 0.21189534719901046, test loss: 0.8148652836954102\n",
      "Iteration 6029 training loss: 0.21187285570663542, test loss: 0.8148824564018035\n",
      "Iteration 6030 training loss: 0.21185036883359948, test loss: 0.8148996282911175\n",
      "Iteration 6031 training loss: 0.2118278865784623, test loss: 0.8149167993633506\n",
      "Iteration 6032 training loss: 0.21180540893978414, test loss: 0.8149339696185016\n",
      "Iteration 6033 training loss: 0.211782935916126, test loss: 0.8149511390565696\n",
      "Iteration 6034 training loss: 0.2117604675060493, test loss: 0.8149683076775536\n",
      "Iteration 6035 training loss: 0.21173800370811616, test loss: 0.8149854754814518\n",
      "Iteration 6036 training loss: 0.2117155445208894, test loss: 0.8150026424682638\n",
      "Iteration 6037 training loss: 0.2116930899429323, test loss: 0.8150198086379883\n",
      "Iteration 6038 training loss: 0.21167063997280888, test loss: 0.8150369739906245\n",
      "Iteration 6039 training loss: 0.21164819460908366, test loss: 0.8150541385261713\n",
      "Iteration 6040 training loss: 0.21162575385032187, test loss: 0.8150713022446279\n",
      "Iteration 6041 training loss: 0.21160331769508933, test loss: 0.8150884651459934\n",
      "Iteration 6042 training loss: 0.21158088614195236, test loss: 0.8151056272302675\n",
      "Iteration 6043 training loss: 0.21155845918947808, test loss: 0.8151227884974486\n",
      "Iteration 6044 training loss: 0.21153603683623406, test loss: 0.8151399489475365\n",
      "Iteration 6045 training loss: 0.2115136190807886, test loss: 0.8151571085805304\n",
      "Iteration 6046 training loss: 0.21149120592171047, test loss: 0.8151742673964294\n",
      "Iteration 6047 training loss: 0.21146879735756918, test loss: 0.8151914253952331\n",
      "Iteration 6048 training loss: 0.21144639338693486, test loss: 0.8152085825769413\n",
      "Iteration 6049 training loss: 0.2114239940083781, test loss: 0.815225738941553\n",
      "Iteration 6050 training loss: 0.21140159922047017, test loss: 0.8152428944890676\n",
      "Iteration 6051 training loss: 0.211379209021783, test loss: 0.815260049219485\n",
      "Iteration 6052 training loss: 0.21135682341088916, test loss: 0.8152772031328045\n",
      "Iteration 6053 training loss: 0.21133444238636157, test loss: 0.8152943562290262\n",
      "Iteration 6054 training loss: 0.2113120659467741, test loss: 0.8153115085081494\n",
      "Iteration 6055 training loss: 0.21128969409070109, test loss: 0.8153286599701733\n",
      "Iteration 6056 training loss: 0.2112673268167174, test loss: 0.8153458106150988\n",
      "Iteration 6057 training loss: 0.2112449641233985, test loss: 0.8153629604429248\n",
      "Iteration 6058 training loss: 0.21122260600932072, test loss: 0.8153801094536517\n",
      "Iteration 6059 training loss: 0.2112002524730606, test loss: 0.8153972576472786\n",
      "Iteration 6060 training loss: 0.2111779035131956, test loss: 0.8154144050238059\n",
      "Iteration 6061 training loss: 0.21115555912830367, test loss: 0.8154315515832335\n",
      "Iteration 6062 training loss: 0.21113321931696324, test loss: 0.8154486973255615\n",
      "Iteration 6063 training loss: 0.21111088407775366, test loss: 0.8154658422507897\n",
      "Iteration 6064 training loss: 0.21108855340925467, test loss: 0.8154829863589182\n",
      "Iteration 6065 training loss: 0.21106622731004648, test loss: 0.8155001296499472\n",
      "Iteration 6066 training loss: 0.21104390577871016, test loss: 0.8155172721238767\n",
      "Iteration 6067 training loss: 0.21102158881382738, test loss: 0.8155344137807069\n",
      "Iteration 6068 training loss: 0.2109992764139801, test loss: 0.8155515546204383\n",
      "Iteration 6069 training loss: 0.21097696857775128, test loss: 0.8155686946430704\n",
      "Iteration 6070 training loss: 0.21095466530372423, test loss: 0.8155858338486042\n",
      "Iteration 6071 training loss: 0.2109323665904829, test loss: 0.8156029722370398\n",
      "Iteration 6072 training loss: 0.21091007243661195, test loss: 0.8156201098083776\n",
      "Iteration 6073 training loss: 0.21088778284069648, test loss: 0.8156372465626179\n",
      "Iteration 6074 training loss: 0.2108654978013223, test loss: 0.8156543824997613\n",
      "Iteration 6075 training loss: 0.21084321731707578, test loss: 0.8156715176198078\n",
      "Iteration 6076 training loss: 0.21082094138654398, test loss: 0.8156886519227585\n",
      "Iteration 6077 training loss: 0.21079867000831432, test loss: 0.8157057854086142\n",
      "Iteration 6078 training loss: 0.21077640318097515, test loss: 0.8157229180773746\n",
      "Iteration 6079 training loss: 0.21075414090311517, test loss: 0.8157400499290409\n",
      "Iteration 6080 training loss: 0.21073188317332373, test loss: 0.8157571809636136\n",
      "Iteration 6081 training loss: 0.21070962999019086, test loss: 0.8157743111810937\n",
      "Iteration 6082 training loss: 0.21068738135230708, test loss: 0.8157914405814816\n",
      "Iteration 6083 training loss: 0.21066513725826355, test loss: 0.8158085691647783\n",
      "Iteration 6084 training loss: 0.21064289770665212, test loss: 0.8158256969309844\n",
      "Iteration 6085 training loss: 0.21062066269606505, test loss: 0.815842823880101\n",
      "Iteration 6086 training loss: 0.2105984322250954, test loss: 0.8158599500121292\n",
      "Iteration 6087 training loss: 0.21057620629233664, test loss: 0.8158770753270693\n",
      "Iteration 6088 training loss: 0.21055398489638294, test loss: 0.815894199824923\n",
      "Iteration 6089 training loss: 0.21053176803582901, test loss: 0.8159113235056906\n",
      "Iteration 6090 training loss: 0.21050955570927035, test loss: 0.8159284463693739\n",
      "Iteration 6091 training loss: 0.2104873479153027, test loss: 0.8159455684159737\n",
      "Iteration 6092 training loss: 0.21046514465252267, test loss: 0.8159626896454908\n",
      "Iteration 6093 training loss: 0.21044294591952742, test loss: 0.8159798100579269\n",
      "Iteration 6094 training loss: 0.21042075171491453, test loss: 0.8159969296532827\n",
      "Iteration 6095 training loss: 0.21039856203728252, test loss: 0.8160140484315601\n",
      "Iteration 6096 training loss: 0.21037637688523014, test loss: 0.8160311663927597\n",
      "Iteration 6097 training loss: 0.21035419625735688, test loss: 0.8160482835368833\n",
      "Iteration 6098 training loss: 0.21033202015226288, test loss: 0.8160653998639323\n",
      "Iteration 6099 training loss: 0.21030984856854887, test loss: 0.8160825153739076\n",
      "Iteration 6100 training loss: 0.21028768150481603, test loss: 0.8160996300668115\n",
      "Iteration 6101 training loss: 0.21026551895966622, test loss: 0.8161167439426444\n",
      "Iteration 6102 training loss: 0.21024336093170193, test loss: 0.8161338570014087\n",
      "Iteration 6103 training loss: 0.21022120741952624, test loss: 0.8161509692431057\n",
      "Iteration 6104 training loss: 0.21019905842174272, test loss: 0.8161680806677367\n",
      "Iteration 6105 training loss: 0.21017691393695567, test loss: 0.8161851912753038\n",
      "Iteration 6106 training loss: 0.21015477396376983, test loss: 0.8162023010658083\n",
      "Iteration 6107 training loss: 0.21013263850079067, test loss: 0.816219410039252\n",
      "Iteration 6108 training loss: 0.21011050754662405, test loss: 0.816236518195637\n",
      "Iteration 6109 training loss: 0.2100883810998767, test loss: 0.8162536255349648\n",
      "Iteration 6110 training loss: 0.21006625915915578, test loss: 0.8162707320572372\n",
      "Iteration 6111 training loss: 0.21004414172306898, test loss: 0.816287837762456\n",
      "Iteration 6112 training loss: 0.21002202879022464, test loss: 0.816304942650623\n",
      "Iteration 6113 training loss: 0.2099999203592318, test loss: 0.8163220467217408\n",
      "Iteration 6114 training loss: 0.2099778164286999, test loss: 0.8163391499758109\n",
      "Iteration 6115 training loss: 0.20995571699723906, test loss: 0.816356252412835\n",
      "Iteration 6116 training loss: 0.20993362206345995, test loss: 0.8163733540328161\n",
      "Iteration 6117 training loss: 0.20991153162597387, test loss: 0.8163904548357551\n",
      "Iteration 6118 training loss: 0.20988944568339277, test loss: 0.8164075548216551\n",
      "Iteration 6119 training loss: 0.20986736423432892, test loss: 0.8164246539905178\n",
      "Iteration 6120 training loss: 0.20984528727739554, test loss: 0.8164417523423456\n",
      "Iteration 6121 training loss: 0.20982321481120617, test loss: 0.8164588498771405\n",
      "Iteration 6122 training loss: 0.20980114683437506, test loss: 0.8164759465949052\n",
      "Iteration 6123 training loss: 0.2097790833455169, test loss: 0.8164930424956416\n",
      "Iteration 6124 training loss: 0.20975702434324717, test loss: 0.8165101375793522\n",
      "Iteration 6125 training loss: 0.20973496982618187, test loss: 0.8165272318460396\n",
      "Iteration 6126 training loss: 0.20971291979293738, test loss: 0.8165443252957061\n",
      "Iteration 6127 training loss: 0.20969087424213093, test loss: 0.8165614179283541\n",
      "Iteration 6128 training loss: 0.20966883317238022, test loss: 0.8165785097439859\n",
      "Iteration 6129 training loss: 0.2096467965823036, test loss: 0.8165956007426045\n",
      "Iteration 6130 training loss: 0.20962476447051984, test loss: 0.8166126909242125\n",
      "Iteration 6131 training loss: 0.20960273683564842, test loss: 0.8166297802888122\n",
      "Iteration 6132 training loss: 0.2095807136763094, test loss: 0.8166468688364062\n",
      "Iteration 6133 training loss: 0.20955869499112342, test loss: 0.816663956566998\n",
      "Iteration 6134 training loss: 0.20953668077871165, test loss: 0.816681043480589\n",
      "Iteration 6135 training loss: 0.2095146710376959, test loss: 0.8166981295771829\n",
      "Iteration 6136 training loss: 0.20949266576669842, test loss: 0.8167152148567824\n",
      "Iteration 6137 training loss: 0.20947066496434233, test loss: 0.8167322993193904\n",
      "Iteration 6138 training loss: 0.20944866862925102, test loss: 0.8167493829650099\n",
      "Iteration 6139 training loss: 0.20942667676004859, test loss: 0.8167664657936432\n",
      "Iteration 6140 training loss: 0.2094046893553598, test loss: 0.8167835478052935\n",
      "Iteration 6141 training loss: 0.20938270641380985, test loss: 0.8168006289999643\n",
      "Iteration 6142 training loss: 0.20936072793402455, test loss: 0.8168177093776584\n",
      "Iteration 6143 training loss: 0.2093387539146304, test loss: 0.8168347889383784\n",
      "Iteration 6144 training loss: 0.20931678435425427, test loss: 0.8168518676821279\n",
      "Iteration 6145 training loss: 0.20929481925152385, test loss: 0.8168689456089102\n",
      "Iteration 6146 training loss: 0.20927285860506722, test loss: 0.8168860227187283\n",
      "Iteration 6147 training loss: 0.2092509024135131, test loss: 0.8169030990115851\n",
      "Iteration 6148 training loss: 0.20922895067549077, test loss: 0.8169201744874843\n",
      "Iteration 6149 training loss: 0.20920700338963016, test loss: 0.816937249146429\n",
      "Iteration 6150 training loss: 0.2091850605545617, test loss: 0.8169543229884223\n",
      "Iteration 6151 training loss: 0.20916312216891642, test loss: 0.8169713960134684\n",
      "Iteration 6152 training loss: 0.20914118823132588, test loss: 0.8169884682215699\n",
      "Iteration 6153 training loss: 0.20911925874042236, test loss: 0.8170055396127305\n",
      "Iteration 6154 training loss: 0.20909733369483846, test loss: 0.8170226101869538\n",
      "Iteration 6155 training loss: 0.20907541309320762, test loss: 0.8170396799442433\n",
      "Iteration 6156 training loss: 0.2090534969341637, test loss: 0.8170567488846024\n",
      "Iteration 6157 training loss: 0.20903158521634116, test loss: 0.8170738170080349\n",
      "Iteration 6158 training loss: 0.2090096779383751, test loss: 0.8170908843145441\n",
      "Iteration 6159 training loss: 0.208987775098901, test loss: 0.8171079508041346\n",
      "Iteration 6160 training loss: 0.20896587669655523, test loss: 0.8171250164768088\n",
      "Iteration 6161 training loss: 0.20894398272997453, test loss: 0.8171420813325714\n",
      "Iteration 6162 training loss: 0.2089220931977961, test loss: 0.8171591453714258\n",
      "Iteration 6163 training loss: 0.20890020809865795, test loss: 0.8171762085933764\n",
      "Iteration 6164 training loss: 0.20887832743119852, test loss: 0.8171932709984258\n",
      "Iteration 6165 training loss: 0.2088564511940569, test loss: 0.8172103325865793\n",
      "Iteration 6166 training loss: 0.2088345793858727, test loss: 0.8172273933578401\n",
      "Iteration 6167 training loss: 0.20881271200528612, test loss: 0.817244453312212\n",
      "Iteration 6168 training loss: 0.20879084905093787, test loss: 0.8172615124496994\n",
      "Iteration 6169 training loss: 0.2087689905214693, test loss: 0.8172785707703064\n",
      "Iteration 6170 training loss: 0.2087471364155224, test loss: 0.817295628274037\n",
      "Iteration 6171 training loss: 0.2087252867317395, test loss: 0.8173126849608954\n",
      "Iteration 6172 training loss: 0.20870344146876377, test loss: 0.8173297408308856\n",
      "Iteration 6173 training loss: 0.20868160062523874, test loss: 0.8173467958840117\n",
      "Iteration 6174 training loss: 0.20865976419980856, test loss: 0.817363850120278\n",
      "Iteration 6175 training loss: 0.20863793219111812, test loss: 0.817380903539689\n",
      "Iteration 6176 training loss: 0.20861610459781255, test loss: 0.8173979561422489\n",
      "Iteration 6177 training loss: 0.20859428141853786, test loss: 0.817415007927962\n",
      "Iteration 6178 training loss: 0.2085724626519405, test loss: 0.8174320588968326\n",
      "Iteration 6179 training loss: 0.20855064829666733, test loss: 0.8174491090488653\n",
      "Iteration 6180 training loss: 0.20852883835136618, test loss: 0.8174661583840643\n",
      "Iteration 6181 training loss: 0.20850703281468497, test loss: 0.8174832069024346\n",
      "Iteration 6182 training loss: 0.20848523168527244, test loss: 0.8175002546039802\n",
      "Iteration 6183 training loss: 0.20846343496177802, test loss: 0.8175173014887059\n",
      "Iteration 6184 training loss: 0.20844164264285153, test loss: 0.817534347556616\n",
      "Iteration 6185 training loss: 0.20841985472714322, test loss: 0.8175513928077157\n",
      "Iteration 6186 training loss: 0.2083980712133042, test loss: 0.8175684372420096\n",
      "Iteration 6187 training loss: 0.20837629209998595, test loss: 0.8175854808595019\n",
      "Iteration 6188 training loss: 0.20835451738584063, test loss: 0.8176025236601975\n",
      "Iteration 6189 training loss: 0.20833274706952085, test loss: 0.8176195656441018\n",
      "Iteration 6190 training loss: 0.20831098114967989, test loss: 0.8176366068112189\n",
      "Iteration 6191 training loss: 0.20828921962497143, test loss: 0.817653647161554\n",
      "Iteration 6192 training loss: 0.20826746249405, test loss: 0.8176706866951122\n",
      "Iteration 6193 training loss: 0.2082457097555704, test loss: 0.8176877254118976\n",
      "Iteration 6194 training loss: 0.20822396140818816, test loss: 0.8177047633119162\n",
      "Iteration 6195 training loss: 0.2082022174505593, test loss: 0.8177218003951724\n",
      "Iteration 6196 training loss: 0.20818047788134036, test loss: 0.8177388366616717\n",
      "Iteration 6197 training loss: 0.2081587426991887, test loss: 0.8177558721114183\n",
      "Iteration 6198 training loss: 0.20813701190276188, test loss: 0.8177729067444184\n",
      "Iteration 6199 training loss: 0.20811528549071817, test loss: 0.8177899405606766\n",
      "Iteration 6200 training loss: 0.2080935634617165, test loss: 0.817806973560198\n",
      "Iteration 6201 training loss: 0.20807184581441635, test loss: 0.8178240057429883\n",
      "Iteration 6202 training loss: 0.20805013254747753, test loss: 0.817841037109052\n",
      "Iteration 6203 training loss: 0.20802842365956065, test loss: 0.8178580676583952\n",
      "Iteration 6204 training loss: 0.20800671914932678, test loss: 0.8178750973910228\n",
      "Iteration 6205 training loss: 0.20798501901543762, test loss: 0.8178921263069404\n",
      "Iteration 6206 training loss: 0.20796332325655525, test loss: 0.8179091544061533\n",
      "Iteration 6207 training loss: 0.2079416318713426, test loss: 0.8179261816886667\n",
      "Iteration 6208 training loss: 0.20791994485846282, test loss: 0.8179432081544864\n",
      "Iteration 6209 training loss: 0.20789826221657998, test loss: 0.8179602338036178\n",
      "Iteration 6210 training loss: 0.20787658394435835, test loss: 0.8179772586360669\n",
      "Iteration 6211 training loss: 0.20785491004046294, test loss: 0.8179942826518385\n",
      "Iteration 6212 training loss: 0.20783324050355942, test loss: 0.8180113058509387\n",
      "Iteration 6213 training loss: 0.2078115753323138, test loss: 0.8180283282333731\n",
      "Iteration 6214 training loss: 0.20778991452539278, test loss: 0.8180453497991476\n",
      "Iteration 6215 training loss: 0.20776825808146357, test loss: 0.8180623705482676\n",
      "Iteration 6216 training loss: 0.20774660599919392, test loss: 0.818079390480739\n",
      "Iteration 6217 training loss: 0.2077249582772522, test loss: 0.8180964095965677\n",
      "Iteration 6218 training loss: 0.20770331491430732, test loss: 0.8181134278957594\n",
      "Iteration 6219 training loss: 0.20768167590902864, test loss: 0.81813044537832\n",
      "Iteration 6220 training loss: 0.20766004126008625, test loss: 0.8181474620442556\n",
      "Iteration 6221 training loss: 0.20763841096615063, test loss: 0.8181644778935719\n",
      "Iteration 6222 training loss: 0.2076167850258929, test loss: 0.8181814929262754\n",
      "Iteration 6223 training loss: 0.20759516343798473, test loss: 0.8181985071423714\n",
      "Iteration 6224 training loss: 0.2075735462010983, test loss: 0.8182155205418665\n",
      "Iteration 6225 training loss: 0.20755193331390637, test loss: 0.818232533124767\n",
      "Iteration 6226 training loss: 0.20753032477508238, test loss: 0.8182495448910782\n",
      "Iteration 6227 training loss: 0.20750872058330005, test loss: 0.818266555840807\n",
      "Iteration 6228 training loss: 0.20748712073723385, test loss: 0.8182835659739592\n",
      "Iteration 6229 training loss: 0.20746552523555872, test loss: 0.8183005752905415\n",
      "Iteration 6230 training loss: 0.20744393407695028, test loss: 0.8183175837905595\n",
      "Iteration 6231 training loss: 0.20742234726008452, test loss: 0.8183345914740203\n",
      "Iteration 6232 training loss: 0.2074007647836381, test loss: 0.8183515983409297\n",
      "Iteration 6233 training loss: 0.2073791866462882, test loss: 0.8183686043912944\n",
      "Iteration 6234 training loss: 0.20735761284671247, test loss: 0.8183856096251205\n",
      "Iteration 6235 training loss: 0.2073360433835893, test loss: 0.8184026140424144\n",
      "Iteration 6236 training loss: 0.20731447825559748, test loss: 0.8184196176431834\n",
      "Iteration 6237 training loss: 0.20729291746141634, test loss: 0.8184366204274334\n",
      "Iteration 6238 training loss: 0.20727136099972593, test loss: 0.8184536223951708\n",
      "Iteration 6239 training loss: 0.2072498088692066, test loss: 0.8184706235464029\n",
      "Iteration 6240 training loss: 0.20722826106853934, test loss: 0.8184876238811354\n",
      "Iteration 6241 training loss: 0.20720671759640588, test loss: 0.8185046233993756\n",
      "Iteration 6242 training loss: 0.2071851784514882, test loss: 0.8185216221011304\n",
      "Iteration 6243 training loss: 0.20716364363246906, test loss: 0.8185386199864063\n",
      "Iteration 6244 training loss: 0.2071421131380316, test loss: 0.8185556170552098\n",
      "Iteration 6245 training loss: 0.20712058696685967, test loss: 0.8185726133075478\n",
      "Iteration 6246 training loss: 0.20709906511763748, test loss: 0.8185896087434277\n",
      "Iteration 6247 training loss: 0.20707754758904998, test loss: 0.818606603362856\n",
      "Iteration 6248 training loss: 0.20705603437978246, test loss: 0.8186235971658395\n",
      "Iteration 6249 training loss: 0.20703452548852097, test loss: 0.8186405901523853\n",
      "Iteration 6250 training loss: 0.2070130209139519, test loss: 0.8186575823225005\n",
      "Iteration 6251 training loss: 0.2069915206547624, test loss: 0.8186745736761921\n",
      "Iteration 6252 training loss: 0.20697002470964002, test loss: 0.8186915642134671\n",
      "Iteration 6253 training loss: 0.20694853307727282, test loss: 0.8187085539343327\n",
      "Iteration 6254 training loss: 0.2069270457563496, test loss: 0.8187255428387958\n",
      "Iteration 6255 training loss: 0.20690556274555943, test loss: 0.8187425309268639\n",
      "Iteration 6256 training loss: 0.20688408404359218, test loss: 0.8187595181985442\n",
      "Iteration 6257 training loss: 0.2068626096491381, test loss: 0.8187765046538437\n",
      "Iteration 6258 training loss: 0.2068411395608881, test loss: 0.8187934902927699\n",
      "Iteration 6259 training loss: 0.2068196737775335, test loss: 0.8188104751153299\n",
      "Iteration 6260 training loss: 0.2067982122977662, test loss: 0.8188274591215312\n",
      "Iteration 6261 training loss: 0.2067767551202788, test loss: 0.8188444423113813\n",
      "Iteration 6262 training loss: 0.20675530224376423, test loss: 0.8188614246848875\n",
      "Iteration 6263 training loss: 0.20673385366691613, test loss: 0.8188784062420572\n",
      "Iteration 6264 training loss: 0.20671240938842847, test loss: 0.818895386982898\n",
      "Iteration 6265 training loss: 0.206690969406996, test loss: 0.8189123669074175\n",
      "Iteration 6266 training loss: 0.20666953372131383, test loss: 0.8189293460156233\n",
      "Iteration 6267 training loss: 0.20664810233007774, test loss: 0.8189463243075226\n",
      "Iteration 6268 training loss: 0.206626675231984, test loss: 0.8189633017831235\n",
      "Iteration 6269 training loss: 0.2066052524257294, test loss: 0.8189802784424336\n",
      "Iteration 6270 training loss: 0.20658383391001126, test loss: 0.8189972542854604\n",
      "Iteration 6271 training loss: 0.2065624196835275, test loss: 0.8190142293122118\n",
      "Iteration 6272 training loss: 0.20654100974497652, test loss: 0.8190312035226958\n",
      "Iteration 6273 training loss: 0.2065196040930573, test loss: 0.8190481769169196\n",
      "Iteration 6274 training loss: 0.20649820272646938, test loss: 0.8190651494948915\n",
      "Iteration 6275 training loss: 0.2064768056439127, test loss: 0.8190821212566195\n",
      "Iteration 6276 training loss: 0.20645541284408786, test loss: 0.8190990922021113\n",
      "Iteration 6277 training loss: 0.20643402432569605, test loss: 0.8191160623313746\n",
      "Iteration 6278 training loss: 0.20641264008743884, test loss: 0.8191330316444182\n",
      "Iteration 6279 training loss: 0.20639126012801853, test loss: 0.8191500001412493\n",
      "Iteration 6280 training loss: 0.2063698844461377, test loss: 0.8191669678218764\n",
      "Iteration 6281 training loss: 0.20634851304049975, test loss: 0.8191839346863075\n",
      "Iteration 6282 training loss: 0.2063271459098084, test loss: 0.8192009007345507\n",
      "Iteration 6283 training loss: 0.20630578305276803, test loss: 0.8192178659666143\n",
      "Iteration 6284 training loss: 0.20628442446808348, test loss: 0.8192348303825061\n",
      "Iteration 6285 training loss: 0.20626307015446013, test loss: 0.8192517939822348\n",
      "Iteration 6286 training loss: 0.20624172011060407, test loss: 0.8192687567658087\n",
      "Iteration 6287 training loss: 0.20622037433522158, test loss: 0.8192857187332356\n",
      "Iteration 6288 training loss: 0.2061990328270198, test loss: 0.8193026798845244\n",
      "Iteration 6289 training loss: 0.20617769558470625, test loss: 0.8193196402196835\n",
      "Iteration 6290 training loss: 0.206156362606989, test loss: 0.8193365997387207\n",
      "Iteration 6291 training loss: 0.20613503389257662, test loss: 0.8193535584416449\n",
      "Iteration 6292 training loss: 0.2061137094401784, test loss: 0.8193705163284648\n",
      "Iteration 6293 training loss: 0.20609238924850387, test loss: 0.819387473399188\n",
      "Iteration 6294 training loss: 0.20607107331626331, test loss: 0.8194044296538243\n",
      "Iteration 6295 training loss: 0.20604976164216748, test loss: 0.8194213850923814\n",
      "Iteration 6296 training loss: 0.20602845422492766, test loss: 0.8194383397148682\n",
      "Iteration 6297 training loss: 0.2060071510632556, test loss: 0.8194552935212935\n",
      "Iteration 6298 training loss: 0.20598585215586376, test loss: 0.819472246511666\n",
      "Iteration 6299 training loss: 0.20596455750146492, test loss: 0.819489198685994\n",
      "Iteration 6300 training loss: 0.20594326709877256, test loss: 0.8195061500442867\n",
      "Iteration 6301 training loss: 0.2059219809465005, test loss: 0.8195231005865529\n",
      "Iteration 6302 training loss: 0.2059006990433633, test loss: 0.819540050312801\n",
      "Iteration 6303 training loss: 0.20587942138807597, test loss: 0.8195569992230404\n",
      "Iteration 6304 training loss: 0.205858147979354, test loss: 0.8195739473172796\n",
      "Iteration 6305 training loss: 0.20583687881591348, test loss: 0.8195908945955277\n",
      "Iteration 6306 training loss: 0.20581561389647096, test loss: 0.8196078410577937\n",
      "Iteration 6307 training loss: 0.2057943532197436, test loss: 0.8196247867040866\n",
      "Iteration 6308 training loss: 0.20577309678444902, test loss: 0.8196417315344154\n",
      "Iteration 6309 training loss: 0.20575184458930545, test loss: 0.8196586755487892\n",
      "Iteration 6310 training loss: 0.20573059663303145, test loss: 0.8196756187472173\n",
      "Iteration 6311 training loss: 0.20570935291434644, test loss: 0.8196925611297087\n",
      "Iteration 6312 training loss: 0.20568811343197005, test loss: 0.8197095026962724\n",
      "Iteration 6313 training loss: 0.20566687818462265, test loss: 0.8197264434469177\n",
      "Iteration 6314 training loss: 0.20564564717102501, test loss: 0.8197433833816543\n",
      "Iteration 6315 training loss: 0.20562442038989842, test loss: 0.8197603225004907\n",
      "Iteration 6316 training loss: 0.20560319783996486, test loss: 0.8197772608034369\n",
      "Iteration 6317 training loss: 0.2055819795199467, test loss: 0.8197941982905017\n",
      "Iteration 6318 training loss: 0.2055607654285668, test loss: 0.8198111349616949\n",
      "Iteration 6319 training loss: 0.2055395555645487, test loss: 0.8198280708170259\n",
      "Iteration 6320 training loss: 0.20551834992661627, test loss: 0.8198450058565042\n",
      "Iteration 6321 training loss: 0.20549714851349407, test loss: 0.8198619400801389\n",
      "Iteration 6322 training loss: 0.20547595132390714, test loss: 0.8198788734879395\n",
      "Iteration 6323 training loss: 0.20545475835658095, test loss: 0.8198958060799162\n",
      "Iteration 6324 training loss: 0.2054335696102417, test loss: 0.8199127378560783\n",
      "Iteration 6325 training loss: 0.20541238508361592, test loss: 0.8199296688164354\n",
      "Iteration 6326 training loss: 0.20539120477543069, test loss: 0.819946598960997\n",
      "Iteration 6327 training loss: 0.20537002868441373, test loss: 0.8199635282897727\n",
      "Iteration 6328 training loss: 0.20534885680929313, test loss: 0.8199804568027728\n",
      "Iteration 6329 training loss: 0.20532768914879768, test loss: 0.8199973845000066\n",
      "Iteration 6330 training loss: 0.20530652570165653, test loss: 0.8200143113814841\n",
      "Iteration 6331 training loss: 0.2052853664665994, test loss: 0.8200312374472147\n",
      "Iteration 6332 training loss: 0.2052642114423566, test loss: 0.8200481626972091\n",
      "Iteration 6333 training loss: 0.20524306062765899, test loss: 0.8200650871314767\n",
      "Iteration 6334 training loss: 0.20522191402123763, test loss: 0.8200820107500273\n",
      "Iteration 6335 training loss: 0.20520077162182465, test loss: 0.8200989335528711\n",
      "Iteration 6336 training loss: 0.20517963342815215, test loss: 0.8201158555400182\n",
      "Iteration 6337 training loss: 0.2051584994389531, test loss: 0.8201327767114784\n",
      "Iteration 6338 training loss: 0.2051373696529609, test loss: 0.8201496970672618\n",
      "Iteration 6339 training loss: 0.20511624406890946, test loss: 0.820166616607379\n",
      "Iteration 6340 training loss: 0.20509512268553315, test loss: 0.8201835353318393\n",
      "Iteration 6341 training loss: 0.20507400550156704, test loss: 0.8202004532406533\n",
      "Iteration 6342 training loss: 0.20505289251574643, test loss: 0.8202173703338316\n",
      "Iteration 6343 training loss: 0.20503178372680744, test loss: 0.8202342866113839\n",
      "Iteration 6344 training loss: 0.20501067913348653, test loss: 0.8202512020733206\n",
      "Iteration 6345 training loss: 0.20498957873452073, test loss: 0.8202681167196522\n",
      "Iteration 6346 training loss: 0.2049684825286476, test loss: 0.8202850305503887\n",
      "Iteration 6347 training loss: 0.2049473905146052, test loss: 0.8203019435655409\n",
      "Iteration 6348 training loss: 0.20492630269113207, test loss: 0.8203188557651188\n",
      "Iteration 6349 training loss: 0.20490521905696732, test loss: 0.8203357671491333\n",
      "Iteration 6350 training loss: 0.20488413961085064, test loss: 0.8203526777175943\n",
      "Iteration 6351 training loss: 0.2048630643515221, test loss: 0.820369587470513\n",
      "Iteration 6352 training loss: 0.20484199327772232, test loss: 0.8203864964078997\n",
      "Iteration 6353 training loss: 0.20482092638819258, test loss: 0.8204034045297649\n",
      "Iteration 6354 training loss: 0.20479986368167444, test loss: 0.8204203118361191\n",
      "Iteration 6355 training loss: 0.20477880515691016, test loss: 0.8204372183269732\n",
      "Iteration 6356 training loss: 0.20475775081264241, test loss: 0.8204541240023376\n",
      "Iteration 6357 training loss: 0.20473670064761446, test loss: 0.8204710288622232\n",
      "Iteration 6358 training loss: 0.20471565466057, test loss: 0.8204879329066409\n",
      "Iteration 6359 training loss: 0.20469461285025348, test loss: 0.8205048361356015\n",
      "Iteration 6360 training loss: 0.2046735752154094, test loss: 0.8205217385491155\n",
      "Iteration 6361 training loss: 0.20465254175478323, test loss: 0.8205386401471941\n",
      "Iteration 6362 training loss: 0.2046315124671207, test loss: 0.8205555409298475\n",
      "Iteration 6363 training loss: 0.2046104873511682, test loss: 0.8205724408970875\n",
      "Iteration 6364 training loss: 0.2045894664056725, test loss: 0.8205893400489248\n",
      "Iteration 6365 training loss: 0.20456844962938098, test loss: 0.8206062383853702\n",
      "Iteration 6366 training loss: 0.20454743702104142, test loss: 0.8206231359064345\n",
      "Iteration 6367 training loss: 0.20452642857940231, test loss: 0.8206400326121296\n",
      "Iteration 6368 training loss: 0.2045054243032124, test loss: 0.8206569285024657\n",
      "Iteration 6369 training loss: 0.20448442419122115, test loss: 0.8206738235774543\n",
      "Iteration 6370 training loss: 0.20446342824217847, test loss: 0.8206907178371066\n",
      "Iteration 6371 training loss: 0.20444243645483484, test loss: 0.8207076112814341\n",
      "Iteration 6372 training loss: 0.20442144882794108, test loss: 0.8207245039104473\n",
      "Iteration 6373 training loss: 0.20440046536024872, test loss: 0.820741395724158\n",
      "Iteration 6374 training loss: 0.20437948605050965, test loss: 0.8207582867225771\n",
      "Iteration 6375 training loss: 0.20435851089747636, test loss: 0.8207751769057164\n",
      "Iteration 6376 training loss: 0.2043375398999018, test loss: 0.8207920662735869\n",
      "Iteration 6377 training loss: 0.20431657305653947, test loss: 0.8208089548262003\n",
      "Iteration 6378 training loss: 0.20429561036614344, test loss: 0.8208258425635677\n",
      "Iteration 6379 training loss: 0.204274651827468, test loss: 0.8208427294857006\n",
      "Iteration 6380 training loss: 0.20425369743926836, test loss: 0.8208596155926107\n",
      "Iteration 6381 training loss: 0.20423274720030002, test loss: 0.8208765008843096\n",
      "Iteration 6382 training loss: 0.2042118011093189, test loss: 0.8208933853608084\n",
      "Iteration 6383 training loss: 0.2041908591650817, test loss: 0.820910269022119\n",
      "Iteration 6384 training loss: 0.20416992136634532, test loss: 0.8209271518682532\n",
      "Iteration 6385 training loss: 0.20414898771186735, test loss: 0.8209440338992223\n",
      "Iteration 6386 training loss: 0.20412805820040586, test loss: 0.8209609151150384\n",
      "Iteration 6387 training loss: 0.20410713283071946, test loss: 0.8209777955157128\n",
      "Iteration 6388 training loss: 0.20408621160156717, test loss: 0.8209946751012576\n",
      "Iteration 6389 training loss: 0.20406529451170866, test loss: 0.8210115538716842\n",
      "Iteration 6390 training loss: 0.2040443815599039, test loss: 0.8210284318270051\n",
      "Iteration 6391 training loss: 0.20402347274491353, test loss: 0.8210453089672315\n",
      "Iteration 6392 training loss: 0.2040025680654987, test loss: 0.8210621852923757\n",
      "Iteration 6393 training loss: 0.20398166752042096, test loss: 0.821079060802449\n",
      "Iteration 6394 training loss: 0.2039607711084425, test loss: 0.8210959354974644\n",
      "Iteration 6395 training loss: 0.20393987882832584, test loss: 0.821112809377433\n",
      "Iteration 6396 training loss: 0.20391899067883418, test loss: 0.8211296824423673\n",
      "Iteration 6397 training loss: 0.20389810665873112, test loss: 0.8211465546922787\n",
      "Iteration 6398 training loss: 0.2038772267667808, test loss: 0.8211634261271807\n",
      "Iteration 6399 training loss: 0.20385635100174784, test loss: 0.8211802967470839\n",
      "Iteration 6400 training loss: 0.20383547936239738, test loss: 0.8211971665520013\n",
      "Iteration 6401 training loss: 0.2038146118474951, test loss: 0.8212140355419448\n",
      "Iteration 6402 training loss: 0.20379374845580714, test loss: 0.8212309037169266\n",
      "Iteration 6403 training loss: 0.20377288918610006, test loss: 0.8212477710769595\n",
      "Iteration 6404 training loss: 0.2037520340371412, test loss: 0.8212646376220549\n",
      "Iteration 6405 training loss: 0.20373118300769807, test loss: 0.8212815033522258\n",
      "Iteration 6406 training loss: 0.20371033609653885, test loss: 0.8212983682674844\n",
      "Iteration 6407 training loss: 0.20368949330243225, test loss: 0.8213152323678431\n",
      "Iteration 6408 training loss: 0.20366865462414743, test loss: 0.821332095653314\n",
      "Iteration 6409 training loss: 0.20364782006045398, test loss: 0.8213489581239098\n",
      "Iteration 6410 training loss: 0.20362698961012213, test loss: 0.8213658197796431\n",
      "Iteration 6411 training loss: 0.2036061632719226, test loss: 0.8213826806205263\n",
      "Iteration 6412 training loss: 0.20358534104462644, test loss: 0.8213995406465721\n",
      "Iteration 6413 training loss: 0.20356452292700533, test loss: 0.8214163998577927\n",
      "Iteration 6414 training loss: 0.20354370891783155, test loss: 0.8214332582542012\n",
      "Iteration 6415 training loss: 0.20352289901587772, test loss: 0.8214501158358102\n",
      "Iteration 6416 training loss: 0.20350209321991694, test loss: 0.8214669726026321\n",
      "Iteration 6417 training loss: 0.20348129152872296, test loss: 0.8214838285546794\n",
      "Iteration 6418 training loss: 0.20346049394106985, test loss: 0.8215006836919658\n",
      "Iteration 6419 training loss: 0.20343970045573248, test loss: 0.8215175380145031\n",
      "Iteration 6420 training loss: 0.2034189110714858, test loss: 0.8215343915223047\n",
      "Iteration 6421 training loss: 0.2033981257871055, test loss: 0.8215512442153827\n",
      "Iteration 6422 training loss: 0.20337734460136786, test loss: 0.821568096093751\n",
      "Iteration 6423 training loss: 0.20335656751304948, test loss: 0.821584947157422\n",
      "Iteration 6424 training loss: 0.2033357945209276, test loss: 0.8216017974064085\n",
      "Iteration 6425 training loss: 0.20331502562377965, test loss: 0.821618646840724\n",
      "Iteration 6426 training loss: 0.20329426082038393, test loss: 0.8216354954603813\n",
      "Iteration 6427 training loss: 0.2032735001095192, test loss: 0.8216523432653928\n",
      "Iteration 6428 training loss: 0.2032527434899644, test loss: 0.8216691902557727\n",
      "Iteration 6429 training loss: 0.20323199096049924, test loss: 0.8216860364315332\n",
      "Iteration 6430 training loss: 0.20321124251990394, test loss: 0.8217028817926875\n",
      "Iteration 6431 training loss: 0.20319049816695908, test loss: 0.8217197263392494\n",
      "Iteration 6432 training loss: 0.20316975790044572, test loss: 0.8217365700712316\n",
      "Iteration 6433 training loss: 0.2031490217191456, test loss: 0.8217534129886478\n",
      "Iteration 6434 training loss: 0.20312828962184074, test loss: 0.8217702550915106\n",
      "Iteration 6435 training loss: 0.20310756160731386, test loss: 0.8217870963798335\n",
      "Iteration 6436 training loss: 0.20308683767434796, test loss: 0.8218039368536304\n",
      "Iteration 6437 training loss: 0.20306611782172673, test loss: 0.8218207765129143\n",
      "Iteration 6438 training loss: 0.2030454020482342, test loss: 0.8218376153576982\n",
      "Iteration 6439 training loss: 0.20302469035265502, test loss: 0.8218544533879962\n",
      "Iteration 6440 training loss: 0.20300398273377426, test loss: 0.8218712906038212\n",
      "Iteration 6441 training loss: 0.20298327919037745, test loss: 0.8218881270051877\n",
      "Iteration 6442 training loss: 0.20296257972125076, test loss: 0.8219049625921079\n",
      "Iteration 6443 training loss: 0.20294188432518065, test loss: 0.8219217973645959\n",
      "Iteration 6444 training loss: 0.20292119300095424, test loss: 0.8219386313226654\n",
      "Iteration 6445 training loss: 0.20290050574735904, test loss: 0.8219554644663303\n",
      "Iteration 6446 training loss: 0.20287982256318315, test loss: 0.8219722967956039\n",
      "Iteration 6447 training loss: 0.20285914344721506, test loss: 0.8219891283104999\n",
      "Iteration 6448 training loss: 0.2028384683982437, test loss: 0.8220059590110319\n",
      "Iteration 6449 training loss: 0.20281779741505876, test loss: 0.8220227888972139\n",
      "Iteration 6450 training loss: 0.20279713049645018, test loss: 0.8220396179690602\n",
      "Iteration 6451 training loss: 0.20277646764120844, test loss: 0.8220564462265832\n",
      "Iteration 6452 training loss: 0.20275580884812452, test loss: 0.8220732736697981\n",
      "Iteration 6453 training loss: 0.2027351541159899, test loss: 0.822090100298718\n",
      "Iteration 6454 training loss: 0.2027145034435966, test loss: 0.8221069261133575\n",
      "Iteration 6455 training loss: 0.20269385682973695, test loss: 0.8221237511137299\n",
      "Iteration 6456 training loss: 0.2026732142732041, test loss: 0.8221405752998494\n",
      "Iteration 6457 training loss: 0.20265257577279128, test loss: 0.8221573986717301\n",
      "Iteration 6458 training loss: 0.2026319413272926, test loss: 0.8221742212293861\n",
      "Iteration 6459 training loss: 0.20261131093550228, test loss: 0.8221910429728313\n",
      "Iteration 6460 training loss: 0.20259068459621532, test loss: 0.82220786390208\n",
      "Iteration 6461 training loss: 0.20257006230822713, test loss: 0.8222246840171461\n",
      "Iteration 6462 training loss: 0.2025494440703336, test loss: 0.8222415033180441\n",
      "Iteration 6463 training loss: 0.20252882988133097, test loss: 0.8222583218047879\n",
      "Iteration 6464 training loss: 0.20250821974001626, test loss: 0.8222751394773921\n",
      "Iteration 6465 training loss: 0.20248761364518672, test loss: 0.8222919563358703\n",
      "Iteration 6466 training loss: 0.2024670115956402, test loss: 0.8223087723802374\n",
      "Iteration 6467 training loss: 0.202446413590175, test loss: 0.8223255876105074\n",
      "Iteration 6468 training loss: 0.20242581962758993, test loss: 0.8223424020266951\n",
      "Iteration 6469 training loss: 0.20240522970668423, test loss: 0.8223592156288146\n",
      "Iteration 6470 training loss: 0.2023846438262577, test loss: 0.8223760284168801\n",
      "Iteration 6471 training loss: 0.20236406198511064, test loss: 0.8223928403909065\n",
      "Iteration 6472 training loss: 0.20234348418204381, test loss: 0.8224096515509083\n",
      "Iteration 6473 training loss: 0.20232291041585831, test loss: 0.8224264618968996\n",
      "Iteration 6474 training loss: 0.20230234068535602, test loss: 0.8224432714288952\n",
      "Iteration 6475 training loss: 0.20228177498933897, test loss: 0.8224600801469096\n",
      "Iteration 6476 training loss: 0.2022612133266099, test loss: 0.8224768880509576\n",
      "Iteration 6477 training loss: 0.20224065569597208, test loss: 0.8224936951410541\n",
      "Iteration 6478 training loss: 0.20222010209622904, test loss: 0.8225105014172129\n",
      "Iteration 6479 training loss: 0.20219955252618496, test loss: 0.8225273068794495\n",
      "Iteration 6480 training loss: 0.20217900698464442, test loss: 0.8225441115277783\n",
      "Iteration 6481 training loss: 0.20215846547041258, test loss: 0.8225609153622142\n",
      "Iteration 6482 training loss: 0.20213792798229496, test loss: 0.822577718382772\n",
      "Iteration 6483 training loss: 0.2021173945190976, test loss: 0.8225945205894665\n",
      "Iteration 6484 training loss: 0.2020968650796272, test loss: 0.8226113219823126\n",
      "Iteration 6485 training loss: 0.2020763396626907, test loss: 0.8226281225613253\n",
      "Iteration 6486 training loss: 0.20205581826709554, test loss: 0.8226449223265194\n",
      "Iteration 6487 training loss: 0.20203530089164978, test loss: 0.8226617212779098\n",
      "Iteration 6488 training loss: 0.20201478753516186, test loss: 0.8226785194155115\n",
      "Iteration 6489 training loss: 0.20199427819644084, test loss: 0.8226953167393399\n",
      "Iteration 6490 training loss: 0.20197377287429602, test loss: 0.8227121132494097\n",
      "Iteration 6491 training loss: 0.2019532715675374, test loss: 0.8227289089457362\n",
      "Iteration 6492 training loss: 0.20193277427497539, test loss: 0.8227457038283343\n",
      "Iteration 6493 training loss: 0.20191228099542077, test loss: 0.8227624978972192\n",
      "Iteration 6494 training loss: 0.20189179172768504, test loss: 0.8227792911524062\n",
      "Iteration 6495 training loss: 0.2018713064705799, test loss: 0.8227960835939104\n",
      "Iteration 6496 training loss: 0.2018508252229177, test loss: 0.8228128752217474\n",
      "Iteration 6497 training loss: 0.20183034798351132, test loss: 0.8228296660359321\n",
      "Iteration 6498 training loss: 0.20180987475117393, test loss: 0.8228464560364799\n",
      "Iteration 6499 training loss: 0.20178940552471936, test loss: 0.8228632452234064\n",
      "Iteration 6500 training loss: 0.20176894030296177, test loss: 0.8228800335967265\n",
      "Iteration 6501 training loss: 0.20174847908471588, test loss: 0.8228968211564557\n",
      "Iteration 6502 training loss: 0.20172802186879693, test loss: 0.8229136079026097\n",
      "Iteration 6503 training loss: 0.2017075686540206, test loss: 0.822930393835204\n",
      "Iteration 6504 training loss: 0.20168711943920292, test loss: 0.8229471789542538\n",
      "Iteration 6505 training loss: 0.20166667422316065, test loss: 0.822963963259775\n",
      "Iteration 6506 training loss: 0.20164623300471074, test loss: 0.8229807467517829\n",
      "Iteration 6507 training loss: 0.20162579578267092, test loss: 0.822997529430293\n",
      "Iteration 6508 training loss: 0.20160536255585915, test loss: 0.8230143112953211\n",
      "Iteration 6509 training loss: 0.20158493332309388, test loss: 0.8230310923468829\n",
      "Iteration 6510 training loss: 0.2015645080831942, test loss: 0.823047872584994\n",
      "Iteration 6511 training loss: 0.20154408683497962, test loss: 0.8230646520096699\n",
      "Iteration 6512 training loss: 0.20152366957727005, test loss: 0.823081430620927\n",
      "Iteration 6513 training loss: 0.2015032563088859, test loss: 0.8230982084187805\n",
      "Iteration 6514 training loss: 0.20148284702864808, test loss: 0.8231149854032461\n",
      "Iteration 6515 training loss: 0.20146244173537803, test loss: 0.8231317615743402\n",
      "Iteration 6516 training loss: 0.20144204042789748, test loss: 0.8231485369320785\n",
      "Iteration 6517 training loss: 0.2014216431050289, test loss: 0.8231653114764768\n",
      "Iteration 6518 training loss: 0.20140124976559495, test loss: 0.8231820852075505\n",
      "Iteration 6519 training loss: 0.20138086040841902, test loss: 0.8231988581253168\n",
      "Iteration 6520 training loss: 0.20136047503232476, test loss: 0.8232156302297905\n",
      "Iteration 6521 training loss: 0.2013400936361364, test loss: 0.8232324015209882\n",
      "Iteration 6522 training loss: 0.20131971621867875, test loss: 0.8232491719989262\n",
      "Iteration 6523 training loss: 0.2012993427787768, test loss: 0.8232659416636201\n",
      "Iteration 6524 training loss: 0.20127897331525638, test loss: 0.8232827105150863\n",
      "Iteration 6525 training loss: 0.20125860782694344, test loss: 0.8232994785533405\n",
      "Iteration 6526 training loss: 0.20123824631266463, test loss: 0.8233162457783995\n",
      "Iteration 6527 training loss: 0.201217888771247, test loss: 0.8233330121902791\n",
      "Iteration 6528 training loss: 0.20119753520151804, test loss: 0.823349777788996\n",
      "Iteration 6529 training loss: 0.20117718560230577, test loss: 0.8233665425745655\n",
      "Iteration 6530 training loss: 0.2011568399724387, test loss: 0.8233833065470051\n",
      "Iteration 6531 training loss: 0.20113649831074568, test loss: 0.8234000697063306\n",
      "Iteration 6532 training loss: 0.20111616061605625, test loss: 0.8234168320525583\n",
      "Iteration 6533 training loss: 0.2010958268872001, test loss: 0.8234335935857047\n",
      "Iteration 6534 training loss: 0.20107549712300776, test loss: 0.8234503543057861\n",
      "Iteration 6535 training loss: 0.20105517132230993, test loss: 0.823467114212819\n",
      "Iteration 6536 training loss: 0.20103484948393804, test loss: 0.8234838733068202\n",
      "Iteration 6537 training loss: 0.20101453160672367, test loss: 0.8235006315878058\n",
      "Iteration 6538 training loss: 0.20099421768949916, test loss: 0.8235173890557925\n",
      "Iteration 6539 training loss: 0.20097390773109713, test loss: 0.8235341457107967\n",
      "Iteration 6540 training loss: 0.20095360173035082, test loss: 0.8235509015528356\n",
      "Iteration 6541 training loss: 0.20093329968609383, test loss: 0.8235676565819253\n",
      "Iteration 6542 training loss: 0.20091300159716022, test loss: 0.8235844107980829\n",
      "Iteration 6543 training loss: 0.20089270746238463, test loss: 0.8236011642013243\n",
      "Iteration 6544 training loss: 0.20087241728060207, test loss: 0.8236179167916672\n",
      "Iteration 6545 training loss: 0.20085213105064803, test loss: 0.8236346685691276\n",
      "Iteration 6546 training loss: 0.20083184877135848, test loss: 0.8236514195337229\n",
      "Iteration 6547 training loss: 0.20081157044156994, test loss: 0.8236681696854693\n",
      "Iteration 6548 training loss: 0.2007912960601191, test loss: 0.8236849190243845\n",
      "Iteration 6549 training loss: 0.20077102562584356, test loss: 0.8237016675504846\n",
      "Iteration 6550 training loss: 0.20075075913758103, test loss: 0.8237184152637866\n",
      "Iteration 6551 training loss: 0.2007304965941699, test loss: 0.8237351621643079\n",
      "Iteration 6552 training loss: 0.20071023799444881, test loss: 0.8237519082520653\n",
      "Iteration 6553 training loss: 0.2006899833372571, test loss: 0.8237686535270756\n",
      "Iteration 6554 training loss: 0.20066973262143445, test loss: 0.823785397989356\n",
      "Iteration 6555 training loss: 0.20064948584582099, test loss: 0.8238021416389238\n",
      "Iteration 6556 training loss: 0.2006292430092574, test loss: 0.8238188844757955\n",
      "Iteration 6557 training loss: 0.20060900411058472, test loss: 0.8238356264999892\n",
      "Iteration 6558 training loss: 0.20058876914864454, test loss: 0.8238523677115206\n",
      "Iteration 6559 training loss: 0.20056853812227884, test loss: 0.8238691081104084\n",
      "Iteration 6560 training loss: 0.2005483110303302, test loss: 0.8238858476966685\n",
      "Iteration 6561 training loss: 0.20052808787164147, test loss: 0.8239025864703191\n",
      "Iteration 6562 training loss: 0.20050786864505607, test loss: 0.8239193244313773\n",
      "Iteration 6563 training loss: 0.20048765334941787, test loss: 0.8239360615798601\n",
      "Iteration 6564 training loss: 0.20046744198357136, test loss: 0.8239527979157852\n",
      "Iteration 6565 training loss: 0.20044723454636115, test loss: 0.8239695334391696\n",
      "Iteration 6566 training loss: 0.2004270310366326, test loss: 0.8239862681500313\n",
      "Iteration 6567 training loss: 0.20040683145323135, test loss: 0.8240030020483867\n",
      "Iteration 6568 training loss: 0.20038663579500365, test loss: 0.8240197351342545\n",
      "Iteration 6569 training loss: 0.20036644406079615, test loss: 0.8240364674076511\n",
      "Iteration 6570 training loss: 0.20034625624945598, test loss: 0.8240531988685947\n",
      "Iteration 6571 training loss: 0.20032607235983066, test loss: 0.8240699295171027\n",
      "Iteration 6572 training loss: 0.20030589239076824, test loss: 0.8240866593531927\n",
      "Iteration 6573 training loss: 0.20028571634111722, test loss: 0.8241033883768821\n",
      "Iteration 6574 training loss: 0.2002655442097265, test loss: 0.8241201165881886\n",
      "Iteration 6575 training loss: 0.20024537599544556, test loss: 0.8241368439871303\n",
      "Iteration 6576 training loss: 0.2002252116971243, test loss: 0.8241535705737245\n",
      "Iteration 6577 training loss: 0.20020505131361294, test loss: 0.8241702963479884\n",
      "Iteration 6578 training loss: 0.2001848948437624, test loss: 0.8241870213099409\n",
      "Iteration 6579 training loss: 0.20016474228642386, test loss: 0.8242037454595993\n",
      "Iteration 6580 training loss: 0.200144593640449, test loss: 0.8242204687969813\n",
      "Iteration 6581 training loss: 0.20012444890469006, test loss: 0.8242371913221045\n",
      "Iteration 6582 training loss: 0.20010430807799964, test loss: 0.8242539130349874\n",
      "Iteration 6583 training loss: 0.2000841711592308, test loss: 0.8242706339356474\n",
      "Iteration 6584 training loss: 0.20006403814723708, test loss: 0.8242873540241025\n",
      "Iteration 6585 training loss: 0.20004390904087258, test loss: 0.8243040733003711\n",
      "Iteration 6586 training loss: 0.2000237838389916, test loss: 0.8243207917644708\n",
      "Iteration 6587 training loss: 0.2000036625404492, test loss: 0.8243375094164199\n",
      "Iteration 6588 training loss: 0.1999835451441007, test loss: 0.8243542262562361\n",
      "Iteration 6589 training loss: 0.19996343164880193, test loss: 0.8243709422839377\n",
      "Iteration 6590 training loss: 0.19994332205340917, test loss: 0.824387657499543\n",
      "Iteration 6591 training loss: 0.1999232163567792, test loss: 0.8244043719030696\n",
      "Iteration 6592 training loss: 0.19990311455776916, test loss: 0.8244210854945363\n",
      "Iteration 6593 training loss: 0.19988301665523672, test loss: 0.8244377982739609\n",
      "Iteration 6594 training loss: 0.19986292264804006, test loss: 0.824454510241362\n",
      "Iteration 6595 training loss: 0.19984283253503765, test loss: 0.8244712213967572\n",
      "Iteration 6596 training loss: 0.19982274631508862, test loss: 0.8244879317401653\n",
      "Iteration 6597 training loss: 0.19980266398705235, test loss: 0.8245046412716046\n",
      "Iteration 6598 training loss: 0.1997825855497888, test loss: 0.8245213499910939\n",
      "Iteration 6599 training loss: 0.19976251100215842, test loss: 0.8245380578986504\n",
      "Iteration 6600 training loss: 0.19974244034302197, test loss: 0.8245547649942938\n",
      "Iteration 6601 training loss: 0.19972237357124079, test loss: 0.8245714712780416\n",
      "Iteration 6602 training loss: 0.19970231068567665, test loss: 0.8245881767499128\n",
      "Iteration 6603 training loss: 0.19968225168519163, test loss: 0.8246048814099254\n",
      "Iteration 6604 training loss: 0.19966219656864856, test loss: 0.8246215852580986\n",
      "Iteration 6605 training loss: 0.1996421453349105, test loss: 0.8246382882944504\n",
      "Iteration 6606 training loss: 0.19962209798284086, test loss: 0.8246549905189998\n",
      "Iteration 6607 training loss: 0.19960205451130386, test loss: 0.824671691931765\n",
      "Iteration 6608 training loss: 0.19958201491916386, test loss: 0.8246883925327654\n",
      "Iteration 6609 training loss: 0.19956197920528584, test loss: 0.8247050923220186\n",
      "Iteration 6610 training loss: 0.1995419473685351, test loss: 0.8247217912995439\n",
      "Iteration 6611 training loss: 0.19952191940777753, test loss: 0.8247384894653602\n",
      "Iteration 6612 training loss: 0.1995018953218794, test loss: 0.8247551868194861\n",
      "Iteration 6613 training loss: 0.19948187510970739, test loss: 0.8247718833619402\n",
      "Iteration 6614 training loss: 0.19946185877012862, test loss: 0.8247885790927416\n",
      "Iteration 6615 training loss: 0.19944184630201092, test loss: 0.8248052740119091\n",
      "Iteration 6616 training loss: 0.1994218377042222, test loss: 0.8248219681194614\n",
      "Iteration 6617 training loss: 0.19940183297563105, test loss: 0.8248386614154174\n",
      "Iteration 6618 training loss: 0.1993818321151065, test loss: 0.8248553538997964\n",
      "Iteration 6619 training loss: 0.19936183512151787, test loss: 0.8248720455726172\n",
      "Iteration 6620 training loss: 0.1993418419937351, test loss: 0.8248887364338986\n",
      "Iteration 6621 training loss: 0.19932185273062852, test loss: 0.8249054264836599\n",
      "Iteration 6622 training loss: 0.19930186733106892, test loss: 0.8249221157219202\n",
      "Iteration 6623 training loss: 0.19928188579392747, test loss: 0.824938804148698\n",
      "Iteration 6624 training loss: 0.19926190811807595, test loss: 0.8249554917640132\n",
      "Iteration 6625 training loss: 0.19924193430238635, test loss: 0.8249721785678843\n",
      "Iteration 6626 training loss: 0.19922196434573142, test loss: 0.824988864560331\n",
      "Iteration 6627 training loss: 0.19920199824698398, test loss: 0.8250055497413721\n",
      "Iteration 6628 training loss: 0.1991820360050176, test loss: 0.8250222341110272\n",
      "Iteration 6629 training loss: 0.19916207761870627, test loss: 0.8250389176693151\n",
      "Iteration 6630 training loss: 0.19914212308692428, test loss: 0.8250556004162556\n",
      "Iteration 6631 training loss: 0.19912217240854638, test loss: 0.8250722823518678\n",
      "Iteration 6632 training loss: 0.19910222558244797, test loss: 0.8250889634761706\n",
      "Iteration 6633 training loss: 0.19908228260750463, test loss: 0.8251056437891844\n",
      "Iteration 6634 training loss: 0.19906234348259255, test loss: 0.8251223232909276\n",
      "Iteration 6635 training loss: 0.19904240820658836, test loss: 0.8251390019814201\n",
      "Iteration 6636 training loss: 0.1990224767783691, test loss: 0.8251556798606812\n",
      "Iteration 6637 training loss: 0.1990025491968122, test loss: 0.8251723569287308\n",
      "Iteration 6638 training loss: 0.1989826254607957, test loss: 0.8251890331855878\n",
      "Iteration 6639 training loss: 0.19896270556919785, test loss: 0.8252057086312724\n",
      "Iteration 6640 training loss: 0.19894278952089767, test loss: 0.8252223832658037\n",
      "Iteration 6641 training loss: 0.19892287731477423, test loss: 0.8252390570892014\n",
      "Iteration 6642 training loss: 0.19890296894970735, test loss: 0.8252557301014853\n",
      "Iteration 6643 training loss: 0.19888306442457715, test loss: 0.825272402302675\n",
      "Iteration 6644 training loss: 0.1988631637382643, test loss: 0.8252890736927899\n",
      "Iteration 6645 training loss: 0.19884326688964973, test loss: 0.8253057442718503\n",
      "Iteration 6646 training loss: 0.19882337387761503, test loss: 0.8253224140398754\n",
      "Iteration 6647 training loss: 0.19880348470104214, test loss: 0.8253390829968854\n",
      "Iteration 6648 training loss: 0.19878359935881337, test loss: 0.8253557511429\n",
      "Iteration 6649 training loss: 0.19876371784981164, test loss: 0.8253724184779386\n",
      "Iteration 6650 training loss: 0.1987438401729202, test loss: 0.8253890850020217\n",
      "Iteration 6651 training loss: 0.19872396632702263, test loss: 0.8254057507151689\n",
      "Iteration 6652 training loss: 0.19870409631100325, test loss: 0.8254224156174\n",
      "Iteration 6653 training loss: 0.19868423012374653, test loss: 0.8254390797087353\n",
      "Iteration 6654 training loss: 0.19866436776413754, test loss: 0.8254557429891947\n",
      "Iteration 6655 training loss: 0.19864450923106178, test loss: 0.8254724054587975\n",
      "Iteration 6656 training loss: 0.19862465452340514, test loss: 0.8254890671175649\n",
      "Iteration 6657 training loss: 0.19860480364005403, test loss: 0.825505727965516\n",
      "Iteration 6658 training loss: 0.19858495657989525, test loss: 0.8255223880026715\n",
      "Iteration 6659 training loss: 0.19856511334181595, test loss: 0.8255390472290512\n",
      "Iteration 6660 training loss: 0.19854527392470392, test loss: 0.8255557056446757\n",
      "Iteration 6661 training loss: 0.19852543832744715, test loss: 0.8255723632495648\n",
      "Iteration 6662 training loss: 0.19850560654893437, test loss: 0.8255890200437384\n",
      "Iteration 6663 training loss: 0.1984857785880545, test loss: 0.8256056760272172\n",
      "Iteration 6664 training loss: 0.198465954443697, test loss: 0.8256223312000213\n",
      "Iteration 6665 training loss: 0.19844613411475168, test loss: 0.8256389855621715\n",
      "Iteration 6666 training loss: 0.19842631760010893, test loss: 0.8256556391136874\n",
      "Iteration 6667 training loss: 0.19840650489865955, test loss: 0.8256722918545896\n",
      "Iteration 6668 training loss: 0.1983866960092947, test loss: 0.8256889437848985\n",
      "Iteration 6669 training loss: 0.19836689093090595, test loss: 0.8257055949046345\n",
      "Iteration 6670 training loss: 0.19834708966238548, test loss: 0.825722245213818\n",
      "Iteration 6671 training loss: 0.19832729220262574, test loss: 0.8257388947124698\n",
      "Iteration 6672 training loss: 0.19830749855051968, test loss: 0.8257555434006097\n",
      "Iteration 6673 training loss: 0.1982877087049608, test loss: 0.8257721912782587\n",
      "Iteration 6674 training loss: 0.19826792266484278, test loss: 0.8257888383454377\n",
      "Iteration 6675 training loss: 0.19824814042906, test loss: 0.8258054846021664\n",
      "Iteration 6676 training loss: 0.1982283619965071, test loss: 0.8258221300484659\n",
      "Iteration 6677 training loss: 0.19820858736607921, test loss: 0.8258387746843568\n",
      "Iteration 6678 training loss: 0.19818881653667195, test loss: 0.8258554185098599\n",
      "Iteration 6679 training loss: 0.1981690495071813, test loss: 0.8258720615249958\n",
      "Iteration 6680 training loss: 0.19814928627650374, test loss: 0.825888703729785\n",
      "Iteration 6681 training loss: 0.19812952684353613, test loss: 0.8259053451242485\n",
      "Iteration 6682 training loss: 0.19810977120717582, test loss: 0.8259219857084068\n",
      "Iteration 6683 training loss: 0.19809001936632054, test loss: 0.8259386254822808\n",
      "Iteration 6684 training loss: 0.19807027131986846, test loss: 0.8259552644458918\n",
      "Iteration 6685 training loss: 0.19805052706671822, test loss: 0.8259719025992599\n",
      "Iteration 6686 training loss: 0.19803078660576895, test loss: 0.8259885399424066\n",
      "Iteration 6687 training loss: 0.19801104993592006, test loss: 0.8260051764753525\n",
      "Iteration 6688 training loss: 0.19799131705607154, test loss: 0.8260218121981187\n",
      "Iteration 6689 training loss: 0.19797158796512376, test loss: 0.8260384471107258\n",
      "Iteration 6690 training loss: 0.1979518626619774, test loss: 0.8260550812131953\n",
      "Iteration 6691 training loss: 0.1979321411455338, test loss: 0.826071714505548\n",
      "Iteration 6692 training loss: 0.19791242341469464, test loss: 0.8260883469878048\n",
      "Iteration 6693 training loss: 0.19789270946836202, test loss: 0.8261049786599868\n",
      "Iteration 6694 training loss: 0.19787299930543842, test loss: 0.8261216095221158\n",
      "Iteration 6695 training loss: 0.19785329292482678, test loss: 0.8261382395742117\n",
      "Iteration 6696 training loss: 0.1978335903254306, test loss: 0.8261548688162969\n",
      "Iteration 6697 training loss: 0.19781389150615364, test loss: 0.8261714972483919\n",
      "Iteration 6698 training loss: 0.19779419646590013, test loss: 0.8261881248705177\n",
      "Iteration 6699 training loss: 0.19777450520357487, test loss: 0.8262047516826961\n",
      "Iteration 6700 training loss: 0.1977548177180829, test loss: 0.8262213776849484\n",
      "Iteration 6701 training loss: 0.19773513400832982, test loss: 0.8262380028772959\n",
      "Iteration 6702 training loss: 0.1977154540732216, test loss: 0.8262546272597592\n",
      "Iteration 6703 training loss: 0.1976957779116647, test loss: 0.8262712508323603\n",
      "Iteration 6704 training loss: 0.19767610552256595, test loss: 0.8262878735951205\n",
      "Iteration 6705 training loss: 0.1976564369048326, test loss: 0.8263044955480612\n",
      "Iteration 6706 training loss: 0.1976367720573724, test loss: 0.8263211166912037\n",
      "Iteration 6707 training loss: 0.1976171109790935, test loss: 0.8263377370245696\n",
      "Iteration 6708 training loss: 0.19759745366890444, test loss: 0.8263543565481805\n",
      "Iteration 6709 training loss: 0.19757780012571427, test loss: 0.826370975262058\n",
      "Iteration 6710 training loss: 0.19755815034843235, test loss: 0.8263875931662231\n",
      "Iteration 6711 training loss: 0.1975385043359686, test loss: 0.8264042102606981\n",
      "Iteration 6712 training loss: 0.19751886208723335, test loss: 0.8264208265455038\n",
      "Iteration 6713 training loss: 0.1974992236011372, test loss: 0.8264374420206627\n",
      "Iteration 6714 training loss: 0.19747958887659142, test loss: 0.8264540566861958\n",
      "Iteration 6715 training loss: 0.19745995791250753, test loss: 0.8264706705421251\n",
      "Iteration 6716 training loss: 0.19744033070779757, test loss: 0.8264872835884722\n",
      "Iteration 6717 training loss: 0.19742070726137392, test loss: 0.8265038958252591\n",
      "Iteration 6718 training loss: 0.19740108757214947, test loss: 0.8265205072525074\n",
      "Iteration 6719 training loss: 0.1973814716390375, test loss: 0.8265371178702385\n",
      "Iteration 6720 training loss: 0.19736185946095178, test loss: 0.8265537276784748\n",
      "Iteration 6721 training loss: 0.19734225103680636, test loss: 0.8265703366772383\n",
      "Iteration 6722 training loss: 0.19732264636551597, test loss: 0.82658694486655\n",
      "Iteration 6723 training loss: 0.19730304544599542, test loss: 0.8266035522464327\n",
      "Iteration 6724 training loss: 0.19728344827716027, test loss: 0.8266201588169078\n",
      "Iteration 6725 training loss: 0.19726385485792627, test loss: 0.8266367645779974\n",
      "Iteration 6726 training loss: 0.19724426518720978, test loss: 0.8266533695297237\n",
      "Iteration 6727 training loss: 0.1972246792639275, test loss: 0.8266699736721084\n",
      "Iteration 6728 training loss: 0.19720509708699654, test loss: 0.8266865770051741\n",
      "Iteration 6729 training loss: 0.19718551865533449, test loss: 0.826703179528942\n",
      "Iteration 6730 training loss: 0.19716594396785922, test loss: 0.8267197812434349\n",
      "Iteration 6731 training loss: 0.19714637302348922, test loss: 0.826736382148675\n",
      "Iteration 6732 training loss: 0.19712680582114334, test loss: 0.8267529822446837\n",
      "Iteration 6733 training loss: 0.19710724235974078, test loss: 0.8267695815314836\n",
      "Iteration 6734 training loss: 0.19708768263820126, test loss: 0.8267861800090975\n",
      "Iteration 6735 training loss: 0.1970681266554449, test loss: 0.8268027776775467\n",
      "Iteration 6736 training loss: 0.19704857441039217, test loss: 0.826819374536854\n",
      "Iteration 6737 training loss: 0.19702902590196403, test loss: 0.8268359705870414\n",
      "Iteration 6738 training loss: 0.19700948112908195, test loss: 0.8268525658281312\n",
      "Iteration 6739 training loss: 0.1969899400906676, test loss: 0.8268691602601462\n",
      "Iteration 6740 training loss: 0.1969704027856433, test loss: 0.8268857538831084\n",
      "Iteration 6741 training loss: 0.1969508692129316, test loss: 0.8269023466970403\n",
      "Iteration 6742 training loss: 0.1969313393714556, test loss: 0.8269189387019643\n",
      "Iteration 6743 training loss: 0.1969118132601389, test loss: 0.826935529897903\n",
      "Iteration 6744 training loss: 0.1968922908779053, test loss: 0.8269521202848783\n",
      "Iteration 6745 training loss: 0.1968727722236792, test loss: 0.8269687098629136\n",
      "Iteration 6746 training loss: 0.19685325729638523, test loss: 0.8269852986320311\n",
      "Iteration 6747 training loss: 0.1968337460949487, test loss: 0.827001886592253\n",
      "Iteration 6748 training loss: 0.1968142386182951, test loss: 0.8270184737436022\n",
      "Iteration 6749 training loss: 0.19679473486535068, test loss: 0.8270350600861012\n",
      "Iteration 6750 training loss: 0.19677523483504158, test loss: 0.8270516456197724\n",
      "Iteration 6751 training loss: 0.19675573852629488, test loss: 0.8270682303446392\n",
      "Iteration 6752 training loss: 0.19673624593803776, test loss: 0.8270848142607234\n",
      "Iteration 6753 training loss: 0.19671675706919797, test loss: 0.8271013973680486\n",
      "Iteration 6754 training loss: 0.19669727191870362, test loss: 0.8271179796666368\n",
      "Iteration 6755 training loss: 0.1966777904854832, test loss: 0.8271345611565113\n",
      "Iteration 6756 training loss: 0.19665831276846577, test loss: 0.8271511418376946\n",
      "Iteration 6757 training loss: 0.19663883876658067, test loss: 0.8271677217102097\n",
      "Iteration 6758 training loss: 0.19661936847875772, test loss: 0.8271843007740796\n",
      "Iteration 6759 training loss: 0.19659990190392712, test loss: 0.8272008790293266\n",
      "Iteration 6760 training loss: 0.19658043904101954, test loss: 0.8272174564759741\n",
      "Iteration 6761 training loss: 0.19656097988896604, test loss: 0.827234033114045\n",
      "Iteration 6762 training loss: 0.19654152444669798, test loss: 0.827250608943562\n",
      "Iteration 6763 training loss: 0.19652207271314742, test loss: 0.8272671839645482\n",
      "Iteration 6764 training loss: 0.1965026246872466, test loss: 0.8272837581770272\n",
      "Iteration 6765 training loss: 0.1964831803679283, test loss: 0.8273003315810212\n",
      "Iteration 6766 training loss: 0.19646373975412562, test loss: 0.8273169041765535\n",
      "Iteration 6767 training loss: 0.19644430284477218, test loss: 0.8273334759636473\n",
      "Iteration 6768 training loss: 0.19642486963880187, test loss: 0.8273500469423258\n",
      "Iteration 6769 training loss: 0.19640544013514916, test loss: 0.8273666171126124\n",
      "Iteration 6770 training loss: 0.1963860143327489, test loss: 0.8273831864745297\n",
      "Iteration 6771 training loss: 0.1963665922305363, test loss: 0.827399755028101\n",
      "Iteration 6772 training loss: 0.19634717382744699, test loss: 0.8274163227733495\n",
      "Iteration 6773 training loss: 0.19632775912241704, test loss: 0.827432889710299\n",
      "Iteration 6774 training loss: 0.19630834811438294, test loss: 0.8274494558389723\n",
      "Iteration 6775 training loss: 0.19628894080228163, test loss: 0.8274660211593933\n",
      "Iteration 6776 training loss: 0.19626953718505039, test loss: 0.827482585671584\n",
      "Iteration 6777 training loss: 0.19625013726162696, test loss: 0.827499149375569\n",
      "Iteration 6778 training loss: 0.19623074103094945, test loss: 0.8275157122713713\n",
      "Iteration 6779 training loss: 0.19621134849195648, test loss: 0.8275322743590144\n",
      "Iteration 6780 training loss: 0.196191959643587, test loss: 0.8275488356385217\n",
      "Iteration 6781 training loss: 0.19617257448478045, test loss: 0.8275653961099164\n",
      "Iteration 6782 training loss: 0.19615319301447656, test loss: 0.8275819557732226\n",
      "Iteration 6783 training loss: 0.19613381523161555, test loss: 0.8275985146284631\n",
      "Iteration 6784 training loss: 0.1961144411351381, test loss: 0.827615072675662\n",
      "Iteration 6785 training loss: 0.1960950707239853, test loss: 0.8276316299148425\n",
      "Iteration 6786 training loss: 0.19607570399709848, test loss: 0.8276481863460283\n",
      "Iteration 6787 training loss: 0.19605634095341962, test loss: 0.8276647419692433\n",
      "Iteration 6788 training loss: 0.19603698159189104, test loss: 0.827681296784511\n",
      "Iteration 6789 training loss: 0.19601762591145525, test loss: 0.8276978507918544\n",
      "Iteration 6790 training loss: 0.19599827391105562, test loss: 0.8277144039912985\n",
      "Iteration 6791 training loss: 0.19597892558963548, test loss: 0.827730956382866\n",
      "Iteration 6792 training loss: 0.19595958094613883, test loss: 0.8277475079665811\n",
      "Iteration 6793 training loss: 0.19594023997951007, test loss: 0.8277640587424676\n",
      "Iteration 6794 training loss: 0.1959209026886939, test loss: 0.8277806087105491\n",
      "Iteration 6795 training loss: 0.19590156907263553, test loss: 0.8277971578708494\n",
      "Iteration 6796 training loss: 0.19588223913028044, test loss: 0.8278137062233927\n",
      "Iteration 6797 training loss: 0.19586291286057483, test loss: 0.8278302537682026\n",
      "Iteration 6798 training loss: 0.19584359026246492, test loss: 0.8278468005053028\n",
      "Iteration 6799 training loss: 0.19582427133489763, test loss: 0.8278633464347178\n",
      "Iteration 6800 training loss: 0.19580495607682014, test loss: 0.8278798915564715\n",
      "Iteration 6801 training loss: 0.19578564448718014, test loss: 0.8278964358705876\n",
      "Iteration 6802 training loss: 0.19576633656492562, test loss: 0.82791297937709\n",
      "Iteration 6803 training loss: 0.1957470323090051, test loss: 0.8279295220760029\n",
      "Iteration 6804 training loss: 0.1957277317183674, test loss: 0.8279460639673506\n",
      "Iteration 6805 training loss: 0.19570843479196182, test loss: 0.8279626050511567\n",
      "Iteration 6806 training loss: 0.195689141528738, test loss: 0.827979145327446\n",
      "Iteration 6807 training loss: 0.1956698519276462, test loss: 0.8279956847962422\n",
      "Iteration 6808 training loss: 0.19565056598763672, test loss: 0.8280122234575694\n",
      "Iteration 6809 training loss: 0.1956312837076606, test loss: 0.8280287613114519\n",
      "Iteration 6810 training loss: 0.19561200508666912, test loss: 0.8280452983579141\n",
      "Iteration 6811 training loss: 0.19559273012361408, test loss: 0.8280618345969802\n",
      "Iteration 6812 training loss: 0.19557345881744748, test loss: 0.8280783700286741\n",
      "Iteration 6813 training loss: 0.19555419116712203, test loss: 0.8280949046530206\n",
      "Iteration 6814 training loss: 0.1955349271715906, test loss: 0.8281114384700439\n",
      "Iteration 6815 training loss: 0.1955156668298065, test loss: 0.828127971479768\n",
      "Iteration 6816 training loss: 0.19549641014072358, test loss: 0.8281445036822176\n",
      "Iteration 6817 training loss: 0.19547715710329608, test loss: 0.8281610350774173\n",
      "Iteration 6818 training loss: 0.19545790771647847, test loss: 0.8281775656653912\n",
      "Iteration 6819 training loss: 0.1954386619792258, test loss: 0.828194095446164\n",
      "Iteration 6820 training loss: 0.19541941989049344, test loss: 0.8282106244197597\n",
      "Iteration 6821 training loss: 0.19540018144923724, test loss: 0.8282271525862032\n",
      "Iteration 6822 training loss: 0.19538094665441338, test loss: 0.8282436799455192\n",
      "Iteration 6823 training loss: 0.19536171550497844, test loss: 0.8282602064977316\n",
      "Iteration 6824 training loss: 0.1953424879998895, test loss: 0.8282767322428657\n",
      "Iteration 6825 training loss: 0.19532326413810402, test loss: 0.8282932571809459\n",
      "Iteration 6826 training loss: 0.19530404391857975, test loss: 0.8283097813119964\n",
      "Iteration 6827 training loss: 0.19528482734027497, test loss: 0.8283263046360426\n",
      "Iteration 6828 training loss: 0.19526561440214835, test loss: 0.8283428271531087\n",
      "Iteration 6829 training loss: 0.19524640510315888, test loss: 0.8283593488632194\n",
      "Iteration 6830 training loss: 0.19522719944226608, test loss: 0.8283758697663994\n",
      "Iteration 6831 training loss: 0.19520799741842976, test loss: 0.828392389862674\n",
      "Iteration 6832 training loss: 0.19518879903061018, test loss: 0.8284089091520676\n",
      "Iteration 6833 training loss: 0.19516960427776806, test loss: 0.828425427634605\n",
      "Iteration 6834 training loss: 0.19515041315886442, test loss: 0.828441945310311\n",
      "Iteration 6835 training loss: 0.1951312256728607, test loss: 0.8284584621792106\n",
      "Iteration 6836 training loss: 0.19511204181871886, test loss: 0.8284749782413284\n",
      "Iteration 6837 training loss: 0.19509286159540118, test loss: 0.8284914934966896\n",
      "Iteration 6838 training loss: 0.19507368500187025, test loss: 0.8285080079453191\n",
      "Iteration 6839 training loss: 0.1950545120370893, test loss: 0.828524521587242\n",
      "Iteration 6840 training loss: 0.19503534270002165, test loss: 0.8285410344224831\n",
      "Iteration 6841 training loss: 0.1950161769896313, test loss: 0.8285575464510675\n",
      "Iteration 6842 training loss: 0.1949970149048825, test loss: 0.8285740576730197\n",
      "Iteration 6843 training loss: 0.19497785644474003, test loss: 0.8285905680883658\n",
      "Iteration 6844 training loss: 0.1949587016081688, test loss: 0.82860707769713\n",
      "Iteration 6845 training loss: 0.19493955039413455, test loss: 0.8286235864993381\n",
      "Iteration 6846 training loss: 0.194920402801603, test loss: 0.828640094495015\n",
      "Iteration 6847 training loss: 0.1949012588295405, test loss: 0.8286566016841856\n",
      "Iteration 6848 training loss: 0.19488211847691372, test loss: 0.8286731080668751\n",
      "Iteration 6849 training loss: 0.1948629817426899, test loss: 0.8286896136431092\n",
      "Iteration 6850 training loss: 0.1948438486258364, test loss: 0.8287061184129129\n",
      "Iteration 6851 training loss: 0.19482471912532118, test loss: 0.828722622376311\n",
      "Iteration 6852 training loss: 0.19480559324011248, test loss: 0.8287391255333296\n",
      "Iteration 6853 training loss: 0.19478647096917917, test loss: 0.8287556278839936\n",
      "Iteration 6854 training loss: 0.19476735231149014, test loss: 0.8287721294283283\n",
      "Iteration 6855 training loss: 0.194748237266015, test loss: 0.8287886301663593\n",
      "Iteration 6856 training loss: 0.1947291258317236, test loss: 0.8288051300981115\n",
      "Iteration 6857 training loss: 0.19471001800758633, test loss: 0.828821629223611\n",
      "Iteration 6858 training loss: 0.1946909137925739, test loss: 0.8288381275428829\n",
      "Iteration 6859 training loss: 0.19467181318565735, test loss: 0.8288546250559528\n",
      "Iteration 6860 training loss: 0.1946527161858081, test loss: 0.8288711217628456\n",
      "Iteration 6861 training loss: 0.19463362279199817, test loss: 0.8288876176635878\n",
      "Iteration 6862 training loss: 0.19461453300319975, test loss: 0.8289041127582041\n",
      "Iteration 6863 training loss: 0.1945954468183857, test loss: 0.8289206070467208\n",
      "Iteration 6864 training loss: 0.19457636423652896, test loss: 0.8289371005291629\n",
      "Iteration 6865 training loss: 0.19455728525660304, test loss: 0.8289535932055563\n",
      "Iteration 6866 training loss: 0.19453820987758189, test loss: 0.8289700850759264\n",
      "Iteration 6867 training loss: 0.19451913809843963, test loss: 0.8289865761402995\n",
      "Iteration 6868 training loss: 0.19450006991815122, test loss: 0.8290030663987006\n",
      "Iteration 6869 training loss: 0.19448100533569146, test loss: 0.8290195558511553\n",
      "Iteration 6870 training loss: 0.19446194435003594, test loss: 0.8290360444976904\n",
      "Iteration 6871 training loss: 0.19444288696016057, test loss: 0.8290525323383304\n",
      "Iteration 6872 training loss: 0.19442383316504153, test loss: 0.8290690193731022\n",
      "Iteration 6873 training loss: 0.19440478296365554, test loss: 0.8290855056020311\n",
      "Iteration 6874 training loss: 0.19438573635497963, test loss: 0.8291019910251425\n",
      "Iteration 6875 training loss: 0.19436669333799128, test loss: 0.8291184756424631\n",
      "Iteration 6876 training loss: 0.1943476539116683, test loss: 0.8291349594540182\n",
      "Iteration 6877 training loss: 0.19432861807498894, test loss: 0.829151442459834\n",
      "Iteration 6878 training loss: 0.19430958582693184, test loss: 0.8291679246599365\n",
      "Iteration 6879 training loss: 0.19429055716647606, test loss: 0.8291844060543513\n",
      "Iteration 6880 training loss: 0.19427153209260106, test loss: 0.8292008866431049\n",
      "Iteration 6881 training loss: 0.19425251060428655, test loss: 0.8292173664262228\n",
      "Iteration 6882 training loss: 0.1942334927005128, test loss: 0.8292338454037315\n",
      "Iteration 6883 training loss: 0.19421447838026043, test loss: 0.8292503235756568\n",
      "Iteration 6884 training loss: 0.19419546764251044, test loss: 0.829266800942025\n",
      "Iteration 6885 training loss: 0.19417646048624426, test loss: 0.8292832775028618\n",
      "Iteration 6886 training loss: 0.19415745691044367, test loss: 0.8292997532581939\n",
      "Iteration 6887 training loss: 0.19413845691409076, test loss: 0.8293162282080472\n",
      "Iteration 6888 training loss: 0.19411946049616824, test loss: 0.8293327023524476\n",
      "Iteration 6889 training loss: 0.19410046765565903, test loss: 0.8293491756914217\n",
      "Iteration 6890 training loss: 0.19408147839154646, test loss: 0.8293656482249958\n",
      "Iteration 6891 training loss: 0.19406249270281434, test loss: 0.8293821199531958\n",
      "Iteration 6892 training loss: 0.19404351058844677, test loss: 0.8293985908760482\n",
      "Iteration 6893 training loss: 0.19402453204742828, test loss: 0.8294150609935794\n",
      "Iteration 6894 training loss: 0.19400555707874387, test loss: 0.8294315303058155\n",
      "Iteration 6895 training loss: 0.19398658568137886, test loss: 0.8294479988127833\n",
      "Iteration 6896 training loss: 0.19396761785431887, test loss: 0.8294644665145086\n",
      "Iteration 6897 training loss: 0.19394865359655009, test loss: 0.8294809334110184\n",
      "Iteration 6898 training loss: 0.19392969290705903, test loss: 0.8294973995023384\n",
      "Iteration 6899 training loss: 0.19391073578483253, test loss: 0.8295138647884956\n",
      "Iteration 6900 training loss: 0.1938917822288578, test loss: 0.8295303292695164\n",
      "Iteration 6901 training loss: 0.19387283223812274, test loss: 0.8295467929454273\n",
      "Iteration 6902 training loss: 0.1938538858116152, test loss: 0.8295632558162548\n",
      "Iteration 6903 training loss: 0.19383494294832368, test loss: 0.8295797178820258\n",
      "Iteration 6904 training loss: 0.19381600364723706, test loss: 0.8295961791427662\n",
      "Iteration 6905 training loss: 0.1937970679073445, test loss: 0.829612639598503\n",
      "Iteration 6906 training loss: 0.19377813572763566, test loss: 0.8296290992492628\n",
      "Iteration 6907 training loss: 0.19375920710710065, test loss: 0.8296455580950725\n",
      "Iteration 6908 training loss: 0.19374028204472973, test loss: 0.8296620161359579\n",
      "Iteration 6909 training loss: 0.19372136053951372, test loss: 0.8296784733719467\n",
      "Iteration 6910 training loss: 0.19370244259044384, test loss: 0.8296949298030651\n",
      "Iteration 6911 training loss: 0.19368352819651163, test loss: 0.82971138542934\n",
      "Iteration 6912 training loss: 0.19366461735670903, test loss: 0.8297278402507988\n",
      "Iteration 6913 training loss: 0.19364571007002834, test loss: 0.8297442942674668\n",
      "Iteration 6914 training loss: 0.1936268063354624, test loss: 0.8297607474793722\n",
      "Iteration 6915 training loss: 0.19360790615200424, test loss: 0.8297771998865409\n",
      "Iteration 6916 training loss: 0.1935890095186474, test loss: 0.8297936514890005\n",
      "Iteration 6917 training loss: 0.1935701164343858, test loss: 0.8298101022867775\n",
      "Iteration 6918 training loss: 0.1935512268982136, test loss: 0.8298265522798993\n",
      "Iteration 6919 training loss: 0.1935323409091257, test loss: 0.829843001468392\n",
      "Iteration 6920 training loss: 0.19351345846611692, test loss: 0.8298594498522833\n",
      "Iteration 6921 training loss: 0.19349457956818278, test loss: 0.8298758974315998\n",
      "Iteration 6922 training loss: 0.19347570421431912, test loss: 0.8298923442063688\n",
      "Iteration 6923 training loss: 0.19345683240352218, test loss: 0.8299087901766168\n",
      "Iteration 6924 training loss: 0.19343796413478853, test loss: 0.8299252353423717\n",
      "Iteration 6925 training loss: 0.19341909940711513, test loss: 0.8299416797036602\n",
      "Iteration 6926 training loss: 0.19340023821949945, test loss: 0.8299581232605089\n",
      "Iteration 6927 training loss: 0.19338138057093907, test loss: 0.8299745660129455\n",
      "Iteration 6928 training loss: 0.1933625264604323, test loss: 0.8299910079609972\n",
      "Iteration 6929 training loss: 0.1933436758869776, test loss: 0.8300074491046908\n",
      "Iteration 6930 training loss: 0.1933248288495739, test loss: 0.8300238894440538\n",
      "Iteration 6931 training loss: 0.19330598534722052, test loss: 0.8300403289791134\n",
      "Iteration 6932 training loss: 0.19328714537891706, test loss: 0.8300567677098968\n",
      "Iteration 6933 training loss: 0.19326830894366365, test loss: 0.8300732056364315\n",
      "Iteration 6934 training loss: 0.19324947604046075, test loss: 0.8300896427587443\n",
      "Iteration 6935 training loss: 0.19323064666830916, test loss: 0.8301060790768631\n",
      "Iteration 6936 training loss: 0.1932118208262101, test loss: 0.8301225145908147\n",
      "Iteration 6937 training loss: 0.1931929985131652, test loss: 0.8301389493006271\n",
      "Iteration 6938 training loss: 0.19317417972817646, test loss: 0.8301553832063269\n",
      "Iteration 6939 training loss: 0.1931553644702462, test loss: 0.8301718163079422\n",
      "Iteration 6940 training loss: 0.19313655273837718, test loss: 0.8301882486055003\n",
      "Iteration 6941 training loss: 0.19311774453157257, test loss: 0.8302046800990284\n",
      "Iteration 6942 training loss: 0.19309893984883592, test loss: 0.830221110788554\n",
      "Iteration 6943 training loss: 0.19308013868917104, test loss: 0.8302375406741052\n",
      "Iteration 6944 training loss: 0.1930613410515822, test loss: 0.8302539697557089\n",
      "Iteration 6945 training loss: 0.19304254693507422, test loss: 0.8302703980333929\n",
      "Iteration 6946 training loss: 0.19302375633865204, test loss: 0.8302868255071845\n",
      "Iteration 6947 training loss: 0.19300496926132105, test loss: 0.8303032521771118\n",
      "Iteration 6948 training loss: 0.19298618570208712, test loss: 0.8303196780432027\n",
      "Iteration 6949 training loss: 0.19296740565995646, test loss: 0.830336103105484\n",
      "Iteration 6950 training loss: 0.19294862913393557, test loss: 0.8303525273639835\n",
      "Iteration 6951 training loss: 0.19292985612303148, test loss: 0.8303689508187294\n",
      "Iteration 6952 training loss: 0.1929110866262515, test loss: 0.8303853734697492\n",
      "Iteration 6953 training loss: 0.19289232064260337, test loss: 0.8304017953170705\n",
      "Iteration 6954 training loss: 0.19287355817109514, test loss: 0.8304182163607214\n",
      "Iteration 6955 training loss: 0.19285479921073528, test loss: 0.8304346366007295\n",
      "Iteration 6956 training loss: 0.1928360437605327, test loss: 0.8304510560371225\n",
      "Iteration 6957 training loss: 0.19281729181949667, test loss: 0.8304674746699283\n",
      "Iteration 6958 training loss: 0.19279854338663666, test loss: 0.8304838924991752\n",
      "Iteration 6959 training loss: 0.19277979846096283, test loss: 0.8305003095248905\n",
      "Iteration 6960 training loss: 0.19276105704148538, test loss: 0.8305167257471022\n",
      "Iteration 6961 training loss: 0.19274231912721526, test loss: 0.8305331411658384\n",
      "Iteration 6962 training loss: 0.1927235847171635, test loss: 0.8305495557811273\n",
      "Iteration 6963 training loss: 0.1927048538103416, test loss: 0.8305659695929967\n",
      "Iteration 6964 training loss: 0.19268612640576152, test loss: 0.8305823826014744\n",
      "Iteration 6965 training loss: 0.19266740250243544, test loss: 0.8305987948065888\n",
      "Iteration 6966 training loss: 0.1926486820993761, test loss: 0.8306152062083673\n",
      "Iteration 6967 training loss: 0.19262996519559647, test loss: 0.8306316168068388\n",
      "Iteration 6968 training loss: 0.19261125179010996, test loss: 0.8306480266020307\n",
      "Iteration 6969 training loss: 0.19259254188193037, test loss: 0.8306644355939719\n",
      "Iteration 6970 training loss: 0.19257383547007184, test loss: 0.8306808437826894\n",
      "Iteration 6971 training loss: 0.19255513255354892, test loss: 0.8306972511682127\n",
      "Iteration 6972 training loss: 0.19253643313137656, test loss: 0.8307136577505692\n",
      "Iteration 6973 training loss: 0.19251773720257, test loss: 0.8307300635297871\n",
      "Iteration 6974 training loss: 0.1924990447661449, test loss: 0.830746468505895\n",
      "Iteration 6975 training loss: 0.19248035582111733, test loss: 0.8307628726789207\n",
      "Iteration 6976 training loss: 0.19246167036650372, test loss: 0.8307792760488931\n",
      "Iteration 6977 training loss: 0.19244298840132087, test loss: 0.8307956786158399\n",
      "Iteration 6978 training loss: 0.19242430992458592, test loss: 0.8308120803797899\n",
      "Iteration 6979 training loss: 0.19240563493531646, test loss: 0.8308284813407711\n",
      "Iteration 6980 training loss: 0.1923869634325303, test loss: 0.8308448814988125\n",
      "Iteration 6981 training loss: 0.19236829541524594, test loss: 0.8308612808539418\n",
      "Iteration 6982 training loss: 0.19234963088248191, test loss: 0.8308776794061873\n",
      "Iteration 6983 training loss: 0.19233096983325737, test loss: 0.8308940771555782\n",
      "Iteration 6984 training loss: 0.19231231226659157, test loss: 0.8309104741021424\n",
      "Iteration 6985 training loss: 0.19229365818150448, test loss: 0.8309268702459089\n",
      "Iteration 6986 training loss: 0.1922750075770162, test loss: 0.8309432655869056\n",
      "Iteration 6987 training loss: 0.19225636045214728, test loss: 0.8309596601251616\n",
      "Iteration 6988 training loss: 0.19223771680591864, test loss: 0.8309760538607052\n",
      "Iteration 6989 training loss: 0.19221907663735166, test loss: 0.8309924467935647\n",
      "Iteration 6990 training loss: 0.1922004399454679, test loss: 0.831008838923769\n",
      "Iteration 6991 training loss: 0.19218180672928944, test loss: 0.8310252302513472\n",
      "Iteration 6992 training loss: 0.19216317698783872, test loss: 0.831041620776327\n",
      "Iteration 6993 training loss: 0.19214455072013856, test loss: 0.8310580104987381\n",
      "Iteration 6994 training loss: 0.1921259279252121, test loss: 0.8310743994186082\n",
      "Iteration 6995 training loss: 0.1921073086020828, test loss: 0.8310907875359667\n",
      "Iteration 6996 training loss: 0.1920886927497747, test loss: 0.831107174850842\n",
      "Iteration 6997 training loss: 0.192070080367312, test loss: 0.8311235613632628\n",
      "Iteration 6998 training loss: 0.19205147145371934, test loss: 0.8311399470732586\n",
      "Iteration 6999 training loss: 0.19203286600802186, test loss: 0.8311563319808576\n",
      "Iteration 7000 training loss: 0.19201426402924485, test loss: 0.8311727160860887\n",
      "Iteration 7001 training loss: 0.19199566551641412, test loss: 0.8311890993889804\n",
      "Iteration 7002 training loss: 0.1919770704685558, test loss: 0.8312054818895627\n",
      "Iteration 7003 training loss: 0.19195847888469642, test loss: 0.8312218635878634\n",
      "Iteration 7004 training loss: 0.1919398907638629, test loss: 0.8312382444839117\n",
      "Iteration 7005 training loss: 0.19192130610508243, test loss: 0.8312546245777371\n",
      "Iteration 7006 training loss: 0.1919027249073827, test loss: 0.8312710038693678\n",
      "Iteration 7007 training loss: 0.19188414716979166, test loss: 0.8312873823588335\n",
      "Iteration 7008 training loss: 0.19186557289133768, test loss: 0.8313037600461629\n",
      "Iteration 7009 training loss: 0.1918470020710496, test loss: 0.8313201369313845\n",
      "Iteration 7010 training loss: 0.1918284347079564, test loss: 0.8313365130145282\n",
      "Iteration 7011 training loss: 0.19180987080108766, test loss: 0.8313528882956228\n",
      "Iteration 7012 training loss: 0.19179131034947308, test loss: 0.8313692627746974\n",
      "Iteration 7013 training loss: 0.19177275335214308, test loss: 0.8313856364517805\n",
      "Iteration 7014 training loss: 0.19175419980812813, test loss: 0.8314020093269024\n",
      "Iteration 7015 training loss: 0.19173564971645923, test loss: 0.8314183814000917\n",
      "Iteration 7016 training loss: 0.19171710307616774, test loss: 0.8314347526713777\n",
      "Iteration 7017 training loss: 0.19169855988628523, test loss: 0.8314511231407896\n",
      "Iteration 7018 training loss: 0.19168002014584387, test loss: 0.8314674928083562\n",
      "Iteration 7019 training loss: 0.19166148385387605, test loss: 0.8314838616741075\n",
      "Iteration 7020 training loss: 0.19164295100941467, test loss: 0.8315002297380722\n",
      "Iteration 7021 training loss: 0.1916244216114928, test loss: 0.8315165970002798\n",
      "Iteration 7022 training loss: 0.19160589565914393, test loss: 0.8315329634607601\n",
      "Iteration 7023 training loss: 0.19158737315140212, test loss: 0.831549329119542\n",
      "Iteration 7024 training loss: 0.19156885408730157, test loss: 0.8315656939766547\n",
      "Iteration 7025 training loss: 0.19155033846587685, test loss: 0.8315820580321278\n",
      "Iteration 7026 training loss: 0.19153182628616314, test loss: 0.8315984212859908\n",
      "Iteration 7027 training loss: 0.1915133175471957, test loss: 0.8316147837382734\n",
      "Iteration 7028 training loss: 0.19149481224801027, test loss: 0.8316311453890043\n",
      "Iteration 7029 training loss: 0.19147631038764293, test loss: 0.8316475062382135\n",
      "Iteration 7030 training loss: 0.19145781196513026, test loss: 0.8316638662859308\n",
      "Iteration 7031 training loss: 0.19143931697950908, test loss: 0.831680225532185\n",
      "Iteration 7032 training loss: 0.19142082542981653, test loss: 0.8316965839770063\n",
      "Iteration 7033 training loss: 0.1914023373150903, test loss: 0.8317129416204242\n",
      "Iteration 7034 training loss: 0.19138385263436822, test loss: 0.8317292984624676\n",
      "Iteration 7035 training loss: 0.1913653713866886, test loss: 0.8317456545031671\n",
      "Iteration 7036 training loss: 0.19134689357109022, test loss: 0.8317620097425517\n",
      "Iteration 7037 training loss: 0.19132841918661206, test loss: 0.8317783641806514\n",
      "Iteration 7038 training loss: 0.1913099482322935, test loss: 0.8317947178174957\n",
      "Iteration 7039 training loss: 0.19129148070717436, test loss: 0.8318110706531142\n",
      "Iteration 7040 training loss: 0.19127301661029467, test loss: 0.831827422687537\n",
      "Iteration 7041 training loss: 0.19125455594069507, test loss: 0.8318437739207936\n",
      "Iteration 7042 training loss: 0.19123609869741634, test loss: 0.8318601243529139\n",
      "Iteration 7043 training loss: 0.1912176448794997, test loss: 0.8318764739839273\n",
      "Iteration 7044 training loss: 0.19119919448598682, test loss: 0.8318928228138642\n",
      "Iteration 7045 training loss: 0.19118074751591957, test loss: 0.8319091708427544\n",
      "Iteration 7046 training loss: 0.19116230396834027, test loss: 0.8319255180706272\n",
      "Iteration 7047 training loss: 0.19114386384229168, test loss: 0.8319418644975132\n",
      "Iteration 7048 training loss: 0.1911254271368168, test loss: 0.8319582101234417\n",
      "Iteration 7049 training loss: 0.191106993850959, test loss: 0.831974554948443\n",
      "Iteration 7050 training loss: 0.19108856398376214, test loss: 0.8319908989725471\n",
      "Iteration 7051 training loss: 0.1910701375342703, test loss: 0.8320072421957834\n",
      "Iteration 7052 training loss: 0.19105171450152797, test loss: 0.8320235846181825\n",
      "Iteration 7053 training loss: 0.19103329488458004, test loss: 0.8320399262397743\n",
      "Iteration 7054 training loss: 0.19101487868247174, test loss: 0.8320562670605887\n",
      "Iteration 7055 training loss: 0.19099646589424862, test loss: 0.832072607080656\n",
      "Iteration 7056 training loss: 0.19097805651895666, test loss: 0.832088946300006\n",
      "Iteration 7057 training loss: 0.19095965055564218, test loss: 0.8321052847186688\n",
      "Iteration 7058 training loss: 0.1909412480033518, test loss: 0.8321216223366751\n",
      "Iteration 7059 training loss: 0.19092284886113262, test loss: 0.8321379591540541\n",
      "Iteration 7060 training loss: 0.19090445312803195, test loss: 0.8321542951708366\n",
      "Iteration 7061 training loss: 0.1908860608030976, test loss: 0.8321706303870527\n",
      "Iteration 7062 training loss: 0.1908676718853777, test loss: 0.8321869648027327\n",
      "Iteration 7063 training loss: 0.1908492863739207, test loss: 0.8322032984179066\n",
      "Iteration 7064 training loss: 0.1908309042677754, test loss: 0.832219631232605\n",
      "Iteration 7065 training loss: 0.19081252556599115, test loss: 0.832235963246858\n",
      "Iteration 7066 training loss: 0.1907941502676173, test loss: 0.8322522944606956\n",
      "Iteration 7067 training loss: 0.19077577837170395, test loss: 0.8322686248741487\n",
      "Iteration 7068 training loss: 0.19075740987730128, test loss: 0.8322849544872469\n",
      "Iteration 7069 training loss: 0.19073904478345988, test loss: 0.8323012833000212\n",
      "Iteration 7070 training loss: 0.19072068308923085, test loss: 0.832317611312502\n",
      "Iteration 7071 training loss: 0.19070232479366553, test loss: 0.8323339385247193\n",
      "Iteration 7072 training loss: 0.19068396989581565, test loss: 0.8323502649367037\n",
      "Iteration 7073 training loss: 0.1906656183947332, test loss: 0.8323665905484858\n",
      "Iteration 7074 training loss: 0.1906472702894707, test loss: 0.8323829153600958\n",
      "Iteration 7075 training loss: 0.19062892557908093, test loss: 0.8323992393715647\n",
      "Iteration 7076 training loss: 0.19061058426261704, test loss: 0.8324155625829226\n",
      "Iteration 7077 training loss: 0.19059224633913244, test loss: 0.8324318849941997\n",
      "Iteration 7078 training loss: 0.19057391180768116, test loss: 0.8324482066054273\n",
      "Iteration 7079 training loss: 0.19055558066731737, test loss: 0.8324645274166356\n",
      "Iteration 7080 training loss: 0.19053725291709558, test loss: 0.8324808474278553\n",
      "Iteration 7081 training loss: 0.19051892855607078, test loss: 0.832497166639117\n",
      "Iteration 7082 training loss: 0.19050060758329831, test loss: 0.8325134850504513\n",
      "Iteration 7083 training loss: 0.19048228999783381, test loss: 0.8325298026618889\n",
      "Iteration 7084 training loss: 0.19046397579873328, test loss: 0.8325461194734607\n",
      "Iteration 7085 training loss: 0.19044566498505305, test loss: 0.8325624354851969\n",
      "Iteration 7086 training loss: 0.19042735755584997, test loss: 0.8325787506971288\n",
      "Iteration 7087 training loss: 0.1904090535101809, test loss: 0.8325950651092868\n",
      "Iteration 7088 training loss: 0.19039075284710358, test loss: 0.8326113787217017\n",
      "Iteration 7089 training loss: 0.19037245556567556, test loss: 0.8326276915344045\n",
      "Iteration 7090 training loss: 0.19035416166495506, test loss: 0.832644003547426\n",
      "Iteration 7091 training loss: 0.19033587114400066, test loss: 0.8326603147607965\n",
      "Iteration 7092 training loss: 0.1903175840018712, test loss: 0.8326766251745477\n",
      "Iteration 7093 training loss: 0.19029930023762587, test loss: 0.8326929347887098\n",
      "Iteration 7094 training loss: 0.19028101985032428, test loss: 0.8327092436033141\n",
      "Iteration 7095 training loss: 0.1902627428390263, test loss: 0.8327255516183912\n",
      "Iteration 7096 training loss: 0.19024446920279228, test loss: 0.8327418588339726\n",
      "Iteration 7097 training loss: 0.19022619894068274, test loss: 0.8327581652500885\n",
      "Iteration 7098 training loss: 0.1902079320517589, test loss: 0.8327744708667706\n",
      "Iteration 7099 training loss: 0.19018966853508193, test loss: 0.8327907756840495\n",
      "Iteration 7100 training loss: 0.19017140838971358, test loss: 0.8328070797019566\n",
      "Iteration 7101 training loss: 0.190153151614716, test loss: 0.8328233829205222\n",
      "Iteration 7102 training loss: 0.19013489820915142, test loss: 0.8328396853397781\n",
      "Iteration 7103 training loss: 0.19011664817208276, test loss: 0.8328559869597553\n",
      "Iteration 7104 training loss: 0.19009840150257307, test loss: 0.8328722877804844\n",
      "Iteration 7105 training loss: 0.19008015819968588, test loss: 0.832888587801997\n",
      "Iteration 7106 training loss: 0.19006191826248495, test loss: 0.8329048870243242\n",
      "Iteration 7107 training loss: 0.19004368169003452, test loss: 0.8329211854474972\n",
      "Iteration 7108 training loss: 0.19002544848139907, test loss: 0.832937483071547\n",
      "Iteration 7109 training loss: 0.19000721863564352, test loss: 0.8329537798965051\n",
      "Iteration 7110 training loss: 0.1899889921518331, test loss: 0.8329700759224026\n",
      "Iteration 7111 training loss: 0.1899707690290335, test loss: 0.8329863711492707\n",
      "Iteration 7112 training loss: 0.1899525492663105, test loss: 0.8330026655771405\n",
      "Iteration 7113 training loss: 0.18993433286273048, test loss: 0.8330189592060441\n",
      "Iteration 7114 training loss: 0.18991611981736006, test loss: 0.8330352520360118\n",
      "Iteration 7115 training loss: 0.18989791012926632, test loss: 0.8330515440670757\n",
      "Iteration 7116 training loss: 0.18987970379751654, test loss: 0.8330678352992669\n",
      "Iteration 7117 training loss: 0.1898615008211784, test loss: 0.8330841257326165\n",
      "Iteration 7118 training loss: 0.18984330119932005, test loss: 0.8331004153671563\n",
      "Iteration 7119 training loss: 0.18982510493100982, test loss: 0.8331167042029173\n",
      "Iteration 7120 training loss: 0.18980691201531646, test loss: 0.8331329922399316\n",
      "Iteration 7121 training loss: 0.18978872245130912, test loss: 0.8331492794782303\n",
      "Iteration 7122 training loss: 0.18977053623805726, test loss: 0.833165565917845\n",
      "Iteration 7123 training loss: 0.18975235337463073, test loss: 0.8331818515588069\n",
      "Iteration 7124 training loss: 0.18973417386009955, test loss: 0.8331981364011479\n",
      "Iteration 7125 training loss: 0.18971599769353434, test loss: 0.8332144204448994\n",
      "Iteration 7126 training loss: 0.18969782487400597, test loss: 0.8332307036900928\n",
      "Iteration 7127 training loss: 0.1896796554005856, test loss: 0.8332469861367603\n",
      "Iteration 7128 training loss: 0.18966148927234483, test loss: 0.8332632677849328\n",
      "Iteration 7129 training loss: 0.1896433264883555, test loss: 0.833279548634642\n",
      "Iteration 7130 training loss: 0.18962516704768997, test loss: 0.8332958286859201\n",
      "Iteration 7131 training loss: 0.18960701094942084, test loss: 0.8333121079387986\n",
      "Iteration 7132 training loss: 0.189588858192621, test loss: 0.833328386393309\n",
      "Iteration 7133 training loss: 0.18957070877636364, test loss: 0.8333446640494829\n",
      "Iteration 7134 training loss: 0.18955256269972268, test loss: 0.8333609409073524\n",
      "Iteration 7135 training loss: 0.18953441996177198, test loss: 0.8333772169669488\n",
      "Iteration 7136 training loss: 0.18951628056158587, test loss: 0.8333934922283046\n",
      "Iteration 7137 training loss: 0.18949814449823907, test loss: 0.8334097666914511\n",
      "Iteration 7138 training loss: 0.1894800117708067, test loss: 0.8334260403564198\n",
      "Iteration 7139 training loss: 0.18946188237836406, test loss: 0.8334423132232435\n",
      "Iteration 7140 training loss: 0.189443756319987, test loss: 0.8334585852919532\n",
      "Iteration 7141 training loss: 0.18942563359475145, test loss: 0.8334748565625808\n",
      "Iteration 7142 training loss: 0.18940751420173396, test loss: 0.8334911270351589\n",
      "Iteration 7143 training loss: 0.18938939814001132, test loss: 0.8335073967097189\n",
      "Iteration 7144 training loss: 0.18937128540866063, test loss: 0.8335236655862929\n",
      "Iteration 7145 training loss: 0.18935317600675936, test loss: 0.8335399336649129\n",
      "Iteration 7146 training loss: 0.18933506993338534, test loss: 0.8335562009456108\n",
      "Iteration 7147 training loss: 0.18931696718761676, test loss: 0.8335724674284184\n",
      "Iteration 7148 training loss: 0.18929886776853208, test loss: 0.833588733113368\n",
      "Iteration 7149 training loss: 0.18928077167521026, test loss: 0.8336049980004917\n",
      "Iteration 7150 training loss: 0.18926267890673049, test loss: 0.8336212620898213\n",
      "Iteration 7151 training loss: 0.18924458946217226, test loss: 0.833637525381389\n",
      "Iteration 7152 training loss: 0.1892265033406155, test loss: 0.8336537878752274\n",
      "Iteration 7153 training loss: 0.1892084205411405, test loss: 0.8336700495713679\n",
      "Iteration 7154 training loss: 0.18919034106282775, test loss: 0.833686310469843\n",
      "Iteration 7155 training loss: 0.1891722649047583, test loss: 0.8337025705706846\n",
      "Iteration 7156 training loss: 0.1891541920660134, test loss: 0.8337188298739253\n",
      "Iteration 7157 training loss: 0.18913612254567472, test loss: 0.833735088379597\n",
      "Iteration 7158 training loss: 0.18911805634282405, test loss: 0.8337513460877322\n",
      "Iteration 7159 training loss: 0.18909999345654394, test loss: 0.833767602998363\n",
      "Iteration 7160 training loss: 0.18908193388591688, test loss: 0.8337838591115214\n",
      "Iteration 7161 training loss: 0.18906387763002597, test loss: 0.8338001144272403\n",
      "Iteration 7162 training loss: 0.18904582468795453, test loss: 0.8338163689455514\n",
      "Iteration 7163 training loss: 0.18902777505878623, test loss: 0.8338326226664874\n",
      "Iteration 7164 training loss: 0.18900972874160515, test loss: 0.8338488755900807\n",
      "Iteration 7165 training loss: 0.18899168573549566, test loss: 0.8338651277163631\n",
      "Iteration 7166 training loss: 0.18897364603954242, test loss: 0.8338813790453676\n",
      "Iteration 7167 training loss: 0.1889556096528306, test loss: 0.8338976295771267\n",
      "Iteration 7168 training loss: 0.18893757657444551, test loss: 0.8339138793116724\n",
      "Iteration 7169 training loss: 0.18891954680347298, test loss: 0.833930128249037\n",
      "Iteration 7170 training loss: 0.18890152033899912, test loss: 0.8339463763892535\n",
      "Iteration 7171 training loss: 0.18888349718011027, test loss: 0.8339626237323541\n",
      "Iteration 7172 training loss: 0.18886547732589326, test loss: 0.8339788702783716\n",
      "Iteration 7173 training loss: 0.1888474607754352, test loss: 0.833995116027338\n",
      "Iteration 7174 training loss: 0.1888294475278236, test loss: 0.8340113609792864\n",
      "Iteration 7175 training loss: 0.18881143758214625, test loss: 0.8340276051342491\n",
      "Iteration 7176 training loss: 0.18879343093749126, test loss: 0.8340438484922589\n",
      "Iteration 7177 training loss: 0.18877542759294716, test loss: 0.8340600910533478\n",
      "Iteration 7178 training loss: 0.18875742754760272, test loss: 0.8340763328175492\n",
      "Iteration 7179 training loss: 0.1887394308005472, test loss: 0.8340925737848954\n",
      "Iteration 7180 training loss: 0.18872143735087002, test loss: 0.8341088139554191\n",
      "Iteration 7181 training loss: 0.18870344719766116, test loss: 0.8341250533291531\n",
      "Iteration 7182 training loss: 0.18868546034001063, test loss: 0.8341412919061298\n",
      "Iteration 7183 training loss: 0.18866747677700918, test loss: 0.8341575296863819\n",
      "Iteration 7184 training loss: 0.18864949650774748, test loss: 0.8341737666699428\n",
      "Iteration 7185 training loss: 0.1886315195313169, test loss: 0.8341900028568449\n",
      "Iteration 7186 training loss: 0.1886135458468089, test loss: 0.8342062382471208\n",
      "Iteration 7187 training loss: 0.18859557545331546, test loss: 0.8342224728408033\n",
      "Iteration 7188 training loss: 0.1885776083499288, test loss: 0.8342387066379259\n",
      "Iteration 7189 training loss: 0.18855964453574137, test loss: 0.8342549396385207\n",
      "Iteration 7190 training loss: 0.18854168400984622, test loss: 0.8342711718426208\n",
      "Iteration 7191 training loss: 0.18852372677133655, test loss: 0.834287403250259\n",
      "Iteration 7192 training loss: 0.18850577281930597, test loss: 0.8343036338614681\n",
      "Iteration 7193 training loss: 0.18848782215284846, test loss: 0.8343198636762816\n",
      "Iteration 7194 training loss: 0.18846987477105817, test loss: 0.8343360926947322\n",
      "Iteration 7195 training loss: 0.18845193067302976, test loss: 0.8343523209168525\n",
      "Iteration 7196 training loss: 0.18843398985785825, test loss: 0.8343685483426759\n",
      "Iteration 7197 training loss: 0.1884160523246389, test loss: 0.8343847749722353\n",
      "Iteration 7198 training loss: 0.18839811807246723, test loss: 0.8344010008055635\n",
      "Iteration 7199 training loss: 0.18838018710043936, test loss: 0.8344172258426941\n",
      "Iteration 7200 training loss: 0.18836225940765142, test loss: 0.8344334500836593\n",
      "Iteration 7201 training loss: 0.18834433499320014, test loss: 0.8344496735284933\n",
      "Iteration 7202 training loss: 0.18832641385618254, test loss: 0.834465896177228\n",
      "Iteration 7203 training loss: 0.18830849599569582, test loss: 0.8344821180298976\n",
      "Iteration 7204 training loss: 0.18829058141083774, test loss: 0.8344983390865346\n",
      "Iteration 7205 training loss: 0.18827267010070622, test loss: 0.834514559347172\n",
      "Iteration 7206 training loss: 0.18825476206439962, test loss: 0.8345307788118441\n",
      "Iteration 7207 training loss: 0.18823685730101655, test loss: 0.8345469974805827\n",
      "Iteration 7208 training loss: 0.18821895580965606, test loss: 0.8345632153534219\n",
      "Iteration 7209 training loss: 0.18820105758941744, test loss: 0.8345794324303947\n",
      "Iteration 7210 training loss: 0.1881831626394004, test loss: 0.8345956487115342\n",
      "Iteration 7211 training loss: 0.1881652709587049, test loss: 0.8346118641968739\n",
      "Iteration 7212 training loss: 0.18814738254643132, test loss: 0.8346280788864473\n",
      "Iteration 7213 training loss: 0.1881294974016803, test loss: 0.8346442927802873\n",
      "Iteration 7214 training loss: 0.1881116155235529, test loss: 0.8346605058784271\n",
      "Iteration 7215 training loss: 0.18809373691115044, test loss: 0.8346767181809008\n",
      "Iteration 7216 training loss: 0.1880758615635746, test loss: 0.8346929296877412\n",
      "Iteration 7217 training loss: 0.18805798947992736, test loss: 0.8347091403989819\n",
      "Iteration 7218 training loss: 0.18804012065931117, test loss: 0.8347253503146561\n",
      "Iteration 7219 training loss: 0.18802225510082865, test loss: 0.8347415594347977\n",
      "Iteration 7220 training loss: 0.18800439280358275, test loss: 0.8347577677594398\n",
      "Iteration 7221 training loss: 0.18798653376667704, test loss: 0.8347739752886156\n",
      "Iteration 7222 training loss: 0.18796867798921504, test loss: 0.8347901820223593\n",
      "Iteration 7223 training loss: 0.1879508254703008, test loss: 0.8348063879607038\n",
      "Iteration 7224 training loss: 0.18793297620903868, test loss: 0.834822593103683\n",
      "Iteration 7225 training loss: 0.1879151302045334, test loss: 0.8348387974513306\n",
      "Iteration 7226 training loss: 0.18789728745589002, test loss: 0.8348550010036795\n",
      "Iteration 7227 training loss: 0.1878794479622138, test loss: 0.8348712037607641\n",
      "Iteration 7228 training loss: 0.1878616117226105, test loss: 0.8348874057226172\n",
      "Iteration 7229 training loss: 0.18784377873618616, test loss: 0.834903606889273\n",
      "Iteration 7230 training loss: 0.18782594900204713, test loss: 0.8349198072607649\n",
      "Iteration 7231 training loss: 0.1878081225193, test loss: 0.8349360068371269\n",
      "Iteration 7232 training loss: 0.18779029928705196, test loss: 0.8349522056183921\n",
      "Iteration 7233 training loss: 0.18777247930441024, test loss: 0.8349684036045951\n",
      "Iteration 7234 training loss: 0.18775466257048262, test loss: 0.8349846007957686\n",
      "Iteration 7235 training loss: 0.1877368490843771, test loss: 0.8350007971919471\n",
      "Iteration 7236 training loss: 0.18771903884520197, test loss: 0.8350169927931638\n",
      "Iteration 7237 training loss: 0.18770123185206597, test loss: 0.835033187599453\n",
      "Iteration 7238 training loss: 0.18768342810407806, test loss: 0.8350493816108482\n",
      "Iteration 7239 training loss: 0.18766562760034772, test loss: 0.8350655748273835\n",
      "Iteration 7240 training loss: 0.18764783033998447, test loss: 0.8350817672490926\n",
      "Iteration 7241 training loss: 0.1876300363220985, test loss: 0.8350979588760091\n",
      "Iteration 7242 training loss: 0.18761224554579997, test loss: 0.8351141497081668\n",
      "Iteration 7243 training loss: 0.18759445801019972, test loss: 0.8351303397456004\n",
      "Iteration 7244 training loss: 0.1875766737144086, test loss: 0.8351465289883427\n",
      "Iteration 7245 training loss: 0.18755889265753808, test loss: 0.8351627174364287\n",
      "Iteration 7246 training loss: 0.18754111483869973, test loss: 0.8351789050898918\n",
      "Iteration 7247 training loss: 0.18752334025700562, test loss: 0.835195091948766\n",
      "Iteration 7248 training loss: 0.187505568911568, test loss: 0.8352112780130854\n",
      "Iteration 7249 training loss: 0.18748780080149957, test loss: 0.8352274632828838\n",
      "Iteration 7250 training loss: 0.18747003592591333, test loss: 0.8352436477581955\n",
      "Iteration 7251 training loss: 0.18745227428392255, test loss: 0.8352598314390544\n",
      "Iteration 7252 training loss: 0.18743451587464097, test loss: 0.8352760143254948\n",
      "Iteration 7253 training loss: 0.18741676069718244, test loss: 0.8352921964175506\n",
      "Iteration 7254 training loss: 0.18739900875066132, test loss: 0.8353083777152559\n",
      "Iteration 7255 training loss: 0.18738126003419225, test loss: 0.8353245582186444\n",
      "Iteration 7256 training loss: 0.1873635145468903, test loss: 0.8353407379277512\n",
      "Iteration 7257 training loss: 0.18734577228787053, test loss: 0.8353569168426093\n",
      "Iteration 7258 training loss: 0.18732803325624872, test loss: 0.8353730949632538\n",
      "Iteration 7259 training loss: 0.18731029745114075, test loss: 0.8353892722897189\n",
      "Iteration 7260 training loss: 0.18729256487166296, test loss: 0.8354054488220382\n",
      "Iteration 7261 training loss: 0.18727483551693191, test loss: 0.8354216245602465\n",
      "Iteration 7262 training loss: 0.1872571093860646, test loss: 0.8354377995043777\n",
      "Iteration 7263 training loss: 0.18723938647817814, test loss: 0.835453973654466\n",
      "Iteration 7264 training loss: 0.18722166679239033, test loss: 0.835470147010546\n",
      "Iteration 7265 training loss: 0.1872039503278189, test loss: 0.8354863195726522\n",
      "Iteration 7266 training loss: 0.1871862370835821, test loss: 0.8355024913408182\n",
      "Iteration 7267 training loss: 0.18716852705879866, test loss: 0.835518662315079\n",
      "Iteration 7268 training loss: 0.18715082025258736, test loss: 0.8355348324954686\n",
      "Iteration 7269 training loss: 0.1871331166640674, test loss: 0.8355510018820218\n",
      "Iteration 7270 training loss: 0.18711541629235842, test loss: 0.8355671704747727\n",
      "Iteration 7271 training loss: 0.18709771913658024, test loss: 0.8355833382737555\n",
      "Iteration 7272 training loss: 0.18708002519585307, test loss: 0.8355995052790051\n",
      "Iteration 7273 training loss: 0.18706233446929746, test loss: 0.8356156714905559\n",
      "Iteration 7274 training loss: 0.1870446469560343, test loss: 0.835631836908442\n",
      "Iteration 7275 training loss: 0.18702696265518468, test loss: 0.835648001532698\n",
      "Iteration 7276 training loss: 0.18700928156587013, test loss: 0.8356641653633586\n",
      "Iteration 7277 training loss: 0.18699160368721252, test loss: 0.8356803284004582\n",
      "Iteration 7278 training loss: 0.18697392901833404, test loss: 0.835696490644032\n",
      "Iteration 7279 training loss: 0.18695625755835707, test loss: 0.8357126520941135\n",
      "Iteration 7280 training loss: 0.18693858930640456, test loss: 0.8357288127507381\n",
      "Iteration 7281 training loss: 0.18692092426159954, test loss: 0.8357449726139397\n",
      "Iteration 7282 training loss: 0.18690326242306549, test loss: 0.8357611316837538\n",
      "Iteration 7283 training loss: 0.1868856037899262, test loss: 0.8357772899602144\n",
      "Iteration 7284 training loss: 0.1868679483613058, test loss: 0.8357934474433563\n",
      "Iteration 7285 training loss: 0.1868502961363287, test loss: 0.8358096041332145\n",
      "Iteration 7286 training loss: 0.18683264711411965, test loss: 0.8358257600298229\n",
      "Iteration 7287 training loss: 0.18681500129380374, test loss: 0.8358419151332172\n",
      "Iteration 7288 training loss: 0.18679735867450642, test loss: 0.8358580694434313\n",
      "Iteration 7289 training loss: 0.18677971925535342, test loss: 0.8358742229605005\n",
      "Iteration 7290 training loss: 0.18676208303547065, test loss: 0.8358903756844598\n",
      "Iteration 7291 training loss: 0.18674445001398465, test loss: 0.835906527615343\n",
      "Iteration 7292 training loss: 0.18672682019002204, test loss: 0.8359226787531857\n",
      "Iteration 7293 training loss: 0.18670919356270996, test loss: 0.8359388290980229\n",
      "Iteration 7294 training loss: 0.18669157013117557, test loss: 0.8359549786498887\n",
      "Iteration 7295 training loss: 0.1866739498945467, test loss: 0.8359711274088184\n",
      "Iteration 7296 training loss: 0.1866563328519512, test loss: 0.8359872753748468\n",
      "Iteration 7297 training loss: 0.18663871900251755, test loss: 0.8360034225480092\n",
      "Iteration 7298 training loss: 0.1866211083453743, test loss: 0.8360195689283397\n",
      "Iteration 7299 training loss: 0.18660350087965036, test loss: 0.836035714515874\n",
      "Iteration 7300 training loss: 0.18658589660447508, test loss: 0.8360518593106464\n",
      "Iteration 7301 training loss: 0.18656829551897805, test loss: 0.8360680033126927\n",
      "Iteration 7302 training loss: 0.1865506976222892, test loss: 0.836084146522047\n",
      "Iteration 7303 training loss: 0.18653310291353875, test loss: 0.836100288938745\n",
      "Iteration 7304 training loss: 0.18651551139185735, test loss: 0.8361164305628213\n",
      "Iteration 7305 training loss: 0.18649792305637575, test loss: 0.8361325713943111\n",
      "Iteration 7306 training loss: 0.18648033790622526, test loss: 0.8361487114332495\n",
      "Iteration 7307 training loss: 0.18646275594053743, test loss: 0.8361648506796715\n",
      "Iteration 7308 training loss: 0.18644517715844405, test loss: 0.8361809891336123\n",
      "Iteration 7309 training loss: 0.18642760155907737, test loss: 0.8361971267951068\n",
      "Iteration 7310 training loss: 0.1864100291415698, test loss: 0.8362132636641907\n",
      "Iteration 7311 training loss: 0.18639245990505418, test loss: 0.8362293997408985\n",
      "Iteration 7312 training loss: 0.1863748938486637, test loss: 0.8362455350252659\n",
      "Iteration 7313 training loss: 0.18635733097153176, test loss: 0.8362616695173274\n",
      "Iteration 7314 training loss: 0.18633977127279214, test loss: 0.8362778032171186\n",
      "Iteration 7315 training loss: 0.18632221475157898, test loss: 0.8362939361246752\n",
      "Iteration 7316 training loss: 0.18630466140702664, test loss: 0.8363100682400318\n",
      "Iteration 7317 training loss: 0.18628711123826994, test loss: 0.8363261995632237\n",
      "Iteration 7318 training loss: 0.18626956424444377, test loss: 0.8363423300942866\n",
      "Iteration 7319 training loss: 0.1862520204246837, test loss: 0.8363584598332554\n",
      "Iteration 7320 training loss: 0.18623447977812527, test loss: 0.8363745887801658\n",
      "Iteration 7321 training loss: 0.1862169423039046, test loss: 0.8363907169350524\n",
      "Iteration 7322 training loss: 0.18619940800115797, test loss: 0.8364068442979512\n",
      "Iteration 7323 training loss: 0.18618187686902204, test loss: 0.8364229708688976\n",
      "Iteration 7324 training loss: 0.18616434890663378, test loss: 0.8364390966479268\n",
      "Iteration 7325 training loss: 0.18614682411313044, test loss: 0.8364552216350742\n",
      "Iteration 7326 training loss: 0.18612930248764964, test loss: 0.8364713458303749\n",
      "Iteration 7327 training loss: 0.18611178402932937, test loss: 0.836487469233865\n",
      "Iteration 7328 training loss: 0.18609426873730775, test loss: 0.8365035918455795\n",
      "Iteration 7329 training loss: 0.18607675661072345, test loss: 0.836519713665554\n",
      "Iteration 7330 training loss: 0.18605924764871531, test loss: 0.8365358346938241\n",
      "Iteration 7331 training loss: 0.18604174185042252, test loss: 0.8365519549304251\n",
      "Iteration 7332 training loss: 0.1860242392149846, test loss: 0.8365680743753924\n",
      "Iteration 7333 training loss: 0.18600673974154136, test loss: 0.8365841930287624\n",
      "Iteration 7334 training loss: 0.18598924342923293, test loss: 0.8366003108905697\n",
      "Iteration 7335 training loss: 0.18597175027719978, test loss: 0.8366164279608501\n",
      "Iteration 7336 training loss: 0.18595426028458273, test loss: 0.8366325442396396\n",
      "Iteration 7337 training loss: 0.1859367734505228, test loss: 0.8366486597269733\n",
      "Iteration 7338 training loss: 0.18591928977416158, test loss: 0.8366647744228873\n",
      "Iteration 7339 training loss: 0.18590180925464056, test loss: 0.8366808883274168\n",
      "Iteration 7340 training loss: 0.18588433189110196, test loss: 0.8366970014405978\n",
      "Iteration 7341 training loss: 0.18586685768268804, test loss: 0.8367131137624659\n",
      "Iteration 7342 training loss: 0.18584938662854145, test loss: 0.836729225293057\n",
      "Iteration 7343 training loss: 0.1858319187278053, test loss: 0.8367453360324063\n",
      "Iteration 7344 training loss: 0.18581445397962285, test loss: 0.8367614459805502\n",
      "Iteration 7345 training loss: 0.18579699238313768, test loss: 0.836777555137524\n",
      "Iteration 7346 training loss: 0.18577953393749375, test loss: 0.8367936635033636\n",
      "Iteration 7347 training loss: 0.1857620786418354, test loss: 0.8368097710781048\n",
      "Iteration 7348 training loss: 0.18574462649530707, test loss: 0.8368258778617834\n",
      "Iteration 7349 training loss: 0.18572717749705367, test loss: 0.8368419838544349\n",
      "Iteration 7350 training loss: 0.1857097316462204, test loss: 0.8368580890560957\n",
      "Iteration 7351 training loss: 0.18569228894195286, test loss: 0.8368741934668017\n",
      "Iteration 7352 training loss: 0.1856748493833968, test loss: 0.8368902970865884\n",
      "Iteration 7353 training loss: 0.18565741296969832, test loss: 0.8369063999154918\n",
      "Iteration 7354 training loss: 0.18563997970000395, test loss: 0.8369225019535478\n",
      "Iteration 7355 training loss: 0.18562254957346042, test loss: 0.8369386032007922\n",
      "Iteration 7356 training loss: 0.18560512258921485, test loss: 0.836954703657261\n",
      "Iteration 7357 training loss: 0.18558769874641462, test loss: 0.8369708033229907\n",
      "Iteration 7358 training loss: 0.18557027804420742, test loss: 0.8369869021980169\n",
      "Iteration 7359 training loss: 0.1855528604817413, test loss: 0.8370030002823754\n",
      "Iteration 7360 training loss: 0.1855354460581645, test loss: 0.8370190975761022\n",
      "Iteration 7361 training loss: 0.1855180347726258, test loss: 0.8370351940792337\n",
      "Iteration 7362 training loss: 0.18550062662427413, test loss: 0.8370512897918055\n",
      "Iteration 7363 training loss: 0.18548322161225872, test loss: 0.8370673847138546\n",
      "Iteration 7364 training loss: 0.1854658197357292, test loss: 0.8370834788454159\n",
      "Iteration 7365 training loss: 0.18544842099383546, test loss: 0.8370995721865262\n",
      "Iteration 7366 training loss: 0.18543102538572767, test loss: 0.8371156647372213\n",
      "Iteration 7367 training loss: 0.18541363291055638, test loss: 0.8371317564975379\n",
      "Iteration 7368 training loss: 0.18539624356747245, test loss: 0.8371478474675116\n",
      "Iteration 7369 training loss: 0.18537885735562706, test loss: 0.8371639376471787\n",
      "Iteration 7370 training loss: 0.18536147427417152, test loss: 0.8371800270365753\n",
      "Iteration 7371 training loss: 0.1853440943222578, test loss: 0.8371961156357376\n",
      "Iteration 7372 training loss: 0.1853267174990378, test loss: 0.8372122034447025\n",
      "Iteration 7373 training loss: 0.18530934380366407, test loss: 0.8372282904635053\n",
      "Iteration 7374 training loss: 0.18529197323528923, test loss: 0.8372443766921828\n",
      "Iteration 7375 training loss: 0.1852746057930663, test loss: 0.8372604621307713\n",
      "Iteration 7376 training loss: 0.18525724147614858, test loss: 0.8372765467793067\n",
      "Iteration 7377 training loss: 0.18523988028368976, test loss: 0.8372926306378257\n",
      "Iteration 7378 training loss: 0.18522252221484384, test loss: 0.8373087137063645\n",
      "Iteration 7379 training loss: 0.18520516726876493, test loss: 0.8373247959849591\n",
      "Iteration 7380 training loss: 0.1851878154446077, test loss: 0.8373408774736465\n",
      "Iteration 7381 training loss: 0.18517046674152704, test loss: 0.8373569581724628\n",
      "Iteration 7382 training loss: 0.18515312115867807, test loss: 0.8373730380814441\n",
      "Iteration 7383 training loss: 0.18513577869521633, test loss: 0.8373891172006271\n",
      "Iteration 7384 training loss: 0.1851184393502976, test loss: 0.8374051955300486\n",
      "Iteration 7385 training loss: 0.18510110312307812, test loss: 0.8374212730697445\n",
      "Iteration 7386 training loss: 0.18508377001271417, test loss: 0.8374373498197514\n",
      "Iteration 7387 training loss: 0.1850664400183625, test loss: 0.8374534257801055\n",
      "Iteration 7388 training loss: 0.1850491131391802, test loss: 0.8374695009508437\n",
      "Iteration 7389 training loss: 0.18503178937432466, test loss: 0.8374855753320029\n",
      "Iteration 7390 training loss: 0.18501446872295346, test loss: 0.8375016489236187\n",
      "Iteration 7391 training loss: 0.18499715118422463, test loss: 0.8375177217257282\n",
      "Iteration 7392 training loss: 0.18497983675729643, test loss: 0.8375337937383676\n",
      "Iteration 7393 training loss: 0.1849625254413275, test loss: 0.8375498649615741\n",
      "Iteration 7394 training loss: 0.1849452172354766, test loss: 0.8375659353953838\n",
      "Iteration 7395 training loss: 0.1849279121389031, test loss: 0.8375820050398338\n",
      "Iteration 7396 training loss: 0.1849106101507664, test loss: 0.8375980738949599\n",
      "Iteration 7397 training loss: 0.18489331127022637, test loss: 0.8376141419607995\n",
      "Iteration 7398 training loss: 0.18487601549644314, test loss: 0.8376302092373891\n",
      "Iteration 7399 training loss: 0.1848587228285771, test loss: 0.8376462757247651\n",
      "Iteration 7400 training loss: 0.18484143326578908, test loss: 0.8376623414229645\n",
      "Iteration 7401 training loss: 0.18482414680724002, test loss: 0.8376784063320238\n",
      "Iteration 7402 training loss: 0.18480686345209138, test loss: 0.8376944704519799\n",
      "Iteration 7403 training loss: 0.18478958319950475, test loss: 0.8377105337828696\n",
      "Iteration 7404 training loss: 0.18477230604864223, test loss: 0.8377265963247295\n",
      "Iteration 7405 training loss: 0.18475503199866591, test loss: 0.8377426580775964\n",
      "Iteration 7406 training loss: 0.18473776104873849, test loss: 0.8377587190415072\n",
      "Iteration 7407 training loss: 0.1847204931980228, test loss: 0.8377747792164986\n",
      "Iteration 7408 training loss: 0.18470322844568213, test loss: 0.8377908386026073\n",
      "Iteration 7409 training loss: 0.1846859667908799, test loss: 0.8378068971998709\n",
      "Iteration 7410 training loss: 0.18466870823277995, test loss: 0.8378229550083256\n",
      "Iteration 7411 training loss: 0.1846514527705464, test loss: 0.8378390120280079\n",
      "Iteration 7412 training loss: 0.18463420040334372, test loss: 0.8378550682589554\n",
      "Iteration 7413 training loss: 0.18461695113033652, test loss: 0.8378711237012051\n",
      "Iteration 7414 training loss: 0.1845997049506899, test loss: 0.8378871783547934\n",
      "Iteration 7415 training loss: 0.18458246186356916, test loss: 0.8379032322197579\n",
      "Iteration 7416 training loss: 0.18456522186814, test loss: 0.8379192852961345\n",
      "Iteration 7417 training loss: 0.18454798496356836, test loss: 0.8379353375839612\n",
      "Iteration 7418 training loss: 0.18453075114902046, test loss: 0.8379513890832748\n",
      "Iteration 7419 training loss: 0.1845135204236629, test loss: 0.8379674397941119\n",
      "Iteration 7420 training loss: 0.18449629278666246, test loss: 0.83798348971651\n",
      "Iteration 7421 training loss: 0.18447906823718638, test loss: 0.8379995388505058\n",
      "Iteration 7422 training loss: 0.1844618467744021, test loss: 0.8380155871961368\n",
      "Iteration 7423 training loss: 0.1844446283974774, test loss: 0.8380316347534397\n",
      "Iteration 7424 training loss: 0.1844274131055804, test loss: 0.8380476815224512\n",
      "Iteration 7425 training loss: 0.1844102008978794, test loss: 0.8380637275032095\n",
      "Iteration 7426 training loss: 0.18439299177354304, test loss: 0.838079772695751\n",
      "Iteration 7427 training loss: 0.1843757857317405, test loss: 0.8380958171001129\n",
      "Iteration 7428 training loss: 0.18435858277164094, test loss: 0.8381118607163324\n",
      "Iteration 7429 training loss: 0.184341382892414, test loss: 0.8381279035444467\n",
      "Iteration 7430 training loss: 0.18432418609322954, test loss: 0.8381439455844932\n",
      "Iteration 7431 training loss: 0.18430699237325773, test loss: 0.8381599868365087\n",
      "Iteration 7432 training loss: 0.18428980173166923, test loss: 0.838176027300531\n",
      "Iteration 7433 training loss: 0.18427261416763469, test loss: 0.8381920669765967\n",
      "Iteration 7434 training loss: 0.18425542968032527, test loss: 0.8382081058647438\n",
      "Iteration 7435 training loss: 0.18423824826891233, test loss: 0.8382241439650086\n",
      "Iteration 7436 training loss: 0.1842210699325677, test loss: 0.8382401812774294\n",
      "Iteration 7437 training loss: 0.18420389467046333, test loss: 0.838256217802043\n",
      "Iteration 7438 training loss: 0.1841867224817715, test loss: 0.8382722535388865\n",
      "Iteration 7439 training loss: 0.18416955336566493, test loss: 0.838288288487998\n",
      "Iteration 7440 training loss: 0.18415238732131642, test loss: 0.8383043226494138\n",
      "Iteration 7441 training loss: 0.18413522434789933, test loss: 0.838320356023172\n",
      "Iteration 7442 training loss: 0.1841180644445871, test loss: 0.8383363886093103\n",
      "Iteration 7443 training loss: 0.18410090761055353, test loss: 0.8383524204078652\n",
      "Iteration 7444 training loss: 0.1840837538449728, test loss: 0.838368451418875\n",
      "Iteration 7445 training loss: 0.18406660314701934, test loss: 0.8383844816423762\n",
      "Iteration 7446 training loss: 0.18404945551586782, test loss: 0.8384005110784074\n",
      "Iteration 7447 training loss: 0.18403231095069333, test loss: 0.8384165397270052\n",
      "Iteration 7448 training loss: 0.18401516945067117, test loss: 0.8384325675882071\n",
      "Iteration 7449 training loss: 0.18399803101497705, test loss: 0.8384485946620511\n",
      "Iteration 7450 training loss: 0.1839808956427868, test loss: 0.8384646209485743\n",
      "Iteration 7451 training loss: 0.18396376333327666, test loss: 0.8384806464478146\n",
      "Iteration 7452 training loss: 0.18394663408562323, test loss: 0.8384966711598092\n",
      "Iteration 7453 training loss: 0.18392950789900328, test loss: 0.8385126950845961\n",
      "Iteration 7454 training loss: 0.1839123847725939, test loss: 0.8385287182222125\n",
      "Iteration 7455 training loss: 0.1838952647055727, test loss: 0.838544740572696\n",
      "Iteration 7456 training loss: 0.1838781476971172, test loss: 0.8385607621360842\n",
      "Iteration 7457 training loss: 0.18386103374640553, test loss: 0.838576782912415\n",
      "Iteration 7458 training loss: 0.18384392285261603, test loss: 0.8385928029017262\n",
      "Iteration 7459 training loss: 0.18382681501492731, test loss: 0.838608822104055\n",
      "Iteration 7460 training loss: 0.18380971023251827, test loss: 0.838624840519439\n",
      "Iteration 7461 training loss: 0.18379260850456813, test loss: 0.8386408581479166\n",
      "Iteration 7462 training loss: 0.1837755098302565, test loss: 0.8386568749895247\n",
      "Iteration 7463 training loss: 0.18375841420876318, test loss: 0.8386728910443015\n",
      "Iteration 7464 training loss: 0.18374132163926818, test loss: 0.8386889063122845\n",
      "Iteration 7465 training loss: 0.183724232120952, test loss: 0.8387049207935121\n",
      "Iteration 7466 training loss: 0.18370714565299537, test loss: 0.8387209344880211\n",
      "Iteration 7467 training loss: 0.18369006223457926, test loss: 0.8387369473958501\n",
      "Iteration 7468 training loss: 0.183672981864885, test loss: 0.838752959517036\n",
      "Iteration 7469 training loss: 0.18365590454309424, test loss: 0.8387689708516178\n",
      "Iteration 7470 training loss: 0.18363883026838887, test loss: 0.8387849813996325\n",
      "Iteration 7471 training loss: 0.18362175903995104, test loss: 0.8388009911611182\n",
      "Iteration 7472 training loss: 0.18360469085696335, test loss: 0.838817000136113\n",
      "Iteration 7473 training loss: 0.1835876257186085, test loss: 0.838833008324654\n",
      "Iteration 7474 training loss: 0.18357056362406968, test loss: 0.8388490157267801\n",
      "Iteration 7475 training loss: 0.18355350457253017, test loss: 0.8388650223425287\n",
      "Iteration 7476 training loss: 0.18353644856317378, test loss: 0.8388810281719379\n",
      "Iteration 7477 training loss: 0.18351939559518438, test loss: 0.8388970332150452\n",
      "Iteration 7478 training loss: 0.18350234566774629, test loss: 0.8389130374718893\n",
      "Iteration 7479 training loss: 0.18348529878004424, test loss: 0.8389290409425076\n",
      "Iteration 7480 training loss: 0.18346825493126287, test loss: 0.8389450436269386\n",
      "Iteration 7481 training loss: 0.18345121412058749, test loss: 0.8389610455252198\n",
      "Iteration 7482 training loss: 0.18343417634720358, test loss: 0.8389770466373895\n",
      "Iteration 7483 training loss: 0.18341714161029682, test loss: 0.8389930469634854\n",
      "Iteration 7484 training loss: 0.1834001099090533, test loss: 0.8390090465035461\n",
      "Iteration 7485 training loss: 0.18338308124265942, test loss: 0.8390250452576093\n",
      "Iteration 7486 training loss: 0.18336605561030178, test loss: 0.8390410432257133\n",
      "Iteration 7487 training loss: 0.18334903301116734, test loss: 0.8390570404078961\n",
      "Iteration 7488 training loss: 0.18333201344444333, test loss: 0.8390730368041958\n",
      "Iteration 7489 training loss: 0.1833149969093173, test loss: 0.8390890324146505\n",
      "Iteration 7490 training loss: 0.18329798340497708, test loss: 0.8391050272392985\n",
      "Iteration 7491 training loss: 0.1832809729306108, test loss: 0.839121021278178\n",
      "Iteration 7492 training loss: 0.18326396548540685, test loss: 0.839137014531327\n",
      "Iteration 7493 training loss: 0.18324696106855393, test loss: 0.8391530069987837\n",
      "Iteration 7494 training loss: 0.18322995967924113, test loss: 0.8391689986805867\n",
      "Iteration 7495 training loss: 0.18321296131665765, test loss: 0.8391849895767736\n",
      "Iteration 7496 training loss: 0.1831959659799932, test loss: 0.8392009796873829\n",
      "Iteration 7497 training loss: 0.18317897366843755, test loss: 0.8392169690124532\n",
      "Iteration 7498 training loss: 0.18316198438118095, test loss: 0.8392329575520224\n",
      "Iteration 7499 training loss: 0.1831449981174139, test loss: 0.8392489453061291\n",
      "Iteration 7500 training loss: 0.18312801487632707, test loss: 0.839264932274811\n",
      "Iteration 7501 training loss: 0.18311103465711165, test loss: 0.8392809184581073\n",
      "Iteration 7502 training loss: 0.18309405745895885, test loss: 0.8392969038560555\n",
      "Iteration 7503 training loss: 0.18307708328106048, test loss: 0.8393128884686943\n",
      "Iteration 7504 training loss: 0.18306011212260836, test loss: 0.8393288722960622\n",
      "Iteration 7505 training loss: 0.18304314398279478, test loss: 0.8393448553381975\n",
      "Iteration 7506 training loss: 0.1830261788608123, test loss: 0.839360837595139\n",
      "Iteration 7507 training loss: 0.1830092167558536, test loss: 0.8393768190669241\n",
      "Iteration 7508 training loss: 0.18299225766711202, test loss: 0.8393927997535917\n",
      "Iteration 7509 training loss: 0.18297530159378073, test loss: 0.8394087796551809\n",
      "Iteration 7510 training loss: 0.1829583485350536, test loss: 0.8394247587717291\n",
      "Iteration 7511 training loss: 0.18294139849012447, test loss: 0.8394407371032758\n",
      "Iteration 7512 training loss: 0.18292445145818773, test loss: 0.8394567146498588\n",
      "Iteration 7513 training loss: 0.18290750743843795, test loss: 0.8394726914115166\n",
      "Iteration 7514 training loss: 0.18289056643006996, test loss: 0.8394886673882884\n",
      "Iteration 7515 training loss: 0.1828736284322789, test loss: 0.8395046425802115\n",
      "Iteration 7516 training loss: 0.18285669344426023, test loss: 0.839520616987326\n",
      "Iteration 7517 training loss: 0.18283976146520975, test loss: 0.839536590609669\n",
      "Iteration 7518 training loss: 0.18282283249432338, test loss: 0.8395525634472805\n",
      "Iteration 7519 training loss: 0.18280590653079754, test loss: 0.8395685355001979\n",
      "Iteration 7520 training loss: 0.18278898357382875, test loss: 0.8395845067684602\n",
      "Iteration 7521 training loss: 0.18277206362261397, test loss: 0.8396004772521061\n",
      "Iteration 7522 training loss: 0.18275514667635043, test loss: 0.8396164469511743\n",
      "Iteration 7523 training loss: 0.18273823273423548, test loss: 0.8396324158657037\n",
      "Iteration 7524 training loss: 0.18272132179546696, test loss: 0.839648383995732\n",
      "Iteration 7525 training loss: 0.18270441385924302, test loss: 0.8396643513412992\n",
      "Iteration 7526 training loss: 0.18268750892476188, test loss: 0.8396803179024435\n",
      "Iteration 7527 training loss: 0.18267060699122223, test loss: 0.8396962836792033\n",
      "Iteration 7528 training loss: 0.182653708057823, test loss: 0.8397122486716173\n",
      "Iteration 7529 training loss: 0.18263681212376343, test loss: 0.8397282128797243\n",
      "Iteration 7530 training loss: 0.18261991918824305, test loss: 0.8397441763035636\n",
      "Iteration 7531 training loss: 0.18260302925046157, test loss: 0.8397601389431734\n",
      "Iteration 7532 training loss: 0.18258614230961917, test loss: 0.8397761007985931\n",
      "Iteration 7533 training loss: 0.18256925836491614, test loss: 0.8397920618698607\n",
      "Iteration 7534 training loss: 0.18255237741555322, test loss: 0.8398080221570156\n",
      "Iteration 7535 training loss: 0.18253549946073136, test loss: 0.8398239816600963\n",
      "Iteration 7536 training loss: 0.18251862449965175, test loss: 0.8398399403791421\n",
      "Iteration 7537 training loss: 0.18250175253151596, test loss: 0.8398558983141915\n",
      "Iteration 7538 training loss: 0.1824848835555258, test loss: 0.8398718554652833\n",
      "Iteration 7539 training loss: 0.18246801757088346, test loss: 0.8398878118324566\n",
      "Iteration 7540 training loss: 0.18245115457679123, test loss: 0.8399037674157503\n",
      "Iteration 7541 training loss: 0.18243429457245175, test loss: 0.8399197222152036\n",
      "Iteration 7542 training loss: 0.18241743755706816, test loss: 0.8399356762308549\n",
      "Iteration 7543 training loss: 0.18240058352984356, test loss: 0.8399516294627436\n",
      "Iteration 7544 training loss: 0.1823837324899816, test loss: 0.8399675819109084\n",
      "Iteration 7545 training loss: 0.1823668844366861, test loss: 0.8399835335753885\n",
      "Iteration 7546 training loss: 0.1823500393691612, test loss: 0.8399994844562225\n",
      "Iteration 7547 training loss: 0.1823331972866112, test loss: 0.8400154345534501\n",
      "Iteration 7548 training loss: 0.18231635818824088, test loss: 0.8400313838671096\n",
      "Iteration 7549 training loss: 0.18229952207325525, test loss: 0.8400473323972404\n",
      "Iteration 7550 training loss: 0.18228268894085953, test loss: 0.8400632801438818\n",
      "Iteration 7551 training loss: 0.18226585879025925, test loss: 0.8400792271070726\n",
      "Iteration 7552 training loss: 0.1822490316206603, test loss: 0.8400951732868519\n",
      "Iteration 7553 training loss: 0.18223220743126883, test loss: 0.8401111186832584\n",
      "Iteration 7554 training loss: 0.18221538622129121, test loss: 0.8401270632963321\n",
      "Iteration 7555 training loss: 0.18219856798993422, test loss: 0.8401430071261113\n",
      "Iteration 7556 training loss: 0.1821817527364047, test loss: 0.8401589501726356\n",
      "Iteration 7557 training loss: 0.18216494045991005, test loss: 0.8401748924359442\n",
      "Iteration 7558 training loss: 0.18214813115965778, test loss: 0.8401908339160763\n",
      "Iteration 7559 training loss: 0.18213132483485578, test loss: 0.8402067746130708\n",
      "Iteration 7560 training loss: 0.18211452148471208, test loss: 0.8402227145269672\n",
      "Iteration 7561 training loss: 0.1820977211084352, test loss: 0.8402386536578041\n",
      "Iteration 7562 training loss: 0.18208092370523382, test loss: 0.8402545920056218\n",
      "Iteration 7563 training loss: 0.1820641292743169, test loss: 0.840270529570459\n",
      "Iteration 7564 training loss: 0.18204733781489366, test loss: 0.8402864663523546\n",
      "Iteration 7565 training loss: 0.1820305493261737, test loss: 0.8403024023513483\n",
      "Iteration 7566 training loss: 0.18201376380736695, test loss: 0.8403183375674793\n",
      "Iteration 7567 training loss: 0.18199698125768346, test loss: 0.840334272000787\n",
      "Iteration 7568 training loss: 0.18198020167633353, test loss: 0.8403502056513109\n",
      "Iteration 7569 training loss: 0.18196342506252805, test loss: 0.8403661385190899\n",
      "Iteration 7570 training loss: 0.18194665141547786, test loss: 0.8403820706041634\n",
      "Iteration 7571 training loss: 0.1819298807343943, test loss: 0.8403980019065711\n",
      "Iteration 7572 training loss: 0.1819131130184889, test loss: 0.840413932426352\n",
      "Iteration 7573 training loss: 0.18189634826697343, test loss: 0.8404298621635459\n",
      "Iteration 7574 training loss: 0.1818795864790601, test loss: 0.840445791118192\n",
      "Iteration 7575 training loss: 0.1818628276539612, test loss: 0.8404617192903296\n",
      "Iteration 7576 training loss: 0.18184607179088944, test loss: 0.8404776466799984\n",
      "Iteration 7577 training loss: 0.18182931888905784, test loss: 0.8404935732872381\n",
      "Iteration 7578 training loss: 0.1818125689476796, test loss: 0.8405094991120872\n",
      "Iteration 7579 training loss: 0.18179582196596822, test loss: 0.840525424154586\n",
      "Iteration 7580 training loss: 0.1817790779431376, test loss: 0.8405413484147739\n",
      "Iteration 7581 training loss: 0.1817623368784017, test loss: 0.8405572718926899\n",
      "Iteration 7582 training loss: 0.18174559877097504, test loss: 0.8405731945883745\n",
      "Iteration 7583 training loss: 0.18172886362007223, test loss: 0.8405891165018663\n",
      "Iteration 7584 training loss: 0.1817121314249081, test loss: 0.8406050376332052\n",
      "Iteration 7585 training loss: 0.1816954021846981, test loss: 0.8406209579824312\n",
      "Iteration 7586 training loss: 0.18167867589865752, test loss: 0.8406368775495829\n",
      "Iteration 7587 training loss: 0.18166195256600223, test loss: 0.8406527963347009\n",
      "Iteration 7588 training loss: 0.18164523218594836, test loss: 0.8406687143378244\n",
      "Iteration 7589 training loss: 0.18162851475771216, test loss: 0.8406846315589928\n",
      "Iteration 7590 training loss: 0.18161180028051033, test loss: 0.8407005479982463\n",
      "Iteration 7591 training loss: 0.18159508875355973, test loss: 0.8407164636556239\n",
      "Iteration 7592 training loss: 0.18157838017607758, test loss: 0.8407323785311656\n",
      "Iteration 7593 training loss: 0.18156167454728142, test loss: 0.8407482926249111\n",
      "Iteration 7594 training loss: 0.1815449718663889, test loss: 0.8407642059369002\n",
      "Iteration 7595 training loss: 0.18152827213261807, test loss: 0.8407801184671725\n",
      "Iteration 7596 training loss: 0.18151157534518736, test loss: 0.8407960302157675\n",
      "Iteration 7597 training loss: 0.18149488150331522, test loss: 0.8408119411827252\n",
      "Iteration 7598 training loss: 0.18147819060622067, test loss: 0.8408278513680854\n",
      "Iteration 7599 training loss: 0.18146150265312283, test loss: 0.8408437607718878\n",
      "Iteration 7600 training loss: 0.18144481764324102, test loss: 0.8408596693941722\n",
      "Iteration 7601 training loss: 0.1814281355757951, test loss: 0.840875577234978\n",
      "Iteration 7602 training loss: 0.18141145645000498, test loss: 0.8408914842943458\n",
      "Iteration 7603 training loss: 0.18139478026509107, test loss: 0.840907390572315\n",
      "Iteration 7604 training loss: 0.18137810702027382, test loss: 0.8409232960689252\n",
      "Iteration 7605 training loss: 0.1813614367147741, test loss: 0.8409392007842167\n",
      "Iteration 7606 training loss: 0.181344769347813, test loss: 0.8409551047182289\n",
      "Iteration 7607 training loss: 0.18132810491861193, test loss: 0.8409710078710019\n",
      "Iteration 7608 training loss: 0.1813114434263926, test loss: 0.8409869102425759\n",
      "Iteration 7609 training loss: 0.18129478487037698, test loss: 0.8410028118329904\n",
      "Iteration 7610 training loss: 0.18127812924978726, test loss: 0.8410187126422854\n",
      "Iteration 7611 training loss: 0.18126147656384597, test loss: 0.8410346126705012\n",
      "Iteration 7612 training loss: 0.18124482681177587, test loss: 0.841050511917677\n",
      "Iteration 7613 training loss: 0.18122817999280014, test loss: 0.8410664103838538\n",
      "Iteration 7614 training loss: 0.181211536106142, test loss: 0.8410823080690707\n",
      "Iteration 7615 training loss: 0.18119489515102513, test loss: 0.8410982049733678\n",
      "Iteration 7616 training loss: 0.1811782571266735, test loss: 0.8411141010967859\n",
      "Iteration 7617 training loss: 0.1811616220323112, test loss: 0.8411299964393639\n",
      "Iteration 7618 training loss: 0.1811449898671627, test loss: 0.8411458910011423\n",
      "Iteration 7619 training loss: 0.1811283606304529, test loss: 0.8411617847821614\n",
      "Iteration 7620 training loss: 0.1811117343214065, test loss: 0.8411776777824611\n",
      "Iteration 7621 training loss: 0.1810951109392491, test loss: 0.8411935700020818\n",
      "Iteration 7622 training loss: 0.18107849048320615, test loss: 0.8412094614410626\n",
      "Iteration 7623 training loss: 0.1810618729525035, test loss: 0.8412253520994445\n",
      "Iteration 7624 training loss: 0.18104525834636734, test loss: 0.8412412419772674\n",
      "Iteration 7625 training loss: 0.181028646664024, test loss: 0.8412571310745717\n",
      "Iteration 7626 training loss: 0.18101203790470013, test loss: 0.8412730193913971\n",
      "Iteration 7627 training loss: 0.18099543206762275, test loss: 0.8412889069277837\n",
      "Iteration 7628 training loss: 0.18097882915201918, test loss: 0.841304793683772\n",
      "Iteration 7629 training loss: 0.18096222915711682, test loss: 0.8413206796594022\n",
      "Iteration 7630 training loss: 0.18094563208214345, test loss: 0.841336564854714\n",
      "Iteration 7631 training loss: 0.18092903792632717, test loss: 0.8413524492697483\n",
      "Iteration 7632 training loss: 0.18091244668889633, test loss: 0.8413683329045449\n",
      "Iteration 7633 training loss: 0.18089585836907957, test loss: 0.8413842157591442\n",
      "Iteration 7634 training loss: 0.1808792729661057, test loss: 0.8414000978335864\n",
      "Iteration 7635 training loss: 0.18086269047920392, test loss: 0.8414159791279119\n",
      "Iteration 7636 training loss: 0.18084611090760377, test loss: 0.8414318596421608\n",
      "Iteration 7637 training loss: 0.18082953425053486, test loss: 0.8414477393763736\n",
      "Iteration 7638 training loss: 0.1808129605072272, test loss: 0.84146361833059\n",
      "Iteration 7639 training loss: 0.1807963896769111, test loss: 0.8414794965048512\n",
      "Iteration 7640 training loss: 0.1807798217588171, test loss: 0.8414953738991973\n",
      "Iteration 7641 training loss: 0.180763256752176, test loss: 0.8415112505136683\n",
      "Iteration 7642 training loss: 0.18074669465621884, test loss: 0.8415271263483047\n",
      "Iteration 7643 training loss: 0.18073013547017716, test loss: 0.8415430014031473\n",
      "Iteration 7644 training loss: 0.18071357919328243, test loss: 0.8415588756782356\n",
      "Iteration 7645 training loss: 0.18069702582476663, test loss: 0.8415747491736109\n",
      "Iteration 7646 training loss: 0.18068047536386206, test loss: 0.8415906218893131\n",
      "Iteration 7647 training loss: 0.18066392780980103, test loss: 0.8416064938253829\n",
      "Iteration 7648 training loss: 0.18064738316181633, test loss: 0.8416223649818607\n",
      "Iteration 7649 training loss: 0.18063084141914101, test loss: 0.8416382353587866\n",
      "Iteration 7650 training loss: 0.18061430258100833, test loss: 0.8416541049562019\n",
      "Iteration 7651 training loss: 0.1805977666466519, test loss: 0.8416699737741461\n",
      "Iteration 7652 training loss: 0.1805812336153055, test loss: 0.8416858418126603\n",
      "Iteration 7653 training loss: 0.1805647034862033, test loss: 0.8417017090717849\n",
      "Iteration 7654 training loss: 0.1805481762585796, test loss: 0.8417175755515602\n",
      "Iteration 7655 training loss: 0.1805316519316692, test loss: 0.8417334412520274\n",
      "Iteration 7656 training loss: 0.18051513050470688, test loss: 0.8417493061732263\n",
      "Iteration 7657 training loss: 0.18049861197692793, test loss: 0.8417651703151979\n",
      "Iteration 7658 training loss: 0.1804820963475678, test loss: 0.8417810336779826\n",
      "Iteration 7659 training loss: 0.18046558361586226, test loss: 0.8417968962616211\n",
      "Iteration 7660 training loss: 0.1804490737810473, test loss: 0.8418127580661542\n",
      "Iteration 7661 training loss: 0.1804325668423593, test loss: 0.8418286190916217\n",
      "Iteration 7662 training loss: 0.18041606279903472, test loss: 0.8418444793380654\n",
      "Iteration 7663 training loss: 0.18039956165031046, test loss: 0.8418603388055251\n",
      "Iteration 7664 training loss: 0.18038306339542365, test loss: 0.8418761974940415\n",
      "Iteration 7665 training loss: 0.1803665680336116, test loss: 0.841892055403656\n",
      "Iteration 7666 training loss: 0.18035007556411203, test loss: 0.8419079125344086\n",
      "Iteration 7667 training loss: 0.1803335859861629, test loss: 0.8419237688863402\n",
      "Iteration 7668 training loss: 0.18031709929900228, test loss: 0.8419396244594916\n",
      "Iteration 7669 training loss: 0.18030061550186877, test loss: 0.8419554792539032\n",
      "Iteration 7670 training loss: 0.18028413459400106, test loss: 0.841971333269616\n",
      "Iteration 7671 training loss: 0.18026765657463817, test loss: 0.841987186506671\n",
      "Iteration 7672 training loss: 0.1802511814430194, test loss: 0.8420030389651085\n",
      "Iteration 7673 training loss: 0.1802347091983843, test loss: 0.8420188906449697\n",
      "Iteration 7674 training loss: 0.18021823983997265, test loss: 0.842034741546295\n",
      "Iteration 7675 training loss: 0.18020177336702467, test loss: 0.8420505916691254\n",
      "Iteration 7676 training loss: 0.1801853097787806, test loss: 0.8420664410135018\n",
      "Iteration 7677 training loss: 0.1801688490744811, test loss: 0.842082289579465\n",
      "Iteration 7678 training loss: 0.1801523912533672, test loss: 0.8420981373670559\n",
      "Iteration 7679 training loss: 0.18013593631467992, test loss: 0.8421139843763149\n",
      "Iteration 7680 training loss: 0.18011948425766086, test loss: 0.8421298306072836\n",
      "Iteration 7681 training loss: 0.1801030350815516, test loss: 0.8421456760600023\n",
      "Iteration 7682 training loss: 0.18008658878559422, test loss: 0.8421615207345121\n",
      "Iteration 7683 training loss: 0.18007014536903096, test loss: 0.842177364630854\n",
      "Iteration 7684 training loss: 0.1800537048311043, test loss: 0.8421932077490689\n",
      "Iteration 7685 training loss: 0.1800372671710572, test loss: 0.8422090500891977\n",
      "Iteration 7686 training loss: 0.18002083238813255, test loss: 0.8422248916512812\n",
      "Iteration 7687 training loss: 0.18000440048157373, test loss: 0.8422407324353608\n",
      "Iteration 7688 training loss: 0.17998797145062445, test loss: 0.8422565724414772\n",
      "Iteration 7689 training loss: 0.17997154529452847, test loss: 0.8422724116696712\n",
      "Iteration 7690 training loss: 0.17995512201253, test loss: 0.8422882501199842\n",
      "Iteration 7691 training loss: 0.17993870160387343, test loss: 0.8423040877924566\n",
      "Iteration 7692 training loss: 0.17992228406780345, test loss: 0.8423199246871302\n",
      "Iteration 7693 training loss: 0.17990586940356507, test loss: 0.8423357608040458\n",
      "Iteration 7694 training loss: 0.1798894576104034, test loss: 0.842351596143244\n",
      "Iteration 7695 training loss: 0.17987304868756399, test loss: 0.8423674307047662\n",
      "Iteration 7696 training loss: 0.1798566426342926, test loss: 0.8423832644886537\n",
      "Iteration 7697 training loss: 0.1798402394498353, test loss: 0.8423990974949473\n",
      "Iteration 7698 training loss: 0.17982383913343827, test loss: 0.8424149297236885\n",
      "Iteration 7699 training loss: 0.17980744168434815, test loss: 0.8424307611749179\n",
      "Iteration 7700 training loss: 0.17979104710181182, test loss: 0.8424465918486769\n",
      "Iteration 7701 training loss: 0.17977465538507623, test loss: 0.8424624217450066\n",
      "Iteration 7702 training loss: 0.17975826653338886, test loss: 0.8424782508639482\n",
      "Iteration 7703 training loss: 0.17974188054599735, test loss: 0.8424940792055428\n",
      "Iteration 7704 training loss: 0.1797254974221495, test loss: 0.8425099067698318\n",
      "Iteration 7705 training loss: 0.1797091171610936, test loss: 0.842525733556856\n",
      "Iteration 7706 training loss: 0.17969273976207797, test loss: 0.842541559566657\n",
      "Iteration 7707 training loss: 0.1796763652243514, test loss: 0.8425573847992756\n",
      "Iteration 7708 training loss: 0.1796599935471628, test loss: 0.8425732092547537\n",
      "Iteration 7709 training loss: 0.17964362472976145, test loss: 0.8425890329331318\n",
      "Iteration 7710 training loss: 0.17962725877139687, test loss: 0.8426048558344519\n",
      "Iteration 7711 training loss: 0.17961089567131872, test loss: 0.8426206779587546\n",
      "Iteration 7712 training loss: 0.17959453542877715, test loss: 0.8426364993060815\n",
      "Iteration 7713 training loss: 0.17957817804302234, test loss: 0.8426523198764738\n",
      "Iteration 7714 training loss: 0.17956182351330502, test loss: 0.842668139669973\n",
      "Iteration 7715 training loss: 0.1795454718388759, test loss: 0.8426839586866204\n",
      "Iteration 7716 training loss: 0.17952912301898602, test loss: 0.8426997769264574\n",
      "Iteration 7717 training loss: 0.17951277705288693, test loss: 0.8427155943895249\n",
      "Iteration 7718 training loss: 0.1794964339398301, test loss: 0.8427314110758648\n",
      "Iteration 7719 training loss: 0.1794800936790676, test loss: 0.8427472269855183\n",
      "Iteration 7720 training loss: 0.17946375626985137, test loss: 0.8427630421185265\n",
      "Iteration 7721 training loss: 0.17944742171143396, test loss: 0.8427788564749313\n",
      "Iteration 7722 training loss: 0.17943109000306806, test loss: 0.8427946700547736\n",
      "Iteration 7723 training loss: 0.17941476114400656, test loss: 0.8428104828580955\n",
      "Iteration 7724 training loss: 0.17939843513350276, test loss: 0.8428262948849377\n",
      "Iteration 7725 training loss: 0.17938211197081014, test loss: 0.842842106135342\n",
      "Iteration 7726 training loss: 0.17936579165518243, test loss: 0.84285791660935\n",
      "Iteration 7727 training loss: 0.1793494741858736, test loss: 0.8428737263070033\n",
      "Iteration 7728 training loss: 0.179333159562138, test loss: 0.842889535228343\n",
      "Iteration 7729 training loss: 0.17931684778323015, test loss: 0.8429053433734103\n",
      "Iteration 7730 training loss: 0.17930053884840483, test loss: 0.8429211507422474\n",
      "Iteration 7731 training loss: 0.17928423275691707, test loss: 0.8429369573348958\n",
      "Iteration 7732 training loss: 0.17926792950802237, test loss: 0.8429527631513967\n",
      "Iteration 7733 training loss: 0.1792516291009762, test loss: 0.8429685681917918\n",
      "Iteration 7734 training loss: 0.1792353315350344, test loss: 0.8429843724561225\n",
      "Iteration 7735 training loss: 0.1792190368094532, test loss: 0.8430001759444307\n",
      "Iteration 7736 training loss: 0.1792027449234889, test loss: 0.8430159786567576\n",
      "Iteration 7737 training loss: 0.1791864558763982, test loss: 0.8430317805931452\n",
      "Iteration 7738 training loss: 0.17917016966743796, test loss: 0.843047581753635\n",
      "Iteration 7739 training loss: 0.17915388629586546, test loss: 0.843063382138268\n",
      "Iteration 7740 training loss: 0.1791376057609381, test loss: 0.8430791817470872\n",
      "Iteration 7741 training loss: 0.17912132806191355, test loss: 0.8430949805801331\n",
      "Iteration 7742 training loss: 0.17910505319804979, test loss: 0.8431107786374478\n",
      "Iteration 7743 training loss: 0.17908878116860508, test loss: 0.8431265759190729\n",
      "Iteration 7744 training loss: 0.17907251197283788, test loss: 0.8431423724250501\n",
      "Iteration 7745 training loss: 0.17905624561000696, test loss: 0.8431581681554212\n",
      "Iteration 7746 training loss: 0.17903998207937136, test loss: 0.8431739631102275\n",
      "Iteration 7747 training loss: 0.17902372138019035, test loss: 0.8431897572895114\n",
      "Iteration 7748 training loss: 0.17900746351172342, test loss: 0.8432055506933143\n",
      "Iteration 7749 training loss: 0.17899120847323044, test loss: 0.8432213433216778\n",
      "Iteration 7750 training loss: 0.1789749562639715, test loss: 0.8432371351746438\n",
      "Iteration 7751 training loss: 0.1789587068832068, test loss: 0.843252926252254\n",
      "Iteration 7752 training loss: 0.17894246033019706, test loss: 0.8432687165545504\n",
      "Iteration 7753 training loss: 0.1789262166042031, test loss: 0.8432845060815746\n",
      "Iteration 7754 training loss: 0.17890997570448597, test loss: 0.8433002948333685\n",
      "Iteration 7755 training loss: 0.17889373763030708, test loss: 0.843316082809974\n",
      "Iteration 7756 training loss: 0.1788775023809281, test loss: 0.843331870011433\n",
      "Iteration 7757 training loss: 0.17886126995561086, test loss: 0.8433476564377868\n",
      "Iteration 7758 training loss: 0.1788450403536176, test loss: 0.843363442089078\n",
      "Iteration 7759 training loss: 0.17882881357421074, test loss: 0.8433792269653478\n",
      "Iteration 7760 training loss: 0.1788125896166528, test loss: 0.8433950110666388\n",
      "Iteration 7761 training loss: 0.17879636848020686, test loss: 0.8434107943929927\n",
      "Iteration 7762 training loss: 0.17878015016413612, test loss: 0.8434265769444508\n",
      "Iteration 7763 training loss: 0.17876393466770396, test loss: 0.8434423587210558\n",
      "Iteration 7764 training loss: 0.17874772199017416, test loss: 0.843458139722849\n",
      "Iteration 7765 training loss: 0.1787315121308107, test loss: 0.8434739199498732\n",
      "Iteration 7766 training loss: 0.17871530508887778, test loss: 0.8434896994021691\n",
      "Iteration 7767 training loss: 0.17869910086363994, test loss: 0.8435054780797799\n",
      "Iteration 7768 training loss: 0.17868289945436194, test loss: 0.8435212559827471\n",
      "Iteration 7769 training loss: 0.17866670086030867, test loss: 0.8435370331111126\n",
      "Iteration 7770 training loss: 0.17865050508074562, test loss: 0.8435528094649184\n",
      "Iteration 7771 training loss: 0.17863431211493824, test loss: 0.8435685850442067\n",
      "Iteration 7772 training loss: 0.17861812196215224, test loss: 0.8435843598490195\n",
      "Iteration 7773 training loss: 0.1786019346216538, test loss: 0.8436001338793986\n",
      "Iteration 7774 training loss: 0.1785857500927091, test loss: 0.8436159071353863\n",
      "Iteration 7775 training loss: 0.1785695683745849, test loss: 0.8436316796170248\n",
      "Iteration 7776 training loss: 0.1785533894665478, test loss: 0.8436474513243557\n",
      "Iteration 7777 training loss: 0.17853721336786516, test loss: 0.8436632222574217\n",
      "Iteration 7778 training loss: 0.17852104007780403, test loss: 0.8436789924162643\n",
      "Iteration 7779 training loss: 0.17850486959563228, test loss: 0.8436947618009262\n",
      "Iteration 7780 training loss: 0.17848870192061764, test loss: 0.8437105304114492\n",
      "Iteration 7781 training loss: 0.17847253705202823, test loss: 0.8437262982478754\n",
      "Iteration 7782 training loss: 0.17845637498913247, test loss: 0.8437420653102471\n",
      "Iteration 7783 training loss: 0.178440215731199, test loss: 0.8437578315986065\n",
      "Iteration 7784 training loss: 0.17842405927749672, test loss: 0.8437735971129957\n",
      "Iteration 7785 training loss: 0.17840790562729483, test loss: 0.8437893618534565\n",
      "Iteration 7786 training loss: 0.17839175477986263, test loss: 0.8438051258200316\n",
      "Iteration 7787 training loss: 0.17837560673446984, test loss: 0.8438208890127636\n",
      "Iteration 7788 training loss: 0.17835946149038645, test loss: 0.8438366514316937\n",
      "Iteration 7789 training loss: 0.17834331904688253, test loss: 0.843852413076865\n",
      "Iteration 7790 training loss: 0.17832717940322862, test loss: 0.8438681739483191\n",
      "Iteration 7791 training loss: 0.17831104255869537, test loss: 0.8438839340460991\n",
      "Iteration 7792 training loss: 0.17829490851255375, test loss: 0.8438996933702464\n",
      "Iteration 7793 training loss: 0.17827877726407504, test loss: 0.8439154519208034\n",
      "Iteration 7794 training loss: 0.17826264881253062, test loss: 0.8439312096978132\n",
      "Iteration 7795 training loss: 0.17824652315719222, test loss: 0.8439469667013169\n",
      "Iteration 7796 training loss: 0.17823040029733184, test loss: 0.8439627229313574\n",
      "Iteration 7797 training loss: 0.17821428023222174, test loss: 0.8439784783879777\n",
      "Iteration 7798 training loss: 0.17819816296113436, test loss: 0.8439942330712195\n",
      "Iteration 7799 training loss: 0.1781820484833426, test loss: 0.8440099869811247\n",
      "Iteration 7800 training loss: 0.17816593679811926, test loss: 0.8440257401177365\n",
      "Iteration 7801 training loss: 0.1781498279047378, test loss: 0.8440414924810968\n",
      "Iteration 7802 training loss: 0.17813372180247156, test loss: 0.8440572440712483\n",
      "Iteration 7803 training loss: 0.17811761849059443, test loss: 0.844072994888233\n",
      "Iteration 7804 training loss: 0.1781015179683804, test loss: 0.8440887449320934\n",
      "Iteration 7805 training loss: 0.17808542023510376, test loss: 0.8441044942028723\n",
      "Iteration 7806 training loss: 0.17806932529003905, test loss: 0.8441202427006123\n",
      "Iteration 7807 training loss: 0.1780532331324611, test loss: 0.8441359904253546\n",
      "Iteration 7808 training loss: 0.17803714376164495, test loss: 0.8441517373771432\n",
      "Iteration 7809 training loss: 0.17802105717686587, test loss: 0.8441674835560197\n",
      "Iteration 7810 training loss: 0.17800497337739948, test loss: 0.8441832289620267\n",
      "Iteration 7811 training loss: 0.17798889236252147, test loss: 0.8441989735952068\n",
      "Iteration 7812 training loss: 0.17797281413150803, test loss: 0.8442147174556023\n",
      "Iteration 7813 training loss: 0.17795673868363548, test loss: 0.8442304605432565\n",
      "Iteration 7814 training loss: 0.17794066601818037, test loss: 0.8442462028582107\n",
      "Iteration 7815 training loss: 0.17792459613441952, test loss: 0.8442619444005083\n",
      "Iteration 7816 training loss: 0.17790852903163012, test loss: 0.8442776851701914\n",
      "Iteration 7817 training loss: 0.17789246470908937, test loss: 0.8442934251673031\n",
      "Iteration 7818 training loss: 0.17787640316607486, test loss: 0.8443091643918857\n",
      "Iteration 7819 training loss: 0.17786034440186457, test loss: 0.8443249028439815\n",
      "Iteration 7820 training loss: 0.17784428841573657, test loss: 0.8443406405236337\n",
      "Iteration 7821 training loss: 0.17782823520696917, test loss: 0.8443563774308842\n",
      "Iteration 7822 training loss: 0.17781218477484095, test loss: 0.8443721135657761\n",
      "Iteration 7823 training loss: 0.17779613711863082, test loss: 0.8443878489283521\n",
      "Iteration 7824 training loss: 0.17778009223761795, test loss: 0.8444035835186542\n",
      "Iteration 7825 training loss: 0.17776405013108162, test loss: 0.844419317336726\n",
      "Iteration 7826 training loss: 0.17774801079830152, test loss: 0.8444350503826097\n",
      "Iteration 7827 training loss: 0.1777319742385575, test loss: 0.8444507826563479\n",
      "Iteration 7828 training loss: 0.17771594045112968, test loss: 0.8444665141579832\n",
      "Iteration 7829 training loss: 0.17769990943529845, test loss: 0.8444822448875584\n",
      "Iteration 7830 training loss: 0.17768388119034442, test loss: 0.8444979748451167\n",
      "Iteration 7831 training loss: 0.1776678557155485, test loss: 0.8445137040307003\n",
      "Iteration 7832 training loss: 0.17765183301019186, test loss: 0.844529432444352\n",
      "Iteration 7833 training loss: 0.17763581307355586, test loss: 0.8445451600861144\n",
      "Iteration 7834 training loss: 0.17761979590492216, test loss: 0.8445608869560305\n",
      "Iteration 7835 training loss: 0.17760378150357262, test loss: 0.8445766130541433\n",
      "Iteration 7836 training loss: 0.17758776986878944, test loss: 0.8445923383804951\n",
      "Iteration 7837 training loss: 0.17757176099985492, test loss: 0.8446080629351295\n",
      "Iteration 7838 training loss: 0.17755575489605188, test loss: 0.8446237867180879\n",
      "Iteration 7839 training loss: 0.17753975155666304, test loss: 0.8446395097294145\n",
      "Iteration 7840 training loss: 0.17752375098097165, test loss: 0.8446552319691512\n",
      "Iteration 7841 training loss: 0.17750775316826115, test loss: 0.8446709534373416\n",
      "Iteration 7842 training loss: 0.17749175811781515, test loss: 0.8446866741340276\n",
      "Iteration 7843 training loss: 0.1774757658289175, test loss: 0.8447023940592532\n",
      "Iteration 7844 training loss: 0.17745977630085244, test loss: 0.8447181132130603\n",
      "Iteration 7845 training loss: 0.1774437895329044, test loss: 0.8447338315954925\n",
      "Iteration 7846 training loss: 0.17742780552435794, test loss: 0.8447495492065921\n",
      "Iteration 7847 training loss: 0.17741182427449806, test loss: 0.8447652660464026\n",
      "Iteration 7848 training loss: 0.1773958457826099, test loss: 0.8447809821149661\n",
      "Iteration 7849 training loss: 0.17737987004797884, test loss: 0.8447966974123264\n",
      "Iteration 7850 training loss: 0.1773638970698906, test loss: 0.8448124119385262\n",
      "Iteration 7851 training loss: 0.17734792684763104, test loss: 0.8448281256936079\n",
      "Iteration 7852 training loss: 0.17733195938048632, test loss: 0.8448438386776153\n",
      "Iteration 7853 training loss: 0.17731599466774292, test loss: 0.8448595508905912\n",
      "Iteration 7854 training loss: 0.17730003270868747, test loss: 0.8448752623325774\n",
      "Iteration 7855 training loss: 0.17728407350260683, test loss: 0.8448909730036184\n",
      "Iteration 7856 training loss: 0.1772681170487882, test loss: 0.844906682903757\n",
      "Iteration 7857 training loss: 0.177252163346519, test loss: 0.8449223920330355\n",
      "Iteration 7858 training loss: 0.1772362123950869, test loss: 0.8449381003914973\n",
      "Iteration 7859 training loss: 0.17722026419377981, test loss: 0.8449538079791858\n",
      "Iteration 7860 training loss: 0.17720431874188583, test loss: 0.844969514796143\n",
      "Iteration 7861 training loss: 0.17718837603869345, test loss: 0.8449852208424131\n",
      "Iteration 7862 training loss: 0.1771724360834913, test loss: 0.845000926118039\n",
      "Iteration 7863 training loss: 0.17715649887556825, test loss: 0.8450166306230632\n",
      "Iteration 7864 training loss: 0.17714056441421358, test loss: 0.8450323343575292\n",
      "Iteration 7865 training loss: 0.17712463269871653, test loss: 0.84504803732148\n",
      "Iteration 7866 training loss: 0.17710870372836685, test loss: 0.8450637395149588\n",
      "Iteration 7867 training loss: 0.17709277750245436, test loss: 0.8450794409380086\n",
      "Iteration 7868 training loss: 0.17707685402026932, test loss: 0.8450951415906728\n",
      "Iteration 7869 training loss: 0.17706093328110206, test loss: 0.8451108414729942\n",
      "Iteration 7870 training loss: 0.17704501528424324, test loss: 0.8451265405850165\n",
      "Iteration 7871 training loss: 0.17702910002898375, test loss: 0.845142238926782\n",
      "Iteration 7872 training loss: 0.17701318751461473, test loss: 0.8451579364983348\n",
      "Iteration 7873 training loss: 0.17699727774042767, test loss: 0.8451736332997175\n",
      "Iteration 7874 training loss: 0.17698137070571401, test loss: 0.8451893293309736\n",
      "Iteration 7875 training loss: 0.1769654664097658, test loss: 0.8452050245921461\n",
      "Iteration 7876 training loss: 0.1769495648518751, test loss: 0.8452207190832782\n",
      "Iteration 7877 training loss: 0.17693366603133426, test loss: 0.8452364128044138\n",
      "Iteration 7878 training loss: 0.17691776994743594, test loss: 0.845252105755595\n",
      "Iteration 7879 training loss: 0.17690187659947312, test loss: 0.8452677979368662\n",
      "Iteration 7880 training loss: 0.17688598598673874, test loss: 0.84528348934827\n",
      "Iteration 7881 training loss: 0.1768700981085263, test loss: 0.8452991799898495\n",
      "Iteration 7882 training loss: 0.17685421296412937, test loss: 0.8453148698616492\n",
      "Iteration 7883 training loss: 0.1768383305528418, test loss: 0.8453305589637109\n",
      "Iteration 7884 training loss: 0.17682245087395773, test loss: 0.8453462472960785\n",
      "Iteration 7885 training loss: 0.17680657392677143, test loss: 0.8453619348587957\n",
      "Iteration 7886 training loss: 0.17679069971057762, test loss: 0.8453776216519058\n",
      "Iteration 7887 training loss: 0.17677482822467108, test loss: 0.8453933076754515\n",
      "Iteration 7888 training loss: 0.17675895946834694, test loss: 0.8454089929294765\n",
      "Iteration 7889 training loss: 0.1767430934409005, test loss: 0.8454246774140243\n",
      "Iteration 7890 training loss: 0.17672723014162733, test loss: 0.8454403611291381\n",
      "Iteration 7891 training loss: 0.1767113695698233, test loss: 0.8454560440748616\n",
      "Iteration 7892 training loss: 0.17669551172478448, test loss: 0.8454717262512378\n",
      "Iteration 7893 training loss: 0.17667965660580728, test loss: 0.8454874076583105\n",
      "Iteration 7894 training loss: 0.17666380421218808, test loss: 0.8455030882961229\n",
      "Iteration 7895 training loss: 0.1766479545432238, test loss: 0.8455187681647183\n",
      "Iteration 7896 training loss: 0.17663210759821157, test loss: 0.8455344472641404\n",
      "Iteration 7897 training loss: 0.17661626337644853, test loss: 0.8455501255944325\n",
      "Iteration 7898 training loss: 0.17660042187723238, test loss: 0.8455658031556383\n",
      "Iteration 7899 training loss: 0.1765845830998608, test loss: 0.8455814799478009\n",
      "Iteration 7900 training loss: 0.17656874704363199, test loss: 0.8455971559709642\n",
      "Iteration 7901 training loss: 0.176552913707844, test loss: 0.8456128312251711\n",
      "Iteration 7902 training loss: 0.17653708309179553, test loss: 0.8456285057104662\n",
      "Iteration 7903 training loss: 0.1765212551947853, test loss: 0.8456441794268915\n",
      "Iteration 7904 training loss: 0.17650543001611235, test loss: 0.8456598523744919\n",
      "Iteration 7905 training loss: 0.17648960755507592, test loss: 0.8456755245533104\n",
      "Iteration 7906 training loss: 0.17647378781097553, test loss: 0.8456911959633904\n",
      "Iteration 7907 training loss: 0.17645797078311082, test loss: 0.8457068666047752\n",
      "Iteration 7908 training loss: 0.17644215647078196, test loss: 0.8457225364775093\n",
      "Iteration 7909 training loss: 0.17642634487328912, test loss: 0.8457382055816357\n",
      "Iteration 7910 training loss: 0.17641053598993275, test loss: 0.8457538739171976\n",
      "Iteration 7911 training loss: 0.17639472982001364, test loss: 0.8457695414842397\n",
      "Iteration 7912 training loss: 0.17637892636283267, test loss: 0.8457852082828048\n",
      "Iteration 7913 training loss: 0.17636312561769107, test loss: 0.8458008743129365\n",
      "Iteration 7914 training loss: 0.17634732758389035, test loss: 0.845816539574679\n",
      "Iteration 7915 training loss: 0.17633153226073214, test loss: 0.8458322040680751\n",
      "Iteration 7916 training loss: 0.17631573964751854, test loss: 0.8458478677931691\n",
      "Iteration 7917 training loss: 0.17629994974355154, test loss: 0.8458635307500048\n",
      "Iteration 7918 training loss: 0.17628416254813364, test loss: 0.8458791929386251\n",
      "Iteration 7919 training loss: 0.17626837806056753, test loss: 0.8458948543590744\n",
      "Iteration 7920 training loss: 0.17625259628015608, test loss: 0.8459105150113962\n",
      "Iteration 7921 training loss: 0.17623681720620246, test loss: 0.845926174895634\n",
      "Iteration 7922 training loss: 0.17622104083801016, test loss: 0.845941834011832\n",
      "Iteration 7923 training loss: 0.1762052671748827, test loss: 0.8459574923600335\n",
      "Iteration 7924 training loss: 0.17618949621612398, test loss: 0.8459731499402823\n",
      "Iteration 7925 training loss: 0.1761737279610381, test loss: 0.8459888067526224\n",
      "Iteration 7926 training loss: 0.17615796240892956, test loss: 0.8460044627970977\n",
      "Iteration 7927 training loss: 0.1761421995591029, test loss: 0.846020118073751\n",
      "Iteration 7928 training loss: 0.17612643941086295, test loss: 0.8460357725826271\n",
      "Iteration 7929 training loss: 0.1761106819635147, test loss: 0.8460514263237694\n",
      "Iteration 7930 training loss: 0.1760949272163637, test loss: 0.8460670792972215\n",
      "Iteration 7931 training loss: 0.17607917516871535, test loss: 0.8460827315030276\n",
      "Iteration 7932 training loss: 0.17606342581987555, test loss: 0.8460983829412314\n",
      "Iteration 7933 training loss: 0.17604767916915035, test loss: 0.8461140336118765\n",
      "Iteration 7934 training loss: 0.176031935215846, test loss: 0.8461296835150074\n",
      "Iteration 7935 training loss: 0.17601619395926918, test loss: 0.8461453326506674\n",
      "Iteration 7936 training loss: 0.17600045539872647, test loss: 0.8461609810189004\n",
      "Iteration 7937 training loss: 0.17598471953352507, test loss: 0.8461766286197498\n",
      "Iteration 7938 training loss: 0.17596898636297212, test loss: 0.8461922754532605\n",
      "Iteration 7939 training loss: 0.1759532558863752, test loss: 0.8462079215194763\n",
      "Iteration 7940 training loss: 0.175937528103042, test loss: 0.8462235668184401\n",
      "Iteration 7941 training loss: 0.1759218030122805, test loss: 0.8462392113501968\n",
      "Iteration 7942 training loss: 0.175906080613399, test loss: 0.8462548551147898\n",
      "Iteration 7943 training loss: 0.1758903609057059, test loss: 0.8462704981122635\n",
      "Iteration 7944 training loss: 0.1758746438885099, test loss: 0.8462861403426613\n",
      "Iteration 7945 training loss: 0.17585892956112, test loss: 0.8463017818060272\n",
      "Iteration 7946 training loss: 0.17584321792284535, test loss: 0.8463174225024059\n",
      "Iteration 7947 training loss: 0.17582750897299537, test loss: 0.8463330624318406\n",
      "Iteration 7948 training loss: 0.17581180271087976, test loss: 0.8463487015943757\n",
      "Iteration 7949 training loss: 0.1757960991358084, test loss: 0.8463643399900549\n",
      "Iteration 7950 training loss: 0.1757803982470914, test loss: 0.8463799776189223\n",
      "Iteration 7951 training loss: 0.17576470004403916, test loss: 0.8463956144810221\n",
      "Iteration 7952 training loss: 0.17574900452596234, test loss: 0.846411250576398\n",
      "Iteration 7953 training loss: 0.17573331169217177, test loss: 0.8464268859050942\n",
      "Iteration 7954 training loss: 0.17571762154197856, test loss: 0.8464425204671552\n",
      "Iteration 7955 training loss: 0.1757019340746941, test loss: 0.8464581542626245\n",
      "Iteration 7956 training loss: 0.1756862492896298, test loss: 0.8464737872915461\n",
      "Iteration 7957 training loss: 0.17567056718609766, test loss: 0.8464894195539641\n",
      "Iteration 7958 training loss: 0.17565488776340968, test loss: 0.8465050510499234\n",
      "Iteration 7959 training loss: 0.17563921102087815, test loss: 0.8465206817794672\n",
      "Iteration 7960 training loss: 0.17562353695781555, test loss: 0.8465363117426398\n",
      "Iteration 7961 training loss: 0.17560786557353475, test loss: 0.8465519409394856\n",
      "Iteration 7962 training loss: 0.17559219686734864, test loss: 0.846567569370048\n",
      "Iteration 7963 training loss: 0.17557653083857058, test loss: 0.8465831970343722\n",
      "Iteration 7964 training loss: 0.17556086748651403, test loss: 0.8465988239325019\n",
      "Iteration 7965 training loss: 0.1755452068104927, test loss: 0.8466144500644807\n",
      "Iteration 7966 training loss: 0.17552954880982044, test loss: 0.8466300754303534\n",
      "Iteration 7967 training loss: 0.17551389348381163, test loss: 0.8466457000301643\n",
      "Iteration 7968 training loss: 0.17549824083178067, test loss: 0.8466613238639571\n",
      "Iteration 7969 training loss: 0.17548259085304216, test loss: 0.8466769469317764\n",
      "Iteration 7970 training loss: 0.17546694354691103, test loss: 0.8466925692336656\n",
      "Iteration 7971 training loss: 0.17545129891270236, test loss: 0.8467081907696704\n",
      "Iteration 7972 training loss: 0.17543565694973176, test loss: 0.8467238115398337\n",
      "Iteration 7973 training loss: 0.17542001765731466, test loss: 0.8467394315442003\n",
      "Iteration 7974 training loss: 0.17540438103476694, test loss: 0.8467550507828147\n",
      "Iteration 7975 training loss: 0.17538874708140478, test loss: 0.8467706692557204\n",
      "Iteration 7976 training loss: 0.1753731157965444, test loss: 0.8467862869629621\n",
      "Iteration 7977 training loss: 0.1753574871795025, test loss: 0.846801903904584\n",
      "Iteration 7978 training loss: 0.17534186122959577, test loss: 0.8468175200806304\n",
      "Iteration 7979 training loss: 0.1753262379461413, test loss: 0.8468331354911458\n",
      "Iteration 7980 training loss: 0.17531061732845638, test loss: 0.8468487501361746\n",
      "Iteration 7981 training loss: 0.17529499937585846, test loss: 0.8468643640157607\n",
      "Iteration 7982 training loss: 0.17527938408766544, test loss: 0.8468799771299483\n",
      "Iteration 7983 training loss: 0.17526377146319516, test loss: 0.8468955894787823\n",
      "Iteration 7984 training loss: 0.17524816150176592, test loss: 0.8469112010623068\n",
      "Iteration 7985 training loss: 0.17523255420269618, test loss: 0.8469268118805662\n",
      "Iteration 7986 training loss: 0.17521694956530456, test loss: 0.8469424219336046\n",
      "Iteration 7987 training loss: 0.17520134758891007, test loss: 0.8469580312214668\n",
      "Iteration 7988 training loss: 0.17518574827283184, test loss: 0.8469736397441967\n",
      "Iteration 7989 training loss: 0.17517015161638927, test loss: 0.8469892475018391\n",
      "Iteration 7990 training loss: 0.17515455761890206, test loss: 0.8470048544944382\n",
      "Iteration 7991 training loss: 0.17513896627969003, test loss: 0.8470204607220386\n",
      "Iteration 7992 training loss: 0.17512337759807325, test loss: 0.8470360661846846\n",
      "Iteration 7993 training loss: 0.17510779157337206, test loss: 0.8470516708824206\n",
      "Iteration 7994 training loss: 0.17509220820490715, test loss: 0.8470672748152911\n",
      "Iteration 7995 training loss: 0.17507662749199926, test loss: 0.8470828779833403\n",
      "Iteration 7996 training loss: 0.17506104943396938, test loss: 0.8470984803866133\n",
      "Iteration 7997 training loss: 0.17504547403013887, test loss: 0.8471140820251538\n",
      "Iteration 7998 training loss: 0.17502990127982923, test loss: 0.8471296828990069\n",
      "Iteration 7999 training loss: 0.17501433118236223, test loss: 0.8471452830082169\n",
      "Iteration 8000 training loss: 0.1749987637370598, test loss: 0.847160882352828\n",
      "Iteration 8001 training loss: 0.17498319894324416, test loss: 0.8471764809328852\n",
      "Iteration 8002 training loss: 0.17496763680023783, test loss: 0.8471920787484329\n",
      "Iteration 8003 training loss: 0.17495207730736345, test loss: 0.8472076757995152\n",
      "Iteration 8004 training loss: 0.17493652046394395, test loss: 0.8472232720861769\n",
      "Iteration 8005 training loss: 0.1749209662693025, test loss: 0.8472388676084629\n",
      "Iteration 8006 training loss: 0.17490541472276241, test loss: 0.8472544623664171\n",
      "Iteration 8007 training loss: 0.17488986582364743, test loss: 0.8472700563600847\n",
      "Iteration 8008 training loss: 0.1748743195712813, test loss: 0.84728564958951\n",
      "Iteration 8009 training loss: 0.17485877596498817, test loss: 0.8473012420547374\n",
      "Iteration 8010 training loss: 0.17484323500409232, test loss: 0.8473168337558119\n",
      "Iteration 8011 training loss: 0.17482769668791834, test loss: 0.8473324246927778\n",
      "Iteration 8012 training loss: 0.17481216101579103, test loss: 0.8473480148656801\n",
      "Iteration 8013 training loss: 0.17479662798703535, test loss: 0.8473636042745629\n",
      "Iteration 8014 training loss: 0.1747810976009766, test loss: 0.8473791929194712\n",
      "Iteration 8015 training loss: 0.17476556985694028, test loss: 0.8473947808004494\n",
      "Iteration 8016 training loss: 0.17475004475425207, test loss: 0.8474103679175421\n",
      "Iteration 8017 training loss: 0.17473452229223793, test loss: 0.8474259542707945\n",
      "Iteration 8018 training loss: 0.17471900247022407, test loss: 0.8474415398602507\n",
      "Iteration 8019 training loss: 0.17470348528753687, test loss: 0.8474571246859558\n",
      "Iteration 8020 training loss: 0.174687970743503, test loss: 0.8474727087479543\n",
      "Iteration 8021 training loss: 0.17467245883744936, test loss: 0.8474882920462907\n",
      "Iteration 8022 training loss: 0.174656949568703, test loss: 0.84750387458101\n",
      "Iteration 8023 training loss: 0.1746414429365913, test loss: 0.8475194563521569\n",
      "Iteration 8024 training loss: 0.17462593894044184, test loss: 0.8475350373597759\n",
      "Iteration 8025 training loss: 0.17461043757958247, test loss: 0.847550617603912\n",
      "Iteration 8026 training loss: 0.17459493885334115, test loss: 0.8475661970846099\n",
      "Iteration 8027 training loss: 0.1745794427610462, test loss: 0.8475817758019142\n",
      "Iteration 8028 training loss: 0.17456394930202612, test loss: 0.8475973537558696\n",
      "Iteration 8029 training loss: 0.17454845847560962, test loss: 0.8476129309465216\n",
      "Iteration 8030 training loss: 0.1745329702811257, test loss: 0.8476285073739139\n",
      "Iteration 8031 training loss: 0.1745174847179035, test loss: 0.8476440830380924\n",
      "Iteration 8032 training loss: 0.17450200178527245, test loss: 0.8476596579391009\n",
      "Iteration 8033 training loss: 0.17448652148256227, test loss: 0.8476752320769846\n",
      "Iteration 8034 training loss: 0.17447104380910286, test loss: 0.8476908054517887\n",
      "Iteration 8035 training loss: 0.17445556876422424, test loss: 0.8477063780635575\n",
      "Iteration 8036 training loss: 0.17444009634725688, test loss: 0.847721949912336\n",
      "Iteration 8037 training loss: 0.1744246265575313, test loss: 0.8477375209981691\n",
      "Iteration 8038 training loss: 0.17440915939437823, test loss: 0.8477530913211017\n",
      "Iteration 8039 training loss: 0.17439369485712888, test loss: 0.8477686608811789\n",
      "Iteration 8040 training loss: 0.17437823294511437, test loss: 0.8477842296784451\n",
      "Iteration 8041 training loss: 0.17436277365766625, test loss: 0.8477997977129453\n",
      "Iteration 8042 training loss: 0.17434731699411632, test loss: 0.847815364984725\n",
      "Iteration 8043 training loss: 0.17433186295379646, test loss: 0.8478309314938279\n",
      "Iteration 8044 training loss: 0.1743164115360389, test loss: 0.8478464972403001\n",
      "Iteration 8045 training loss: 0.17430096274017606, test loss: 0.8478620622241859\n",
      "Iteration 8046 training loss: 0.17428551656554056, test loss: 0.8478776264455301\n",
      "Iteration 8047 training loss: 0.1742700730114653, test loss: 0.8478931899043785\n",
      "Iteration 8048 training loss: 0.1742546320772834, test loss: 0.8479087526007749\n",
      "Iteration 8049 training loss: 0.17423919376232816, test loss: 0.8479243145347652\n",
      "Iteration 8050 training loss: 0.1742237580659332, test loss: 0.847939875706394\n",
      "Iteration 8051 training loss: 0.1742083249874323, test loss: 0.8479554361157063\n",
      "Iteration 8052 training loss: 0.17419289452615946, test loss: 0.8479709957627469\n",
      "Iteration 8053 training loss: 0.17417746668144898, test loss: 0.8479865546475612\n",
      "Iteration 8054 training loss: 0.17416204145263534, test loss: 0.8480021127701941\n",
      "Iteration 8055 training loss: 0.1741466188390532, test loss: 0.8480176701306901\n",
      "Iteration 8056 training loss: 0.17413119884003753, test loss: 0.8480332267290948\n",
      "Iteration 8057 training loss: 0.1741157814549235, test loss: 0.848048782565453\n",
      "Iteration 8058 training loss: 0.17410036668304651, test loss: 0.84806433763981\n",
      "Iteration 8059 training loss: 0.1740849545237422, test loss: 0.8480798919522105\n",
      "Iteration 8060 training loss: 0.17406954497634639, test loss: 0.8480954455026998\n",
      "Iteration 8061 training loss: 0.1740541380401952, test loss: 0.8481109982913229\n",
      "Iteration 8062 training loss: 0.17403873371462492, test loss: 0.8481265503181247\n",
      "Iteration 8063 training loss: 0.17402333199897208, test loss: 0.8481421015831506\n",
      "Iteration 8064 training loss: 0.17400793289257352, test loss: 0.8481576520864459\n",
      "Iteration 8065 training loss: 0.17399253639476608, test loss: 0.8481732018280549\n",
      "Iteration 8066 training loss: 0.17397714250488713, test loss: 0.8481887508080233\n",
      "Iteration 8067 training loss: 0.17396175122227409, test loss: 0.8482042990263958\n",
      "Iteration 8068 training loss: 0.17394636254626453, test loss: 0.8482198464832182\n",
      "Iteration 8069 training loss: 0.1739309764761965, test loss: 0.8482353931785355\n",
      "Iteration 8070 training loss: 0.17391559301140805, test loss: 0.8482509391123922\n",
      "Iteration 8071 training loss: 0.17390021215123755, test loss: 0.8482664842848341\n",
      "Iteration 8072 training loss: 0.1738848338950236, test loss: 0.848282028695906\n",
      "Iteration 8073 training loss: 0.17386945824210495, test loss: 0.8482975723456535\n",
      "Iteration 8074 training loss: 0.17385408519182072, test loss: 0.8483131152341216\n",
      "Iteration 8075 training loss: 0.17383871474351018, test loss: 0.8483286573613551\n",
      "Iteration 8076 training loss: 0.17382334689651274, test loss: 0.8483441987273995\n",
      "Iteration 8077 training loss: 0.17380798165016817, test loss: 0.8483597393323006\n",
      "Iteration 8078 training loss: 0.17379261900381648, test loss: 0.8483752791761023\n",
      "Iteration 8079 training loss: 0.1737772589567977, test loss: 0.8483908182588513\n",
      "Iteration 8080 training loss: 0.17376190150845233, test loss: 0.8484063565805917\n",
      "Iteration 8081 training loss: 0.17374654665812095, test loss: 0.8484218941413696\n",
      "Iteration 8082 training loss: 0.1737311944051444, test loss: 0.8484374309412293\n",
      "Iteration 8083 training loss: 0.1737158447488638, test loss: 0.8484529669802171\n",
      "Iteration 8084 training loss: 0.17370049768862045, test loss: 0.8484685022583778\n",
      "Iteration 8085 training loss: 0.17368515322375586, test loss: 0.8484840367757566\n",
      "Iteration 8086 training loss: 0.17366981135361176, test loss: 0.8484995705323986\n",
      "Iteration 8087 training loss: 0.17365447207753013, test loss: 0.8485151035283494\n",
      "Iteration 8088 training loss: 0.17363913539485323, test loss: 0.8485306357636544\n",
      "Iteration 8089 training loss: 0.17362380130492344, test loss: 0.848546167238359\n",
      "Iteration 8090 training loss: 0.17360846980708342, test loss: 0.848561697952508\n",
      "Iteration 8091 training loss: 0.17359314090067607, test loss: 0.8485772279061473\n",
      "Iteration 8092 training loss: 0.17357781458504454, test loss: 0.8485927570993221\n",
      "Iteration 8093 training loss: 0.173562490859532, test loss: 0.8486082855320776\n",
      "Iteration 8094 training loss: 0.17354716972348214, test loss: 0.848623813204459\n",
      "Iteration 8095 training loss: 0.1735318511762387, test loss: 0.8486393401165124\n",
      "Iteration 8096 training loss: 0.17351653521714572, test loss: 0.8486548662682822\n",
      "Iteration 8097 training loss: 0.1735012218455474, test loss: 0.8486703916598147\n",
      "Iteration 8098 training loss: 0.17348591106078812, test loss: 0.8486859162911543\n",
      "Iteration 8099 training loss: 0.17347060286221264, test loss: 0.8487014401623475\n",
      "Iteration 8100 training loss: 0.1734552972491659, test loss: 0.848716963273439\n",
      "Iteration 8101 training loss: 0.17343999422099296, test loss: 0.8487324856244745\n",
      "Iteration 8102 training loss: 0.17342469377703917, test loss: 0.848748007215499\n",
      "Iteration 8103 training loss: 0.17340939591665014, test loss: 0.8487635280465589\n",
      "Iteration 8104 training loss: 0.17339410063917157, test loss: 0.8487790481176991\n",
      "Iteration 8105 training loss: 0.17337880794394964, test loss: 0.8487945674289646\n",
      "Iteration 8106 training loss: 0.1733635178303305, test loss: 0.8488100859804013\n",
      "Iteration 8107 training loss: 0.17334823029766058, test loss: 0.8488256037720548\n",
      "Iteration 8108 training loss: 0.17333294534528665, test loss: 0.8488411208039703\n",
      "Iteration 8109 training loss: 0.17331766297255558, test loss: 0.8488566370761935\n",
      "Iteration 8110 training loss: 0.17330238317881455, test loss: 0.8488721525887697\n",
      "Iteration 8111 training loss: 0.1732871059634109, test loss: 0.8488876673417448\n",
      "Iteration 8112 training loss: 0.17327183132569215, test loss: 0.848903181335164\n",
      "Iteration 8113 training loss: 0.17325655926500624, test loss: 0.8489186945690728\n",
      "Iteration 8114 training loss: 0.1732412897807011, test loss: 0.848934207043517\n",
      "Iteration 8115 training loss: 0.17322602287212505, test loss: 0.8489497187585417\n",
      "Iteration 8116 training loss: 0.1732107585386265, test loss: 0.8489652297141929\n",
      "Iteration 8117 training loss: 0.17319549677955415, test loss: 0.8489807399105158\n",
      "Iteration 8118 training loss: 0.17318023759425702, test loss: 0.8489962493475565\n",
      "Iteration 8119 training loss: 0.1731649809820841, test loss: 0.8490117580253598\n",
      "Iteration 8120 training loss: 0.1731497269423849, test loss: 0.8490272659439722\n",
      "Iteration 8121 training loss: 0.17313447547450891, test loss: 0.8490427731034386\n",
      "Iteration 8122 training loss: 0.17311922657780604, test loss: 0.8490582795038047\n",
      "Iteration 8123 training loss: 0.17310398025162618, test loss: 0.8490737851451164\n",
      "Iteration 8124 training loss: 0.17308873649531967, test loss: 0.8490892900274191\n",
      "Iteration 8125 training loss: 0.17307349530823707, test loss: 0.8491047941507586\n",
      "Iteration 8126 training loss: 0.1730582566897289, test loss: 0.8491202975151801\n",
      "Iteration 8127 training loss: 0.17304302063914623, test loss: 0.8491358001207299\n",
      "Iteration 8128 training loss: 0.17302778715584016, test loss: 0.8491513019674534\n",
      "Iteration 8129 training loss: 0.17301255623916195, test loss: 0.8491668030553954\n",
      "Iteration 8130 training loss: 0.1729973278884633, test loss: 0.8491823033846032\n",
      "Iteration 8131 training loss: 0.17298210210309603, test loss: 0.8491978029551213\n",
      "Iteration 8132 training loss: 0.17296687888241205, test loss: 0.8492133017669956\n",
      "Iteration 8133 training loss: 0.17295165822576372, test loss: 0.8492287998202723\n",
      "Iteration 8134 training loss: 0.17293644013250348, test loss: 0.8492442971149965\n",
      "Iteration 8135 training loss: 0.17292122460198397, test loss: 0.849259793651214\n",
      "Iteration 8136 training loss: 0.17290601163355812, test loss: 0.8492752894289712\n",
      "Iteration 8137 training loss: 0.1728908012265791, test loss: 0.8492907844483127\n",
      "Iteration 8138 training loss: 0.17287559338040026, test loss: 0.8493062787092848\n",
      "Iteration 8139 training loss: 0.17286038809437515, test loss: 0.8493217722119338\n",
      "Iteration 8140 training loss: 0.17284518536785753, test loss: 0.8493372649563045\n",
      "Iteration 8141 training loss: 0.17282998520020146, test loss: 0.8493527569424434\n",
      "Iteration 8142 training loss: 0.17281478759076116, test loss: 0.8493682481703957\n",
      "Iteration 8143 training loss: 0.17279959253889107, test loss: 0.8493837386402074\n",
      "Iteration 8144 training loss: 0.17278440004394588, test loss: 0.8493992283519246\n",
      "Iteration 8145 training loss: 0.17276921010528049, test loss: 0.8494147173055925\n",
      "Iteration 8146 training loss: 0.17275402272225, test loss: 0.8494302055012575\n",
      "Iteration 8147 training loss: 0.17273883789420968, test loss: 0.849445692938965\n",
      "Iteration 8148 training loss: 0.1727236556205152, test loss: 0.8494611796187609\n",
      "Iteration 8149 training loss: 0.17270847590052227, test loss: 0.8494766655406912\n",
      "Iteration 8150 training loss: 0.17269329873358688, test loss: 0.8494921507048014\n",
      "Iteration 8151 training loss: 0.17267812411906525, test loss: 0.8495076351111376\n",
      "Iteration 8152 training loss: 0.1726629520563138, test loss: 0.8495231187597461\n",
      "Iteration 8153 training loss: 0.17264778254468913, test loss: 0.849538601650672\n",
      "Iteration 8154 training loss: 0.17263261558354825, test loss: 0.8495540837839612\n",
      "Iteration 8155 training loss: 0.1726174511722481, test loss: 0.8495695651596603\n",
      "Iteration 8156 training loss: 0.1726022893101461, test loss: 0.8495850457778145\n",
      "Iteration 8157 training loss: 0.17258712999659967, test loss: 0.8496005256384698\n",
      "Iteration 8158 training loss: 0.17257197323096665, test loss: 0.8496160047416723\n",
      "Iteration 8159 training loss: 0.17255681901260492, test loss: 0.8496314830874678\n",
      "Iteration 8160 training loss: 0.17254166734087273, test loss: 0.8496469606759021\n",
      "Iteration 8161 training loss: 0.1725265182151284, test loss: 0.8496624375070214\n",
      "Iteration 8162 training loss: 0.1725113716347306, test loss: 0.8496779135808717\n",
      "Iteration 8163 training loss: 0.1724962275990382, test loss: 0.8496933888974983\n",
      "Iteration 8164 training loss: 0.17248108610741014, test loss: 0.8497088634569483\n",
      "Iteration 8165 training loss: 0.17246594715920585, test loss: 0.8497243372592663\n",
      "Iteration 8166 training loss: 0.17245081075378466, test loss: 0.8497398103044991\n",
      "Iteration 8167 training loss: 0.17243567689050635, test loss: 0.8497552825926925\n",
      "Iteration 8168 training loss: 0.17242054556873088, test loss: 0.8497707541238922\n",
      "Iteration 8169 training loss: 0.17240541678781834, test loss: 0.849786224898145\n",
      "Iteration 8170 training loss: 0.17239029054712912, test loss: 0.8498016949154963\n",
      "Iteration 8171 training loss: 0.17237516684602372, test loss: 0.849817164175992\n",
      "Iteration 8172 training loss: 0.17236004568386307, test loss: 0.8498326326796782\n",
      "Iteration 8173 training loss: 0.17234492706000806, test loss: 0.8498481004266013\n",
      "Iteration 8174 training loss: 0.17232981097382, test loss: 0.8498635674168067\n",
      "Iteration 8175 training loss: 0.1723146974246603, test loss: 0.8498790336503408\n",
      "Iteration 8176 training loss: 0.17229958641189064, test loss: 0.8498944991272498\n",
      "Iteration 8177 training loss: 0.17228447793487286, test loss: 0.8499099638475796\n",
      "Iteration 8178 training loss: 0.17226937199296913, test loss: 0.8499254278113764\n",
      "Iteration 8179 training loss: 0.17225426858554174, test loss: 0.8499408910186855\n",
      "Iteration 8180 training loss: 0.1722391677119531, test loss: 0.8499563534695543\n",
      "Iteration 8181 training loss: 0.17222406937156615, test loss: 0.8499718151640279\n",
      "Iteration 8182 training loss: 0.17220897356374373, test loss: 0.8499872761021524\n",
      "Iteration 8183 training loss: 0.17219388028784907, test loss: 0.8500027362839747\n",
      "Iteration 8184 training loss: 0.1721787895432455, test loss: 0.8500181957095398\n",
      "Iteration 8185 training loss: 0.17216370132929673, test loss: 0.8500336543788946\n",
      "Iteration 8186 training loss: 0.17214861564536652, test loss: 0.8500491122920852\n",
      "Iteration 8187 training loss: 0.17213353249081895, test loss: 0.8500645694491578\n",
      "Iteration 8188 training loss: 0.1721184518650183, test loss: 0.8500800258501581\n",
      "Iteration 8189 training loss: 0.1721033737673289, test loss: 0.8500954814951321\n",
      "Iteration 8190 training loss: 0.17208829819711563, test loss: 0.8501109363841266\n",
      "Iteration 8191 training loss: 0.17207322515374335, test loss: 0.8501263905171873\n",
      "Iteration 8192 training loss: 0.1720581546365771, test loss: 0.8501418438943606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f735edc62a0>,\n",
       " <matplotlib.lines.Line2D at 0x7f7358188e30>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ### ( 12 lines of code)\n",
    "def grad(predictions, labels, in_features):\n",
    "    \"\"\" Gradient function with sigmoid activation\n",
    "    Args:\n",
    "        predictions: model predicted value, a 2d array with shape (# samples, 1)\n",
    "        labels: labeled value from data set, a 2d array with shape (# samples, 1)\n",
    "        in_features: feature matrix, a 2d array with shape (# samples, # pixels)\n",
    "    Returns:\n",
    "        dw: row vector of BCE loss partial derivatives w.r.t. weights, 2d array with shape (1, # features)\n",
    "        db: scalar of BCE loss partial derivatives w.r.t. bias\n",
    "    \"\"\"\n",
    "    dw = 1 / labels.shape[0] * (predictions - labels).T @ in_features\n",
    "    db = np.mean(predictions - labels)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.random.normal(0, 1e-4, (1, features_train.shape[1]))\n",
    "b = np.random.normal(0, 1e-4)\n",
    "num_iters = 8192\n",
    "learning_rate = 0.001\n",
    "losses_train, losses_test = [], []\n",
    "# Optimization loop\n",
    "for i in range(num_iters):\n",
    "    preds_train = forward(features_train, w, b)\n",
    "    loss_train = bce_loss(preds_train, labels_train)\n",
    "    loss_test = bce_loss(forward(features_test, w, b), labels_test)\n",
    "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    dw, db = grad(preds_train, labels_train, features_train)\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "### END CODE HERE ### \n",
    "\n",
    "\n",
    "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "You can treat the model output, $\\mathbf{\\hat{y}}$ as the probabilities of the images being classified as dog pictures. Set the classification boundary to be 0.5, we can categorize an image as a cat image ($^{(m)}y <= 0.5$) or a dog image ($^{(m)}y > 0.5$). We'll evaluate the model on both training and test dataset.\n",
    "### $\\color{violet}{\\textbf{(15\\%) Exercise 5: Evaluation on Test Data}}$\n",
    "Calculate classification accuracy using the trained model.\n",
    "\n",
    "**$\\color{red}{\\textbf{Note: model accuracy on training data has to be greater than 0.98}}$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on training data: 0.9892280071813285\n",
      "Model accuracy on test data: 0.6071428571428571\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on training data\n",
    "categories_train = preds_train > 0.5  # Convert predictions over 0.5 to 1 (True), otherwise to 0 (False)\n",
    "is_correct_train = categories_train == labels_train  # Find out which predictions are correct\n",
    "num_correct_train = np.sum(is_correct_train)  # Calculate how many correct predictions are made\n",
    "acc_train = num_correct_train / labels_train.shape[0]  # Calculate accuracy rate: # correct predictions / # samples\n",
    "print(f\"Model accuracy on training data: {acc_train}\")\n",
    "\n",
    "### START CODE HERE ### ( 6 lines of code)\n",
    "# Accuracy on test data\n",
    "preds_test = forward(features_test, w, b)  # make predictions on test features\n",
    "categories_test = preds_test > 0.5  # Convert predictions over 0.5 to 1 (True), otherwise to 0 (False)\n",
    "is_correct_test = categories_test==labels_test  # Find out which predictions are correct\n",
    "num_correct_test = np.sum(is_correct_test)  # Calculate how many correct predictions are made\n",
    "acc_test = num_correct_test / labels_test.shape[0]  # Calculate accuracy rate: correct # / total #\n",
    "### END CODE HERE ###\n",
    "print(f\"Model accuracy on test data: {acc_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may notice the big difference bewteen the prediction accuracy on training data vs. test data. What could be the causes of this phenomenon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test\n",
    "Download new images to this folder and try to use your model to classify them.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(15\\%) Exercise 6: Test Model with New Image}}$\n",
    "**$\\color{red}{\\textbf{Note: you can use two provided pictures to test your code, but please upload at least one new picture for the final test.}}$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11838/3096496798.py:21: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  predictions = sigmoid(in_features @ weight.T + bias)\n",
      "/tmp/ipykernel_11838/3096496798.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  y = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 12 lines of code)\n",
    "image_raw = cv.imread('Hello_Kitty.jpg')  # read the raw image from a file\n",
    "image_rgb = cv.cvtColor(image_raw, cv.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "image_resize = cv.resize(image_rgb, (100, 100))  # Resize image to shape (100, 100, 3)\n",
    "image_flatten = image.reshape(1, -1)  # Flatten image array to a row vector with shape (1, 200*200*3)\n",
    "image_rescale = image_flatten / 255.  # rescale pixel value from 0~255 to 0.~1.\n",
    "dog_likelihood = forward(image_rescale, w, b)  # predict new image with your model\n",
    "### END CODE HERE ### \n",
    "\n",
    "is_dog = dog_likelihood > 0.5\n",
    "if is_dog.squeeze():\n",
    "    print(\"Woof!\") \n",
    "else:\n",
    "    print(\"Meow!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have finished this assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3321",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
