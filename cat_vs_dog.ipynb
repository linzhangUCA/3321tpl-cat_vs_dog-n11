{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat vs. Dog (Binary Classification) with N11 Model \n",
    "\n",
    "## Background\n",
    "Binary classification in the context of images, is the process of teaching a model to categorize a given image into one of two possible classes. \n",
    "We need to classify images to automate tasks that require visual understanding, enabling computers to interpret and make decisions based on the content of a picture.\n",
    "This ability to automatically categorize images unlocks countless applications across various industries, from healthcare to transportation.\n",
    "\n",
    "- Classify a skin lesion as either benign or malignant (cancerous) from a photograph, assisting dermatologists in early cancer detection. \n",
    "- An autonomous vehicle needs to classify objects as pedestrians or other objects.\n",
    "- Classify different types of crops from an aerial photo, assess plant health by identifying diseased vs. healthy crops.\n",
    "- Unlock your smartphone with your face is a process to identify authorized personnel vs. unauthorized intruders.\n",
    "\n",
    "## Objectives:\n",
    "- Practice neural network model with a hidden layer.\n",
    "- Review binary classification.\n",
    "- Get started using [PyTorch](https://pytorch.org/).\n",
    "\n",
    " <font color=582c83> \n",
    "\n",
    "## Exercises:\n",
    "1. (5%) Data Preprocessing\n",
    "2. (15%) Exercise 2: N11 Model Construction\n",
    "3. (10%) Exercise 3: Model Assessment\n",
    "4. (20%) Exercise 4: Gradients Computation\n",
    "5. (40%) Exercise 5: Model Optimization\n",
    "6. (6%) Exercise 6: New Images Test\n",
    "7. (4%) Exercise 7: Model Analyses\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation with PyTorch\n",
    "\n",
    "### 1.1 Create Annotation Files\n",
    "Data is not always stored in NumPy arrays. Most cases, you will have to organize and annotate the raw data stored in your hard drive. For image data, you want it to be organized as the following way.\n",
    "``` console\n",
    "root/dog/xxx.jpg\n",
    "root/dog/xxy.jpg\n",
    "root/dog/xxz.jpg\n",
    "\n",
    "root/cat/123.jpg\n",
    "root/cat/456.jpg\n",
    "root/cat/789.jpg\n",
    "```\n",
    "To grab image information and store them in an comma-seperated values (CSV) file:\n",
    "1. Visit the data directory, grab all images' paths and corresponding categories.\n",
    "2. Save the paths and categories of images in an `.csv` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Locate train and validation directories\n",
    "root_dir = \"./dataset\"  # locate dataset directory from this repo in the whole system\n",
    "train_cat_dir = Path(root_dir) / \"training\" / \"cats\"\n",
    "train_dog_dir = Path(root_dir) / \"training\" / \"dogs\"\n",
    "val_cat_dir = Path(root_dir) / \"validation\" / \"cats\"\n",
    "val_dog_dir = Path(root_dir) / \"validation\" / \"dogs\"\n",
    "\n",
    "# Glob training files\n",
    "train_cat_files = list(train_cat_dir.glob(\"**/*.jpg\"))\n",
    "train_dog_files = list(train_dog_dir.glob(\"**/*.jpg\"))\n",
    "print(f\"There are {len(train_cat_files)} cat images, and {len(train_dog_files)} dog images in the training dataset\")\n",
    "df_train = pd.DataFrame({\n",
    "    'path': list(train_cat_files) + list(train_dog_files),\n",
    "    'label': ['cat'] * len(list(train_cat_files)) + ['dog'] * len(list(train_dog_files))\n",
    "})\n",
    "df_train.to_csv('annotations_train.csv', header=False, index=False)  \n",
    "\n",
    "# Glob validation files\n",
    "val_cat_files = list(val_cat_dir.glob(\"**/*.jpg\"))\n",
    "val_dog_files = list(val_dog_dir.glob(\"**/*.jpg\"))\n",
    "print(f\"There are {len(val_cat_files)} cat images, and {len(val_dog_files)} dog images in the validation dataset\")\n",
    "df_val = pd.DataFrame({\n",
    "    'path': list(val_cat_files) + list(val_dog_files),\n",
    "    'label': ['cat'] * len(list(val_cat_files)) + ['dog'] * len(list(val_dog_files))\n",
    "})\n",
    "df_val.to_csv('annotations_val.csv', header=False, index=False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Manage Datasets using [PyTorch](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)\n",
    "1. Inherit the `Dataset` class to build a customized `CatDogDataset` class.\n",
    "2. Instantiate the customized class to a `dataset_train` and `dataset_test` .\n",
    "3. Further create dataloaders to shuffle the data and access the full matrix of the features and the targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import decode_image, ImageReadMode\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(3321)\n",
    "\n",
    "class CatDogDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        self.imgs_info = pd.read_csv(annotations_file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_info)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs_info.iloc[idx, 0]\n",
    "        raw_img = decode_image(img_path, mode=ImageReadMode.RGB)\n",
    "        # resize image to (64, 64)\n",
    "        image = Resize((64, 64))(raw_img)\n",
    "        category = 1. if self.imgs_info.iloc[idx, 1] == 'dog' else 0.\n",
    "        sample = {'image': image, 'category': category}\n",
    "\n",
    "        return sample\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocess the Data\n",
    "A typical binary classification dataset is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a target vector $\\mathbf{y} = [^{(1)}y, ^{(2)}y, ..., ^{(M)}y]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x}$ is a normalized and flattened image array, and $^{(m)}y \\in \\{0, 1\\}$.\n",
    "\n",
    "- A colored image is usually represented by a 3-dimensional array with shape **`(width, height, 3)`** or **`(3, width, height)`**.\n",
    "- When a digital image is loaded, each pixel bears an integer value ranged **0~255** to represent the color intensity.\n",
    "\n",
    "![](https://miro.medium.com/v2/format:webp/1*pFywKuWmz7Xk07OXxPiX2Q.png)\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features/images and targets/labels\n",
    "dataset_train = CatDogDataset(annotations_file='annotations_train.csv')\n",
    "dataset_val = CatDogDataset(annotations_file='annotations_val.csv')\n",
    "\n",
    "# Create data loaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1000, shuffle=True)  # large batch size to get all data in one batch\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1000, shuffle=False)\n",
    "# Extract all data from dataloaders\n",
    "raw_data_train = next(iter(dataloader_train))\n",
    "raw_data_val = next(iter(dataloader_val))\n",
    "\n",
    "# Separate features from targets \n",
    "raw_features_train = raw_data_train['image'].numpy()\n",
    "raw_labels_train = raw_data_train['category'].numpy()\n",
    "raw_features_val = raw_data_val['image'].numpy()\n",
    "raw_labels_val = raw_data_val['category'].numpy()\n",
    "# Uncomment 4 lines below to exam the raw data\n",
    "# print(f\"Raw training features shape: {raw_features_train.shape}, raw trainging labels shape: {raw_labels_train.shape}\")\n",
    "# print(f\"Raw validation features shape: {raw_features_val.shape}, raw validation labels shape: {raw_labels_val.shape}\")\n",
    "# print(f\"A sample of raw features array:\\n {raw_features_train[0]}\")\n",
    "# print(f\"First 10 raw training labels: {raw_labels_train[:10]}\")\n",
    "\n",
    "# Visualize a few sample images\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "for i in range(4):\n",
    "    sample_img = raw_features_train[i]\n",
    "    sample_cls = raw_labels_train[i]\n",
    "    axs[i] = plt.subplot(1, 4, i + 1)\n",
    "    axs[i].set_title(f'{\"dog\" if sample_cls == 1. else \"cat\"}')\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(np.transpose(sample_img, (1, 2, 0)))  # image is already in (H, W, C) format\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Rerun this cell to get different samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=582C83>(5%) Exercise 1: Data Preprocessing</font>\n",
    "1. Rescale pixel color intensity values within range: `[0, 1]` using float numbers.\n",
    "2. Reshape feature array to `(# samples, # pixels)`\n",
    "3. Reshape label array to `(# samples, 1)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "features_train = None\n",
    "labels_train = None\n",
    "features_val = None\n",
    "labels_val = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Processed training features shape: {features_train.shape}, a sample:\\n {features_train[0]}\")\n",
    "print(f\"Processed training labels shape: {labels_train.shape}\")\n",
    "print(f\"Processed validation features shape: {features_val.shape}, a sample:\\n {features_val[0]}\")\n",
    "print(f\"Processed validation labels shape: {labels_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Processed training features shape: (557, 12288)\n",
    "Processed training labels shape: (557, 1)\n",
    "Processed validation features shape: (140, 12288)\n",
    "Processed validation labels shape: (140, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Input Single-Hidden Layer Single-Output Model\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{\\mathbf{y}} &= \\sigma(\\mathbf{X}^{[1]} \\cdot \\mathbf{w}^{[2]T} + b^{[2]}) \\\\\n",
    "                     &= \\sigma(\\sigma(\\mathbf{X}^{[0]} \\cdot \\mathbf{W}^{[1]T} + \\mathbf{b}^{[1]}) \\cdot \\mathbf{w}^{[2]T} + b^{[2]})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}^{[0]}$ is the input features matrix, $\\mathbf{X}^{[1]}$ is the hidden features matrix, and $\\hat{\\mathbf{y}}$ is model predictions matrix. \n",
    "The model is governed by 2 layers of parameters: $\\mathbf{W}^{[1]}$, $\\mathbf{b}^{[1]}$, $\\mathbf{w}^{[2]}$, and $b^{[2]}$. \n",
    "$\\sigma(\\cdot)$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).\n",
    "Please refer to lecture [slides](https://linzhanguca.github.io/_docs/deep_learning-2025/0929/nnN11.pdf) for the shape of each matrix/array.\n",
    "\n",
    "### <font color=#582C83>(15%) Exercise 2: N11 Model Construction </font> \n",
    "1. Let the linear model: $\\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{w}^T + b$ to take feature matrix $\\mathbf{X}$ as the input and output a transformed/intermediate feature matrix $\\mathbf{Z}$.\n",
    "2. Apply sigmoid function on $\\mathbf{Z}$, so that the prediction will be: $\\mathbf{\\hat{y}} = \\sigma(\\mathbf{Z}) = 1 / (1 + e^{-\\mathbf{Z}})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(in_features):\n",
    "    \"\"\" Sigmoid function\n",
    "    Args:\n",
    "        in_features: independent variable, could be an arrary of any shape or a scalar. \n",
    "    Returns:\n",
    "        activated_features: dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    activated_features = None\n",
    "    ### END CODE HERE ###\n",
    "    return activated_features\n",
    "\n",
    "def linear(in_features, weights, biases):\n",
    "    \"\"\" Linear function\n",
    "    Args:\n",
    "        in_features: input feature matrix, 2d array with shape (# samples, # input features)\n",
    "        weights: weight parameter matrix, 2d array with shape (# next layer features , # input features)\n",
    "        biases: bias parameter vector, 2d array with shape (1, # next layer features)\n",
    "    Returns:\n",
    "        linear_output: linear model output feature matrix, 2d array with shape (# samples, # next layer features)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    linear_output = None\n",
    "    ### END CODE HERE ###\n",
    "    return linear_output\n",
    "\n",
    "def forward(in_features, params):\n",
    "    \"\"\" Forward function\n",
    "    Args:\n",
    "        in_features: feature matrix, 2d array with shape (# samples, # pixels)\n",
    "        params: a dictionary of weights and biases\n",
    "    Returns:\n",
    "        predictions: predicted probabilities, a column vector or 2d array with shape (# samples, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    features_1 = None\n",
    "    predictions = None\n",
    "    ### END CODE HERE ###\n",
    "    return predictions, features_1\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "params_dummy = {\n",
    "    'W1': np.random.normal(size=(5, features_train.shape[1])),\n",
    "    'b1': np.random.normal(size=(1, 5)),\n",
    "    'W2': np.random.normal(size=(1, 5)),\n",
    "    'b2': np.random.normal()\n",
    "}\n",
    "preds_dummy, X1_dummy = forward(features_train, params_dummy)\n",
    "print(f\"Hidden layer (X1) shape: {X1_dummy.shape}\")\n",
    "print(f\"Hidden layer (X1) samples:\\n{X1_dummy[:3]}\")\n",
    "print(f\"Shape of dummy predictions: {preds_dummy.shape}\")\n",
    "print(f\"Dummy prediction samples:\\n{preds_dummy[:3]}\")\n",
    "print(f\"Dummy predicted classes samples:\\n{(preds_dummy[:3] >= 0.5).astype(int)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Hidden layer (X1) shape: (557, 5)\n",
    "Hidden layer (X1) samples:\n",
    "[[1.00000000e+00 1.00000000e+00 3.21777360e-19 9.24947905e-02\n",
    "  1.00000000e+00]\n",
    " [1.00000000e+00 1.00000000e+00 8.60858067e-30 5.80196383e-42\n",
    "  1.00000000e+00]\n",
    " [1.00000000e+00 1.00000000e+00 1.71898486e-20 4.23640927e-26\n",
    "  1.00000000e+00]]\n",
    "Shape of dummy predictions: (557, 1)\n",
    "Dummy prediction samples:\n",
    "[[0.47896982]\n",
    " [0.49083586]\n",
    " [0.49083586]]\n",
    "Dummy predicted classes samples:\n",
    "[[0]\n",
    " [0]\n",
    " [0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Model\n",
    "\n",
    "### 3.1. Binary Cross Entropy Loss\n",
    "$$\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y}) = \\frac{1}{M} \\Sigma [-\\mathbf{y} \\log \\hat{\\mathbf{y}} - (1 - \\mathbf{y}) \\log(1 - \\hat{\\mathbf{y}})]$$\n",
    "\n",
    "### 3.2. Accuracy\n",
    "$$Acc = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "### <font color=#582c83>(10%) Exercise 3: Model Assessment </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute the Binary Cross Entropy (BCE) loss.\n",
    "    \n",
    "    Args:\n",
    "        predictions: model predicted values, a 2d array with shape (# samples, 1)\n",
    "        labels: labeled values from data set, a 2d array with shape (# samples, 1)\n",
    "    Returns:\n",
    "        loss_value: averaged BCE error, a scalar\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    loss_value = None\n",
    "    ### END CODE HERE ###\n",
    "    return loss_value\n",
    "    \n",
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: model predicted values, a 2d array with shape (# samples, 1)\n",
    "        labels: labeled values from data set, a 2d array with shape (# samples, 1)\n",
    "    Returns:\n",
    "        acc: accuracy, a scalar\n",
    "    \"\"\"\n",
    "    ### START CODING HERE ### (≈ 2 lines)\n",
    "    predicted_classes = None\n",
    "    acc = None\n",
    "    return acc\n",
    "    ### END CODING HERE ###\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Dummy model BCE loss: {bce_loss(preds_dummy, labels_train)}\")\n",
    "print(f\"Dummy model accuracy: {accuracy(preds_dummy, labels_train) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Dummy model BCE loss: 0.7176122919341839\n",
    "Dummy model accuracy: 48.473967684021545%\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Back-Propagate Gradients of Loss\n",
    "Compute gradients of loss so that we can update model parameters later.\n",
    "$$\n",
    "\\nabla \\mathcal{L} = \n",
    "\\begin{bmatrix} \n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{[1]}} &\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[1]}} &\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{[2]}} &\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The computation of the gradients starts from the last layer, then back-propagate to the first layer. \n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{[2]}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}} \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{Z}^{[2]}} \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{w}^{[2]}} = \\frac{1}{M} (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\cdot \\mathbf{X}^{[1]} \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}} \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{Z}^{[2]}} \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial b^{[2]}} = \\overline{(\\hat{\\mathbf{y}} - \\mathbf{y})} \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}^{[1]}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}} \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{Z}^{[2]}} \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{X}^{[1]}} = (\\hat{\\mathbf{y}} - \\mathbf{y}) \\cdot \\mathbf{w}^{[2]} \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{[1]}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}^{[1]}} \\frac{\\partial \\mathbf{X}^{[1]}}{\\partial \\mathbf{Z}^{[1]}} \\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{W}^{[1]}} = \\frac{1}{M} [(\\hat{\\mathbf{y}} - \\mathbf{y}) \\cdot \\mathbf{w}^{[2]} * \\mathbf{X}^{[1]} * (1 - \\mathbf{X}^{[1]})]^T \\cdot \\mathbf{X}^{[0]} \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[1]}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}^{[1]}} \\frac{\\partial \\mathbf{X}^{[1]}}{\\partial \\mathbf{Z}^{[1]}} \\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{b}^{[1]}} = \\overline{(\\hat{\\mathbf{y}} - \\mathbf{y}) \\cdot \\mathbf{w}^{[2]} * \\mathbf{X}^{[1]} * (1 - \\mathbf{X}^{[1]})}, axis=0, keepdims\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### <font color=#582c83>(20%) Exercise 4: Gradients Computation</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(in_features, features_1, predictions, labels, params):\n",
    "    \"\"\" Gradient function with sigmoid activation\n",
    "    Args:\n",
    "        in_features: feature matrix, a 2d array with shape (# samples, # pixels)\n",
    "        features_1: hidden layer feature matrix, a 2d array with shape (# samples, # hidden features)\n",
    "        predictions: model predicted value, a 2d array with shape (# samples, 1)\n",
    "        labels: labeled value from data set, a 2d array with shape (# samples, 1)\n",
    "        params: a dictionary of weights and biases\n",
    "    Returns:\n",
    "        grads: a dictionary of gradients of weights and biases\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 6 lines)\n",
    "    dL_dW2 = None\n",
    "    dL_db2 = None\n",
    "    dL_dX1 = None\n",
    "    dL_dW1 = None\n",
    "    dL_db1 = None\n",
    "    grads = {\n",
    "        'dW2': dL_dW2,\n",
    "        'db2': dL_db2,\n",
    "        'dW1': dL_dW1,\n",
    "        'db1': dL_db1\n",
    "    }   \n",
    "\n",
    "    return grads\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "grads_dummy = backward(features_train, X1_dummy, preds_dummy, labels_train, params_dummy)\n",
    "for k, v in grads_dummy.items():\n",
    "    print(f\"{k}: {v}\") \n",
    "    print(f\"Shape of {k}: {v.shape}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dW2: [[-0.01219751 -0.01291893  0.02279078 -0.00305472 -0.00633631]]\n",
    "Shape of dW2: (1, 5)\n",
    "db2: -0.011168723623745864\n",
    "Shape of db2: ()\n",
    "dW1: [[-1.83808861e-06 -3.60190685e-07 -4.61408613e-07 ... -5.76820151e-05\n",
    "  -6.07048124e-05 -6.22402946e-05]\n",
    " [-1.77579955e-04 -1.80499871e-04 -1.83843446e-04 ... -1.22578799e-04\n",
    "  -1.09420635e-04 -1.09276882e-04]\n",
    " [ 3.72156964e-04  4.05010601e-04  4.25819595e-04 ...  4.01500770e-04\n",
    "   3.62029743e-04  3.52201928e-04]\n",
    " [ 7.81283167e-06  2.18910660e-05  2.50437615e-05 ... -9.66289643e-06\n",
    "  -1.37238647e-05  2.59703713e-06]\n",
    " [-4.44899727e-05 -4.90495903e-05 -5.15890191e-05 ... -2.62571885e-05\n",
    "  -2.86883308e-05 -2.76296182e-05]]\n",
    "Shape of dW1: (5, 12288)\n",
    "db1: [[-4.70166025e-05 -5.89589280e-04  1.29078047e-03  1.93598262e-04\n",
    "  -1.57950863e-05]]\n",
    "Shape of db1: (1, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update Model Parameters\n",
    "By tweaking the model parameters along the gradient a small step (learning rate, $\\alpha$) iteratively, we expect to bring the model loss down to a reasonable scale.\n",
    "\n",
    "- Initialize parameters\n",
    "- Repeat until converge\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{W}^{[1]} &= \\mathbf{W}^{[1]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{[1]}} \\\\\n",
    "    \\mathbf{b}^{[1]} &= \\mathbf{b}^{[1]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[1]}} \\\\\n",
    "    \\mathbf{w}^{[2]} &= \\mathbf{w}^{[2]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{[1]}} \\\\\n",
    "    b^{[2]} &= b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}\n",
    "\\end{align*}   \n",
    "$$\n",
    "### <font color=582c83>(40%) Exercise 5: Model Optimization</font>\n",
    "1. Define a function to compute gradient of loss\n",
    "2. Perform gradient descent optimization using appropriate iterations and learning rate.\n",
    "    1. Initialize weights and bias\n",
    "    2. Make predictions\n",
    "    3. Log training loss and test loss\n",
    "    4. Update weights and bias\n",
    "    5. Repeat 2 to 5 until converge.\n",
    "    \n",
    "<font color=red> **Bring training (dataset) accuracy over 70%** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 17 lines)\n",
    "\n",
    "# Initialize parameters\n",
    "params = {\n",
    "    'W1': None,\n",
    "    'b1': None,\n",
    "    'W2': None,\n",
    "    'b2': None,\n",
    "}\n",
    "\n",
    "# Set hyperparameters\n",
    "num_iters = None\n",
    "learning_rate = None\n",
    "\n",
    "# Reserve metrics storage\n",
    "losses_train, lossese_val = [], []\n",
    "accuracies_train, accuracies_val = [], []\n",
    "\n",
    "# Optimization iterations\n",
    "for i in range(num_iters):\n",
    "    # Forward pass\n",
    "    preds_train, features_1_train = None\n",
    "    preds_val, features_1_val = None\n",
    "    # Compute loss and accuracy\n",
    "    loss_train = None\n",
    "    loss_val = None\n",
    "    acc_train = None\n",
    "    acc_val = None\n",
    "    losses_train.append(loss_train)\n",
    "    lossese_val.append(loss_val)\n",
    "    accuracies_train.append(acc_train)\n",
    "    accuracies_val.append(acc_val)\n",
    "    # Log progress (every 50 iterations)   \n",
    "    if i % 100 == 0 or i == num_iters - 1:\n",
    "        print(f\"Iter {i}: Train Loss {loss_train}, Train Acc {acc_train}\")\n",
    "        print(f\"Iter {i}: Validation Loss {loss_val}, Validation Acc {acc_val}\")\n",
    "    # Back-propagation\n",
    "    grads = None\n",
    "    # Gradient descent parameter update\n",
    "    params['W1'] = None\n",
    "    params['b1'] = None\n",
    "    params['W2'] = None\n",
    "    params['b2'] = None\n",
    "\n",
    "### END CODE HERE ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training and Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot loss and accuracy curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(num_iters), losses_train, range(num_iters), lossese_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title(\"Loss Decrease\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(num_iters), accuracies_train, range(num_iters), accuracies_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title(\"Accuracy Growth\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Confusion Matrix for Test Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm_train = confusion_matrix(labels_train, (preds_train >= 0.5).astype(int))\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=[\"cat\", \"dog\"])\n",
    "disp_train.plot(cmap='PuRd')\n",
    "plt.title('Confusion Matrix - Training Data')\n",
    "cm_val = confusion_matrix(labels_val, (preds_val >= 0.5).astype(int))\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=[\"cat\", \"dog\"])\n",
    "disp_test.plot(cmap='PuRd')\n",
    "plt.title('Confusion Matrix - Validation Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test and Analyze\n",
    "There are 6 pictures of (3) dogs and (3) cats in this repository. \n",
    "Test these new images using the model you've just trained.\n",
    "Hopefully, all the cats say \"Meow\" and dogs say \"Woof\".\n",
    "\n",
    "### <font color=582c83> (6%) Exercise 6: New Images Test </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"./test_images\"  # locate dataset directory from this repo in the whole system\n",
    "test_cat_dir = Path(test_dir) / \"cats\"\n",
    "test_dog_dir = Path(test_dir) / \"dogs\"\n",
    "\n",
    "\n",
    "# Glob test files\n",
    "test_cat_files = list(test_cat_dir.glob(\"**/*\"))\n",
    "test_dog_files = list(test_dog_dir.glob(\"**/*\"))\n",
    "test_files = test_cat_files + test_dog_files\n",
    "\n",
    "for test_file in test_files:\n",
    "    image_tensor = decode_image(None, mode=ImageReadMode.RGB)  # decode image file to pytorch tensor\n",
    "    image = Resize((None, None))(image_tensor)  # resize image to correct size\n",
    "    image_np = None  # convert image tensor to numpy array\n",
    "    image_flatten = None  # rescale and reshape\n",
    "    dog_likelihood, _ = None  # use model to predict\n",
    "    is_dog = None  # use correct threshold to classify\n",
    "    if is_dog.squeeze():\n",
    "        print(f\"{test_file.name}: Woof!\") \n",
    "    else:\n",
    "        print(f\"{test_file.name}: Meow!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=582c83> (4%) Exercise 7: Model Analyses </font>\n",
    "\n",
    "Was your model performing similar on training data, validation data and test pictures? If not, what could be the cause?\n",
    "> Please write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have finished this assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catdog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
