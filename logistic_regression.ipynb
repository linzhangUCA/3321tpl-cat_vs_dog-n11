{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification - Cats vs. Dogs\n",
    "\n",
    "Welcome to your Third assignment! You will train a simple neural network model to distinguish cat pictures from dog pictures. In this assignment, we'll practice such process under a binary classification context. \n",
    "- A binary classification task is to categorize an object into one of the two classes. \n",
    "- Use all pixels in an image as the features. \n",
    "\n",
    "\n",
    "## Exercises:\n",
    "1. $\\color{violet}{\\textbf{(20\\%) Data Preprocessing}}$\n",
    "2. $\\color{violet}{\\textbf{(5\\%) Logistic Regression Model}}$\n",
    "3. $\\color{violet}{\\textbf{(5\\%) Cross Entropy Loss}}$\n",
    "4. $\\color{violet}{\\textbf{(40\\%) Gradient Descent Optimization}}$\n",
    "5. $\\color{violet}{\\textbf{(15\\%) Evaluation on Test Dataset}}$\n",
    "6. $\\color{violet}{\\textbf{(15\\%) Test Model with New Image}}$\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. $\\color{red}{\\textbf{Modify code out of the designated area at your own risk.}}$\n",
    "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
    "- **Need to install [PyTorch](https://pytorch.org/), [opencv-python](https://pypi.org/project/opencv-python/) and [pandas](https://pandas.pydata.org/).** Use the following command in your terminal.\n",
    "    ```console\n",
    "    pip install torch opencv-python pandas\n",
    "    ```\n",
    "## After this assignment you will:\n",
    "- Be able to train a neural network model to do binary classification task.\n",
    "    - Use sigmoid activation function.\n",
    "    - Learn a new loss function: cross entropy.\n",
    "    - Practice gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data and Create a Dataset\n",
    "### 1.1 Create an Annotation File\n",
    "Data is not always stored in NumPy arrays. Most cases, you will have to organize and annotate the raw data stored in your hard drive. For image data, you want it to be organized as the following way.\n",
    "``` console\n",
    "root/dog/xxx.jpg\n",
    "root/dog/xxy.jpg\n",
    "root/dog/xxz.jpg\n",
    "\n",
    "root/cat/123.jpg\n",
    "root/cat/456.jpg\n",
    "root/cat/789.jpg\n",
    "```\n",
    "To grab image information and store them in an comma-seperated values (CSV) file:\n",
    "1. Visit the data directory, grab all images' paths and corresponding categories.\n",
    "2. Save the paths and categories of images in an `.csv` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Locate train and test directories\n",
    "root_dir = os.path.join(sys.path[0], \"dataset\")  # locate dataset directory from this repo in the whole system\n",
    "train_dir = os.path.join(root_dir, \"train\")\n",
    "test_dir = os.path.join(root_dir, \"test\")\n",
    "categories = ['cats', 'dogs']\n",
    "\n",
    "# Glob training files\n",
    "train_cat_files = glob(os.path.join(train_dir, categories[0], \"*.jpg\"))\n",
    "train_dog_files = glob(os.path.join(train_dir, categories[1], \"*.jpg\"))\n",
    "print(f\"There are {len(train_cat_files)} cat images, and {len(train_dog_files)} dog images in the training dataset\")\n",
    "train_image_files = train_cat_files + train_dog_files\n",
    "train_labels = ['cat'] * len(train_cat_files) + ['dog'] * len(train_dog_files)\n",
    "train_data_dict = {'path': train_image_files, 'label': train_labels}\n",
    "df_train = pd.DataFrame(train_data_dict)\n",
    "# print(df_train)\n",
    "df_train.to_csv('annotation_train.csv', header=False, index=False)\n",
    "\n",
    "# Glob test files\n",
    "test_cat_files = glob(os.path.join(test_dir, categories[0], \"*.jpg\"))\n",
    "test_dog_files = glob(os.path.join(test_dir, categories[1], \"*.jpg\"))\n",
    "print(f\"There are {len(test_cat_files)} cat images, and {len(test_dog_files)} dog images in the test dataset\")\n",
    "test_image_files = test_cat_files + test_dog_files\n",
    "test_labels = ['cat'] * len(test_cat_files) + ['dog'] * len(test_dog_files)\n",
    "test_data_dict = {'path': test_image_files, 'label': test_labels}\n",
    "df_test = pd.DataFrame(test_data_dict)\n",
    "# print(df_test)\n",
    "df_test.to_csv('annotation_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dataset\n",
    "[PyTorch](https://pytorch.org/) as a popuplar deep learning programming framework has a bunch of handy tools that allow you to easily access the data.\n",
    "1. Inherit the `Dataset` class to build a customized `CatsDogsDataset` class.\n",
    "2. Instantiate the customized class to a `dataset_train` and `dataset_test` .\n",
    "3. Further create dataloaders to shuffle the data and access the full matrix of the features and the targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create customized dataset\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        self.imgs_info = pd.read_csv(annotations_file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = self.imgs_info.iloc[idx, 0]\n",
    "        image_raw = cv.imread(img_path)\n",
    "        image_rgb = cv.cvtColor(image_raw, cv.COLOR_BGR2RGB)\n",
    "        image = cv.resize(image_rgb, (100, 100))\n",
    "        category = 1. if self.imgs_info.iloc[idx, 1] == 'dog' else 0.\n",
    "        sample = {'image': image, 'category': category}\n",
    "        return sample\n",
    "\n",
    "# Loop training dataset\n",
    "dataset_train = CatsDogsDataset(annotations_file='annotation_train.csv')\n",
    "for i, sample in enumerate(dataset_train):\n",
    "    image = sample['image']\n",
    "    category = sample['category']\n",
    "    if not i%100:\n",
    "        print(i, image.shape, category)\n",
    "print(i, image.shape, category)\n",
    "    \n",
    "dataset_test = CatsDogsDataset(annotations_file='annotation_test.csv')\n",
    "\n",
    "# Create shuffled data loader \n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1000, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1000, shuffle=True)\n",
    "samples = next(iter(dataloader_train))\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "for i in range(4):\n",
    "    image = samples['image'][i]\n",
    "    category = samples['category'][i]\n",
    "    axs[i] = plt.subplot(1, 4, i + 1)\n",
    "    axs[i].set_title(f'Sample #{i+1}: {category}')\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(image)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Data\n",
    "A typical binary classification dataset is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a target vector $\\mathbf{y} = [^{(1)}y, ^{(2)}y, ..., ^{(M)}y]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x}$ is a normalized and flattened image array, and $^{(m)}y \\in {0, 1}$.\n",
    "\n",
    "- A colored image is usually represented by a **3-dimensional array with shape $(width, height, 3)$**. Where, $width$ indicates number of pixels on horizontal edge, $height$ indicates number of pixels on vertical edge.\n",
    "- Usually, we use an **integer ranged 0~255** to describe a pixel, which is the intensity of that color channel.\n",
    "- At current stage, we would like to convert an image array into a row vector, or a **2d array with shape $(1, width*height*3)$**.\n",
    "\n",
    "![](https://miro.medium.com/v2/format:webp/1*pFywKuWmz7Xk07OXxPiX2Q.png)\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(20\\%) Exercise 1: Data Preprocessing}}$\n",
    "1. Separate raw feature array and target array.\n",
    "2. Reshape feature array and target array.\n",
    "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features/images and targets/labels\n",
    "data_train = next(iter(dataloader_train))\n",
    "data_test = next(iter(dataloader_test))\n",
    "\n",
    "\n",
    "# Separate features from targets \n",
    "raw_feature_train = data_train['image'].numpy()\n",
    "raw_feature_test = data_test['image'].numpy()\n",
    "raw_target_train = data_train['category'].numpy()\n",
    "raw_target_test = data_test['category'].numpy()\n",
    "\n",
    "### START CODE HERE ### (≈ 6 lines of code)\n",
    "# Reshape feature matrix to (M, width*height*3), target vector to (M, 1)\n",
    "reshaped_feature_train = None\n",
    "reshaped_feature_test = None\n",
    "reshaped_target_train = None\n",
    "reshaped_target_test = None\n",
    "\n",
    "# Rescale features within range: 0~1\n",
    "rescaled_feature_train = None\n",
    "rescaled_feature_test = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Finalize data to be used later\n",
    "feature_train = rescaled_feature_train\n",
    "feature_test = rescaled_feature_test\n",
    "target_train = reshaped_target_train\n",
    "target_test = reshaped_target_test\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "print(feature_train.shape, feature_test.shape, target_train.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "(557, 30000) (140, 30000) (557, 1) (140, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward Pass\n",
    "Since a target $^{(m)}y$ is equal to either 0 or 1, we would like to use a sigmoid functionto further transform the output of a linear model. So that the output of such a sythesized model has an output within range 0~1. The new model is also called **Logistic Regression Model**.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 2: Logistic Regression Model}}$\n",
    "1. Let the linear model: $\\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{w}^T + b$ to take feature matrix $\\mathbf{X}$ as the input and output a transformed/intermediate feature matrix $\\mathbf{Z}$.\n",
    "2. Apply sigmoid function on $\\mathbf{Z}$, so that the prediction will be: $\\mathbf{\\hat{y}} = \\sigma(\\mathbf{Z}) = 1 / (1 + e^{-\\mathbf{Z}})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function\n",
    "    Args:\n",
    "        x: independent variable, could be an arrary of any shape or a scalar.\n",
    "    Returns:\n",
    "        y: dependent variable, could be an arrary of any shape or a scalar.\n",
    "    \"\"\"\n",
    "    y = None\n",
    "    return y\n",
    "\n",
    "def forward(feature, weight, bias):\n",
    "    \"\"\" Logistic model function\n",
    "    Args:\n",
    "        feature: feature matrix, 2d-array with shape (# samples, # pixels)\n",
    "        weight: a row vector with shape (1, # pixels)\n",
    "        biase: scalar\n",
    "    Returns:\n",
    "        prediction: model predicted value, a 2-D numpy array with shape (# samples, 1)\n",
    "    \"\"\"\n",
    "    prediction = None\n",
    "    return prediction\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "print(forward(np.random.normal(size=(4, feature_test.shape[1])), np.random.normal(0, 0.01, (1, feature_test.shape[1])), np.random.normal(0, 0.01)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "[[0.78035352]\n",
    " [0.68058492]\n",
    " [0.60273538]\n",
    " [0.41968119]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Binary Cross Entropy\n",
    "It is OK to use a Mean square error function to compute the loss (difference between target and prediction). It is better to use a binary cross entropy function to compute the loss for a binary classification problem. \n",
    "\n",
    "#### $\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y}) = \\frac{1}{M} \\Sigma [-\\mathbf{\\hat{y}} \\log \\mathbf{y} - (1 - \\mathbf{\\hat{y}}) \\log(1 - \\mathbf{y})]$\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 3: Cross Entropy Loss}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy function\n",
    "        Args:\n",
    "            prediction: model predicted value, a 2-D numpy array with shape (# samples, 1)\n",
    "            target: labeled value from data set, a 2-D numpy array with shape (# samples, 1)\n",
    "        Returns:\n",
    "            loss_value: averaged CE error, a scalar\n",
    "    \"\"\"\n",
    "    loss_value = None\n",
    "    return loss_value\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "print(binary_cross_entropy_loss(forward(np.random.normal(size=(4, feature_test.shape[1])), np.random.normal(0, 0.01, (1, feature_test.shape[1])), np.random.normal(0, 0.01)), np.random.randint(0, 2, (4, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "0.5250353081044535\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backpropagation\n",
    "As there was one extra step (sigmoid activation) to compute model predictions, an extra step is needed to compute gradient of the loss.\n",
    "According to the chain rule:\n",
    "\n",
    "#### $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\frac{1}{M} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{Z}} \\cdot \\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{w}}$\n",
    "#### $\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{M} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{Z}} \\cdot \\frac{\\partial \\mathbf{Z}}{\\partial b}$\n",
    "Where,\n",
    "#### $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} = - \\frac{\\mathbf{y}}{\\mathbf{\\hat{y}}} + \\frac{1 -\\mathbf{y}}{1 -\\mathbf{\\hat{y}}}$\n",
    "#### $\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{Z}} = \\sigma (\\mathbf{Z}) (1 - \\sigma(\\mathbf{z})) = \\mathbf{\\hat{y}} (1 - \\mathbf{\\hat{y}})$\n",
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{w}} = \\mathbf{X}$\n",
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial b} = 1$\n",
    "Thus,\n",
    "#### $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\frac{1}{M} [(- \\frac{\\mathbf{y}}{\\mathbf{\\hat{y}}} + \\frac{1 -\\mathbf{y}}{1 -\\mathbf{\\hat{y}}}) \\mathbf{\\hat{y}} (1 - \\mathbf{\\hat{y}})]^T \\cdot \\mathbf{X}$\n",
    "#### $\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{M} (- \\frac{\\mathbf{y}}{\\mathbf{\\hat{y}}} + \\frac{1 -\\mathbf{y}}{1 -\\mathbf{\\hat{y}}}) \\mathbf{\\hat{y}} (1 - \\mathbf{\\hat{y}})$\n",
    "\n",
    "### $\\color{violet}{\\textbf{(40\\%) Exercise 4: Gradient Descent Optimization}}$\n",
    "- Define a function to compute gradient of loss\n",
    "- Perform gradient descent optimization using appropriate iterations and learning rate.\n",
    "    1. Initialize weights and bias\n",
    "    2. Make predictions\n",
    "    3. Update weights and bias\n",
    "    4. Log training loss and test loss\n",
    "    5. Repeat 2 to 5 until converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### START CODE HERE ### (≈ 12 lines of code)\n",
    "def grad(prediction, target, feature):\n",
    "    \"\"\" Gradient function with sigmoid activation\n",
    "    Args:\n",
    "        prediction: model predicted value, a 2d array with shape (# samples, 1)\n",
    "        target: labeled value from data set, a 2d array with shape (# samples, 1)\n",
    "        feature: feature matrix, a 2d array with shape (# samples, # pixels)\n",
    "    Returns:\n",
    "        dL_dw: row vector of BCE loss partial derivatives w.r.t. weights, 2d array with shape (1, # features)\n",
    "        dL_db: scalar of BCE loss partial derivatives w.r.t. bias\n",
    "    \"\"\"\n",
    "    dL_dw = None\n",
    "    dL_db = None\n",
    "    \n",
    "    return dL_dw, dL_db\n",
    "\n",
    "# Initialize parameters\n",
    "w = None\n",
    "b = None\n",
    "num_iters = None\n",
    "learning_rate = None\n",
    "losses_train, losses_test = [], []\n",
    "# Optimization loop\n",
    "for i in range(num_iters):\n",
    "    pred_train = None\n",
    "    dw, db = None\n",
    "    loss_train = None\n",
    "    loss_test = binary_cross_entropy_loss(forward(feature_test, w, b), target_test)\n",
    "    w = None\n",
    "    b = None\n",
    "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "### END CODE HERE ### \n",
    "\n",
    "\n",
    "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "You can treat the model output, $\\mathbf{\\hat{y}}$ as the probabilities of the images being classified as dog pictures. Set the classification threshold to be 0.5, we can categorize an image as a cat image ($^{(m)}y <= 0.5$) or a dog image ($^{(m)}y > 0.5$)\n",
    "### $\\color{violet}{\\textbf{(15\\%) Exercise 5: Evaluation on Test Data}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 6 lines of code)\n",
    "pred_test = None  # make predictions on test features\n",
    "category_test = None  # Use threshold of 0.5 to assign class 0 or 1 to each individual in test dataset. You may need two lines to get the classification done\n",
    "\n",
    "prediction_correctness = None  # Find out which predictions are correct\n",
    "num_correct = None  # Calculate how many correct predictions are made\n",
    "accuracy = None  # Calculate accuracy rate: correct # / total #\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test\n",
    "Download new images to this folder and try to use your model to classify them.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(15\\%) Exercise 6: Test Model with New Image}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 12 lines of code)\n",
    "image_raw = None  # read the raw image from a file\n",
    "image_rgb = None  # Convert BGR to RGB\n",
    "image_resize = None  # Resize image to shape (200, 200, 3)\n",
    "image_flatten = None  # Flatten image array to a row vector with shape (1, 200*200*3)\n",
    "image_rescale = None  # rescale pixel value from 0~255 to 0.~1.\n",
    "dog_likelihood = None  # predict new image with your model\n",
    "### END CODE HERE ### \n",
    "\n",
    "is_dog = dog_likelihood > 0.5\n",
    "if is_dog.squeeze():\n",
    "    print(\"It's dog!\") \n",
    "else:\n",
    "    print(\"It's cat!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have finished this assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
