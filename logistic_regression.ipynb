{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU4TDl3qnNoU"
      },
      "source": [
        "# Logistic Regression Practice\n",
        "**Due: Mondy, 09/19/2022, 2:15 PM**\n",
        "\n",
        "Welcome to your second assignment! You will build a logistic regression \n",
        "classifier to recognize cats in pictures. \n",
        "\n",
        "\n",
        "Contents:\n",
        "1. Exercise 1.1 (5%)\n",
        "2. Exercise 1.2 (10%)\n",
        "3. Exercise 2.1 (10%)\n",
        "4. Exercise 2.2 (30%)\n",
        "5. Exercise 2.3.1 (20%)\n",
        "6. Exercise 2.3.2 (10%)\n",
        "7. Exercise 3 (15%)\n",
        "\n",
        "**Instructions:**\n",
        "- The code between the ### START CODE HERE ### and ### END CODE HERE ### comments will be graded. \n",
        "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
        "\n",
        "**After this assignment you will:**\n",
        "\n",
        "- Be able to build the general architecture of a logistic regression model, including:\n",
        "    - Initializing parameters\n",
        "    - Calculating the cost function and its gradient\n",
        "    - optimizing parameters (gradient descent) \n",
        "- Get more used to vectorization using NumPY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9O8WWMZnNoa"
      },
      "source": [
        "## 1 - Overview of the Dataset ##\n",
        "\n",
        "You are given a dataset containing:\n",
        "- a training set of `m_train` images labeled as cat (y=1) or non-cat (y=0)\n",
        "- a test set of `m_test` images labeled as cat or non-cat\n",
        "\n",
        "Each image is of shape (width, height, 3) where 3 is for the 3 color channels (RGB). Load the data by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoobNZRqnNoZ"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "from utils import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Loading the data (cat/non-cat)\n",
        "train_set_X, train_set_y, test_set_X, test_set_y, classes = load_dataset()\n",
        "print (f\"train_set_X shape: {train_set_X.shape}\")\n",
        "print (f\"train_set_y shape: {train_set_y.shape}\")\n",
        "print (f\"test_set_X shape: {test_set_X.shape}\")\n",
        "print (f\"test_set_y shape: {test_set_y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY3qV-4FnNob"
      },
      "source": [
        "`train_set_X` and `test_set_X` contains images represented in numpy arrays. You can visualize an example by running the following code. Feel free to change the `index` value and re-run to see other images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DizI0WW0nNob"
      },
      "outputs": [],
      "source": [
        "# Example of a picture\n",
        "index = 200  # feel free to change this number\n",
        "plt.imshow(train_set_X[index])\n",
        "print(f\"y = {train_set_y[0, index]}, it's a {classes[train_set_y[0, index]].decode('utf-8')} picture\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNBIQFnnNoc"
      },
      "source": [
        "**(5%) Exercise 1.1: Data Dimensions**\n",
        "\n",
        "It is very common to have mismatched array dimensions in deep learning software development. So, first thing is always to make sure that you know the shapes of your data. \n",
        "\n",
        "Find the values for:\n",
        "- `m_train` (number of training examples)\n",
        "- `m_test` (number of test examples)\n",
        "- `image_size` (width, height, color_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39lJVpUOnNoc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (≈ 3 lines of code)\n",
        "m_train = None\n",
        "m_test = None\n",
        "image_size = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "print (f\"Number of training examples: m_train = {m_train}\")\n",
        "print (f\"Number of testing examples: m_test = {m_test}\")\n",
        "print (f\"Size of each picture: {image_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRefoZ6knNod"
      },
      "source": [
        "**Expected Output:** \n",
        "> \n",
        "```console\n",
        "Number of training examples: m_train = 209\n",
        "Number of testing examples: m_test = 50\n",
        "Size of each picture: (64, 64, 3)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwpC-aCJnNod"
      },
      "source": [
        "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
        "Before we perform the forward pass of logistic regression, reshape every image into a `(1, width*height*color_channels)` arrary or a row vector. \n",
        "So that, our training (and test) dataset is a numpy-array/matrix with `m_train` (`m_test`) rows, where each row represents a flattened image. \n",
        "It is convenient to represent the value of a pixel using a float number within range of [0, 1] instead of using an integer in between [0, 255].\n",
        "Divided each integer pixel value by 255 is usually applied to standardize a image dataset. \n",
        "\n",
        "**(10%) Exercise 1.2: Data Preprocessing**\n",
        "\n",
        "Reshape and standardize the datasets:\n",
        "- flatten the dataset into a numpy array with shape `(m, width*height*color_channels)`\n",
        "- standardize the dataset to convert pixels to float numbers and limit each pixel value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO9lzQPhnNoe"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (≈ 4 lines of code)\n",
        "# flatten images\n",
        "train_set_X_flatten = None\n",
        "test_set_X_flatten = None\n",
        "# standardize\n",
        "train_set_X = None\n",
        "test_set_X = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_X_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_X_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "# print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cGkT_DnNoe"
      },
      "source": [
        "**Expected Output**: \n",
        ">\n",
        "```console\n",
        "train_set_x_flatten shape: (209, 12288)\n",
        "train_set_y shape: (209, 1)\n",
        "test_set_x_flatten shape: (50, 12288)\n",
        "test_set_y shape: (50, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVqdjG9pnNof"
      },
      "source": [
        "## 2 - General Architecture of Logistic Regression algorithm ##\n",
        "\n",
        "It's time to design a simple algorithm to distinguish cat images from non-cat images. We will build a Logistic Regression model to do this job.\n",
        "\n",
        "Refer to the 10th slide from [class materials](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0831/logistic_regression_p1.pdf) for the Mathematical expressions of the algorithm.\n",
        "\n",
        "\n",
        "We need a few steps to train our logistic regression model:\n",
        "1. Initialize the model's parameters (weights and bias)\n",
        "2. Calculate cost (forward propagation)\n",
        "3. Calculate gradient (backward propagation)\n",
        "4. Update parameters\n",
        "5. Repeat step 2 to 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvgeNsUryg2"
      },
      "source": [
        "### 2.1 - Initialize parameters\n",
        "Parameters in a logistic regression model are a weight vector and a bias scalar. \n",
        "the dimension of the weight vector is determined by the numbers of features in data.\n",
        "It is OK to initialize all the parameters at `0.` \n",
        "\n",
        "**(10%) Exercise 2.1:** Zero Initialization\n",
        "\n",
        "Create a function to:\n",
        "- initialize weight vector to be a zero row vector\n",
        "- initialize bias to be 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLArRGannNoh"
      },
      "outputs": [],
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (1, dim) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "        dim -- dimension of the weight vector\n",
        "    \n",
        "    Returns:\n",
        "        w -- initialized vector of shape (1, m)\n",
        "        b -- initialized scalar: 0\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    w = None\n",
        "    b = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (1, dim))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b\n",
        "\n",
        "# test\n",
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (f\"w = {w}\")\n",
        "print (f\"b = {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oidVk27nNoh"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        ">\n",
        "```console\n",
        "w = [[0. 0.]]\n",
        "b = 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_GjvZm4ryg3"
      },
      "source": [
        "### 2.2 - Forward and Backward Propagation\n",
        "\n",
        "Compute cost of $m$ examples in the forward propagation. Compute gradient in the backward propagation\n",
        "Refer to the 11th slide from [logistic regression: part1](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0831/logistic_regression_p1.pdf) for the forward propagation.\n",
        "Refer to the 9th slide from [logistic regression: part2](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0907/lr_p2.pdf) for the backward propagation.\n",
        "\n",
        "**Note: cross entropy loss function is recommended to calculate the cost:**\n",
        "$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(\\hat{y}^{(i)})+(1-y^{(i)})\\log(1-\\hat{y}^{(i)}))$\n",
        "\n",
        "**(30%) Exercise 2.2:** Implement forward and backward propagation\n",
        "- finish a helper function: `sigmoid()`\n",
        "- finish `propagate()` function to compute cost and gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaKanLZGnNoi"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "        z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "        s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s\n",
        "    \n",
        "def propagate(w, b, X, y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "        w -- weights, a row vector\n",
        "        b -- bias, a scalar\n",
        "        X -- data, a matrix\n",
        "        y -- label/ground truth, a column vector (containing 0 if non-cat, 1 if cat)\n",
        "\n",
        "    Return:\n",
        "        grads -- a dictionary stores gradients\n",
        "            dw -- gradient of the cost with respect to w\n",
        "            db -- gradient of the cost with respect to b\n",
        "        cost -- a scalar of negative log-likelihood cost for logistic regression\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    # forward propagation (calculate cost)\n",
        "    yhat = None\n",
        "    cost = None    \n",
        "    # backward propagation (calculate gradient)\n",
        "    dw = None\n",
        "    db = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {'dw': dw,\n",
        "             'db': db}\n",
        "    \n",
        "    return grads, cost\n",
        "\n",
        "# test\n",
        "w, b, X, y = np.array([[1.,2.]]), 2., np.array([[1., 3.], [2., 4.], [-1., -3.2]]), np.array([[1], [0], [1]])\n",
        "grads, cost = propagate(w, b, X, y)\n",
        "print (f\"dw = {grads['dw']}\")\n",
        "print (f\"db = {grads['db']}\")\n",
        "print (f\"cost = {cost}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeQbbWTKnNoi"
      },
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "dw = [[0.99845601 2.39507239]]\n",
        "db = 0.001455578136784208\n",
        "cost = 5.801545319394553\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2iPxv4XnNoi"
      },
      "source": [
        "### 2.3 - Optimization\n",
        "Update the parameters using gradient descent algorithm.\n",
        "The goal is to learn $\\mathbf{w}$ and $b$ by minimizing the cost function $J$.\n",
        "You can refer to the last slide from [logistic regression: part2](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0907/lr_p2.pdf).\n",
        "\n",
        "**(20%) Exercise 2.3.1:** Gradient descent optimization.\n",
        "Finish the following function to compute cost, gradient and one-step update of the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL54R5a2nNoi"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(w, b, X, y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "        w -- weights, a row vector\n",
        "        b -- bias, a scalar\n",
        "        X -- data, a matrix\n",
        "        y -- label/ground truth, a column vector\n",
        "        num_iterations -- number of iterations of the optimization loop\n",
        "        learning_rate -- learning rate of the gradient descent update rule\n",
        "        print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "        params -- a dictionary stores parameters\n",
        "            w -- gradient of the cost with respect to w\n",
        "            b -- gradient of the cost with respect to b\n",
        "        grads -- a dictionary stores gradients\n",
        "        cost -- a scalar of negative log-likelihood cost for logistic regression                \n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        ### START CODE HERE ### (≈ 1-4 lines of code)\n",
        "        # cost and gradient calculation \n",
        "        grads, cost = None, None\n",
        "        dw = None\n",
        "        db = None        \n",
        "        # update params \n",
        "        w = None\n",
        "        b = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # record costs\n",
        "        if not (i+1) % 100:\n",
        "            costs.append(cost)        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and not (i+1) % 100:\n",
        "            print (f\"Cost after iteration {i+1}: {cost}\")\n",
        "    \n",
        "    params = {'w': w,\n",
        "              'b': b}\n",
        "    \n",
        "    grads = {'dw': dw,\n",
        "             'db': db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "# test\n",
        "params, grads, costs = optimize(w, b, X, y, num_iterations= 100, learning_rate = 0.009, print_cost = True)\n",
        "print (f\"w = {params['w']}\")\n",
        "print (f\"b = {params['b']}\")\n",
        "print (f\"dw = {grads['dw']}\")\n",
        "print (f\"db = {grads['db']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLOl4-iGnNoj"
      },
      "source": [
        "**Expected Output**: \n",
        ">\n",
        "```console\n",
        "Cost after iteration 100: 1.0784313398164709\n",
        "w = [[0.19033591 0.12259159]]\n",
        "b = 1.9253598300845747\n",
        "dw = [[0.67752042 1.41625495]]\n",
        "db = 0.21919450454067657\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Z2mv0-nNoj"
      },
      "source": [
        "**(10%) Exercise 2.3.2:** Prediction\n",
        "\n",
        "It is time to use the learned $\\mathbf{w}$ and $b$ to predict the labels for a dataset $\\mathbf{X}$. Finish the `predict()` function. Set a threshold to categorize a $\\hat{y}$ into either 1 (cat) or 0 (non-cat). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4ZfCympnNoj"
      },
      "outputs": [],
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "        w -- weights, a row vector\n",
        "        b -- bias, a scalar\n",
        "        X -- data, a matrix\n",
        "        \n",
        "    Returns:\n",
        "        pred -- a column vector contains all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[0]\n",
        "    pred = np.zeros((m,1))\n",
        "    w = w.reshape(1, X.shape[1])\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 2 line of code)\n",
        "    yhat = None\n",
        "    pred = None\n",
        "    ### END CODE HERE ###\n",
        "    assert(pred.shape == (m, 1))\n",
        "    \n",
        "    return pred\n",
        "\n",
        "# test\n",
        "w = np.array([[0.1124579, 0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1., 1.2], [-1.1, 2.], [-3.2, 0.1]])\n",
        "print (f\"predictions: \\n{predict(w, b, X)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZJYjGHDnNok"
      },
      "source": [
        "**Expected Output**: \n",
        ">\n",
        "```console\n",
        "predictions: \n",
        "[[1.]\n",
        " [1.]\n",
        " [0.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrb94iw4nNok"
      },
      "source": [
        "## 3 - Show Me the Cats ##\n",
        "\n",
        "We will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous sections), in the right order.\n",
        "\n",
        "**(15%) Exercise 3:** Train a logistic regression classifier\n",
        "\n",
        "Finish the `train()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftYsMKfCnNok"
      },
      "outputs": [],
      "source": [
        "def train(X_train, y_train, X_test, y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "     \"\"\"\n",
        "     Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "     Arguments:\n",
        "          X_train -- training dataset, a matrix\n",
        "          y_train -- training labels, a column vector\n",
        "          X_test -- test dataset, a matrix\n",
        "          Y_test -- test labels, a column vector\n",
        "          num_iterations -- number of iterations to optimize the parameters\n",
        "          learning_rate -- learning rate used in the update rule of optimize()\n",
        "          print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "     Returns:\n",
        "          model -- dictionary containing information about the model.\n",
        "     \"\"\"\n",
        "    \n",
        "     ### START CODE HERE ###\n",
        "\n",
        "     # initialize parameters with zeros (≈ 1 line of code)\n",
        "\n",
        "     # Gradient descent (≈ 1 line of code)\n",
        "\n",
        "     # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "\n",
        "     # Predict test/train set examples (≈ 2 lines of code)\n",
        "\n",
        "     ### END CODE HERE ###\n",
        "\n",
        "     # Print train/test Errors\n",
        "     print(f\"train accuracy: {100 - np.mean(np.abs(pred_train - y_train)) * 100} %\")\n",
        "     print(f\"test accuracy: {100 - np.mean(np.abs(pred_test - y_test)) * 100} %\")\n",
        "\n",
        "\n",
        "     model = {\"costs\": costs,\n",
        "          \"Y_prediction_test\": pred_test, \n",
        "          \"Y_prediction_train\" : pred_train, \n",
        "          \"w\" : w, \n",
        "          \"b\" : b,\n",
        "          \"learning_rate\" : learning_rate,\n",
        "          \"num_iterations\": num_iterations}\n",
        "\n",
        "     return model\n",
        "\n",
        "# test\n",
        "model = train(train_set_X, train_set_y, test_set_X, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWrMXVOqnNol"
      },
      "source": [
        "**Expected Output**: \n",
        ">\n",
        "```console\n",
        "Cost after iteration 0: 0.693147\n",
        "Cost after iteration 100: 0.584508\n",
        "Cost after iteration 200: 0.466949\n",
        "Cost after iteration 300: 0.376007\n",
        "Cost after iteration 400: 0.331463\n",
        "Cost after iteration 500: 0.303273\n",
        "Cost after iteration 600: 0.279880\n",
        "Cost after iteration 700: 0.260042\n",
        "Cost after iteration 800: 0.242941\n",
        "Cost after iteration 900: 0.228004\n",
        "Cost after iteration 1000: 0.214820\n",
        "Cost after iteration 1100: 0.203078\n",
        "Cost after iteration 1200: 0.192544\n",
        "Cost after iteration 1300: 0.183033\n",
        "Cost after iteration 1400: 0.174399\n",
        "Cost after iteration 1500: 0.166521\n",
        "Cost after iteration 1600: 0.159305\n",
        "Cost after iteration 1700: 0.152667\n",
        "Cost after iteration 1800: 0.146542\n",
        "Cost after iteration 1900: 0.140872\n",
        "train accuracy: 99.04306220095694 %\n",
        "test accuracy: 70.0 %\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKlgAlTnNol"
      },
      "source": [
        "Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data.Test accuracy is 70%. The model is clearly overfitting the training data. It is actually not too bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITqohWsanNol"
      },
      "outputs": [],
      "source": [
        "# Example of a picture that was wrongly classified.\n",
        "index = 5\n",
        "plt.imshow(test_set_X[index].reshape(image_size))\n",
        "# classes\n",
        "print (\"ground truth = \" + str(test_set_y[index, 0]) + \", you predicted that it is a \\\"\" + classes[int(model[\"Y_prediction_test\"][index,0])].decode(\"utf-8\") +  \"\\\" picture.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o-KEiWXnNom"
      },
      "source": [
        "We can also plot the learning curve which can be expressed with the costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMiDtDxlnNom"
      },
      "outputs": [],
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(model['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(model[\"learning_rate\"]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBE1u2R3nNom"
      },
      "source": [
        "> You can also monitor costs for test dataset. Try to plot both curves in same figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG3oP91wnNon"
      },
      "source": [
        "# Congratulations on finishing this assignment. \n",
        "You can use your own image to test the model. \n",
        "1. Upload an image file into the `images/` folder.\n",
        "2. modify the `file` variable in the following code block according to your needs.\n",
        "3. Run following code block.\n",
        "> A bunch of images are pre-uploaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz65jVINryg7"
      },
      "outputs": [],
      "source": [
        "file = \"cat2.jpg\"   # change this to the name of your image file \n",
        "# preprocess the image to fit your algorithm.\n",
        "fname = \"images/\" + file\n",
        "im = plt.imread(fname)\n",
        "# im = plt.imread(fname)\n",
        "imresize = np.resize(im, image_size)\n",
        "imfloat = imresize/255.\n",
        "imflatten = imfloat.reshape(1,-1)\n",
        "my_predicted_image = predict(model[\"w\"], model[\"b\"], imflatten)\n",
        "plt.imshow(im)\n",
        "print(f\"Your model predicts a {classes[int(np.squeeze(my_predicted_image))].decode('utf-8')} picture\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit ('3.10.6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "d270a79710bdef277cbe1980c659a200a6519333aa892b0e4ae637e197c06188"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}