{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression / Binary Classification - Cats vs. Dogs\n",
    "\n",
    "Welcome to your new assignment! You will train a simple neural network model to distinguish cat pictures from dog pictures. In this assignment, we'll practice such process under a binary classification context. \n",
    "- A binary classification task is to categorize an object into one of the two classes. \n",
    "- Use all pixels in an image as the features. \n",
    "\n",
    "\n",
    "## Exercises:\n",
    "1. $\\color{violet}{\\textbf{(20\\%) Data Preprocessing}}$\n",
    "2. $\\color{violet}{\\textbf{(5\\%) Logistic Regression Model}}$\n",
    "3. $\\color{violet}{\\textbf{(5\\%) Cross Entropy Loss}}$\n",
    "4. $\\color{violet}{\\textbf{(40\\%) Gradient Descent Optimization}}$\n",
    "5. $\\color{violet}{\\textbf{(20\\%) Evaluation on Test Dataset}}$\n",
    "6. $\\color{violet}{\\textbf{(10\\%) Test Model with New Image}}$\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. $\\color{red}{\\textbf{Modify code out of the designated area at your own risk.}}$\n",
    "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
    "- **Need to install [PyTorch](https://pytorch.org/), [opencv-python](https://pypi.org/project/opencv-python/) and [pandas](https://pandas.pydata.org/).** Use the following command in your terminal.\n",
    "    ```console\n",
    "    pip install torch opencv-python pandas\n",
    "    ```\n",
    "## After this assignment you will:\n",
    "- Use sigmoid activation function to transform output of a linear function.\n",
    "- Learn a new loss function: cross entropy.\n",
    "- Apply the binary classifier and calculate classification accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data and Create a Dataset\n",
    "### 1.1 Create an Annotation File\n",
    "Data is not always stored in NumPy arrays. Most cases, you will have to organize and annotate the raw data stored in your hard drive. For image data, you want it to be organized as the following way.\n",
    "``` console\n",
    "root/dog/xxx.jpg\n",
    "root/dog/xxy.jpg\n",
    "root/dog/xxz.jpg\n",
    "\n",
    "root/cat/123.jpg\n",
    "root/cat/456.jpg\n",
    "root/cat/789.jpg\n",
    "```\n",
    "To grab image information and store them in an comma-seperated values (CSV) file:\n",
    "1. Visit the data directory, grab all images' paths and corresponding categories.\n",
    "2. Save the paths and categories of images in an `.csv` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Locate train and test directories\n",
    "root_dir = \"./dataset\"  # locate dataset directory from this repo in the whole system\n",
    "train_dir = os.path.join(root_dir, \"train\")\n",
    "test_dir = os.path.join(root_dir, \"test\")\n",
    "categories = ['cats', 'dogs']\n",
    "\n",
    "# Glob training files\n",
    "train_cat_files = glob(os.path.join(train_dir, categories[0], \"*.jpg\"))\n",
    "train_dog_files = glob(os.path.join(train_dir, categories[1], \"*.jpg\"))\n",
    "print(f\"There are {len(train_cat_files)} cat images, and {len(train_dog_files)} dog images in the training dataset\")\n",
    "train_image_files = train_cat_files + train_dog_files\n",
    "train_labels = ['cat'] * len(train_cat_files) + ['dog'] * len(train_dog_files)\n",
    "train_data_dict = {'path': train_image_files, 'label': train_labels}\n",
    "df_train = pd.DataFrame(train_data_dict)\n",
    "# print(df_train)\n",
    "df_train.to_csv('annotation_train.csv', header=False, index=False)\n",
    "\n",
    "# Glob test files\n",
    "test_cat_files = glob(os.path.join(test_dir, categories[0], \"*.jpg\"))\n",
    "test_dog_files = glob(os.path.join(test_dir, categories[1], \"*.jpg\"))\n",
    "print(f\"There are {len(test_cat_files)} cat images, and {len(test_dog_files)} dog images in the test dataset\")\n",
    "test_image_files = test_cat_files + test_dog_files\n",
    "test_labels = ['cat'] * len(test_cat_files) + ['dog'] * len(test_dog_files)\n",
    "test_data_dict = {'path': test_image_files, 'label': test_labels}\n",
    "df_test = pd.DataFrame(test_data_dict)\n",
    "# print(df_test)\n",
    "df_test.to_csv('annotation_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a Dataset using [PyTorch](https://pytorch.org/)\n",
    "1. Inherit the `Dataset` class to build a customized `CatsDogsDataset` class.\n",
    "2. Instantiate the customized class to a `dataset_train` and `dataset_test` .\n",
    "3. Further create dataloaders to shuffle the data and access the full matrix of the features and the targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create customized dataset\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        self.imgs_info = pd.read_csv(annotations_file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = self.imgs_info.iloc[idx, 0]\n",
    "        image_raw = cv.imread(img_path)\n",
    "        image_rgb = cv.cvtColor(image_raw, cv.COLOR_BGR2RGB)\n",
    "        image = cv.resize(image_rgb, (100, 100))\n",
    "        category = 1. if self.imgs_info.iloc[idx, 1] == 'dog' else 0.\n",
    "        sample = {'image': image, 'category': category}\n",
    "        return sample\n",
    "\n",
    "# Loop training dataset\n",
    "dataset_train = CatsDogsDataset(annotations_file='annotation_train.csv')\n",
    "for i, sample in enumerate(dataset_train):\n",
    "    image = sample['image']\n",
    "    category = sample['category']\n",
    "    if not i%100:\n",
    "        print(i, image.shape, category)\n",
    "print(i, image.shape, category)\n",
    "    \n",
    "dataset_test = CatsDogsDataset(annotations_file='annotation_test.csv')\n",
    "\n",
    "# Create shuffled data loader \n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1000, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1000, shuffle=True)\n",
    "samples = next(iter(dataloader_train))\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "for i in range(4):\n",
    "    image = samples['image'][i]\n",
    "    category = samples['category'][i]\n",
    "    axs[i] = plt.subplot(1, 4, i + 1)\n",
    "    axs[i].set_title(f'Sample #{i+1}: {category}')\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(image)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can re-run the above coding cell to get different samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocess the Data\n",
    "A typical binary classification dataset is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a target vector $\\mathbf{y} = [^{(1)}y, ^{(2)}y, ..., ^{(M)}y]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x}$ is a normalized and flattened image array, and $^{(m)}y \\in \\{0, 1\\}$.\n",
    "\n",
    "- A colored image is usually represented by a **3-dimensional array with shape $(width, height, 3)$**. Where, $width$ indicates number of pixel columns, $height$ indicates number of pixel rows, and 3 indicates 3 color channels (red, green , blue).\n",
    "- When a digital image is loaded, each pixel bears an integer value ranged **0~255** to represent the color intensity.\n",
    "\n",
    "![](https://miro.medium.com/v2/format:webp/1*pFywKuWmz7Xk07OXxPiX2Q.png)\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(20\\%) Exercise 1: Data Preprocessing}}$\n",
    "1. Separate raw feature array and target array.\n",
    "2. Reshape feature array and target array.\n",
    "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features/images and targets/labels\n",
    "data_train = next(iter(dataloader_train))\n",
    "data_test = next(iter(dataloader_test))\n",
    "\n",
    "\n",
    "# Separate features from targets \n",
    "raw_features_train = data_train['image'].numpy()\n",
    "raw_features_test = data_test['image'].numpy()\n",
    "raw_labels_train = data_train['category'].numpy()\n",
    "raw_labels_test = data_test['category'].numpy()\n",
    "print(f\"Raw training data shapes: {raw_features_train.shape}, {raw_labels_train.shape}\")\n",
    "print(f\"Raw test data shapes: {raw_features_test.shape}, {raw_labels_test.shape}\")\n",
    "\n",
    "### START CODE HERE ### (≈ 6 lines of code)\n",
    "# Reshape feature matrix to (M, width*height*3), target vector to (M, 1)\n",
    "reshaped_features_train = None\n",
    "reshaped_features_test = None\n",
    "reshaped_labels_train = None\n",
    "reshaped_labels_test = None\n",
    "\n",
    "# Rescale features within range: 0~1\n",
    "rescaled_features_train = None\n",
    "rescaled_features_test = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Finalize data to be used later\n",
    "features_train = rescaled_features_train\n",
    "features_test = rescaled_features_test\n",
    "labels_train = reshaped_labels_train\n",
    "labels_test = reshaped_labels_test\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Processed training features shape: {features_train.shape}\")\n",
    "print(f\"Processed training labels shape: {labels_train.shape}\")\n",
    "print(f\"Processed test features shape: {features_test.shape}\")\n",
    "print(f\"Processed test labels shape: {labels_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Processed training features shape: (557, 30000)\n",
    "Processed training labels shape: (557, 1)\n",
    "Processed test features shape: (140, 30000)\n",
    "Processed test labels shape: (140, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model\n",
    "Apply a sigmoid function to transform the output of a linear model within range 0~1. The new model is also called **Logistic Regression Model**.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 2: Logistic Regression Model}}$\n",
    "1. Let the linear model: $\\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{w}^T + b$ to take feature matrix $\\mathbf{X}$ as the input and output a transformed/intermediate feature matrix $\\mathbf{Z}$.\n",
    "2. Apply sigmoid function on $\\mathbf{Z}$, so that the prediction will be: $\\mathbf{\\hat{y}} = \\sigma(\\mathbf{Z}) = 1 / (1 + e^{-\\mathbf{Z}})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function\n",
    "    Args:\n",
    "        x: independent variable, could be an arrary of any shape or a scalar.\n",
    "    Returns:\n",
    "        y: dependent variable, could be an arrary of any shape or a scalar.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    y = None\n",
    "    ### END CODE HERE ###\n",
    "    return y\n",
    "\n",
    "def forward(in_features, weight, bias):\n",
    "    \"\"\" Logistic regression model function\n",
    "    Args:\n",
    "        in_features: feature matrix, 2d array with shape (# samples, # pixels)\n",
    "        weight: a row vector with shape (1, # pixels)\n",
    "        biase: a scalar\n",
    "    Returns:\n",
    "        predictions: model predicted values, a column vector or 2d array with shape (# samples, 1)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    predictions = None\n",
    "    ### END CODE HERE ###\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "print(forward(np.random.normal(size=(4, features_test.shape[1])), np.random.normal(0, 0.01, (1, features_test.shape[1])), np.random.normal(0, 0.01)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "[[0.78035352]\n",
    " [0.68058492]\n",
    " [0.60273538]\n",
    " [0.41968119]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary Cross Entropy\n",
    "It is OK to use a Mean Squared Error (MSE) function to compute the model loss. It is better to use a Binary Cross Entropy (BCE) function to assess the model for a binary classification problem. \n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y}) = \\frac{1}{M} \\Sigma [-\\mathbf{y} \\log \\hat{\\mathbf{y}} - (1 - \\mathbf{y}) \\log(1 - \\hat{\\mathbf{y}})]$$\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 3: Cross Entropy Loss}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(predictions, labels):\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy function\n",
    "        Args:\n",
    "            predictions: model predicted values, a 2d array with shape (# samples, 1)\n",
    "            labels: labeled values from data set, a 2d array with shape (# samples, 1)\n",
    "        Returns:\n",
    "            loss_value: averaged CE error, a scalar\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss_value = None\n",
    "    ### END CODE HERE ###\n",
    "    return loss_value\n",
    "    \n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "print(bce_loss(forward(np.random.normal(size=(4, features_test.shape[1])), np.random.normal(0, 0.01, (1, features_test.shape[1])), np.random.normal(0, 0.01)), np.random.randint(0, 2, (4, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "0.5250353081044535\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Optimization\n",
    "### 4.1 Gradient of the Loss\n",
    "By computing the gradient of the loss, we can figure out what would be the best directions to change the model parameters (weight and bias) so that the loss of the model can be reduced.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial w_1} & \\frac{\\partial \\mathcal{L}}{\\partial w_2} & \\dots & \\frac{\\partial \\mathcal{L}}{\\partial w_N} \\end{bmatrix} = \\frac{1}{M} (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\cdot \\mathbf{X}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b}  = \\overline{\\hat{\\mathbf{y}} - \\mathbf{y}} $$\n",
    "\n",
    "### 4.2 Iterative Gradient Descent\n",
    "Then by tweaking the model parameters along the gradient a small step (learning rate, $\\alpha$) iteratively, we expect to bring the model loss down to a reasonable scale.\n",
    "\n",
    "- $\\text{Initialize } \\mathbf{w} \\text{ and } b$\n",
    "- $\\text{Repeat until converge}$\n",
    "    - $\\mathbf{w} = \\mathbf{w} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}$\n",
    "    - $b = b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
    "### $\\color{violet}{\\textbf{(40\\%) Exercise 4: Gradient Descent Optimization}}$\n",
    "1. Define a function to compute gradient of loss\n",
    "2. Perform gradient descent optimization using appropriate iterations and learning rate.\n",
    "    1. Initialize weights and bias\n",
    "    2. Make predictions\n",
    "    3. Log training loss and test loss\n",
    "    4. Update weights and bias\n",
    "    5. Repeat 2 to 5 until converge.\n",
    "    \n",
    "**$\\color{red}{\\textbf{Note: bring training loss below 0.2}}$**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(predictions, labels, in_features):\n",
    "    \"\"\" Gradient function with sigmoid activation\n",
    "    Args:\n",
    "        predictions: model predicted value, a 2d array with shape (# samples, 1)\n",
    "        labels: labeled value from data set, a 2d array with shape (# samples, 1)\n",
    "        in_features: feature matrix, a 2d array with shape (# samples, # pixels)\n",
    "    Returns:\n",
    "        dw: row vector of BCE loss partial derivatives w.r.t. weights, 2d array with shape (1, # features)\n",
    "        db: scalar of BCE loss partial derivatives w.r.t. bias\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = None\n",
    "    db = None\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return dw, db\n",
    "\n",
    "# GD iteration\n",
    "### START CODE HERE ### (≈ 10 lines of code)\n",
    "w = None\n",
    "b = None\n",
    "num_iters = None\n",
    "learning_rate = None\n",
    "losses_train, losses_test = [], []\n",
    "for i in range(num_iters):\n",
    "    preds_train = None  # make prediction using model\n",
    "    loss_train = None  # compute BCE loss using training data\n",
    "    loss_test = None  # compute BCE loss using test data\n",
    "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    dw, db = None  # compute loss gradient\n",
    "    w = None  # update weight\n",
    "    b = None  # update bias\n",
    "### END CODE HERE ### \n",
    "\n",
    "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "You can treat the model output, $\\mathbf{\\hat{y}}$ as the probabilities of the images being classified as dog pictures. Set the classification boundary to be 0.5, we can categorize an image as a cat image ($^{(m)}y <= 0.5$) or a dog image ($^{(m)}y > 0.5$). We'll evaluate the model on both training and test dataset.\n",
    "### $\\color{violet}{\\textbf{(20\\%) Exercise 5: Evaluation on Test Data}}$\n",
    "Calculate classification accuracy using the trained model.\n",
    "\n",
    "**$\\color{red}{\\textbf{Note: model accuracy on training data has to be greater than 0.98}}$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on training data\n",
    "categories_train = preds_train > 0.5  # Convert predictions over 0.5 to 1 (True), otherwise to 0 (False)\n",
    "is_correct_train = categories_train == labels_train  # Find out which predictions are correct\n",
    "num_correct_train = np.sum(is_correct_train)  # Calculate how many correct predictions are made\n",
    "acc_train = num_correct_train / labels_train.shape[0]  # Calculate accuracy rate: # correct predictions / # samples\n",
    "print(f\"Model accuracy on training data: {acc_train}\")\n",
    "\n",
    "### START CODE HERE ### (≈ 5 lines of code)\n",
    "# Accuracy on test data\n",
    "preds_test = None  # make predictions on test features\n",
    "categories_test = None  # Convert predictions over 0.5 to 1 (True), otherwise to 0 (False)\n",
    "is_correct_test = None  # Find out which predictions are correct\n",
    "num_correct_test = None  # Calculate how many correct predictions are made\n",
    "acc_test = None  # Calculate accuracy rate: correct # / total #\n",
    "### END CODE HERE ###\n",
    "print(f\"Model accuracy on test data: {acc_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may notice the big difference bewteen the prediction accuracy on training data vs. test data. What could be the causes of this phenomenon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test\n",
    "Download new images to this folder and try to use your model to classify them.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(10\\%) Exercise 6: Test Model with New Image}}$\n",
    "**$\\color{red}{\\textbf{Note: you can use two provided pictures to test your code, but please upload at least one new picture for the final test.}}$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_raw = cv.imread('Golden_Retriever.jpg')  # read the raw image from a file\n",
    "image_rgb = cv.cvtColor(image_raw, cv.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "image_resize = cv.resize(image_rgb, (100, 100))  # Resize image to shape (100, 100, 3)\n",
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "image_flatten = None  # reshape image array to a row vector with shape (1, 100*100*3)\n",
    "image_rescale = None  # rescale pixel value from 0~255 to 0.~1.\n",
    "dog_likelihood = None  # predict new image with your model\n",
    "### END CODE HERE ### \n",
    "\n",
    "is_dog = dog_likelihood > 0.5\n",
    "if is_dog.squeeze():\n",
    "    print(\"Woof!\") \n",
    "else:\n",
    "    print(\"Meow!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have finished this assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3321",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
